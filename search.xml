<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[delete和truncate以及drop区别]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fdelete%E5%92%8Ctruncate%E4%BB%A5%E5%8F%8Adrop%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[这个题目我自己也被问过，这里简单整理一下。 先来个总结： drop直接删掉表； truncate删除的是表中的数据，再插入数据时自增长的数据id又重新从1开始； delete删除表中数据，可以在后面添加where字句。 日志是否记录 DELETE语句执行删除操作的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。 TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 是否可以回滚 delete 这个操作会被放到 rollback segment 中,事务提交后才生效。truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment 中，不能回滚. 所以在没有备份情况下，谨慎使用 drop 与 truncate。 表和索引占的空间 当表被 TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小。 而 DELETE 操作不会减少表或索引所占用的空间。 drop语句将表所占用的空间全释放掉。 TRUNCATE 和 DELETE 只删除数据，而 DROP 则删除整个表（结构和数据） 所以从干净程度，drop &gt; truncate &gt; delete ok，差不多了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最基础知识小结]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fmysql%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文介绍关于数据库的最最最最基本的一些语法知识，如果这些都不熟悉，建议多多练习，因为后续的文章会比较深入原理。 一、DDL语句 1、创建数据库：create database dbname; 2、删除数据库：drop database dbname; 3、创建表：create table tname; 4、删除表：drop table tname; 5、修改表：略，懒得看 二、DML语句 插入： 1insert into table(字段1，字段2，...) values (value1,value2,...) , (value3,value4,..) 更新： 1update table set 字段=value where ... 删除： 1delete from table where ... 这里要注意下delete和truncate以及drop三者的区别，下篇文章详解。 单表查询： 1select 字段 from table 连表查询方式1： 1select 别名1.字段,别名2.字段 from table1 别名1,table2 别名2 where ... 连表查询方式2： 1select 别名1.字段,别名2.字段 from table1 别名1 join table2 别名2 on ... 这是全连接，这里就要了解一下笛卡儿积，简单来说，最后行数是左边表的函数乘以右边表的行数。详细的可以自行google. 查询不重复的记录： 1select distinct 字段 from table ... 排序：默认是升序 1select 字段 from table where ... order by ... asc/desc limit：主要用于分页 1select * from table where ... order by ... asc/desc limit 起始偏移位置，显示条数 聚合： 1select count(*)/avg(..)/sum(...)/max(...)/min(...) from table group by ... having .... 注意这里的having和where的区别：where是对表结果进行筛选，having 是对查询结果进行筛选，与group by 合用 左连接和右连接 左连接意思就是左表中的记录都在，右表没有匹配项就以null显示。记录数等于左表的行数。 右连接与之同理，尽量转为左连接做。 子查询： 1select * from table where ... in (select ....) 所谓子查询就是根据另一个select的结果再进行筛选，常用的是in,not in,=,!=,exits,not exits union 主要用于两张表中的数据的合并： 1select 字段 from table1 union all select 字段 from table2 要想不重复用union]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常见的面试题]]></title>
    <url>%2F2019%2F01%2F25%2Fnetwork%2F8.%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[对这一块整理一些常见的面试题。 1.TCP三次握手、四次挥手 这部分略。前面已经说的很详细，包括握手为什么不是两次、为什么不是四次，为什么挥手要等2MSL的时间。 2.常见的HTTP状态码及其含义 200 OK：正常返回信息 400 Bad Reqest：客户端请求有语法错误，不能被服务器所理解 401 Unauthorized：请求未经授权，这个状态码必须与WWW-Authenticate报头域一起使用 403 Forbidden：服务器收到请求，但是拒绝提供服务 404 Not Found：请求资源不存在 500 Internal Server Error：服务器发生不可预期的错误 503 Server Unavilable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常 3.Get请求和Post请求的区别 Http报文层面：GET将请求信息放在URL，POST则放在报文体中 数据库层面：GET符合幂等性和安全性(查询不会改变数据库)，POST不符合 其他层面：GET可以被缓存、被存储为书签，而POST不行 4.Cookie和Session的区别 对于session，字面上理解是会话，可以理解为用户与服务端一对一的交互。是一个比较抽象的概念。 但是我们常说的session其实是这里抽象概念的一种实现方式罢了，我觉得没有必要咬文嚼字，下面直接从面试角度来分析一下。 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。 这个Session是保存在服务端的，有一个唯一标识，这个唯一标识对应一个用户。在服务端保存Session的方法很多，内存、数据库、文件都有。 服务端解决了用户标识问题，但是服务端怎么知道此时操作浏览器的用户是谁呢？ 这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。 实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端(放在响应头中返回)，需要在 Cookie 里面记录一个Session ID，以后每次请求(请求头)把这个会话ID发送到服务器，我就知道你是谁了。 有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 总结： Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中； Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 下面说一下很常见的一种写法。比如在单体应用中，我此时登陆你的网站了，你可以将我的信息保存在session中： 12User currentUserInfo = userService.getUserByUsernameAndPasswd(username,password);session.setAttribute("currentUser",currentUserInfo); 下次，我就可以在我们之间的会话中随时获取我的个人信息： 1User currentUser = session.getAttribute("currentUser"); 其实这些就是利用存放在Cookie中的JSESSIONID来实现的。 5.HTTP和HRTTPS的关系 来说一下SSL(Security Sockets Layer，安全套接层) 为网络通信提供安全及数据完整性的一种安全协议 是操作系统对外的API，SSL3.0之后更名为TLS 采用身份验证和数据加密保证网络通信的安全和数据的完整性 HTTPS数据传输流程： 浏览器将支持的加密算法信息发送给服务器 服务器选择一套浏览器支持的加密算法，以证书的形式回发浏览器 浏览器验证证书合法性，并结合证书公钥加密信息发给服务器 服务器使用私钥解密信息，验证哈希，加密响应消息回发浏览器 浏览器解密响应消息，并对消息进行验证，之后进行加密交互数据 这个也就不赘述了，下面直接说说区别。 HTTPS需要到CA申请证书，HTTP不需要 HTTPS密文传输，HTTP明文传输 连接方式不同，HTTPS默认使用443端口，HTTP使用80端口 HTTPS=HTTP+加密+认证+完整性保护，更安全 但是仍然存在一定的风险： 浏览器默认填充http://，请求需要进行跳转，有被劫持的风险 可以使用HSTS(HTTP Strict Transport Security)优化（这个还不未主流，面试问的少） Socket简介 我们知道，进程与进程直接的通信最基本的要求是：可以唯一确定进程。 在本地进程通信中，可以用PID来唯一标识一个进程。 但是PID只在本地唯一，网络中PID冲突的几率还是存在的。 我们知道，到IP层就可以唯一定位到一台主机了，TCP层(tcp协议+端口号)可以唯一定位一台主机中的一个进程。 这样，我们可以通过ip地址+协议+端口号可以唯一标识一台主机的一个进程。这样就可以通过socket进行网络通信了。 socket是对TCP/IP协议的抽象，是操作系统对外开放的接口。 socket起源于unix，而unix是遵从一切皆文件的哲学。Socket是一种基于从打开、读/写、关闭的模式实现的。客户端和服务器各自维护一个文件，在连接建立后，可以供对方读取或者读取对方内容。 socket相关题目 编写一个网络程序，有客户端和服务端，客户端向服务端发送一个字符串，服务器收到字符串之后打印到命令行上，然后向客户端返回该字符串的长度，最后，客户端输出服务端返回的该字符串的长度，分别用TCP和UDP两种方式去实现。 代码地址：https://github.com/sunweiguo/TcpAndUdp/]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础核心-理解类、对象、面向对象编程、面向接口编程]]></title>
    <url>%2F2019%2F01%2F24%2Fjava-basic%2FJAVA%E5%9F%BA%E7%A1%80%E6%A0%B8%E5%BF%83-%E7%90%86%E8%A7%A3%E7%B1%BB%E3%80%81%E5%AF%B9%E8%B1%A1%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E3%80%81%E9%9D%A2%E5%90%91%E6%8E%A5%E5%8F%A3%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是类，什么是对象，什么是面向对象编程，什么是面向接口编程。学习面向对象思想的语言，比如java，第一关可能就是要理解这些概念。下面就来好好琢磨一下。 类和对象的概念 首先总结一下：类是一个模板，对象就是用这个模板创造出来的东西。 比如，男孩，他就是一个模板，男的就行，那么对象是什么呢？就是具体某个男孩，比如男孩BOB，男孩fourColor. 请看下面一张图： 男孩女孩是比较抽象的概念，是模板，左边一排就是其具体的一些对象。你看长的都不一样，有的黑，有的白，有的高，有的矮，国家地区也不一样。但是他们都属于男孩或者女孩。 那么同理，人就是一个类，男孩女孩就是人的子类，因为人可能不仅包括男孩女孩，还包括第三性别这个类。 这里还引出了JAVA特性中的继承。继承简单理解就是父类有的东西(访问级别不能是private)的，那都是你的。比如你老爸的房子，就是属于你的，你出入自由。 人还可以分为胖人和瘦人这个子类。所以只要是抽象的模板，就是一个类。 对象：对象是类的一个实例（对象不是找个女朋友），有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。 类：类是一个模板，它描述一类对象的行为和状态。 下面就拿狗这个类来说事。狗是动物这个类的子类。 Java中创建类 构造器方法说明 需要创造一个类对象出来的时候，要用到这个类的构造器方法，那么啥是构造器方法呢？构造器方法就是创造类时的初始化方法，和类同名的方法，你可以在里面写自己的代码 1234567891011121314151617181920//模版class 类名称 &#123; 访问权限 构造方法名称()&#123; &#125;&#125;//例子public class Dog&#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125;&#125; 一个相对比较完整的类 1234567891011121314151617181920212223242526272829//模版class 类名称 &#123; //构造器方法 //声明成员变量---这个变量属于这个类 //声明成员方法 //在方法里面定义的变量是局部变量，区别于成员变量&#125;//例子public class Dog &#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125; //狗的颜色--成员属性 public String color;//一般是private，赋值用set方法，取值用get方法，这里只是演示 //狗的行为，它会叫---成员方法 private void say()&#123; int i = 0;//局部变量 System.out.println("我会叫：汪汪汪~"); &#125;&#125; 创建对象 语法： 类名 对象名 = new 类名() ; 举例： 12Dog fourcolor ; // 先声明一个 Dog 类的对象 fourcolorfourcolor = new Dog(&quot;fourcolor&quot;) ; // 用 new 关键字实例化 Dog 的对象 fourcolor,此时调用构造方法二 通过Dog这个类可以创造出fourcolor对象.下面我才能操作这个对象： 1234//让它的颜色为黑色fourcolor.color = &quot;black&quot;;//让它叫fourcolor.say(); 面向对象 在理解了什么是类，什么是对象，就可以来说说面向对象到底是什么了。 先来说说面向过程，大家都学习过C语言。C语言就是典型的面向过程的语言。 举个例子：要把大象装进冰箱里，这件事，面向过程的程序员是这样思考的： 把冰箱门儿打开。 把大象装进去。 把冰箱门儿关上。 上面的每一件事都用一个函数来实现。抽象为下面三个函数： openTheDoor()； pushElephant()； closeTheDoor()； 这样不挺好的吗？为什么不用面向过程的这种思维来编程呢，还要搞出什么面向对象来。 需求又来啦： 「我要把大象装微波炉里」 「我要把狮子也装冰箱里」 「我要把大象装冰箱，但是门别关，敞着就行」 这个时候，面向过程的程序员就悲剧了，来一个需求我就写一个函数，我还能下班吗？ 面向对象从另一个角度来解决这个问题。它抛弃了函数，把「对象」作为程序的基本单元。 面向对象的世界里，到处都是对象。即：万物皆对象。 比如人这个类，每个具体的人(对象)都要有这样的属性：身高、体重、年龄。每个人都有这样的行为：吃饭、睡觉、上厕所。 那么，这些通用的属性+方法可以构建一个模板：人这个类。因为每个具体的人（对象）都需要这些基本的东西。当然了，每个人具体什么身高、什么体重、吃什么都是不一样的，所以每个对象一般都是不一样的。但是模板是一样的。 那么，回到刚才的需求，面向对象是如何思考这件事的呢？ 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 就是说，我不用亲自管开门的细节，我只要叫他开门即可。 我们创建的对象，应该是刚刚好能做完它能做的事情，不多做，不少做。多做了容易耦合，各种功能杂糅在一个对象里。比如我有一个对象叫「汽车」，可以「行驶」，可以「载人」，现在的需求是要实现「载人飞行」，就不能重用这个对象，必须新定义一个对象「飞机」来做。如果你给「汽车」插上了翅膀，赋予了它「飞行」的能力，那么新来的同学面对你的代码就会莫名其妙，无从下手。 但是不禁要问：怎么实现这种下达命令就可以自动去执行的效果呢？或者说，我怎么知道它有这个功能啊！ 面向接口编程 现在我们把「数据」和「行为」都封装到了对象里，相当于对象成了一个黑匣子，那我们怎么知道对象具有什么样的能力呢？这个问题的关键就是接口。 因为无论是把大象装进洗衣机还是冰箱，都要求洗衣机或者冰箱有开门和关门的功能。这个时候，我们就可以抽象出来一个接口：【自动门】。这个接口里面定义两个能力：【开门】和【关门】。 让洗衣机、冰箱、微波炉这些带门的东西全部实现【自动门】接口。 这个时候，每个具体的实现可能略有不同，比如冰箱开门是往外拽，但是洗衣机开门可能是往上翻盖子。 此时，我有一个需求，把大象放进冰箱。我一看，冰箱实现了【自动门】这个接口，里面有【开门】和【关门】两个方法，ok，我知道冰箱是可以开门和关门了，那就好办了。我直接下达命令即可。还是跟上面一样的步骤. 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，需要将狮子也装冰箱里。那还是一样： 向冰箱下达「开门」的命令。 向狮子下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，我要把大象装冰箱，但是门别关，敞着就行，那就： 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 是不是很方便？冰箱也可以换，我可以换成任何东西，只要实现了这个接口，这些东西就都有这些能力，那我才不管里面到底怎么实现的呢，直接下达【开门】【关门】命令即可。 这也引入了JAVA特性中另一个特性：封装。外界不知道里面实现细节，只需要知道它的功能和入参即可。 这就是面向过程和面向对象编程的区别，也顺带地理解了什么是面向接口编程。这是学习JAVA最基础也是最核心的点。 整理自： https://tryenough.com/java05 http://www.woshipm.com/pmd/294180.html]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串核心一网打尽]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%B8%E5%BF%83%E4%B8%80%E7%BD%91%E6%89%93%E5%B0%BD%2F</url>
    <content type="text"><![CDATA[对字符串中最核心的点：对象创建和动态加入常量池这些点进行深入分析。 比如有两个面试题： Q1：String s = new String(&quot;abc&quot;); 定义了几个对象。 Q2：如何理解String的intern方法？ A1：对于通过 new 产生的对象，会先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象，然后在堆中创建一个常量池中此 “abc” 对象的拷贝对象。所以答案是：一个或两个。如果常量池中原来没有 ”abc”, 就是两个。如果原来的常量池中存在“abc”时，就是一个。 A2：当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； 字面量和运行时常量池 JVM为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化。为了减少在JVM中创建的字符串的数量，字符串类维护了一个字符串常量池。 在JVM运行时区域的方法区中，有一块区域是运行时常量池，主要用来存储编译期生成的各种字面量和符号引用。 了解过JVM就会知道，在java代码被javac编译之后，文件结构中是包含一部分Constant pool的。比如以下代码： 123public static void main(String[] args) &#123; String s = "abc";&#125; 经过编译后，常量池内容如下： 1234567891011Constant pool: #1 = Methodref #4.#20 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #21 // abc #3 = Class #22 // StringDemo #4 = Class #23 // java/lang/Object ... #16 = Utf8 s .. #21 = Utf8 abc #22 = Utf8 StringDemo #23 = Utf8 java/lang/Object 上面的Class文件中的常量池中，比较重要的几个内容： 123#16 = Utf8 s#21 = Utf8 abc#22 = Utf8 StringDemo 上面几个常量中，s就是前面提到的符号引用，而abc就是前面提到的字面量。而Class文件中的常量池部分的内容，会在运行期被运行时常量池加载进去。 new String创建了几个对象 下面，我们可以来分析下String s = new String(&quot;abc&quot;);创建对象情况了。 这段代码中，我们可以知道的是，在编译期，符号引用s和字面量abc会被加入到Class文件的常量池中。由于是new的方式，在类加载期间，先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象。 在运行期间，在堆中创建一个常量池中此 “abc” 对象的拷贝对象。 运行时常量池的动态扩展 编译期生成的各种字面量和符号引用是运行时常量池中比较重要的一部分来源，但是并不是全部。那么还有一种情况，可以在运行期像运行时常量池中增加常量。那就是String的intern方法。 当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； intern()有两个作用，第一个是将字符串字面量放入常量池（如果池没有的话），第二个是返回这个常量的引用。 一个例子： 123456789String s1 = "hello world";String s2 = new String("hello world");System.out.println("s==s1:"+(s==s1));String s3 = new String("hello world").intern();System.out.println("s==s2:"+(s==s2)); 运行结果是： 12s1==s2:falses2==s3:true 你可以简单的理解为String s1 = &quot;hello world&quot;;和String s3 = new String(&quot;hello world&quot;).intern();做的事情是一样的（但实际有些区别，这里暂不展开）。都是定义一个字符串对象，然后将其字符串字面量保存在常量池中，并把这个字面量的引用返回给定义好的对象引用。 对于String s3 = new String(&quot;hello world&quot;).intern();，在不调intern情况，s3指向的是JVM在堆中创建的那个对象的引用的（如s2）。但是当执行了intern方法时，s3将指向字符串常量池中的那个字符串常量。 由于s1和s3都是字符串常量池中的字面量的引用，所以s1==s3。但是，s2的引用是堆中的对象，所以s2!=s1。 intern的正确用法 不知道，你有没有发现，在String s3 = new String(&quot;abc&quot;).intern();中，其实intern是多余的？ 因为就算不用intern，“abc&quot;作为一个字面量也会被加载到Class文件的常量池”&quot;，进而加入到运行时常量池中，为啥还要多此一举呢？到底什么场景下才会用到intern呢? 在解释这个之前，我们先来看下以下代码： 1234String s1 = "hello";String s2 = "world";String s3 = s1 + s2;String s4 = "hello" + "world"; 在经过反编译后，得到代码如下： 1234String s1 = "hello";String s2 = "world";String s3 = (new StringBuilder()).append(s1).append(s2).toString();String s4 = "helloworld"; 这就是阿里巴巴文档里为什么规定循环拼接字符串不准使用&quot;+&quot;而必须使用StringBuilder，因为反编译出的字节码文件显示每次循环都会 new 出一个 StringBuilder 对象，然后进行append 操作，最后通过 toString 方法返回 String 对象，造成内存资源浪费。 不恰当的方式形如： 1234String str = "start";for (int i = 0; i &lt; 100; i++) &#123; str = str + "hello";&#125; 好了，言归正传，可以发现，同样是字符串拼接，s3和s4在经过编译器编译后的实现方式并不一样。s3被转化成StringBuilder及append，而s4被直接拼接成新的字符串。 如果你感兴趣，你还能发现，String s4 = s1 + s2; 经过编译之后，常量池中是有两个字符串常量的分别是 hello、world（其实hello和world是String s1 = &quot;hello&quot;;和String s2 = &quot;world&quot;;定义出来的），拼接结果helloworld并不在常量池中。 如果代码只有String s4 = &quot;hello&quot; + &quot;world&quot;;，那么常量池中将只有helloworld而没有hello和 world。 究其原因，是因为常量池要保存的是已确定的字面量值。也就是说，对于字符串的拼接，纯字面量和字面量的拼接，会把拼接结果作为常量保存到字符串。 如果在字符串拼接中，有一个参数是非字面量，而是一个变量的话，整个拼接操作会被编译成StringBuilder.append，这种情况编译器是无法知道其确定值的。只有在运行期才能确定。 那么，有了这个特性了，intern就有用武之地了。那就是很多时候，我们在程序中用到的字符串是只有在运行期才能确定的，在编译期是无法确定的，那么也就没办法在编译期被加入到常量池中。 这时候，对于那种可能经常使用的字符串，使用intern进行定义，每次JVM运行到这段代码的时候，就会直接把常量池中该字面值的引用返回，这样就可以减少大量字符串对象的创建了。 总结 第一种情况： 12String str1 = "abc"; System.out.println(str1 == "abc"); 栈中开辟一块空间存放引用str1； String池中开辟一块空间，存放String常量&quot;abc&quot;； 引用str1指向池中String常量&quot;abc&quot;； str1所指代的地址即常量&quot;abc&quot;所在地址，输出为true 第二种情况： 12String str2 = new String("abc"); System.out.println(str2 == "abc"); 栈中开辟一块空间存放引用str2； 堆中开辟一块空间存放一个新建的String对象&quot;abc&quot;； 引用str2指向堆中的新建的String对象&quot;abc&quot;； str2所指代的对象地址为堆中地址，而常量&quot;abc&quot;地址在池中，输出为false； 第三、四种情况 1234567891011121314//（3）String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//str1 和 str2 是字符串常量，所以在编译期就确定了。//str3 中有个 str1 是引用，所以不会在编译期确定。//又因为String是 final 类型的，所以在 str1 + "b" 的时候实际上是创建了一个新的对象，在把新对象的引用传给str3。//（4）final String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//这里和(3)的不同就是给 str1 加上了一个final，这样str1就变成了一个常量。//这样 str3 就可以在编译期中就确定了 这里的细节在上面已经详细说明了。 第五种情况 1234String str1 = "ab"；String str2 = new String("ab");System.out.println(str1== str2);//falseSystem.out.println(str2.intern() == str1);//true 整理自： 我终于搞清楚了和String有关的那点事儿 https://www.jianshu.com/p/2624036c9daa]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[关于java字符串不可变特性的深入理解。 什么是不可变对象？ 众所周知， 在Java中， String类是不可变的。那么到底什么是不可变的对象呢？ 可以这样认为：如果一个对象，在它创建完成之后，不能再改变它的状态，那么这个对象就是不可变的。不能改变状态的意思是，不能改变对象内的成员变量，包括基本数据类型的值不能改变，引用类型的变量不能指向其他的对象，引用类型指向的对象的状态也不能改变。 区分对象和对象的引用 对于Java初学者， 对于String是不可变对象总是存有疑惑。看下面代码： 12345String s = "ABCabc"; System.out.println("s = " + s); s = "123456"; System.out.println("s = " + s); 打印结果: 12s = ABCabcs = 123456 首先创建一个 String 对象 s ，然后让 s 的值为 ABCabc ， 然后又让 s 的值为 123456 。 从打印结果可以看出，s 的值确实改变了。那么怎么还说 String 对象是不可变的呢？ 其实这里存在一个误区：s只是一个String对象的引用，并不是对象本身。 对象在内存中是一块内存区，成员变量越多，这块内存区占的空间越大。 引用只是一个4字节的数据，里面存放了它所指向的对象的地址，通过这个地址可以访问对象。 也就是说，s 只是一个引用，它指向了一个具体的对象，当 s=“123456”; 这句代码执行过之后，又创建了一个新的对象“123456”， 而引用s重新指向了这个新的对象，原来的对象“ABCabc”还在内存中存在，并没有改变。内存结构如下图所示： 为什么String对象是不可变的？ 要理解 String 的不可变性，首先看一下 String 类中都有哪些成员变量。 在JDK1.6中，String 的成员变量有以下几个： 1234567891011121314public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 在JDK1.7和1.8中，String 类做了一些改动，主要是改变了substring方法执行时的行为，这和本文的主题不相关。JDK1.7中 String 类的主要成员变量就剩下了两个： 1234567public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 由以上的代码可以看出， 在Java中 String 类其实就是对字符数组的封装。 JDK6中， value是String封装的数组，offset是String在这个value数组中的起始位置，count是String所占的字符的个数。 在JDK7中，只有一个value变量，也就是value中的所有字符都是属于String这个对象的。这个改变不影响本文的讨论。 除此之外还有一个hash成员变量，是该 String 对象的哈希值的缓存，这个成员变量也和本文的讨论无关。在Java中，数组也是对象。 所以value也只是一个引用，它指向一个真正的数组对象。其实执行了 1String s = “ABCabc"; 这句代码之后，真正的内存布局应该是这样的： value，offset和count这三个变量都是private的，并且没有提供setValue， setOffset和setCount等公共方法来修改这些值，所以在String类的外部无法修改String。也就是说一旦初始化就不能修改， 并且在String类的外部不能访问这三个成员。 此外，value，offset和count这三个变量都是final的， 也就是说在 String 类内部，一旦这三个值初始化了， 也不能被改变。所以可以认为 String 对象是不可变的了。 那么在 String 中，明明存在一些方法，调用他们可以得到改变后的值。这些方法包括substring， replace， replaceAll， toLowerCase等。例如如下代码： 1234String a = "ABCabc"; System.out.println("a = " + a); //ABCabca = a.replace('A', 'a'); System.out.println("a = " + a); //aBCabc 那么a的值看似改变了，其实也是同样的误区。再次说明， a只是一个引用， 不是真正的字符串对象，在调用a.replace('A', 'a')时， 方法内部创建了一个新的String对象，并把这个心的对象重新赋给了引用a。String中replace方法的源码可以说明问题： 1234567891011121314151617181920212223242526public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(buf, true);//new出了新的String对象 &#125; &#125; return this;&#125; String对象真的不可变吗？ 从上文可知String的成员变量是private final 的，也就是初始化之后不可改变。那么在这几个成员中， value比较特殊，因为他是一个引用变量，而不是真正的对象。 value是final修饰的，也就是说final不能再指向其他数组对象，那么我能改变value指向的数组吗？ 比如将数组中的某个位置上的字符变为下划线“_”。 至少在我们自己写的普通代码中不能够做到，因为我们根本不能够访问到这个value引用，更不能通过这个引用去修改数组。 那么用什么方式可以访问私有成员呢？ 没错，用反射， 可以反射出String对象中的value属性， 进而改变通过获得的value引用改变数组的结构。下面是实例代码： 123456789101112131415161718192021public static void testReflection() throws Exception &#123; //创建字符串"Hello World"， 并赋给引用s String s = "Hello World"; System.out.println("s = " + s); //Hello World //获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField("value"); //改变value属性的访问权限 valueFieldOfString.setAccessible(true); //获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); //改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println("s = " + s); //Hello_World &#125; 在这个过程中，s始终引用的同一个 String 对象，但是再反射前后，这个 String 对象发生了变化， 也就是说，通过反射是可以修改所谓的“不可变”对象的。但是一般我们不这么做。 这个反射的实例还可以说明一个问题：如果一个对象，他组合的其他对象的状态是可以改变的，那么这个对象很可能不是不可变对象。例如一个Car对象，它组合了一个Wheel对象，虽然这个Wheel对象声明成了private final 的，但是这个Wheel对象内部的状态可以改变， 那么就不能很好的保证Car对象不可变。 参考： https://blog.csdn.net/zhangjg_blog/article/details/18319521]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer拆箱和装箱]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2FInteger%E6%8B%86%E7%AE%B1%E5%92%8C%E8%A3%85%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[由于笔试经常遇到，所以这里整理一下。将Integer这一块一网打尽。 拆箱和装箱 这里以面试笔试经常出现的Integer类型为例，请看下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; /*第一组*/ Integer i = new Integer(128); Integer i2 = 128; System.out.println(i == i2); Integer i3 = new Integer(127); Integer i4 = 127; System.out.println(i3 == i4); /*第二组*/ Integer i5 = 128; Integer i6 = 128; System.out.println(i5 == i6); Integer i7 = 127; Integer i8 = 127; System.out.println(i7 == i8); /*第三组*/ Integer i9 = new Integer(128); int i10 = 128; System.out.println(i9 == i10); Integer i11 = new Integer(127); int i12 = 127; System.out.println(i11== i12); /*第四组*/ Integer i13 = new Integer(128); Integer i14 = Integer.valueOf(128); System.out.println(i13 == i14); Integer i15 = new Integer(127); Integer i16 = Integer.valueOf(127); System.out.println(i13 == i14); /*第五组*/ Integer i17 = Integer.valueOf(128); Integer i18 = 128; System.out.println(i17 == i18); Integer i19 = Integer.valueOf(127); Integer i20 = 127; System.out.println(i19 == i20);&#125; 执行结果为： 1234567891011121314falsefalsefalsetruetruetruefalsefalsefalsetrue 翻开源码(jdk8)，我们可以看到一个私有的静态类，叫做整形缓存。顾名思义，就是缓存某些整型值，我们可以看到，它默认将-127-128之间数字封装成对象，放进一个常量池中，以后定义类似于Integer a = 1里面的a就可以直接从这个常量池中取对象即可，不需要重新new 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 对于Integer.valueOf(): 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 我们可以看到，也是先看看是不是在-127到128之间的范围，是的话就从 cache 中取出相应的 Integer 对象即可。 第一组中 第一种情况，i 是创建的一个Integer的对象，取值是128。i2 是进行自动装箱的实例，因为这里超出了-128到127的范围，所以是创建了新的Integer对象。由于==比较的是地址，所以两者必然不一样。 第二种情况就不一样了，i4是不需要自己new，而是可以直接从缓存中取，但是i3是new出来的，地址还是不一样。 第二组中 第一种情况是都超出范围了，所以都要自己分别去new，所以不一样 第二种情况是在范围内，都去缓存中取，实际上都指向同一个对象，所以一样 第三组中 i10和i12都是int型，i9和i11与它们比较的时候都要自动拆箱，所以比较的是数值，所以都一样 第四组中 与第一组原理一样 四五组中 与第二组原理一样 所以啊，new Integer()是每次都直接new对象出来，而Integer.valueOf()可能会用到缓存，所以后者效率高一点。 总结 int 和 Integer 在进行比较的时候， Integer 会进行拆箱，转为 int 值与int` 进行比较。 Integer 与 Integer 比较的时候，由于直接赋值的时候会进行自动的装箱，那么这里就需要注意两个问题，一个是 -128&lt;= x&lt;=127 的整数，将会直接缓存在 IntegerCache 中，那么当赋值在这个区间的时候，不会创建新的 Integer 对象，而是从缓存中获取已经创建好的 Integer 对象。二：当大于这个范围的时候，直接 new Integer 来创建 Integer 对象。 new Integer(1) 和 Integer a = 1 不同，前者会创建对象，存储在堆中，而后者因为在-128到127的范围内，不会创建新的对象，而是从 IntegerCache 中获取的。那么 Integer a = 128, 大于该范围的话才会直接通过 new Integer(128)创建对象，进行装箱。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务解决方案思考]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F10%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[在分布式系统中，最头疼的就是分布式事务问题，处理起来一定要小心翼翼。由于没有此方面实战，本文就从理论上看看比较好的分布式事务处理方案。 什么是分布式事务 众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？ 比如用户下单过程。当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 方案1：基于可靠消息服务的分布式事务 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程中，如果任务A处理失败，那么需要进入回滚流程: 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 注意，这个方案需要消息队列具有事务消息的能力，阿里的RocketMQ可以实现这个目标。其他的MQ还不行。 方案2：最大努力通知（定期校对） 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。 如果这两步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第一种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。还有其他的一些解决思路，这里就暂时只描述这些。后续再学习。 参考自：https://juejin.im/post/5aa3c7736fb9a028bb189bca]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[库存扣减问题]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F09%E5%BA%93%E5%AD%98%E6%89%A3%E5%87%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[库存扣减问题一直是秒杀中最关键的一个点。如果把控不好，扣成负数，那可就麻烦了，如么如何保证不会出现超卖还能保证性能呢？ 一、扣减库存问题分析 在提交订单的时候，要扣减库存，对于sql，是这么写的： 1update t_stcok set stock = stock-2 where sku_id = 1 首先这条sql存在超卖问题，很有可能会减成负数。可能会改成如下： 1update t_stcok set stock = stock-2 where sku_id = 1 and stock &gt; 2 这样好像解决了超卖问题。但是引入了新的问题。由于库存牵涉进货、补货等系统，所以是个独立的服务。 并且，比如我是通过MQ去通知库存进行扣减库存，但是由于网络抖动，请求扣减库存没有结果，这个时候可能需要进行重试。重试之后，可能成功了，这个时候，有可能这两次都成功了。那么，一个用户买一样东西，但是库存扣了两遍。这就是幂等。如果不做幂等处理，重试会出现上述这种致命问题。 那么如何做到幂等呢？ 实际上就是追求数据一致性。那么就可以考虑锁来保证，比如我这里用乐观锁来实现： 123select stock,version from t_stock;if(stock &gt; 用户购买数量) update t_stcok set stock = stock-2 where sku_id = 1 and version = last_version 但是，一旦出现并发，那么可能这个用户是执行update失败的，所以还需要去重试(guava retry或者spring retry都可以优雅地实现重试)，直到成功或者库存已经不足。 那么，在少量并发的情况下，可以考虑乐观锁，要不然会大量失败，此时需要用悲观锁： 12select * from t_stock for update;下面执行update操作。。。 在一个事务内，第一句为select for update，那么这一行数据就会被本线程锁住，整个事务执行完才能允许其他线程进来。 存在的问题：一个线程锁住这行数据，那么其他线程都要等待，效率很低。 那么，如何保证数据一致性，还可以提高效率呢？ 对于扣减库存，往往是先在redis中进行扣减库存。redis是单线程，是高速串行执行，不存在并发问题。 如果是单机redis，可以在同一个事务中保证一次性执行: 12345watch stockmultiif stock &gt; count stock = stock - count;exec 但是不能在集群中用（分布在不同节点上时），所以用watch不通用。 redis都是原子操作，比如自增:incrby，用这个就可以判断库存是否够。就是所谓的redis预减库存。 但是在实际中，库存表里有两个字段：库存和锁定库存。 锁定库存是表示多少用户真正下单了，但是还没有支付。锁定库存+库存=总库存，等用户真正支付之后，就可以将锁定库存减掉。那么，此时，redis中需要存库存和锁定库存这两个值，上面单一的原子操作就不行了。 解决方案：redis+lua 为什么要用lua呢？可以用lua将一系列操作封装起来执行，输入自己的参数即可。lua脚本在redis中执行是串行的、原子性的。 OK，下面就实战一波：根据skuId查询缓存中的库存值。 二、查询库存（设置库存） 首先，我们要明确一点，redis中的库存初始值是由后台的系统人工提前配置好的，在进行商品销售时（用户下单时），直接从redis中先进行库存的扣减。 这里呢，我们没有进行初始化，而是在程序中进行判断：如果redis已经有了这个库存值，就将他查询出来返回；否则，就去数据库查询，然后对redis进行初始化。 这里的一个问题是：如果存在并发问题，但是我们初始化两个值（库存值和库存锁定值），这里采用lua脚本，在lua脚本中完成初始化，并且对于两个用户同时进行初始化库存的问题，可以在lua中进行判断,因为redis是单线程，lua也是单线程，不用担心会同时初始化两次。 下面首先写一个接口，根据skuid查询库存(库存和锁定库存)： 123456789101112@RequestMapping("/query/&#123;skuId&#125;")public ApiResult&lt;Stock&gt; queryStock(@PathVariable long skuId)&#123; ApiResult&lt;Stock&gt; result = new ApiResult(Constants.RESP_STATUS_OK,"库存查询成功"); Stock stock = new Stock(); stock.setSkuId(skuId); int stockCount = stockService.queryStock(skuId); stock.setStock(stockCount); result.setData(stock); return result;&#125; service层： 123456789101112131415161718192021222324252627282930@Overridepublic int queryStock(long skuId) &#123; //先查redis Stock stock ; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+skuId; String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+skuId; //只需要查询一个即可，比如我这里只查询库存就行 Object stockObj = redisTemplate.opsForValue().get(stockKey); Integer stockInRedis = null ; if(stockObj!=null)&#123; stockInRedis = Integer.valueOf(stockObj.toString()); &#125; //没有，那么我就需要将数据库中的数据初始化到redis中 if(stockInRedis==null)&#123; //去数据库查询 然后对redis进行初始化 stock = stockMapper.selectBySkuId(skuId); //两个key和两个库存值通过lua脚本塞到redis中 //这里如果发生两个用户并发初始化redis，脚本中会进行判断，如果已经初始化了，脚本就会停止执行 // 设置库存不应该在这配置，应该是后台管理系统进行设置，所以正常情况下，这里redis中应该是必然存在的 //如果是在后台配置，就没有必要这么复杂了 redisUtils.skuStockInit(stockKey,stockLockKey,stock.getStock().toString(),stock.getLockStock().toString()); &#125;else&#123; return stockInRedis;//缓存中有就直接返回 &#125; //缓存结果可能会返回设置不成功，所以还是返回数据库查询结果 return stock.getStock();&#125; 那么这个工具类为： 123456789101112131415161718192021222324252627282930313233/** * 查看redis是否已经初始化好库存初始值，没有就初始化 */public static final String STOCK_CACHE_LUA = "local stock = KEYS[1] " + "local stock_lock = KEYS[2] " + "local stock_val = tonumber(ARGV[1]) " + "local stock_lock_val = tonumber(ARGV[2]) " + "local is_exists = redis.call(\"EXISTS\", stock) " + "if is_exists == 1 then " + " return 0 " + "else " + " redis.call(\"SET\", stock, stock_val) " + " redis.call(\"SET\", stock_lock, stock_lock_val) " + " return 1 " + "end"; /** * @Description 缓存sku库存 以及锁定库存 */public boolean skuStockInit(String stockKey,String stockLockKey,String stock,String stockLock)&#123; //用jedis去执行lua脚本 输入的参数要注意顺序 都是写死的 第一组是key，第二组是stock Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_CACHE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stock, stockLock))); &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 对于lua脚本进行稍微的解释一下： 12345678910111213141516171819202122//第一组数据是key数组；第二组数据是args数组，是与key数组对应的值，就是库存//我们这里第一组为[stockKey,stockLockKey],就是存在redis中的名字，这里是在service层中定义好了//第二组为[50,0]，这个值就是可以从数据库表t_stock中查询出来的//因为执行这段lua脚本的话，说明redis中没有缓存的数据，所以需要先查询数据库，然后将缓存设置好//lua中定义变量用locallocal stock = KEYS[1]local stock_lock = KEYS[2]local stock_val = tonumber(ARGV[1])local stock_lock_val = tonumber(ARGV[2])//再查询一遍缓存是否存在，防止两个线程同时进来设置缓存//存在就不用设置缓存了，否则就设置缓存local is_exists = redis.call("EXISTS", stock)if is_exists == 1 then return 0else redis.call("SET", stock, stock_val) redis.call("SET", stock_lock, stock_lock_val) return 1end 那么，启动工程mama-buy-stock：假如我去查询skuId=1的商品： 第一次库存不存在，那么就会去查询数据库： 我们再来看看redis中的数据： 三、扣减库存 下面来看看扣减库存是如何实现的。因为提交订单后，往往是不止一件商品的，往往购物车内有很多件商品，同时过来，假设有五件商品，但是其中只有一件暂时没有库存了，那么我还是希望其他的四件商品能够卖出去，只是没有库存的商品就不算钱了。所以扣减库存用一个map来装，即Map&lt;skuId,count&gt; controller层： 1234567@RequestMapping("/reduce")public ApiResult&lt;Map&lt;Long,Integer&gt;&gt; reduceStock(@RequestBody List&lt;StockReduce&gt; stockReduceList)&#123; ApiResult result = new ApiResult(Constants.RESP_STATUS_OK,"库存扣减成功"); Map&lt;Long,Integer&gt; resultMap = stockService.reduceStock(stockReduceList); result.setData(resultMap); return result;&#125; service层： 1234567891011121314151617181920212223242526272829303132333435@Override@Transactionalpublic Map&lt;Long, Integer&gt; reduceStock(List&lt;StockReduce&gt; stockReduceList) &#123; Map&lt;Long, Integer&gt; resultMap = new HashMap&lt;&gt;(); //遍历去减redis中库存，增加锁定库存 stockReduceList.stream().forEach(param -&gt; &#123; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+param.getSkuId(); String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+param.getSkuId(); Object result = redisUtils.reduceStock(stockKey, stockLockKey, param.getReduceCount().toString(),//incrby一个负数，就是减 String.valueOf(Math.abs(param.getReduceCount())));//incrby一个正数，就是加 if(result instanceof Long)&#123; //库存不存在或者不足 扣减失败 sku下单失败 记录下来 resultMap.put(param.getSkuId(),-1); &#125;else if (result instanceof List)&#123; //扣减成功 记录扣减流水 List resultList = ((List) result); int stockAftChange = ((Long)resultList.get(0)).intValue(); int stockLockAftChange = ((Long) resultList.get(1)).intValue(); StockFlow stockFlow = new StockFlow(); stockFlow.setOrderNo(param.getOrderNo()); stockFlow.setSkuId(param.getSkuId()); stockFlow.setLockStockAfter(stockLockAftChange); stockFlow.setLockStockBefore(stockLockAftChange+param.getReduceCount()); stockFlow.setLockStockChange(Math.abs(param.getReduceCount())); stockFlow.setStockAfter(stockAftChange); stockFlow.setStockBefore(stockAftChange+Math.abs(param.getReduceCount())); stockFlow.setStockChange(param.getReduceCount()); stockFlowMapper.insertSelective(stockFlow); resultMap.put(param.getSkuId(),1); &#125; &#125;); return resultMap;&#125; 对于redis的操作，基本与上一致： 12345678910111213141516171819202122232425262728293031323334/** * @Description 扣减库存lua脚本 * @Return 0 key不存在 错误 -1 库存不足 返回list 扣减成功 */public static final String STOCK_REDUCE_LUA= "local stock = KEYS[1]\n" + "local stock_lock = KEYS[2]\n" + "local stock_change = tonumber(ARGV[1])\n" + "local stock_lock_change = tonumber(ARGV[2])\n" + "local is_exists = redis.call(\"EXISTS\", stock)\n" + "if is_exists == 1 then\n" + " local stockAftChange = redis.call(\"INCRBY\", stock,stock_change)\n" + " if(stockAftChange&lt;0) then\n" + " redis.call(\"DECRBY\", stock,stock_change)\n" + " return -1\n" + " else \n" + " local stockLockAftChange = redis.call(\"INCRBY\", stock_lock,stock_lock_change)\n" + " return &#123;stockAftChange,stockLockAftChange&#125;\n" + " end " + "else \n" + " return 0\n" + "end"; public Object reduceStock(String stockKey,String stockLockKey,String stockChange,String stockLockChange)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_REDUCE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stockChange, stockLockChange))); &#125;); return result;&#125; 此时，一旦数据库发生异常，那么就会回滚，但是redis中是无法回滚的。这个问题不用担心，因为数据库发生异常是及其严重的问题，是很少会发生的，一旦发生，只需要去这个流水的表中去查看情况，然后去执行脚本去初始化这个redis即可。所以是可以补救的。 但是接口的幂等性还没有做。重复尝试调用这个接口（通常是发生在MQ的失败重传机制，客户端的连续点击一般是可以避免的），可能会重复减redis库存并且重复地去插入流水记录。这个问题该如何解决呢？ 四、redis分布式锁来实现幂等性 主流的方案，比如有用一张表来控制，比如以这个orderID为唯一主键，一旦插入成功，就可以根据这个唯一主键的存在与否判断是否为重复请求（也就是说，这里的扣减库存和插入去重表放在一个事务里，去重表中有一个字段为orderId，全局唯一不重复，用唯一索引进行约束，那么插入的时候判断这个去重表是否可以插入成功，如果不成功，那么数据库操作全部回滚）。 可以用redis分布式锁给这个订单上锁。以订单id为锁，不会影响其他线程来扣减库存，所以不影响性能。 针对这个订单，第一次肯定是可以去扣减库存的，但是第二次再接收到这个请求，那么就要返回已经成功了，不要再重复扣减。 对于reduceStock()这个方法最前面增加锁： 123456789//防止扣减库存时MQ正常重试时的不幂等//以订单ID 加个缓存锁 防止程序短时间重试 重复扣减库存 不用解锁 自己超时Long orderNo = stockReduceList.get(0).getOrderNo();boolean lockResult = redisUtils.distributeLock(Constants.ORDER_RETRY_LOCK+orderNo.toString(),orderNo.toString(),300000);if(!lockResult)&#123; //锁定失败 重复提交 返回一个空map return Collections.EMPTY_MAP;&#125;... 123456789101112131415161718192021222324252627282930313233343536373839404142private static final String LOCK_SUCCESS = "OK";private static final String SET_IF_NOT_EXIST = "NX";private static final String SET_WITH_EXPIRE_TIME = "PX";private static final Long EXCUTE_SUCCESS = 1L;/**lua脚本 在redis中 lua脚本执行是串行的 原子的 */private static final String UNLOCK_LUA= "if redis.call('get', KEYS[1]) == ARGV[1] then " + " return redis.call('del', KEYS[1]) " + "else " + " return 0 " + "end";/** * @Description 获取分布式锁 * @Return boolean */public boolean distributeLock(String lockKey, String requestId, int expireTime)&#123; String result = redisTemplate.execute((RedisCallback&lt;String&gt;) redisConnection -&gt; &#123; JedisCommands commands = (JedisCommands)redisConnection.getNativeConnection(); return commands.set(lockKey,requestId,SET_IF_NOT_EXIST,SET_WITH_EXPIRE_TIME,expireTime);//一条命令实现setnx和setexpire这些操作，原子性 &#125;); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125;/** * @Description 释放分布式锁 * @Return boolean */public boolean releaseDistributelock(String lockKey, String requestId)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(UNLOCK_LUA, Collections.singletonList(lockKey), Collections.singletonList(requestId));//lua脚本中原子性实现：get查询和delete删除这两个操作 &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 注意，这里不需要我们主动去释放分布式锁，只要设置一个大于重试时间的过期时间即可。让它自己删除。 注意redis在集群下做分布式锁，最好要用Redission。这里如果用于集群，如何lua脚本在一个事务里同时操作多个key的时候，如果要保证这个事务生效，就需要保证这几个key都要在同一个节点上。但是，比如我们这里的两个key： 12public static final String CACHE_PRODUCT_STOCK = "product:stock";public static final String CACHE_PRODUCT_STOCK_LOCK = "product:stock:lock"; 因为我们这里要同时对库存和锁定库存这两个key进行操作，需要放在一个事务内执行，不处理的话，一旦他们不在一个节点，那么事务就不会生效，解决方案： 12public static final String CACHE_PRODUCT_STOCK = "&#123;product:stock&#125;";public static final String CACHE_PRODUCT_STOCK_LOCK = "&#123;product:stock&#125;:lock"; 如果加上花括号，那么在进行计算hash值的时候，他们两就会是一样的，会被投放到同一个slot中，自然就保证了在同一个节点上。 五、测试一下]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK平台搭建]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F08ELK%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[因为要完成产品的全文搜索这个功能，所以需要准备一下ES的环境。本节安装ELK。 ELK由Elasticsearch、Logstash和Kibana三部分组件组成。 前言 Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 简单来说，他是个全文搜索引擎，可以快速地储存、搜索和分析海量数据。 Logstash是一个完全开源的工具，它可以把分散的、多样化的日志日志，或者是其他数据源的数据信息进行收集、分析、处理，并将其存储供以后使用。 Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。 你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。 你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。 Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。 一、安装ES 1.1 首先是安装JDK： 12345cd /opt/wget --no-cookies --no-check-certificate --header &quot;Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie&quot; &quot;http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz&quot;tar xzf jdk-8u141-linux-x64.tar.gz 1.2 添加环境变量： 123456789101112vim /etc/profileJAVA_HOME=/opt/jdk1.8.0_141JAVA_JRE=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATHsource /etc/profilejava -version 1.3 下载6.2.4版本： 123456wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gztar -xzvf elasticsearch-6.2.4.tar.gztar -zxvf elasticsearch-6.2.4.tar.gzmv elasticsearch-6.2.4 elasticsearch 1.4 配置sysctl.conf 1234567891011#修改sysctl配置vim /etc/sysctl.conf #添加如下配置vm.max_map_count=262144 #让配置生效sysctl -p #查看配置的数目sysctl -a|grep vm.max_map_count 1.5 elasticsearch从5.0版本之后不允许root账户启动 1234567891011121314151617181920#添加用户adduser dev #设定密码passwd dev #添加权限chown -R dev /opt/elasticsearch #切换用户su dev #查看当前用户who am i #启动./elasticsearch/bin/elasticsearch #后台启动./elasticsearch/bin/elasticsearch -d 1.6 配置limits.conf 123456789101112131415vim /etc/security/limits.conf 把* soft nofile 65535* hard nofile 65535 改为* soft nofile 65536* hard nofile 65536 #切换用户su dev #查看配置是否生效ulimit -Hn 1.7 配置所有用户访问 1vim /opt/elasticsearch/config/elasticsearch.yml 1.8 添加一下内容 1network.host: 0.0.0.0 1.9 重启 12ps -ef | grep elastickill -9 xxxx 1.10 测试： 1curl http://localhost:9200/ 显示： 123456789101112131415&#123; &quot;name&quot; : &quot;MmiaBfA&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;zjX-q5PDRLyrWMy5TiBDkw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.2.4&quot;, &quot;build_hash&quot; : &quot;ccec39f&quot;, &quot;build_date&quot; : &quot;2018-04-12T20:37:28.497551Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.2.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 就说明成功了。 二、安装Kibana 6.2.4 1234567wget https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gztar -zxvf kibana-6.2.4-linux-x86_64.tar.gzmv kibana-6.2.4-linux-x86_64 kibanavim /opt/kibana/config/kibana.yml 2.1 添加以下内容： 123server.port: 5601server.host: &quot;0.0.0.0&quot;elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 2.2 切换到bin目录下，启动即可。 12345#不能关闭终端./kibana #可关闭终端nohup ./kibana &amp; 2.3 开放防火墙和安全组对应的这个端口 浏览器访问：http://106.14.163.235:5601 看到一个控制台页面就成功啦。 2.4 关闭这个进程 1234567891011121314ps -ef|grep kibana ps -ef|grep 5601 都找不到 尝试 使用 fuser -n tcp 5601 kill -9 端口 启动即可 ./kibana或者去这个目录下的.out日志中可以看到看到它占用的pid 三、logstash 1234567891011# 下载wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.4.tar.gz# 解压tar -zxvf logstash-6.2.4.tar.gz# 重命名mv logstash-6.2.4.tar.gz logstash# 进入cd logstash 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 新建一个配置文件 我这里是mysqltones.confinput &#123; stdin &#123; &#125; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/mama-buy-trade&quot; jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;root&quot; jdbc_driver_library =&gt; &quot;/opt/logstash/mysql-connector-java-5.1.46-bin.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; # mysql文件, 也可以直接写SQL语句在此处，如下： statement =&gt; &quot;SELECT * from t_product&quot; # statement_filepath =&gt; &quot;/opt/logstash/conf/jdbc.sql&quot; # 这里类似crontab,可以定制定时操作，比如每10分钟执行一次同步(分 时 天 月 年) schedule =&gt; &quot;*/10 * * * *&quot; type =&gt; &quot;jdbc&quot; # 是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中 record_last_run =&gt; &quot;true&quot; # 是否需要记录某个column 的值,如果record_last_run为真,可以自定义我们需要 track 的 column 名称，此时该参数就要为 true. 否则默认 track 的是 timestamp 的值. use_column_value =&gt; &quot;true&quot; # 如果 use_column_value 为真,需配置此参数. track 的数据库 column 名,该 column 必须是递增的. 一般是mysql主键 tracking_column =&gt; &quot;id&quot; last_run_metadata_path =&gt; &quot;/opt/logstash/conf/last_id&quot; # 是否清除 last_run_metadata_path 的记录,如果为真那么每次都相当于从头开始查询所有的数据库记录 clean_run =&gt; &quot;false&quot; # 是否将 字段(column) 名称转小写 lowercase_column_names =&gt; &quot;false&quot; &#125;&#125;# 此处我不做过滤处理filter &#123;&#125;output &#123; # 输出到elasticsearch的配置 elasticsearch &#123; hosts =&gt; [&quot;127.0.0.1:9200&quot;] index =&gt; &quot;jdbc&quot; # 将&quot;_id&quot;的值设为mysql的autoid字段 document_id =&gt; &quot;%&#123;id&#125;&quot; template_overwrite =&gt; true &#125; # 这里输出调试，正式运行时可以注释掉 stdout &#123; codec =&gt; json_lines &#125;&#125; 12# 启动./bin/logstash -f ./mysqltones.conf 看到这个就说明成功了： 安装mysql数据库 这一步要在执行logstash之前搞定，我的是阿里云centos7.3版本，mysql版本是5.7，安装过程如下： 123456789101112131415161718192021222324252627282930313233343536373839# 下载MySQL源安装包: wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm# 安装MySQL源：yum localinstall mysql57-community-release-el7-8.noarch.rpm # 检查MySQL源安装情况： yum repolist enabled | grep &quot;mysql.*-community.*&quot;# 安装MySQL: yum install mysql-community-server# 启动MySQL: systemctl start mysqld# 查看MySQL状态: systemctl status mysqld# 设置开机启动MySQL：systemctl enable mysqld systemctl daemon-reload# 查找并修改MySQL默认密码（注意密码要符合规范，否则会失败）：grep &apos;temporary password&apos; /var/log/mysqld.log mysql -uroot -p alter user root@localhost identified by &apos;你的新密码&apos;;# 远程连接测试添加远程账户：GRANT ALL PRIVILEGES ON *.* TO &apos;用户&apos;@&apos;%&apos; IDENTIFIED BY &apos;密码&apos; WITH GRANT OPTION;# 立即生效：flush privileges;# 退出MySQL：exit# 最后远程将数据给导入数据库 安装分词器 ik_max_word是分词比较细腻的一款，我们就用它来做分词，首先需要安装一下： 12345678# 直接安装./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.4/elasticsearch-analysis-ik-6.2.4.zip # 重新启动ESps -ef | grep elastickill -9 xxxxsu dev./bin/elasticsearch -d 对这个分词器在kibana中进行测试： 下面结合数据库模拟一下：]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Curator]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F07Curator%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[从技术角度出发，注册一个网站，再高并发的时候，有可能出现用户名重复这样的问题（虽然一般情况下不会出现这种问题），如何解决呢？ 从数据库角度，对于单表，我可以用select .. for update悲观锁实现，或者用version这种乐观锁的思想。 更好的方法是将这个字段添加唯一索引，用数据库来保证不会重复。一旦插入重复，那么就会抛出异常，程序就可以捕获到。 但是，假如我们这里分表了，以上都是针对单表，第一种方案是锁表，不行，设置唯一索引是没有用。怎么办呢？ 解决方案：用ZK做一个分布式锁。 首先准备一个ZK客户端，用的是Curator来连接我们的ZK： 1234567891011121314151617181920@Componentpublic class ZkClient &#123; @Autowired private Parameters parameters; @Bean public CuratorFramework getZkClient()&#123; CuratorFrameworkFactory.Builder builder= CuratorFrameworkFactory.builder() .connectString(parameters.getZkHost()) .connectionTimeoutMs(3000) .retryPolicy(new RetryNTimes(5, 10)); CuratorFramework framework = builder.build(); framework.start(); return framework; &#125;&#125; 注册用一个分布式锁来控制： 1234567891011121314151617181920212223242526272829303132333435@Overridepublic void registerUser(User user) throws Exception &#123; InterProcessLock lock = null; try&#123; lock = new InterProcessMutex(zkClient, Constants.USER_REGISTER_DISTRIBUTE_LOCK_PATH); boolean retry = true; do&#123; if (lock.acquire(3000, TimeUnit.MILLISECONDS))&#123; //查询重复用户 User repeatedUser = userMapper.selectByEmail(user.getEmail()); if(repeatedUser!=null)&#123; throw new MamaBuyException("用户邮箱重复"); &#125; user.setPassword(passwordEncoder.encode(user.getPassword())); user.setNickname("码码购用户"+user.getEmail()); userMapper.insertSelective(user); //跳出循环 retry = false; &#125; //可以适当休息一会...也可以设置重复次数，不要无限循环 &#125;while (retry); &#125;catch (Exception e)&#123; log.error("用户注册异常",e); throw e; &#125;finally &#123; if(lock != null)&#123; try &#123; lock.release(); log.info(user.getEmail()+Thread.currentThread().getName()+"释放锁"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 思路非常简单，就是先尝试上锁，即acquire，但是有可能失败，所以这里用一个超时时间，即3000ms之内上不了锁就失败，进入下一次循环。最后释放锁即可。 ok，这里要来说说ZK实现分布式锁了。这里用了开源客户端Curator，他对于实现分布式锁进行了封装，但是，我还是想了解一下它的实现原理： 每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。 也就是说，最小的那个节点就是Leader，进来判断是不是为那个节点，是的话就可以获取到锁，反之不行。 为什么不能通过大家一起创建节点，如果谁成功了就算获取到了锁。 多个client创建一个同名的节点，如果节点谁创建成功那么表示获取到了锁，创建失败表示没有获取到锁。 答：使用临时顺序节点可以保证获得锁的公平性，及谁先来谁就先得到锁，这种方式是随机获取锁，会造成无序和饥饿。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Session]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F06Spring%20Session%2F</url>
    <content type="text"><![CDATA[在单体应用中，我们经常用http session去管理用户信息，但是到了分布式环境下，显然是不行的，因为session对于不同的机器是隔离的，而http本身是无状态的，那么就无法判断出用户在哪一个服务器上登陆的。这个时候就需要有一个独立的地方存储用户session。spring session可以做到无代码侵入的方式实现分布式session存储。 在spring boot开发中，我们先注册相应bean并且打开相应的注解： 1234567891011121314151617181920212223242526272829@Configuration@EnableRedisHttpSession //(maxInactiveIntervalInSeconds = 604800)//session超时public class HttpSessionConfig &#123; @Autowired private Parameters parameters; @Bean public HttpSessionStrategy httpSessionStrategy()&#123; return new HeaderHttpSessionStrategy(); &#125; @Bean public JedisConnectionFactory connectionFactory()&#123; JedisConnectionFactory connectionFactory = new JedisConnectionFactory(); String redisHost = parameters.getRedisNode().split(":")[0]; int redisPort = Integer.valueOf(parameters.getRedisNode().split(":")[1]); connectionFactory.setTimeout(2000); connectionFactory.setHostName(redisHost); connectionFactory.setPort(redisPort);// connectionFactory.setPassword(parameters.getRedisAuth()); return connectionFactory; &#125;&#125; ok，这样子其实就配置好了，一开始我也云里雾里的，这是啥玩意？ 其实官网的文档中讲的是最准确的。所以还是官网看看吧！ ok，来spring session的官网(https://spring.io/projects/spring-session)来看看把，我们来看看1.3.4GA版本的文档(https://docs.spring.io/spring-session/docs/1.3.4.RELEASE/reference/html5/#httpsession-rest). spring session可以存在很多介质中，比如我们的数据源，比如redis，甚至是mongodb等。但是我们常用的是存在redis中，结合redis的过期机制来做。 所以其实我们只要关心如何跟redis整合，以及restful接口。 我们可以看到一开始文档就告诉我们要配置一下HttpSessionStrategy和存储介质。从HttpSessionStrategy语义就能大致看出配置的是它的策略，是基于header的策略。这个是什么意思，下面会提到。 那么我们就来看看文档吧！ 好了，我们知道了它的基本原理，下面来看看是如何在restful接口中实现用户session的管理的： 也就是说要想在restful接口应用中用这种方式，直接告诉spring session:return new HeaderHttpSessionStrategy();即可。进入源码我们就会知道，它默认给这个header里面放置的一条类似于token的名字是private String headerName = &quot;x-auth-token&quot;;。 那么在用户登陆成功之后，到底存到是什么呢，先来看看响应数据的header里面是什么： 这一串数字正好可以跟redis中对应上，我们可以先来redis中看看到底在里面存储了啥玩意： 我们已经看到了想要看到的一串字符串，这里解释一下redis中存储的东西： spring:session是默认的Redis HttpSession前缀（redis中，我们常用’:’作为分割符） 每一个session都会有三个相关的key，第一个key(spring:session:sessions:37...)最为重要，它是一个HASH数据结构，将内存中的session信息序列化到了redis中。如本项目中用户信息,还有一些meta信息，如创建时间，最后访问时间等。 另外两个key，一个是spring:session:expiration，还有一个是spring:session:sessions:expires，前者是一个SET类型，后者是一个STRING类型，可能会有读者发出这样的疑问，redis自身就有过期时间的设置方式TTL，为什么要额外添加两个key来维持session过期的特性呢？redis清除过期key的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致session不被清除。于是引入了专门的expiresKey，来专门负责session的清除，包括我们自己在使用redis时也需要关注这一点。 这样子，就可以用独立的redis来存储用户的信息，通过前端传来的header里面的token，就可以到redis拿出当前登陆用户的信息了。 OK，在解决了spring session的问题之后，下面就可以来实现登陆啦： controller: 1234567891011121314@RequestMapping("login")public ApiResult login(@RequestBody @Valid User user, HttpSession session)&#123; ApiResult&lt;UserElement&gt; result = new ApiResult&lt;&gt;(Constants.RESP_STATUS_OK,"登录成功"); UserElement ue= userService.login(user); if(ue != null)&#123; if(session.getAttribute(Constants.REQUEST_USER_SESSION) == null)&#123; session.setAttribute(Constants.REQUEST_USER_SESSION,ue); &#125; result.setData(ue); &#125; return result;&#125; 就跟以前一样，将session直接存进去就可以了。 12345678910111213141516171819202122@Overridepublic UserElement login(User user) &#123; UserElement ue = null; User userExist = userMapper.selectByEmail(user.getEmail()); if(userExist != null)&#123; //对密码与数据库密码进行校验 boolean result = passwordEncoder.matches(user.getPassword(),userExist.getPassword()); if(!result)&#123; throw new MamaBuyException("密码错误"); &#125;else&#123; //校验全部通过，登陆通过 ue = new UserElement(); ue.setUserId(userExist.getId()); ue.setEmail(userExist.getEmail()); ue.setNickname(userExist.getNickname()); ue.setUuid(userExist.getUuid()); &#125; &#125;else &#123; throw new MamaBuyException("用户不存在"); &#125; return ue;&#125;]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式ID生成策略]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F05%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[分布式环境下如何保证ID的不重复呢？一般我们可能会想到用UUID来实现嘛。但是UUID一般可以获取当前时间的毫秒数再加点随机数，但是在高并发下仍然可能重复。最重要的是，如果我要用这种UUID来生成分表的唯一ID的话，重复不谈，这种随机的字符串对于我们的innodb存储引擎的插入效率是很低的。所以我们生成的ID如果作为主键，最好有两种特性：分布式唯一和有序。 唯一性就不用说了，有序保证了对索引字段的插入的高效性。我们来具体看看ShardingJDBC的分布式ID生成策略是如何保证。 snowflake算法 sharding-jdbc的分布式ID采用twitter开源的snowflake算法，不需要依赖任何第三方组件，这样其扩展性和维护性得到最大的简化；但是snowflake算法的缺陷（强依赖时间，如果时钟回拨，就会生成重复的ID）。 雪花算法是由Twitter公布的分布式主键生成算法，它能够保证不同进程主键的不重复性，以及相同进程主键的有序性。 在同一个进程中，它首先是通过时间位保证不重复，如果时间相同则是通过序列位保证。 同时由于时间位是单调递增的，且各个服务器如果大体做了时间同步，那么生成的主键在分布式环境可以认为是总体有序的，这就保证了对索引字段的插入的高效性。例如MySQL的Innodb存储引擎的主键。 使用雪花算法生成的主键，二进制表示形式包含4部分，从高位到低位分表为：1bit符号位、41bit时间戳位、10bit工作进程位以及12bit序列号位。 雪花算法主键的详细结构见下图。 符号位(1bit) 预留的符号位，恒为零。 时间戳位(41bit) 41位的时间戳可以容纳的毫秒数是2的41次幂，一年所使用的毫秒数是：365 * 24 * 60 * 60 * 1000。通过计算可知： 1Math.pow(2, 41) / (365 * 24 * 60 * 60 * 1000L); 结果约等于69.73年。ShardingSphere的雪花算法的时间纪元从2016年11月1日零点开始，可以使用到2086年，相信能满足绝大部分系统的要求。 工作进程位(10bit) 该标志在Java进程内是唯一的，如果是分布式应用部署应保证每个工作进程的id是不同的。该值默认为0，可通过调用静态方法DefaultKeyGenerator.setWorkerId()设置。 序列号位(12bit) 该序列是用来在同一个毫秒内生成不同的ID。如果在这个毫秒内生成的数量超过4096(2的12次幂)，那么生成器会等待到下个毫秒继续生成。 时钟回拨 服务器时钟回拨会导致产生重复序列，因此默认分布式主键生成器提供了一个最大容忍的时钟回拨毫秒数。 如果时钟回拨的时间超过最大容忍的毫秒数阈值，则程序报错；如果在可容忍的范围内，默认分布式主键生成器会等待时钟同步到最后一次主键生成的时间后再继续工作。 最大容忍的时钟回拨毫秒数的默认值为0，可通过调用静态方法DefaultKeyGenerator.setMaxTolerateTimeDifferenceMilliseconds()设置。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springMVC全局异常+spring包扫描包隔离+spring事务传播]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F04springMVC%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%2Bspring%E5%8C%85%E6%89%AB%E6%8F%8F%E5%8C%85%E9%9A%94%E7%A6%BB%2Bspring%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[在开发中，springMVC全局异常+spring包扫描包隔离+spring事务传播这三个不可能不会遇到。下面来好好说说他们吧。 1、全局异常引入原因 假设在我们的login.do的controller方法中第一行增加一句： 1int i = 1/0; 重新启动服务器进行用户登录操作，那么就会抛出异常： 123java.lang.ArithmeticException: / by zero com.swg.controller.portal.UserController.login(UserController.java:37) ...其他的堆栈信息 这些信息会直接显示在网页上，如果是关于数据库的错误，同样，会详细地将数据库中的字段都显示在页面上，这对于我们的项目来说是存在很大的安全隐患的。这个时候，需要用全局异常来处理，如果发生异常，我们就对其进行拦截，并且在页面上显示我们给出的提示信息。 对于SpringBoot，一般全局异常是: 1234567891011121314151617@ControllerAdvice@ResponseBody@Slf4jpublic class ExceptionHandlerAdvice &#123; @ExceptionHandler(Exception.class) public ServerResponse handleException(Exception e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(Constants.RESP_STATUS_INTERNAL_ERROR,"系统异常，请稍后再试"); &#125; @ExceptionHandler(SnailmallException.class) public ServerResponse handleException(SnailmallException e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(e.getExceptionStatus(),e.getMessage()); &#125;&#125; 2、引入全局异常 12345678910111213@Slf4j@Componentpublic class ExceptionResolver implements HandlerExceptionResolver&#123; @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) &#123; log.error("exception:&#123;&#125;",httpServletRequest.getRequestURI(),e); ModelAndView mv = new ModelAndView(new MappingJacksonJsonView()); mv.addObject("status",ResponseEnum.ERROR.getCode()); mv.addObject("msg","接口异常，详情请查看日志中的异常信息"); mv.addObject("data",e.toString()); return mv; &#125;&#125; 那么，再执行登陆操作之后，就不会在页面上直接显示异常信息了。有效地屏蔽了关键信息。 3、spring和springmvc配置文件的优化 3.1 包隔离优化 在编写全局异常之前，先进行了包隔离和优化，一期中的扫描包的写法是： 12345&lt;!--spring:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt;&lt;!--springmvc:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt; 即spring和springmvc扫描包下面的所有的bean和controller.优化后的代码配置： 1234567891011#spring&lt;context:component-scan base-package="com.swg" annotation-config="true"&gt;&lt;!--将controller的扫描排除掉--&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt;#springmvc&lt;context:component-scan base-package="com.swg.controller" annotation-config="true" use-default-filters="false"&gt;&lt;!--添加白名单，只扫描controller，总之要将service给排除掉即可--&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt; 这样做的原因是：Spring和SpringMVC是有父子容器关系的，而且正是因为这个才往往会出现包扫描的问题。 针对包扫描只要记住以下几点即可： spring是父容器，springmvc是子容器，子容器可以访问父容器的bean,父容器不能访问子容器的bean。 只有顶级容器（spring）才有加强的事务能力，而springmvc容器的service是没有的。 如果springmvc不配置包扫描的话，页面404. 3.2 事务的传播机制 针对事务，不得不展开说明spring事务的几种传播机制了。在 spring 的 TransactionDefinition 接口中一共定义了七种事务传播属性： PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择（默认）。 PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 4、补充 Spring默认情况下，会对运行期例外(RunTimeException)，即uncheck异常，进行事务回滚。如果遇到checked异常就不回滚。如何改变默认规则： 让checked例外也回滚：在整个方法前加上 @Transactional(rollbackFor=Exception.class) 让unchecked例外不回滚： @Transactional(notRollbackFor=RunTimeException.class) 不需要事务管理的(只查询的)方法：@Transactional(propagation=Propagation.NOT_SUPPORTED) 5、那么什么是嵌套事务呢？ 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫做save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点在于那个save point，看以下几个问题： 问题1：如果子事务回滚，会发生什么？ 父事务会回到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。 问题2：如果父事务回滚，会发生什么？ 父事务回滚，子事务也会跟着回滚，为什么呢？因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理/ 问题3：父事务先提交，然后子事务再提交；还是子事务先提交，然后父事务再提交呢？ 答案是第二种情况，子事务是父事务的一部分，由父事务同意提交。 6、spring配置文件的一些理解： 容器 在Spring整体框架的核心概念中，容器是核心思想，就是用来管理Bean的整个生命周期的，而在一个项目中，容器不一定只有一个，Spring中可以包括多个容器，而且容器有上下层关系，目前最常见的一种场景就是在一个项目中引入Spring和SpringMVC这两个框架，那么它其实就是两个容器，Spring是父容器，SpringMVC是其子容器，并且在Spring父容器中注册的Bean对于SpringMVC容器中是可见的，而在SpringMVC容器中注册的Bean对于Spring父容器中是不可见的，也就是子容器可以看见父容器中的注册的Bean，反之就不行。 1&lt;context:component-scan base-package="com.springmvc.test" /&gt; 我们可以使用统一的如下注解配置来对Bean进行批量注册，而不需要再给每个Bean单独使用xml的方式进行配置。 从Spring提供的参考手册中我们得知该配置的功能是扫描配置的base-package包下的所有使用了@Component注解的类，并且将它们自动注册到容器中，同时也扫描@Controller，@Service，@Respository这三个注解，因为他们是继承自@Component。 1&lt;context:annotation-config/&gt; 其实有了上面的配置，这个是可以省略掉的，因为上面的配置会默认打开以下配置。以下配置会默认声明了@Required、@Autowired、 @PostConstruct、@PersistenceContext、@Resource、@PreDestroy等注解。 1&lt;mvc:annotation-driven /&gt; 这个是SpringMVC必须要配置的，因为它声明了@RequestMapping、@RequestBody、@ResponseBody等。并且，该配置默认加载很多的参数绑定方法，比如json转换解析器等。 7、总结 在实际工程中会包括很多配置，我们按照官方推荐根据不同的业务模块来划分不同容器中注册不同类型的Bean：Spring父容器负责所有其他非@Controller注解的Bean的注册，而SpringMVC只负责@Controller注解的Bean的注册，使得他们各负其责、明确边界。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis实现分布式锁]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F03redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[为了讲解redis分布式锁，我将引入一个场景：定时关单。因为往往订单服务是一个集群，那么定时器会同时触发这些集群去取消订单，显然是浪费机器资源的，所以目的是：只让其中一台机器去执行取消订单即可。这里可以用分布式锁来实现。 项目是从练手项目中截取出来的，框架是基于SSM的XML形式构成，所以下面还涉及一点XMl对于定时器spring schedule的配置内容。 1、引入目标 定时自动对超过两个小时还未支付的订单对其进行取消，并且重置库存。 2、配置 首先是spring配置文件引入spring-schedule 123456xmlns:task="http://www.springframework.org/schema/task"...http://www.springframework.org/schema/taskhttp://www.springframework.org/schema/task/spring-task.xsd...&lt;task:annotation-driven/&gt; 补充：针对applicationContext-datasource.xml中的dataSource读取配置文件的信息无法展现的问题，在spring的配置文件中增加一条配置： 1&lt;context:property-placeholder location="classpath:datasource.properties"/&gt; 3、定时调度代码 此代码的主要功能是：定时调用取消订单服务。 1234567891011121314@Component@Slf4jpublic class CloseOrderTask &#123; @Autowired private OrderService orderService; @Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍的时候执行 public void closeOrderTaskV1()&#123; log.info("关闭订单定时任务启动"); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); log.info("关闭订单定时任务结束"); &#125;&#125; @Component一定要加，否则spring扫描不到。 close.order.task.time.hour 也是配置在snailmall.properties中的，这里配置的是默认的2，即两个小时，下订单超过两个小时仍然不支付，就取消该订单。 对于orderService里面的具体方法： 这里是关单的具体逻辑，细节是行锁。这段代码只要知道他是具体关单的逻辑即可，不需要仔细了解代码。 1234567891011121314151617181920212223242526@Overridepublic void closeOrder(int hour) &#123; Date closeDateTime = DateUtils.addHours(new Date(),-hour); //找到状态为未支付并且下单时间是早于当前检测时间的两个小时的时间,就将其置为取消 //SELECT &lt;include refid="Base_Column_List"/&gt; from mmall_order WHERE status = #&#123;status&#125; &lt;![CDATA[ and create_time &lt;= #&#123;date&#125; ]]&gt; order by create_time desc List&lt;Order&gt; orderList = orderMapper.selectOrderStatusByCreateTime(Const.OrderStatusEnum.NO_PAY.getCode(),DateTimeUtil.dateToStr(closeDateTime)); for(Order order:orderList)&#123; List&lt;OrderItem&gt; orderItemList = orderItemMapper.getByOrderNo(order.getOrderNo()); for(OrderItem orderItem:orderItemList)&#123; //一定要用主键where条件，防止锁表。同时必须是支持MySQL的InnoDB. Integer stock = productMapper.selectStockByProductId(orderItem.getProductId()); if(stock == null)&#123; continue; &#125; //更新产品库存 Product product = new Product(); product.setId(orderItem.getProductId()); product.setStock(stock+orderItem.getQuantity()); productMapper.updateByPrimaryKeySelective(product); &#125; //关闭order //UPDATE mmall_order set status = 0 where id = #&#123;id&#125; orderMapper.closeOrderByOrderId(order.getId()); log.info("关闭订单OrderNo:&#123;&#125;",order.getOrderNo()); &#125;&#125; 这样，debug启动项目，一分钟后就会自动执行closeOrderTaskV1方法了。找一个未支付的订单，进行相应测试。 4、存在的问题 经过实验发现，同时部署两台tomcat服务器，执行定时任务的时候是两台都同时执行的，显然不符合我们集群的目标，我们只需要在同一时间只有一台服务器执行这个定时任务即可。那么解决方案就是引入redis分布式锁。 redis实现分布式锁，核心命令式setnx命令。所以阅读下面，您需要对redis分布式锁的基本实现原理必须要先有一定的认识才行。 5、第一种方案 第一步：setnx进去，如果成功，说明塞入redis成功，抢占到锁 第二步：抢到锁之后，先设置一下过期时间，即后面如果执行不到delete，也会将这个锁自动释放掉，防止死锁 第三步：关闭订单，删除redis锁 存在的问题：如果因为tomcat关闭或tomcat进程在执行closeOrder()方法的时候，即还没来得及设置锁的过期时间的时候，这个时候会造成死锁。需要改进。 123456789101112131415161718192021222324252627//第一个版本，在突然关闭tomcat的时候有可能出现死锁@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV2()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; log.info("关闭订单定时任务结束");&#125;private void closeOrder(String lockName) &#123; //给锁一个过期时间，如果因为某个原因导致下面的锁没有被删除，造成死锁 RedisShardPoolUtil.expire(lockName,50); log.info("获取&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); //关闭订单之后就立即删除这个锁 RedisShardPoolUtil.del(lockName); log.info("释放&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); System.out.println("=============================================");&#125; 6、改进 图看不清，可以重新打开一个窗口看。具体的逻辑代码： 12345678910111213141516171819202122232425262728@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV3()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; //在没有拿到锁的情况下，也要进行相应的判断，确保不死锁 String lockValueStr = RedisShardPoolUtil.get(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); //如果判断锁是存在的并且现在已经超时了，那么我们这个进程就有机会去占有这把锁 if(lockValueStr != null &amp;&amp; System.currentTimeMillis() &gt; Long.parseLong(lockValueStr))&#123; //当前进程进行get set操作，拿到老的key，再塞进新的超时时间 String getSetResult = RedisShardPoolUtil.getset(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); //如果拿到的是空的，说明老的锁已经释放，那么当前进程有权占有这把锁进行操作； //如果拿到的不是空的，说明老的锁仍然占有，并且这次getset拿到的key与上面查询get得到的key一样的话，说明没有被其他进程刷新，那么本进程还是有权占有这把锁进行操作 if(getSetResult == null || (getSetResult != null &amp;&amp; StringUtils.equals(lockValueStr,getSetResult)))&#123; closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125; log.info("关闭订单定时任务结束");&#125; 这样两次的防死锁措施，不仅可以防止死锁，还可以提高效率。 7、扩展 mysql四种事务隔离机制 read uncommitted:读取未提交内容 两个线程，其中一个线程执行了更新操作，但是没有提交，另一个线程在事务内就会读到该线程未提交的数据。 read committed:读取提交内容（不可重复读） 针对第一种情况，一个线程在一个事务内不会读取另一个线程未提交的数据了。但是，读到了另一个线程更新后提交的数据，也就是说重复读表的时候，数据会不一致。显然这种情况也是不合理的，所以叫不可重复读。 repeatable read:可重复读（默认） 可重复读，显然解决2中的问题，即一个线程在一个事务内不会再读取到另一个线程提交的数据，保证了该线程在这个事务内的数据的一致性。 对于某些情况，这种方案会出现幻影读，他对于更新操作是没有任何问题的了，但是对于插入操作，有可能在一个事务内读到新插入的数据（但是MySQL中用多版本并发控制机制解决了这个问题），所以默认使用的就是这个机制，没有任何问题。 serializable:序列化 略。 存储引擎 MySQL默认使用的是InnoDB，支持事务。还有例如MyISAM,这种存储引擎不支持事务，只支持只读操作，在用到数据的修改的地方，一般都是用默认的InnoDB存储引擎。 索引的一个注意点 一般类型为normal和unique，用btree实现，对于联合索引(字段1和字段2)，在执行查询的时候，例如 1select * from xxx where 字段1="xxx" ... 是可以利用到索引的高性能查询的，但是如果是 1select * from xxx where 字段2="xxx" ... 效率跟普通的查询时一样的，因为用索引进行查询，最左边的那个字段必须要有，否则无效。 扩展的内容知识顺便提一下，在数据库这一块，会详细介绍一下。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson实现Redis分布式锁原理]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F02Redisson%E5%AE%9E%E7%8E%B0Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[我们可以自己来实现一个redis分布式锁，但是如何用Redisson优雅地实现呢？本文探讨一下它的原理。 用Redisson来实现分布式锁异常地简单，形如： 还支持redis单实例、redis哨兵、redis cluster、redis master-slave等各种部署架构，都可以给你完美实现。 加锁 原理图： 现在某个客户端要加锁。如果该客户端面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。紧接着，就会发送一段lua脚本到redis上，那段lua脚本如下所示： 为啥要用lua脚本呢？因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的原子性。 解释一下这段脚本的意思。 这里的KEYS[1]代表的是你加锁的那个key的名字。这个key就是我们常看到的： 1RLock lock = redisson.getLock("myLock"); 中的myLock，我就是对这个key进行加锁。 这里的ARGV[1]代表的就是锁key的默认生存时间，默认30秒。ARGV[2]代表的是加锁的客户端的ID:比如8743c9c0-0795-4907-87fd-6c719a6b4586:1 第一段if判断语句，就是相当于用exists myLock命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。如何加锁呢？很简单，用下面的命令：hset myLock。 执行完hest之后，设置了一个hash数据结构：8743c9c0-0795-4907-87fd-6c719a6b4586:1 1，这行命令执行后，会出现一个类似下面的数据结构： 紧接着会执行pexpire myLock 30000命令，设置myLock这个锁key的生存时间是30秒。好了，到此为止，ok，加锁完成了。 锁互斥 那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？很简单，第一个if判断会执行exists myLock，发现myLock这个锁key已经存在了。接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。 所以这个客户端2两个if都不能进入，只能执行最后的pttl myLock，返回值代表了myLock这个锁key的剩余生存时间。比如还剩15000毫秒的生存时间。此时客户端2会进入一个while循环，不停的尝试加锁。 watch dog自动延期机制 客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？ 简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制 看一下代码，相同的客户进来，会进入第二个if，会执行hincrby，即增1，那么这个hash结构就会变成： 释放锁 如果执行lock.unlock()，就可以释放分布式锁，此时的业务逻辑也是非常简单的。其实说白了，就是每次都对myLock数据结构中的那个加锁次数减1。如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用：del myLock命令，从redis里删除这个key。然后呢，另外的客户端2就可以尝试完成加锁了。 这就是所谓的分布式锁的开源Redisson框架的实现机制。 存在的问题 其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。 但是复制的这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。 假设客户端1在redis master上获得锁，然后主机宕机，redis slave成为新的redis master，但是还未同步到redis slave上，但是客户端1已经觉得自己获取到了锁。 此时，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，此时就会发生多个客户端完成对一个key的加锁。这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring事务的传播行为]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F01%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BC%A0%E6%92%AD%E8%A1%8C%E4%B8%BA%2F</url>
    <content type="text"><![CDATA[经常听到别人说事务传播行为，那到底什么是事务的传播行为呢？ 1.什么是事务？ 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。 这里注意，其实事务就是数据库才能保证的，所以抛开数据库层面来谈事务本身就是不存在的，所以事务的概念就是数据库一系列操作的一个完整单元。 2.什么是ACID？ ACID是指数据库管理系统在写入或更新资料的过程中，为保证事务是正确可靠的，所必须具备的四个特性：原子性、一致性、隔离性、持久性。 Atomicity：一个事务中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 Isolation：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔 离分为不同级别，包括读未提交(Read uncommitted)、读提交(read committed)、可重复读(repeatable read)和串行化(Serializable)。 Durability：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 3.spring事务传播行为 在我们用SSM开发项目的时候，我们一般都是将事务设置在Service层 那么当我们调用Service层的一个方法的时候它能够保证我们的这个方法中执行的所有的对数据库的更新操作保持在一个事务中，在事务层里面调用的这些方法要么全部成功，要么全部失败。那么事务的传播特性也是从这里说起的。 如果你在你的`Service`层的这个方法中，除了调用了`Dao`层的方法之外，还调用了本类的其他的`Service`方法，那么在调用其他的`Service`方法的时候，这个事务是怎么规定的呢，我必须保证我在我方法里调用的这个方法与我本身的方法处在同一个事务中，否则如果保证事物的一致性。事务的传播特性就是解决这个问题的. 在Spring中有针对传播特性的多种配置我们大多数情况下只用其中的一种:PROPGATION_REQUIRED：这个配置项的意思是说当我调用service层的方法的时候开启一个事务(具体调用那一层的方法开始创建事务，要看你的aop的配置),那么在调用这个service层里面的其他的方法的时候,如果当前方法产生了事务就用当前方法产生的事务，否则就创建一个新的事务。这个工作使由Spring来帮助我们完成的。 默认情况下当发生`RuntimeException`的情况下，事务才会回滚，所以要注意一下：如果你在程序发生错误的情况下，有自己的异常处理机制定义自己的`Exception`，必须从`RuntimeException`类继承，这样事务才会回滚！ 4.事务隔离级别 1、Serializable：最严格的级别，事务串行执行，资源消耗最大； 2、REPEATABLE READ：保证了一个事务不会修改已经由另一个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 3、READ COMMITTED:大多数主流数据库的默认事务等级，保证了一个事务不会读到另一个并行事务已修改但未提交的数据，避免了“脏读取”。该级别适用于大多数系统。 4、Read Uncommitted：保证了读取过程中不会读取到非法数据。 5.总结 本文的重点是在于理解事务的传播行为这个概念，从事务的概念，到事务的ACID介绍，引出事务传播传播行为和隔离级别这两个概念加以理解。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap]]></title>
    <url>%2F2019%2F01%2F22%2Fjava-collection%2F12.ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[对于并发场景下，推荐使用线程安全的 concurrentHashMap ，而不是 HashMap 或者是 HashTable .concurrentHashMap在JDK7和JDK8中的实现原理是不一样的。本文分别对其核心思想和方法进行阐述。 一、JDK7实现 ConcurrentHashMap 的内部细分了若干个小的 HashMap ，称之为段（ SEGMENT ）。 ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment ，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel ( Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 1.1 get方法 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值. 内部 HashEntry 类 ： 12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 1.2 put方法 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。 首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。 而 ConcurrentHashMap 不一样，它是在将数据插入之前检查是否需要扩容，之后再做插入操作。 1.3 size方法 每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。 但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。 在 JDK7 中，第一种方案他会使用不加锁的模式去尝试多次计算 ConcurrentHashMap 的 size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的。 第二种方案是如果第一种方案不符合，他就会给每个 Segment 加上锁，然后计算 ConcurrentHashMap 的 size 返回。其源码实现: 12345678910111213141516171819202122232425262728293031323334353637public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 其中 12// 锁之前重试次数static final int RETRIES_BEFORE_LOCK = 2; 二、JDK8实现 jdk8 中的 ConcurrentHashMap 数据结构和实现与 jdk7 还是有着明显的差异。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 也将 jdk7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。 其中的 val next 都用了 volatile 修饰，保证了可见性。 2.1 put方法 重点来看看 put 函数： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足(不需要初始化、Node不为空、不需要扩容)，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 2.2 get方法 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 都不满足那就按照链表的方式遍历获取值。 2.3 size方法 JDK8 实现相比 JDK7 简单很多，只有一种方案，我们直接看 size() 代码： 123456789101112131415161718192021public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; //获取baseCount值 long sum = baseCount; //遍历CounterCell数组全部加到baseCount上，它们的和就是size if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; 可能你会有所疑问，ConcurrentHashMap 中的 baseCount 属性不就是记录的所有键值对的总数吗？直接返回它不就行了吗？ 之所以没有这么做，是因为我们的 addCount 方法用于 CAS 更新 baseCount，但很有可能在高并发的情况下，更新失败，那么这些节点虽然已经被添加到哈希表中了，但是数量却没有被统计。 还好，addCount 方法在更新 baseCount 失败的时候，会调用 fullAddCount 将这些失败的结点包装成一个 CounterCell 对象，保存在 CounterCell 数组中。那么整张表实际的 size 其实是 baseCount 加上 CounterCell数组中元素的个数。 三、总结 并发情况下请使用concurrentHashMap 在jdk7中，用的是分段锁，默认是12段，那么并发量最多也就12. get不加锁，第一次hash定位到segment，第二次hash定位到元素，元素值是用volatile保证内存可见性 put需要加锁，hash定位到segment后，先检查是否需要扩容再插入。 size先使用不加锁的模式去尝试多次计算size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入；如果不一致，给每个 Segment 加上锁再依次去计算个数 在jdk8中，采用了 CAS + synchronized 来保证并发安全性 put的过程比较复杂，简单来说是：先计算hash定位到node—》判断是否初始化—》如果node为空则表示可以插入，用cas插入—》判断是否需要扩容—》如果不需要初始化、Node不为空、不需要扩容，则利用 synchronized 锁写入数据—》判断是否需要转换为红黑树 get就比较简单，直接根据hash定位到node，然后以链表或者红黑树的方式拿到 size方法就一种方案：baseCount+CounterCell[]中所有元素 整理自： https://crossoverjie.top/JCSprout/#/thread/ConcurrentHashMap?id=size-方法 https://www.jianshu.com/p/e99e3fcface4]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap死循环问题]]></title>
    <url>%2F2019%2F01%2F21%2Fjava-collection%2F11.HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JDK1.7或者更老的版本中在多线程情况下是会存在死循环问题，究其原因是put过程中的resize方法在调用transfer方法的时候导致的死锁。这次我们来看看到底是哪里出了问题！ 核心源码 在JDK8中，内部已经调整，解决了死循环问题，是如何解决的呢？将JDK7中头插入法改为末端插入。就是这么简单。关于这个，可以查看jdk8源码中的resize方法。 上面提到是由于put时出现问题，那么先来到put()中看看： 我们看到，put一个不存在的新元素，必然增加一个节点，我们进入这个增加节点的方法： 检查是否需要扩容，需要的话就resize: 下面就是对链表数据进行迁移： 核心的代码就是这么多，首先要强调一下：两个线程进来，是分别建立了两个独立的扩容后的数组，比如这里是两个长度为4的数组。老的数组为2个数就是唯一的。所以在第一步，线程2运行结束时，老的数组元素已经空了。 下面先演示一下正常的rehash过程。 正常情况 假设了我们的hash算法就是简单的用 key mod 一下数组(hash表)的大小 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。 接下来的三个步骤是Hash表 resize 成4，然后所有的 &lt;key,value&gt; 重新 rehash 的过程 注意到，在JDK7中，是按照头插入法依次插入的。所以7插到了3前面。 并发情况 1.初始情况 假设我们有两个线程。我用红色和浅蓝色标注了一下。 对于第一个线程先执行完这一行，然后挂起，此时 e 和 next 都附好值了： 而让线程二执行完成。于是我们有下面的这个样子： 因为Thread1的 e 指向了 key(3) ，而 next 指向了 key(7) ，其在 Thread2 rehash后，指向了 Thread2 重组后的链表。 2.Thread1被调度回来执行 先是执行 newTalbe[i] = e ：此时线程1的第三个位置就是指向元素3; 然后是 e = next，导致了 e 指向了 key(7) ; 而下一次循环的 next = e.next 导致了 next 指向了 key(3) ; 3.一切安好 线程一接着工作。把 key(7) 摘下来，放到 newTable[i] 的第一个，然后把 e 和 next 往下移。 4.环形链接出现 e.next = newTable[i] 导致 key(3).next 指向了 key(7) 注意：此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 自己的简单整理 这里还是比较绕的，理解的最好方式左边放源码，右边放图，中间用草稿纸画一画。 那么，这里我在对其过程尽可能地讲明白一点。我们先确定7和3会全部落到扩容后的下标为3的位置(3%4=3,7%4=3)。 规定线程1开辟的数组为 arr1 ，线程2开辟的数组为 arr2; 1. 初始状态 线程一： e -&gt; key3 , next -&gt; key7 线程二： 数组3号位置 arr2[3] -&gt; key7 -&gt; key3 注意此时 key7 指向 key3 . 我们要明确一下，发生死循环，是指在put操作完毕之后，最终生成的数组中有死循环引用才行，千万不要一开始看线程一种key3指向key7，然后线程二种key7指向key3就是死循环了。。。 2. 线程一继续执行 i = 3 e.next = key7,此时 e=key3 ,所以是 key3.next = key7（这是线程1的初始状态决定的） arr1[3] 指向 key3 e 为 key7 3.由于e不为空，所以还会循环： 上一步 e 为 key7，所以 next = key7.next ，到线程2中一看是 key3 ，所以 next = key3（线程2中key7.next就是key3） i = 3 e.next = key3------注意，这里就是Key7指向了key3,key7的next引用下面没有变过，所以这里做一下记录，即key7指向key3 newTable[3] = key7 e = key3 4.由于e不为空，所以还会循环： 上一步 e=key3 , next=null i=3 key3.next = key7，注意,由于key7已经指向了key3，此时key3又指向key7,发生死循环 newTable[3] = key3 e = null 5.e为null，跳出循环。 此时发现key3又指向了key7。发生死循环。 整理自:https://coolshell.cn/articles/9606.html/comment-page-2#comments]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux重要的一些命令]]></title>
    <url>%2F2019%2F01%2F21%2Flinux%E9%87%8D%E8%A6%81%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[很多命令之所以重要，是因为用它们会大幅提高工作效率。所以，熟悉它们是我们必须要做的一件事情。下面着重提取了find、grep、管道、wc、awk以及sed等几个命令来看看如何使用。 一、Linux体系结构 对这幅图进行详细说明一下。 体系结构主要分为用户态(用户上层活动)和内核态 内核：本质是一段管理计算机硬件设备的程序，这个程序直接管理硬件：包括CPU、内存空间、硬盘接口、网络接口等。所有的计算机操作都要通过内核来操作。 系统调用：内核的访问接口，是一种不能再简化的操作(可以认为系统调用已经是最小的原子操作，上层完成一个功能要依托于若干系统调用才能完成) 由于系统调用比较基础，要完成一个功能需要很多系统调用组合才能实现，对于程序员来说比较复杂。这个时候怎么办呢？我们可以调用公共函数库：系统调用的组合拳。简化程序员操作。 Shell也是一种特殊的应用程序，是一个命令解释器，可以编程。 Shell下通系统调用，上通各种应用，是上层和下层之间粘合的胶水，让不同程序可以偕同工作。 在没有图形界面之前，用户通过shell命令行或者可编程的shell脚本可以完成很多事情。 二、如何根据文件名检索文件 find 在指定目录下查找文件 语法 find path [options] params 2.1 精确查找文件 比如我在当前目录(可能在子目录下)下找一个文件叫做test.java： 1find -name "test.java" 这个指令就可以在本目录以及子目录下递归查找这个文件了。 实例：精确查询名字叫snailmall-api-gateway-8080.jar这个文件： 2.2 全局搜索 如果是全局查找，也很简单，无非是从根目录开始递归查找。 1find / -name "test.java" 实例：我对这台服务器全局查找文件名以snailmall开头的所有文件： 2.3 模糊查询 如果找以test打头的文件： 1find -name "test*" 即用 * 通配符就可以模糊查询到以 test 打头的文件。 实例，我的这台服务器上部署了几个关于商城的服务，这个目录下我放了jar包和相应的启动信息文件。我对其进行模糊查询： 2.4 忽略大小写 1find -iname "test*" 三、如何根据文件内的内容进行检索 3.1 grep命令 grep 查找文件里符合条件的字符串 语法：grep [options] pattern file 比如 test.java 中有一句话是： 1System.out.println("i love java"); 那么如何查找 test.java 中的 java 呢？ 1grep "java" test* 这句话意思就是查找以 test 打头的文件中的包含 java 字符串所在的行。 直接输入： 1grep "hello" 会等待用户输入文本。然后再对输入的内容进行检索。 3.2 管道操作符 | 可将指令连接起来，前一个指令的输出作为后一个指令的输入。 1find / | grep "test" 作用同： 1find / -name "test" 注意： 只有前一个指令正确才会再处理。 管道右边命令必须能接收标准输入流，否则传递过程中数据会被抛弃 3.3 grep结合管道 1grep 'xxx' hello.info 可以将 xxx 所在的行全部筛选出来，但是还是特别多，我比如关心这每一行中某个字段的信息，比如是 param[xx12]这种信息。如何实现筛选呢？ 1grep 'xxx' hello.info | grep -o 'param\[[0-9a-z]*\]' 这样就只把类似于 param[xx12] 这样的信息清晰地展现出来。 如何过滤掉不要的信息呢？可以用： 1grep -v 比如我们查询 tomcat 进程信息： 1ps -ef | grep tomcat 我们会发现，不仅 tomcat 的信息展现出来了，执行 grep 命令本身的进程信息也展示出来了。我们要将这个 grep 命令过滤掉，只展现 tomcat 进程信息，可以： 1ps -ef | grep tomcat | grep -v "grep" 这样就把 grep 进程信息过滤掉了。 四、如何对文件内容做统计 awk 一次读取一行文本，按输入分隔符进行切片，切成多个组成部分 将切片直接保存再内建的变量中，$1$2…($0表示行的全部) 支持对单个切片的判断，支持循环判断，默认分隔符为空格 语法：awk [options] ‘cmd’ file 有这样一个文件text1.txt： 123456789proto Recv-Q Send-Q Local Address Foreign Address statetcp 0 48 111.34.134.2:ssh 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.13.2:s0 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 1 48 localhots:webcac 124.213.2.12:12565 ESTABLISHEDtcp 1 48 111.34.134.2:s1 124.213.2.12:12565 ESTABLISHEDudp 1 48 111.34.134.2:s2 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.134.2:s3 124.213.2.12:12565 ESTABLISHED 列出切分出来的第一列和第二列： 1awk '&#123;print $1,$2&#125;' test1.txt 结果： 筛选出第一列为tcp和第二列为1的所在行，将这些行数据全部打印出来： 1awk '$1="tcp" &amp;&amp; $2==1&#123;print $0&#125;' test1.txt 结果： 打印带有表头的数据： 1awk '($1="tcp" &amp;&amp; $2==1) || NR==1 &#123;print $0&#125;' test1.txt 默认是以空格分隔，那么以逗号或者其他符号可以吗？答案当然是可以。对于这样的文件text2.txt： 12345adas,123wqe,54412321,dddfsdaasd,1235465547,fjigj 1awk -F "," '&#123;print $2&#125;' text2.txt 五、WC统计 有一个文件test2.txt，里面的内容是： 123swg123eh shwfshsfswg7 121 32n dswg17328 123swg1 2h1jhwjqbsjwqbsh ddddh wg ehdedhd dhsjh 六、sed命令 sed是一个很好的文件处理工具，本身是一个管道命令，主要是以行为单位进行处理，可以将数据行进行替换、删除、新增、选取等特定工作 sed [-n/e/f/r/i] ‘command’ 输入文本 常用选项： -n∶使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN的资料一般都会被列出到萤幕上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e∶直接在指令列模式上进行 sed 的动作编辑； -f∶直接将 sed 的动作写在一个档案内， -f filename 则可以执行 filename 内的sed 动作； -r∶sed 的动作支援的是延伸型正规表示法的语法。(预设是基础正规表示法语法) -i∶直接修改读取的档案内容，而不是由萤幕输出。 常用命令： a ∶新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ∶取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ∶删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ∶插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ∶列印，亦即将某个选择的资料印出。通常 p 会与参数 sed -n 一起运作～ s ∶取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 这里我就主要看一下批量替换这个功能。 如果只是给一个文件中的若干字符串批量替换，只需要： 1sed -i "s/oldstring/newstring/g" filename 如果是对某一路径下很多的文件批量替换： 1sed -i &quot;s/oldstring/newstring/g&quot; `grep oldstring -rl path` 其中，oldstring是待被替换的字符串，newstring是待替换oldstring的新字符串，grep操作主要是按照所给的路径查找oldstring，path是所替换文件的路径； -i选项是直接在文件中替换，不在终端输出； -r选项是所给的path中的目录递归查找； -l选项是输出所有匹配到oldstring的文件； 这里只是模拟一下，将目录下的所有文件进行批量修改：]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[站点文章汇总]]></title>
    <url>%2F2019%2F01%2F21%2F%E7%AB%99%E7%82%B9%E6%96%87%E7%AB%A0%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[☝️☝️☝️🔝🔝🔝本文为置顶文章，为了方便管理和查阅，在这里详细展示目录索引，看完索引，你就知道本站的大体内容啦！我相信一定会给小伙伴们一些收获！ 计算机网络🐭🐭🐭 这一部分主要是关于HTTP和TCP的必备知识。 《计算机网络相关系列》： 《从下到上看五层模型》 《从上到下看五层模型》 《HTTP的前世今生》 《TCP协议入门》 《TCP三次握手和四次挥手》 《HTTP基础知识提炼》 《一步一步理解HTTPS》 《一些常见的面试题》 JAVA容器🐱🐱🐱 这一部分是JAVA容器一系列文章，主要讲了常用JAVA容器的源码和一些特性，面试必问点。 《JAVA容器》： 《ArrayList/Vector》 《LinkedList》 《CopyOnWriteArrayList》 《HashCode/Equals》 《HashMap》 《HashSet》 《LinkedHashMap》 《HashMap和LinkedHashMap遍历机制》 《HashTable》 《LinkedHashSet》 《JDK7中HashMap死循环原因剖析》 《ConcurrentHashMap》 Linux&amp;操作系统🐶🐶🐶 一些必备的Liunx相关的知识点整理。 《Linux》： 《Linux面试重要命令》 《操作系统相关》： 《面试-进程与线程》 JAVA虚拟机相关🐹🐹🐹 主要是介绍JVM相关知识。轻松以应付面试。 JAVA核心基础知识🐺🐺🐺 主要是介绍比较核心的JAVA基础知识，属于JAVA基础进阶。 《Integer拆箱和装箱》： 《String为什么不可变》： 《java字符串核心一网打尽》： 《JAVA基础核心-理解类、对象、面向对象编程、面向接口编程》： JAVA多线程🐸🐸🐸 多线程这一块比较棘手，且学且保重。 Redis🐯🐯🐯 系统学习redis的笔记整理。 MySQL数据库🐨🐨🐨 作为必备技能，用法和原理都要会。 算法🐻🐻🐻 算法这一块也是面试痛点和难点，头发越来越少了呢！ Spring🐷🐷🐷 大厂必问啊啊啊啊，源码终究还是要读的~ Spring Cloud相关🐮🐮🐮 这一块就比较偏实践了。分布式。。。路漫漫。。。 Zookeeper🐗🐗🐗 作为当今分布式协调中心，核心的Paxos算法你不想了解一下吗？ 杂记🐵🐵🐵 在这个板块，不划分类别，文章尽可能地简短，也可谓之记忆碎片。 《技术短文杂记》： 《spring事务的传播行为》 《Redisson实现Redis分布式锁原理》 《redis实现分布式锁》 《springMVC全局异常+spring包扫描包隔离+spring事务传播》 《分布式ID生成策略》 《Spring Session》 《Curator》 《ELK平台搭建》 《库存扣减问题》 《分布式事务解决方案思考》 实战作品🐰🐰🐰 记录一些实战作品，代码主要存放在github上。 《我的实战》： 《快乐蜗牛商城代码》 《码码购分布式电商实战代码》]]></content>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F10.LinkedHashSet%2F</url>
    <content type="text"><![CDATA[HashSet 和 LinkedHashSet 的关系类似于 HashMap 和 LinkedHashMap 的关系，即后者维护双向链表，实现迭代顺序可为插入顺序或是访问顺序。所以也就轻松加愉快快速了解一下即可。 从源码中可以看到其空的构造函数为： 123public LinkedHashSet() &#123; super(16, .75f, true);&#125; 这个super即父类是HashSet，从它的继承关系就可以显然看到： 123public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable 那么HashSet内部的数据结构就是一个 HashMap，其方法的内部几乎就是在调用 HashMap 的方法。 LinkedHashSet 首先我们需要知道的是它是一个 Set 的实现，所以它其中存的肯定不是键值对，而是值。此实现与 HashSet 的不同之处在于，LinkedHashSet 维护着一个运行于所有条目的双向循环链表。 这一切都与LinkedHashMap类似。 LinkedHashSet 内部有个属性 accessOrder 控制着遍历次序。默认情况下该值为 false ,即按插入排序访问。如果将该值设置为 true 的话，则按访问次序排序(即最近最少使用算法，最近最少使用的放在链表头部，最近访问的则在链表尾部)。 一、 示例 HashSet的遍历： 123456789101112public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new HashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125;&#125; 输出结果是： aaa ccc bbb eee LinkedHashSet的遍历： 1234567891011121314public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new LinkedHashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); linkedHashSet.add(null); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; 输出结果是： aaa eee ccc bbb null 可以看到与输入顺序是一致的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashtable]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F9.HashTable%2F</url>
    <content type="text"><![CDATA[Hashtable 是个过时的集合类，不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换。但这并不是我们不去了解它的理由。最起码 Hashtable 和 HashMap 的面试题在面试中经常被问到。 一、前言 Hashtable和HashMap，从存储结构和实现来讲基本上都是相同的。 它和HashMap的最大的不同是它是线程安全的，另外它不允许key和value为null。 为了能在哈希表中成功地保存和取出对象，用作key的对象必须实现hashCode方法和equals方法。 二、fail-fast机制 iterator方法返回的迭代器是fail-fast的。如果在迭代器被创建后hashtable被结构型地修改了，除了迭代器自己的remove方法，迭代器会抛出一个ConcurrentModificationException异常。 因此，面对在并发的修改，迭代器干脆利落的失败，而不是冒险的继续。 关于这个的理解，其实在上一章讲LinkedHashMap中的第八点提到： 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 简单说，就是两个线程同时分别进行修改和遍历时，会抛出这个异常。 面试题：集合在遍历过程中是否可以删除元素，为什么迭代器就可以安全删除元素？ 集合在使用 for 循环迭代的过程中不允许使用，集合本身的 remove 方法删除元素，如果进行错误操作将会导致 ConcurrentModificationException 异常的发生 Iterator 可以删除访问的当前元素(current)，一旦删除的元素是Iterator 对象中 next 所正在引用的，在 Iterator 删除元素通过 修改 modCount 与 expectedModCount 的值，可以使下次在调用 remove 的方法时候两者仍然相同因此不会有异常产生。 迭代器的fail-fast机制并不能得到保证，它不能够保证一定出现该错误。一般来说，fail-fast会尽最大努力抛出ConcurrentModificationException异常。因此，为提高此类操作的正确性而编写一个依赖于此异常的程序是错误的做法，正确做法是：ConcurrentModificationException 应该仅用于检测 bug。 Hashtable是线程安全的。如果不需要线程安全的实现是不需要的，推荐使用HashMap代替Hashtable。如果需要线程安全的实现，推荐使用java.util.concurrent.ConcurrentHashMap代替Hashtable。 二、继承关系 123public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable&#123;&#125; extends Dictionary&lt;K,V&gt;：Dictionary类是一个抽象类，用来存储键/值对，作用和Map类相似。 implements Map&lt;K,V&gt;：实现了Map，实现了Map中声明的操作和default方法。 hashMap以及TreeMap的源码，都没有继承于这个类。不过当我看到注释中的解释也就明白了，其 Dictionary 源码注释是这样的：NOTE: This class is obsolete. New implementations should implement the Map interface, rather than extending this class. 该话指出 Dictionary 这个类过时了，新的实现类应该实现Map接口。 三、属性 1234567891011121314//哈希表private transient Entry&lt;?,?&gt;[] table;//记录哈希表中键值对的个数private transient int count;//扩容的阈值private int threshold;//负载因子private float loadFactor;//hashtable被结构型修改的次数。private transient int modCount = 0; HashTable并没有像HashMap那样定义了很多的常量，而是直接写死在了方法里。 Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。 四、构造函数 12345678/** * 使用默认初始化容量（11）和默认负载因子（0.75）来构造一个空的hashtable. * * 这里可以看到，Hashtable默认初始化容量为16，而HashMap的默认初始化容量为11。 */public Hashtable() &#123; this(11, 0.75f);&#125; 我们可以获取到这些信息：HashTable默认的初始化容量为11（与HashMap不同），负载因子默认为0.75（与HashMap相同）。而正因为默认初始化容量的不同，同时也没有对容量做调整的策略，所以可以先推断出，HashTable使用的哈希函数跟HashMap是不一样的（事实也确实如此）。 五、重要方法 5.1 get方法 12345678910111213public synchronized V get(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //通过哈希函数，计算出key对应的桶的位置 int index = (hash &amp; 0x7FFFFFFF) % tab.length; //遍历该桶的所有元素，寻找该key for (Entry&lt;?,?&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return (V)e.value; &#125; &#125; return null;&#125; 这里可以看到，Hashtable和HashMap确认key在数组中的索引的方法不同。 Hashtable通过index = (hash &amp; 0x7FFFFFFF) % tab.length;来确认 HashMap通过i = (n - 1) &amp; hash;来确认 跟HashMap相比，HashTable的get方法非常简单。我们首先可以看见get方法使用了synchronized来修饰，所以它能保证线程安全。并且它是通过链表的方式来处理冲突的。另外，我们还可以看见HashTable并没有像HashMap那样封装一个哈希函数，而是直接把哈希函数写在了方法中。而哈希函数也是比较简单的，它仅对哈希表的长度进行了取模。 5.2 put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public synchronized V put(K key, V value) &#123; // 确认value不为null if (value == null) &#123; throw new NullPointerException(); &#125; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //找到key在table中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") //获取key所在索引的entry Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，判断key是否已经存在 for(; entry != null ; entry = entry.next) &#123; //如果key已经存在 if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; //保存旧的value V old = entry.value; //替换value entry.value = value; //返回旧的value return old; &#125; &#125; //如果key在hashtable不是已经存在，就直接将键值对添加到table中，返回null addEntry(hash, key, value, index); return null;&#125;private void addEntry(int hash, K key, V value, int index) &#123; modCount++; Entry&lt;?,?&gt; tab[] = table; //哈希表的键值对个数达到了阈值，则进行扩容 if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; //把新节点插入桶中（头插法） tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++;&#125; 从代码中可以总结出Hashtable的put方法的总体思路： 确认value不为null。如果为null，则抛出异常 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key已经存在，替换value，返回旧的value 如果key在hashtable不是已经存在，就直接添加，否则直接将键值对添加到table中，返回null 在方法中可以看到，在遍历桶中元素时，是按照链表的方式遍历的。可以印证，HashMap的桶中可能为链表或者树。但Hashtable的桶中只可能是链表。 5.3 remove方法 12345678910111213141516171819202122232425public synchronized V remove(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //计算key在hashtable中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，如果entry中存在key为参数key的键值对，就删除键值对，并返回键值对的value for(Entry&lt;K,V&gt; prev = null ; e != null ; prev = e, e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; modCount++; if (prev != null) &#123; prev.next = e.next; &#125; else &#123; tab[index] = e.next; &#125; count--; V oldValue = e.value; e.value = null; return oldValue; &#125; &#125; //如果不存在key为参数key的键值对，返回value return null;&#125; 从代码中可以总结出Hashtable的remove方法的总体思路： 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key存在，删除key映射的键值对，返回旧的value 如果key在hashtable不存在，返回null 5.4 rehash方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 增加hashtable的容量，为了更有效地存放和找到它的entry。 * 当键值对的数量超过了临界值（capacity*load factor）这个方法自动调用 * 长度变为原来的2倍+1 * */@SuppressWarnings("unchecked")protected void rehash() &#123; //记录旧容量 int oldCapacity = table.length; //记录旧桶的数组 Entry&lt;?,?&gt;[] oldMap = table; // overflow-conscious code //新的容量为旧的容量的2倍+1 int newCapacity = (oldCapacity &lt;&lt; 1) + 1; //如果新的容量大于容量的最大值MAX_ARRAY_SIZE if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; //如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; //如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE newCapacity = MAX_ARRAY_SIZE; &#125; //创建新的数组，容量为新容量 Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; //结构性修改次数+1 modCount++; //计算扩容的临界值 threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; //将旧的数组中的键值对转移到新数组中 for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125;&#125; 看完代码，我们可以总结出rehash的总体思路为： 新建变量新的容量，值为旧的容量的2倍+1 如果新的容量大于容量的最大值MAX_ARRAY_SIZE 如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE 创建新的数组，容量为新容量 将旧的数组中的键值对转移到新数组中 这里可以看到，一般情况下，HashMap扩容后容量变为原来的两倍，而Hashtable扩容后容量变为原来的两倍加一。 HashTable的rehash方法相当于HashMap的resize方法。跟HashMap那种巧妙的rehash方式相比，HashTable的rehash过程需要对每个键值对都重新计算哈希值，而比起异或和与操作，取模是一个非常耗时的操作，所以这也是导致效率较低的原因之一。 六、遍历 可以使用与HashMap一样的遍历方式，但是由于历史原因，多了Enumeration的方式。 针对Enumeration，这里与iterator进行对比一下。 相同点 Iterator和Enumeration都可以对某些容器进行遍历。 Iterator和Enumeration都是接口。 不同点 Iterator有对容器进行修改的方法。而Enumeration只能遍历。 Iterator支持fail-fast，而Enumeration不支持。 Iterator比Enumeration覆盖范围广，基本所有容器中都有Iterator迭代器，而只有Vector、Hashtable有Enumeration。 Enumeration在JDK 1.0就已经存在了，而Iterator是JDK2.0新加的接口。 七、Hashtable与HashMap对比 HashTable的应用非常广泛，HashMap是新框架中用来代替HashTable的类，也就是说建议使用HashMap。 下面着重比较一下二者的区别： 1.继承不同 Hashtable是基于陈旧的Dictionary类的，HashMap是java1.2引进的Map接口的一个实现。 2.同步 Hashtable 中的方法是同步的，保证了Hashtable中的对象是线程安全的。 HashMap中的方法在缺省情况下是非同步的,HashMap中的对象并不是线程安全的。在多线程并发的环境下，可以直接使用Hashtable，但是要使用HashMap的话就要自己增加同步处理了。 3.效率 单线程中, HashMap的效率大于Hashtable。因为同步的要求会影响执行的效率，所以如果你不需要线程安全的集合，HashMap是Hashtable的轻量级实现，这样可以避免由于同步带来的不必要的性能开销，从而提高效率。 4.null值 Hashtable中，key和value都不允许出现null值，否则出现NullPointerException。 在HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键，而应该用containsKey()方法来判断。 5.遍历方式 Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable可以使用Enumeration的方式。 6.容量 Hashtable和HashMap它们两个内部实现方式的数组的初始大小和扩容的方式。 HashTable中hash数组默认大小是11，增加的方式是 old*2+1。 HashMap中hash数组的默认大小是16，而且一定是2的指数。 八、总结 无论什么时候有多个线程访问相同实例的可能时，就应该使用Hashtable，反之使用HashMap。非线程安全的数据结构能带来更好的性能。 如果在将来有一种可能—你需要按顺序获得键值对的方案时，HashMap是一个很好的选择，因为有HashMap的一个子类 LinkedHashMap。 所以如果你想可预测的按顺序迭代（默认按插入的顺序），你可以很方便用LinkedHashMap替换HashMap。反观要是使用的Hashtable就没那么简单了。 如果有多个线程访问HashMap，Collections.synchronizedMap（）可以代替，总的来说HashMap更灵活，或者直接用并发容器ConcurrentHashMap。 整理自： http://blog.csdn.net/panweiwei1994/article/details/77428710 http://blog.csdn.net/u013124587/article/details/52655042]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap和LinkedHashMap遍历机制]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F8.HashMap%E5%92%8CLinkedHashMap%E9%81%8D%E5%8E%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本篇单独讲一下HashMap和LinkedHashMap遍历方式。 一、对HashMap和LinkedHashMap遍历的几种方法 这里以HashMap为例，LinkedHashMap一样的方式。 12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println("key=" + next.getKey() + " value=" + next.getValue());&#125; 123456Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println("key=" + key + " value=" + map.get(key));&#125; 123map.forEach((key,value)-&gt;&#123; System.out.println("key=" + key + " value=" + value);&#125;); 强烈建议使用第一种 EntrySet 进行遍历。 第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。 我们知道，HashMap的输出顺序与元素的输入顺序无关，LinkedHashMap可以按照输入顺序输出，也可以根据读取元素的顺序输出。这一现象，已经在上一篇中展示出来了。 二、HashMap的遍历机制 HashMap 提供了两个遍历访问其内部元素Entry&lt;k,v&gt;的接口： Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()-------&gt;返回此映射所包含的映射关系的 Set 视图。 Set&lt;K&gt; keySet()--------&gt;返回此映射中所包含的键的 Set 视图。 实际上，第二个接口表示的Key的顺序，和第一个接口返回的Entry顺序是对应的，也就是说：这两种接口对HashMap的元素遍历的顺序相相同的。 那么，HashMap遍历内部Entry&lt;K,V&gt; 的顺序是什么呢？ 搞清楚这个问题，先要知道其内部结构是怎样的。 HashMap在存储Entry对象的时候，是根据Key的hash值判定存储到Entry[] table数组的哪一个索引值表示的链表上。 对HashMap遍历Entry对象的顺序和Entry对象的存储顺序之间没有任何关系。 HashMap散列图、Hashtable散列表是按“有利于随机查找的散列(hash)的顺序”。并非按输入顺序。遍历时只能全部输出，而没有顺序。甚至可以rehash()重新散列，来获得更利于随机存取的内部顺序。 所以对HashMap的遍历，由内部的机制决定的，这个机制是只考虑利于快速存取，不考虑输入等顺序。 三、LinkedHashMap 的遍历机制 LinkedHashMap 是HashMap的子类，它可以实现对容器内Entry的存储顺序和对Entry的遍历顺序保持一致。 为了实现这个功能，LinkedHashMap内部使用了一个Entry类型的双向链表，用这个双向链表记录Entry的存储顺序。当需要对该Map进行遍历的时候，实际上是遍历的是这个双向链表。 LinkedHashMap内部使用的LinkedHashMap.Entry类继承自Map.Entry类，在其基础上增加了LinkedHashMap.Entry类型的两个字段，用来引用该Entry在双向链表中的前面的Entry对象和后面的Entry对象。 它的内部会在Map.Entry类的基础上，增加两个Entry类型的引用：before，after。LinkedHashMap使用一个双向连表，将其内部所有的Entry串起来。 1234LinkedHashMap linkedHashMap = new LinkedHashMap(); linkedHashMap.put("name","louis"); linkedHashMap.put("age","24"); linkedHashMap.put("sex","male"); 对LinkedHashMap进行遍历的策略： 从 header.after 指向的Entry对象开始，然后一直沿着此链表遍历下去，直到某个entry.after == header 为止，完成遍历。 根据Entry&lt;K,V&gt;插入LinkedHashMap的顺序进行遍历的方式叫做：按插入顺序遍历。 另外，LinkedHashMap还支持一种遍历顺序，叫做：Get读取顺序。 如果LinkedHashMap的这个Get读取遍历顺序开启，那么，当我们在LinkedHashMap上调用get(key) 方法时，会导致内部key对应的Entry在双向链表中的位置移动到双向链表的最后。 四、遍历机制的总结 HashMap对元素的遍历顺序跟Entry插入的顺序无关，而LinkedHashMap对元素的遍历顺序可以跟Entry&lt;K,V&gt;插入的顺序保持一致：从双向。 当LinkedHashMap处于Get获取顺序遍历模式下，当执行get() 操作时，会将对应的Entry&lt;k,v&gt;移到遍历的最后位置。 LinkedHashMap处于按插入顺序遍历的模式下，如果新插入的&lt;key,value&gt; 对应的key已经存在，对应的Entry在遍历顺序中的位置并不会改变。 除了遍历顺序外，其他特性HashMap和LinkedHashMap基本相同。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F7.LinkedHashMap%2F</url>
    <content type="text"><![CDATA[大多数情况下，只要不涉及线程安全问题， Map 基本都可以使用 HashMap ，不过 HashMap 有一个问题，就是迭代 HashMap 的顺序并不是 HashMap 放置的顺序，也就是无序。 HashMap 的这一缺点往往会带来困扰，因为有些场景，我们期待一个有序的 Map。 篇幅有点长，但是在理解了HashMap之后就比较简单了。 这个时候，LinkedHashMap就闪亮登场了，它虽然增加了时间和空间上的开销，但是可以解决有排序需求的场景。 它的底层是继承于 HashMap 实现的，由一个双向循环链表所构成。 LinkedHashMap 的排序方式有两种： 根据写入顺序排序。 根据访问顺序排序。 其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。 一、LinkedHashMap数据结构 LinkedHashMap是通过哈希表和双向循环链表实现的，它通过维护一个双向循环链表来保证对哈希表迭代时的有序性，而这个有序是指键值对插入的顺序。 我们可以看出，遍历所有元素只需要从header开始遍历即可，一直遍历到下一个元素是header结束。 另外，当向哈希表中重复插入某个键的时候，不会影响到原来的有序性。也就是说，假设你插入的键的顺序为1、2、3、4，后来再次插入2，迭代时的顺序还是1、2、3、4，而不会因为后来插入的2变成1、3、4、2。（但其实我们可以改变它的规则，使它变成1、3、4、2） LinkedHashMap的实现主要分两部分，一部分是哈希表，另外一部分是链表。哈希表部分继承了HashMap，拥有了HashMap那一套高效的操作，所以我们要看的就是LinkedHashMap中链表的部分，了解它是如何来维护有序性的。 二、demo示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public static void main(String[] args) &#123; /** * HashMap插入数据，遍历输出无序 */ System.out.println("----------HashMap插入数据--------"); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("apple", "a"); map.put("watermelon", "b"); map.put("banana", "c"); map.put("peach", "d"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap插入数据，遍历，默认以插入顺序为序 */ System.out.println("----------LinkedHashMap插入数据,按照插入顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap = new LinkedHashMap&lt;&gt;(); linkedHashMap.put("apple", "a"); linkedHashMap.put("watermelon", "b"); linkedHashMap.put("banana", "c"); linkedHashMap.put("peach", "d"); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = linkedHashMap.entrySet().iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; /** * LinkedHashMap插入数据，设置accessOrder=true实现使得其遍历顺序按照访问的顺序输出，这里先用get方法来演示 */ System.out.println("----------LinkedHashMap插入数据,accessOrder=true:按照访问顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap2 = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); linkedHashMap2.put("apple", "aa"); linkedHashMap2.put("watermelon", "bb"); linkedHashMap2.put("banana", "cc"); linkedHashMap2.put("peach", "dd"); linkedHashMap2.get("banana");//banana移动到了内部的链表末尾 linkedHashMap2.get("apple");//apple移动到了内部的链表末尾 Iterator iter2 = linkedHashMap2.entrySet().iterator(); while (iter2.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter2.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap的put方法在accessOrder=true的情况下 */ System.out.println("-----------"); linkedHashMap2.put("watermelon", "bb");//watermelon移动到了内部的链表末尾 linkedHashMap2.put("stawbarrey", "ee");//末尾插入新元素stawbarrey linkedHashMap2.put(null, null);//插入新的节点 null Iterator iter3 = linkedHashMap2.entrySet().iterator(); while (iter3.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter3.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; &#125; 输出结果是: 12345678910111213141516171819202122----------HashMap插入数据--------banana=capple=apeach=dwatermelon=b----------LinkedHashMap插入数据,按照插入顺序进行排序--------apple=awatermelon=bbanana=cpeach=d----------LinkedHashMap插入数据,按照访问顺序进行排序--------watermelon=bbpeach=ddbanana=cc//banana到了末尾apple=aa//apple到了末尾-----------peach=ddbanana=ccapple=aawatermelon=bb//watermelon到了链表末尾stawbarrey=ee//新插入的放在末尾null=null//新插入的放在末尾 三、属性 LinkedHashMap可以认为是HashMap+LinkedList，即它既使用HashMap操作数据结构，又使用LinkedList维护插入元素的先后顺序 3.1 继承关系 123public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; LinkedHashMap是HashMap的子类，自然LinkedHashMap也就继承了HashMap中所有非private的方法。所以它已经从 HashMap 那里继承了与哈希表相关的操作了，那么在LinkedHashMap中，它可以专注于链表实现的那部分，所以与链表实现相关的属性如下。 3.2 属性介绍 1234567891011121314151617//LinkedHashMap的链表节点继承了HashMap的节点，而且每个节点都包含了前指针和后指针，所以这里可以看出它是一个双向链表static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125;//头指针transient LinkedHashMap.Entry&lt;K,V&gt; head;//尾指针transient LinkedHashMap.Entry&lt;K,V&gt; tail;//默认为false。当为true时，表示链表中键值对的顺序与每个键的插入顺序一致，也就是说重复插入键，也会更新顺序//简单来说，为false时，就是上面所指的1、2、3、4的情况；为true时，就是1、3、4、2的情况final boolean accessOrder; 五、构造方法 1234public LinkedHashMap() &#123; super(); accessOrder = false;&#125; 其实就是调用的 HashMap 的构造方法: HashMap 实现： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap init();&#125; 可以看到里面有一个空的 init()，具体是由 LinkedHashMap 来实现的： 12345@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 其实也就是对 header 进行了初始化。 六、添加元素 LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法. newNode() 会在HashMap的putVal() 方法里被调用，putVal() 方法会在批量插入数据putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) 或者插入单个数据public V put(K key, V value)时被调用。 LinkedHashMap重写了newNode(),在每次构建新节点时，通过linkNodeLast(p);将新节点链接在内部双向链表的尾部。 12345678910111213141516171819//在构建新节点时，构建的是`LinkedHashMap.Entry` 不再是`Node`.Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125;//将新增的节点，连接在链表的尾部private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; //集合之前是空的 if (last == null) head = p; else &#123;//将新节点连接在链表的尾部 p.before = last; last.after = p; &#125;&#125; 以及HashMap专门预留给LinkedHashMap的afterNodeAccess() 、afterNodeInsertion() 、afterNodeRemoval() 方法。 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 如果你没有注意到注释的解释的话，你可能会很奇怪为什么会有三个空方法，而且有不少地方还调用过它们。其实这三个方法表示的是在访问、插入、删除某个节点之后，进行一些处理，它们在LinkedHashMap有各自的实现。LinkedHashMap正是通过重写这三个方法来保证链表的插入、删除的有序性。 123456789101112131415161718//回调函数，新节点插入之后回调,判断是否需要删除最老插入的节点。//如果实现LruCache会用到这个方法。void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; //LinkedHashMap 默认返回false 则不删除节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;//LinkedHashMap 默认返回false 则不删除节点。 //返回true 代表要删除最早的节点。//通常构建一个LruCache会在达到Cache的上限是返回trueprotected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; void afterNodeInsertion(boolean evict)以及boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) 是构建LruCache需要的回调，在这可以忽略它们。 七、删除元素 LinkedHashMap也没有重写remove() 方法，因为它的删除逻辑和HashMap并无区别。 但它重写了afterNodeRemoval() 这个回调方法。该方法会在Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) 方法中回调，removeNode() 会在所有涉及到删除节点的方法中被调用，上文分析过，是删除节点操作的真正执行者。 1234567891011121314151617//在删除节点e时，同步将e从双向链表上删除void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //待删除节点 p 的前置后置节点都置空 p.before = p.after = null; //如果前置节点是null，则现在的头结点应该是后置节点a if (b == null) head = a; else//否则将前置节点b的后置节点指向a b.after = a; //同理如果后置节点时null ，则尾节点应是b if (a == null) tail = b; else//否则更新后置节点a的前置节点为b a.before = b;&#125; 八、查询元素 LinkedHashMap重写了get()和getOrDefault() 方法： 12345678910111213141516public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125;public V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return defaultValue; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 对比HashMap中的实现,LinkedHashMap只是增加了在成员变量(构造函数时赋值)accessOrder为true的情况下，要去回调void afterNodeAccess(Node&lt;K,V&gt; e) 函数。 1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 在afterNodeAccess() 函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。 12345678910111213141516171819202122232425262728293031void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last;//原尾节点 //如果accessOrder 是true ，且原尾节点不等于e if (accessOrder &amp;&amp; (last = tail) != e) &#123; //节点e强转成双向链表节点p LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //p现在是尾节点， 后置节点一定是null p.after = null; //如果p的前置节点是null，则p以前是头结点，所以更新现在的头结点是p的后置节点a if (b == null) head = a; else//否则更新p的前直接点b的后置节点为 a b.after = a; //如果p的后置节点不是null，则更新后置节点a的前置节点为b if (a != null) a.before = b; else//如果原本p的后置节点是null，则p就是尾节点。 此时 更新last的引用为 p的前置节点b last = b; if (last == null) //原本尾节点是null 则，链表中就一个节点 head = p; else &#123;//否则 更新 当前节点p的前置节点为 原尾节点last， last的后置节点是p p.before = last; last.after = p; &#125; //尾节点的引用赋值成p tail = p; //修改modCount。 ++modCount; &#125;&#125; 图示(注意这个图，1和6也应该是连在一起的，因为是双向循环链表，所以视为一个小错误)： 说明：从图中可以看到，结点3链接到了尾结点后面。 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 九、判断元素是否存在 它重写了该方法，相比HashMap的实现，更为高效。 123456789public boolean containsValue(Object value) &#123; //遍历一遍链表，去比较有没有value相等的节点，并返回 for (LinkedHashMap.Entry&lt;K,V&gt; e = head; e != null; e = e.after) &#123; V v = e.value; if (v == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; return false;&#125; 对比HashMap，是用两个for循环遍历，相对低效。 12345678910111213public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false;&#125; 十、替换某个元素 1234567891011121314// 用dst替换srcprivate void transferLinks(LinkedHashMap.Entry&lt;K,V&gt; src, LinkedHashMap.Entry&lt;K,V&gt; dst) &#123; LinkedHashMap.Entry&lt;K,V&gt; b = dst.before = src.before; LinkedHashMap.Entry&lt;K,V&gt; a = dst.after = src.after; if (b == null) head = dst; else b.after = dst; if (a == null) tail = dst; else a.before = dst;&#125; 十二、总结 LinkedHashMap相对于HashMap的源码比，是很简单的。因为大树底下好乘凉。它继承了HashMap，仅重写了几个方法，以改变它迭代遍历时的顺序。这也是其与HashMap相比最大的不同。 在每次插入数据，或者访问、修改数据时，会增加节点、或调整链表的节点顺序。以决定迭代时输出的顺序。 accessOrder默认是false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。为true时，可以在这基础之上构建一个LruCache. LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法.在每次构建新节点时，将新节点链接在内部双向链表的尾部 accessOrder=true的模式下,在afterNodeAccess()函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。值得注意的是，afterNodeAccess()函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 nextNode() 就是迭代器里的next()方法 。该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。 而双链表节点的顺序在LinkedHashMap的增、删、改、查时都会更新。以满足按照插入顺序输出，还是访问顺序输出。 它与HashMap比，还有一个小小的优化，重写了containsValue()方法，直接遍历内部链表去比对value值是否相等。 整理自： http://blog.csdn.net/zxt0601/article/details/77429150 http://wiki.jikexueyuan.com/project/java-collection/linkedhashmap.html http://blog.csdn.net/u013124587/article/details/52659741 http://www.cnblogs.com/leesf456/p/5248868.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F6.HashSet%2F</url>
    <content type="text"><![CDATA[HashSet 是一个不允许存储重复元素的集合，它是基于 HashMap 实现的， HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成。所以只要理解了 HashMap，HashSet 就水到渠成了。 成员变量 首先了解下HashSet的成员变量: 1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 发现主要就两个变量: map ：用于存放最终数据的。 PRESENT ：是所有写入map的value值。 构造方法 1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 构造函数很简单，利用了HashMap初始化了map。 add 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 比较关键的就是这个 add() 方法。 可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会受到影响，这样就保证了 HashSet 中只能存放不重复的元素。 该方法如果添加的是在 HashSet 中不存在的，则返回 true；如果添加的元素已经存在，返回 false。其原因在于我们之前提到的关于 HashMap 的 put 方法。该方法在添加 key 不重复的键值对的时候，会返回 null。 总结 HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。 所以 HashMap 会出现的问题 HashSet 依然不能避免。 对于 HashSet 中保存的对象，请注意正确重写其 equals 和 hashCode 方法，以保证放入的对象的唯一性。这两个方法是比较重要的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F5.HashMap%2F</url>
    <content type="text"><![CDATA[HashMap基本是面试必问的点，因为这个数据结构用的太频繁了，jdk1.8中的优化也是比较巧妙。有必要去深入探讨一下。但是涉及的内容比较多，这里只先探讨jdk8中HashMap的实现，至于jdk7中HashMap的死循环问题、红黑树的原理等都不会在本篇文章扩展到。其他的文章将会再去探讨整理。 本篇文章较长，高能预警。 一、前言 之前的List，讲了ArrayList、LinkedList，最后讲到了CopyOnWriteArrayList，就前两者而言，反映的是两种思想： （1）ArrayList以数组形式实现，顺序插入、查找快，插入、删除较慢 （2）LinkedList以链表形式实现，顺序插入、查找较慢，插入、删除方便 那么是否有一种数据结构能够结合上面两种的优点呢？有，答案就是HashMap。 HashMap是一种非常常见、方便和有用的集合，是一种键值对（K-V）形式的存储结构，在有了HashCode的基础后，下面将还是用图示的方式解读HashMap的实现原理。 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 二、HashMap的结构 其中哈希表是一个数组，我们经常把数组中的每一个节点称为一个桶，哈希表中的每个节点都用来存储一个键值对。 在插入元素时，如果发生冲突（即多个键值对映射到同一个桶上）的话，就会通过链表的形式来解决冲突。 因为一个桶上可能存在多个键值对，所以在查找的时候，会先通过key的哈希值先定位到桶，再遍历桶上的所有键值对，找出key相等的键值对，从而来获取value。 如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数： 容量 负载因子 容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。 三、继承关系 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 说明： HashMap继承自AbstractMap，AbstractMap是Map接口的骨干实现，AbstractMap中实现了Map中最重要最常用和方法，这样HashMap继承AbstractMap就不需要实现Map的所有方法，让HashMap减少了大量的工作。 四、属性 123456789101112131415161718192021222324252627//默认的初始容量为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//最大的容量上限为2^30static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的负载因子为0.75static final float DEFAULT_LOAD_FACTOR = 0.75f;//变成树型结构的临界值为8static final int TREEIFY_THRESHOLD = 8;//恢复链式结构的临界值为6static final int UNTREEIFY_THRESHOLD = 6;/** * 哈希表的最小树形化容量 * 当哈希表中的容量大于这个值时，表中的桶才能进行树形化 * 否则桶内元素太多时会扩容，而不是树形化 * 为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD */static final int MIN_TREEIFY_CAPACITY = 64;//哈希表transient Node&lt;K,V&gt;[] table;//哈希表中键值对的个数transient int size;//哈希表被修改的次数transient int modCount;//它是通过capacity*load factor计算出来的，当size到达这个值时，就会进行扩容操作int threshold;//负载因子final float loadFactor; 4.1 几个属性的详细说明 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍（为什么是两倍下文会说明）。 默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子`Load factor`的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子`loadFactor`的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意size和table的长度length、容纳最大键值对数量threshold的区别。 而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，因为常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考 http://blog.csdn.net/liuqiyao_01/article/details/14475159 ，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。下文会说明。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，并且链表的长度超过64时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 这里着重提一下MIN_TREEIFY_CAPACITY字段，容易与TREEIFY_THRESHOLD打架，TREEIFY_THRESHOLD是指桶中元素达到8个，就将其本来的链表结构改为红黑树，提高查询的效率。MIN_TREEIFY_CAPACITY是指最小树化的哈希表元素个数，也就是说，小于这个值，就算你(数组)桶里的元素数量大于8了，还是要用链表存储，只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。注意扩容是数组的复制。 4.2 Node结构 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 例如程序执行下面代码： 1map.put("美团","小美"); 系统将调用&quot;美团&quot;这个key的hashCode()方法得到其hashCode值（该方法适用于每个Java对象）。 然后再通过Hash算法来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。 当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。 那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法(5.4节)和扩容机制(5.5节)。下文会讲到。 五、方法 5.1 get方法 123456789101112131415161718192021222324252627282930//get方法主要调用的是getNode方法，所以重点要看getNode方法的实现public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //如果哈希表不为空 &amp;&amp; key对应的桶上不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //是否直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //判断是否有后续节点 if ((e = first.next) != null) &#123; //如果当前的桶是采用红黑树处理冲突，则调用红黑树的get方法去获取节点 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //不是红黑树的话，那就是传统的链式结构了，通过循环的方法判断链中是否存在该key do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 实现步骤大致如下： 通过hash值获取该key映射到的桶。 桶上的key就是要查找的key，则直接命中。 桶上的key不是要查找的key，则查看后续节点： 如果后续节点是树节点，通过调用树的方法查找该key。 如果后续节点是链式节点，则通过循环遍历链查找该key。 5.2 put方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法的具体实现也是在putVal方法中，所以我们重点看下面的putVal方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果哈希表为空，则先创建一个哈希表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果当前桶没有碰撞冲突，则直接把键值对插入，完事 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果桶上节点的key与当前key重复，那你就是我要找的节点了 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果是采用红黑树的方式处理冲突，则通过红黑树的putTreeVal方法去插入这个键值对 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //否则就是传统的链式结构 else &#123; //采用循环遍历的方式，判断链中是否有重复的key for (int binCount = 0; ; ++binCount) &#123; //到了链尾还没找到重复的key，则说明HashMap没有包含该键 if ((e = p.next) == null) &#123; //创建一个新节点插入到尾部 p.next = newNode(hash, key, value, null); //如果链的长度大于TREEIFY_THRESHOLD这个临界值，则把链变为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //找到了重复的key if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //这里表示在上面的操作中找到了重复的键，所以这里把该键的值替换为新值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //判断是否需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 1234567891011121314151617181920final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; put方法比较复杂，实现步骤大致如下： 先通过hash值计算出key映射到哪个桶。 如果桶上没有碰撞冲突，则直接插入。 如果出现碰撞冲突了，则需要处理冲突： 如果该桶使用红黑树处理冲突，则调用红黑树的方法插入。 否则采用传统的链式方法插入。如果链的长度到达临界值，则把链转变为红黑树。 如果桶中存在重复的键，则为该键替换新值。 如果size大于阈值，则进行扩容。 5.3 remove方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//remove方法的具体实现在removeNode方法中，所以我们重点看下面的removeNode方法public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果当前key映射到的桶不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //如果桶上的节点就是要找的key，则直接命中 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //如果是以红黑树处理冲突，则构建一个树节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); //如果是以链式的方式处理冲突，则通过遍历链表来寻找节点 else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //比对找到的key的value跟要删除的是否匹配 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //通过调用红黑树的方法来删除节点 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //使用链表的操作来删除节点 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 5.4 hash方法(确定哈希桶数组索引位置) 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。 注意get方法和put方法源码中都需要先计算key映射到哪个桶上，然后才进行之后的操作，计算的主要代码如下： 1(n - 1) &amp; hash 上面代码中的n指的是哈希表的大小，hash指的是key的哈希值，hash是通过下面这个方法计算出来的，采用了二次哈希的方式，其中key的hashCode方法是一个native方法： 123456static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 总结就是：由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2^n 。这样用 2^n - 1 做位运算与取模效果一致，并且效率还要高出许多。这样回答了上文中：好的Hash算法到底是什么。 5.5 resize方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //计算扩容后的大小 if (oldCap &gt; 0) &#123; //如果当前容量超过最大容量，则无法进行扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //没超过最大值则扩为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //新的resize阈值 threshold = newThr; //创建新的哈希表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; //遍历旧哈希表的每个桶，重新计算桶里元素的新位置 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //如果桶上只有一个键值对，则直接插入 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果是通过红黑树来处理冲突的，则调用相关方法把树分离开 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果采用链式处理冲突 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //通过上面讲的方法来计算节点的新位置 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。 例如，原来的容量为32，那么应该拿hash跟31（0x11111）做与操作；在扩容扩到了64的容量之后，应该拿hash跟63（0x111111）做与操作。新容量跟原来相比只是多了一个bit位，假设原来的位置在23，那么当新增的那个bit位的计算结果为0时，那么该节点还是在23；相反，计算结果为1时，则该节点会被分配到23+31的桶上。 这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。回答了上文中好的扩容机制。 六、总结 HashMap的结构底层是一个数组，每个数组元素是一个桶，后面可能会连着一串因为碰撞而聚在一起的(key,value)节点，以链表的形式或者树的形式挂着 按照原来的拉链法来解决冲突，如果一个桶上的冲突很严重的话，是会导致哈希表的效率降低至O（n），而通过红黑树的方式，可以把效率改进至O（logn）。相比链式结构的节点，树型结构的节点会占用比较多的空间，所以这是一种以空间换时间的改进方式。 threshold是数组长度扩容的临界值 modCount字段主要用来记录HashMap内部结构发生变化的次数，这里结构变化必须是新的值塞进来或者某个值删除这种类型，而不是仅仅是覆盖 只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。 好的hash算法：由于在计算中位运算比取模运算效率高的多，所以HashMap规定数组的长度为 2^n 。这样用 2^n - 1 与 hash 做位运算与取模效果一致，并且效率还要高出许多。 好的扩容机制：因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。 还有就是要记住put的过程。 整理自： https://zhuanlan.zhihu.com/p/21673805 http://blog.csdn.net/u013124587/article/details/52649867]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashcode/Equals]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F4.hashcode%E5%92%8Cequals%2F</url>
    <content type="text"><![CDATA[hashcode涉及到集合HashMap等集合，此篇侧重于了解hashcode和equals方法的作用的原理。有助于下一篇HashMap的理解。 一、Hash是什么 Hash是散列的意思，就是把任意长度的输入，通过散列算法变换成固定长度的输出，该输出就是散列值。这个玩意还可以做加密。 不同关键字经过散列算法变换后可能得到同一个散列地址，这种现象称为碰撞。 如果两个Hash值不同（前提是同一Hash算法），那么这两个Hash值对应的原始输入必定不同 二、什么是hashcode HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的。 如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置。 如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写。为什么呢？下文会说。 三、HashCode有什么用 比方说Set里面已经有1000个元素了，那么第1001个元素进来的时候，最多可能调用1000次equals方法，如果equals方法写得复杂，对比的东西特别多，那么效率会大大降低。 使用HashCode就不一样了，比方说HashSet，底层是基于HashMap实现的，先通过HashCode取一个模，这样一下子就固定到某个位置了，如果这个位置上没有元素，那么就可以肯定HashSet中必定没有和新添加的元素equals的元素，就可以直接存放了，都不需要比较； 如果这个位置上有元素了，逐一比较，比较的时候先比较HashCode，HashCode都不同接下去都不用比了，肯定不一样，HashCode相等，再equals比较，没有相同的元素就存，有相同的元素就不存。 如果原来的Set里面有相同的元素，只要HashCode的生成方式定义得好（不重复），不管Set里面原来有多少元素，只需要执行一次的equals就可以了。这样一来，实际调用equals方法的次数大大降低，提高了效率。 当俩个对象的`hashCode`值相同的时候，`Hashset`会将对象保存在同一个位置，但是他们`equals`返回`false`，所以实际上这个位置采用链式结构来保存多个对象。 四、为什么重写Object的equals()方法尽量要重写Object的hashCode()方法 面临问题：若两个对象equals相等，但由于不在一个区间，因为hashCode的值在重写之前是对内存地址计算得出，所以根本没有机会进行比较，会被认为是不同的对象(这就是为什么还要重写hashcode方法了)。所以Java对于eqauls方法和hashCode方法是这样规定的： 1 如果两个对象相同(equals为true)，那么它们的hashCode值一定要相同。也告诉我们重写equals方法，一定要重写hashCode方法，也就是说hashCode值要和类中的成员变量挂上钩，对象相同–&gt;成员变量相同—-&gt;hashCode值一定相同。 2 如果两个对象的hashCode相同(只是映射到同一个位置而已)，它们并不一定相同，这里的对象相同指的是用eqauls方法比较。 简单来说，如果只重写equals方法而不重写hashcode方法，会导致重复元素的产生。具体通过下面的例子进行说明。 五、举例 6.1 Student类 很简单，定义了id和name两个字段，无参和有参构造函数，toString方法。 1234567891011121314151617181920public class Student &#123; private int id; private String name; get(),set()略... public Student()&#123;&#125; public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; @Override public String toString() &#123; return "Student [id=" + id + ", name=" + name + "]"; &#125;&#125; 6.2 main方法 1234567891011121314151617181920public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 执行结果： 1234set集合容量为: 3Student [id=1, name=hh]---1735600054Student [id=1, name=hh]---356573597Student [id=2, name=gg]---21685669 我们可以看到，只要是new的对象，他们的hashcode是不一样的。所以，就会认为他们是不一样的对象。所以，集合里面数量为3. 6.3 只重写equals()方法，而不重写HashCode()方法 输出： 1234set集合容量为: 3Student [id=2, name=gg]---2018699554Student [id=1, name=hh]---366712642Student [id=1, name=hh]---1829164700 结论：覆盖equals（Object obj）但不覆盖hashCode(),导致数据不唯一性。 在这里，其实我们可以看到，student1和student2其实是一个对象，但是由于都是new并且没有重写hashcode导致他们变成了两个不一样的对象。 分析： （1）当执行set.add(student1)时，集合为空，直接存入集合； （2）当执行set.add(student2)时，首先判断该对象（student2）的hashCode值所在的存储区域是否有相同的hashCode，因为没有覆盖hashCode方法，所以jdk使用默认Object的hashCode方法，返回内存地址转换后的整数，因为不同对象的地址值不同，所以这里不存在与student2相同hashCode值的对象，因此jdk默认不同hashCode值，equals一定返回false，所以直接存入集合。 （3）当执行set.add(student3)时,与2同理。 （4）当最后执行set.add(student1)时，因为student1已经存入集合，同一对象返回的hashCode值是一样的，继续判断equals是否返回true，因为是同一对象所以返回true。此时jdk认为该对象已经存在于集合中，所以舍弃。 6.4 只重写HashCode()方法，equals()方法直接返回false 1234set集合容量为: 3Student [id=1, name=hh]---4320Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 按照上面的分析，可能会觉得里面应该装4个，因为两次add的student1，虽然他们的hashcode一样，但是equals直接返回false，那么应该判定为两个不同的对象。但是结果确跟我们预想的不一样。 分析： 首先student1和student2的对象比较hashCode，因为重写了HashCode方法，所以hashcode相等,然后比较他们两的equals方法，因为equals方法始终返回false,所以student1和student2也是不相等的，所以student2也被放进了set 首先student1(student2)和student3的对象比较hashCode，不相等，所以student3放进set中 最后再看最后重复添加的student1,与第一个student1的hashCode是相等的，在比较equals方法，因为equals返回false,所以student1和student4不相等;同样，student2和student4也是不相等的;student3和student4的hashcode都不相等，所以肯定不相等的，所以最后一个重复的student1应该可以放到set集合中，那么结果应该是size:4,那为什么会是3呢？ 这时候我们就需要查看HashSet的源码了，下面是HashSet中的add方法的源码： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 这里我们可以看到其实HashSet是基于HashMap实现的，我们在点击HashMap的put方法，源码如下： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 首先是判断hashCode是否相等，不相等的话，直接跳过，相等的话，然后再来比较这两个对象是否相等或者这两个对象的equals方法，因为是进行的或操作，所以只要有一个成立即可，那这里我们就可以解释了，其实上面的那个集合的大小是3,因为最后的一个r1没有放进去，以为r1==r1返回true的，所以没有放进去了。所以集合的大小是3，如果我们将hashCode方法设置成始终返回false的话，这个集合就是4了。 6.5 同时重写 我的写法是： 12345678910111213141516171819@Overridepublic int hashCode() &#123; int result = 17; result = result * 31 + name.hashCode(); result = result * 31 + id; return result;&#125;@Overridepublic boolean equals(Object obj) &#123; if(obj == this) return true; if(!(obj instanceof Student)) return false; Student o = (Student)obj; return o.name.equals(name) &amp;&amp; o.id == id;&#125; 结果： 123set集合容量为: 2Student [id=2, name=gg]---118515Student [id=1, name=hh]---119506 达到我们预期的效果。 六、内存泄露 我们上面实验了重写equals和hashcode方法，执行main，执行结果是： 123set集合容量为: 2Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 将main方法改为： 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); //------新增的开始------- student3.setId(11); set.remove(student3); System.out.println("set集合容量为: "+set.size()); //------新增的结束------- Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 运行结果是： 1234set集合容量为: 2set集合容量为: 2Student [id=1, name=hh]---4320Student [id=11, name=gg]---4598 我们调用了remove删除student3对象，以为删除了student3,但事实上并没有删除，这就叫做内存泄露，就是不用的对象但是他还在内存中。所以我们多次这样操作之后，内存就爆了。 原因： 在调用remove方法的时候，会先使用对象的hashCode值去找到这个对象，然后进行删除，这种问题就是因为我们在修改了对象student3的id属性的值，又因为RectObject对象的hashCode方法中有id值参与运算,所以student3对象的hashCode就发生改变了，所以remove方法中并没有找到student3了，所以删除失败。即student3的hashCode变了，但是他存储的位置没有更新，仍然在原来的位置上，所以当我们用他的新的hashCode去找肯定是找不到了。 总结： 上面的这个内存泄露告诉我一个信息：如果我们将对象的属性值参与了hashCode的运算中，在进行删除的时候，就不能对其属性值进行修改，否则会出现严重的问题。 七、总结 hashCode是为了提高在散列结构存储中查找的效率，在线性表中没有作用。 equals和hashCode需要同时覆盖。 若两个对象equals返回true，则hashCode有必要也返回相同的int数。 若两个对象equals返回false，则hashCode不一定返回不同的int数,但为不相等的对象生成不同hashCode值可以提高哈希表的性能。 若两个对象hashCode返回相同int数，则equals不一定返回true。 同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 整理自： http://blog.csdn.net/haobaworenle/article/details/53819838 http://www.cnblogs.com/xrq730/p/4842028.html http://blog.csdn.net/qq_21688757/article/details/53067814 http://blog.csdn.net/fyxxq/article/details/42066843]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CopyOnWriteArrayList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F3.CopyOnWriteArrayList%2F</url>
    <content type="text"><![CDATA[CopyOnWriteArrayList是ArrayList的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 CopyOnWriteArrayList是一个写时复制的容器，采用了读写分离的思想。通俗点来讲，在对容器进行写操作时，不直接修改当前容器，而是先对当前容器进行拷贝得到一个副本，然后对副本进行写操作，最后再将原容器的引用指向拷贝出来的副本。这样做的好处就是可以对容器进行并发读而不用进行加锁。 一、类的继承关系 12public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 含义不需要再赘述了。 二、类的属性 12345678910111213141516171819202122/** 用于在对数组产生写操作的方法加锁. */final transient ReentrantLock lock = new ReentrantLock();/** 底层的存储结构. */private transient volatile Object[] array;/** 反射机制. */private static final sun.misc.Unsafe UNSAFE;/** lock域的内存偏移量.是通过反射拿到的 */private static final long lockOffset;static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = CopyOnWriteArrayList.class; lockOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("lock")); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 三、数组末尾添加一个元素 12345678910111213141516171819202122public boolean add(E e) &#123; // 可重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 元素数组 Object[] elements = getArray(); // 数组长度 int len = elements.length; // 复制数组 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将要添加的元素放到副本数组的末尾去 newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 基本原理很简单，就是对当前数组加锁，内部复制一个新数组，处理完毕，修改引用即可，达到最终一致的效果。 四、如果没有这个元素则添加 12345public boolean addIfAbsent(E e) &#123; Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false : addIfAbsent(e, snapshot);&#125; 该函数用于添加元素（如果数组中不存在，则添加；否则，不添加，直接返回）。如何可以保证多线程环境下不会重复添加元素？ 答案：通过快照数组和当前数组进行对比来确定是否一致，确保添加元素的线程安全 12345678910111213141516171819202122232425262728293031323334private boolean addIfAbsent(E e, Object[] snapshot) &#123; // 重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 获取数组 Object[] current = getArray(); // 数组长度 int len = current.length; if (snapshot != current) &#123; // 快照不等于当前数组，对数组进行了修改 // 取较小者 int common = Math.min(snapshot.length, len); for (int i = 0; i &lt; common; i++) // 遍历 if (current[i] != snapshot[i] &amp;&amp; eq(e, current[i])) // 当前数组的元素与快照的元素不相等并且e与当前元素相等 // 表示在snapshot与current之间修改了数组，并且设置了数组某一元素为e，已经存在 // 返回false return false; if (indexOf(e, current, common, len) &gt;= 0) // 在当前数组中找到e元素 // 返回false return false; &#125; // 复制数组 Object[] newElements = Arrays.copyOf(current, len + 1); // 对数组len索引的元素赋值为e newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 该函数的流程如下： 获取锁，获取当前数组为current，current长度为len，判断数组之前的快照snapshot是否等于当前数组current，若不相等，则进入步骤2；否则，进入步骤3 不相等，表示在snapshot与current之间，对数组进行了修改，直接返回false结束; 说明当前数组等于快照数组，说明数组没有被改变。在当前数组中索引指定元素，若能够找到，说明已经存在此元素，直接返回false结束；否则进入4 说明没有当前要插入的元素，通过数组复制的方式添加到末尾 无论如何，都要释放锁 五、获取指定索引的元素 1234567public E get(int index) &#123; return get(getArray(), index);&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 通过写时复制的方式，CopyOnWriteArrayList 的 get 方法不用加锁也可以保证线程安全，所以 CopyOnWriteArrayList 并发读的效率是非常高的，它是直接通过数组下标获取元素的。 六、总结 简单而言要记住它的三个特点： CopyOnWriteArrayList 是一个并发的数组容器，它的底层实现是数组。 CopyOnWriteArrayList 采用写时复制的方式来保证线程安全。 通过写时复制的方式，可以高效的进行并发读，但是对于写操作，每次都要进行加锁以及拷贝副本，效率非常低，所以 CopyOnWriteArrayList 仅适合读多写少的场景。 Vector虽然是线程安全的，但是只是一种相对的线程安全而不是绝对的线程安全，它只能够保证增、删、改、查的单个操作一定是原子的，不会被打断，但是如果组合起来用，并不能保证线程安全性。 CopyOnWriteArrayList在并发下不会产生任何的线程安全问题，也就是绝对的线程安全 另外，有两点必须讲一下。 我认为CopyOnWriteArrayList这个并发组件，其实反映的是两个十分重要的分布式理念： （1）读写分离 我们读取CopyOnWriteArrayList的时候读取的是CopyOnWriteArrayList中的Object[] array，但是修改的时候，操作的是一个新的Object[] array，读和写操作的不是同一个对象，这就是读写分离。这种技术数据库用的非常多，在高并发下为了缓解数据库的压力，即使做了缓存也要对数据库做读写分离，读的时候使用读库，写的时候使用写库，然后读库、写库之间进行一定的同步，这样就避免同一个库上读、写的IO操作太多 （2）最终一致 对CopyOnWriteArrayList来说，线程1读取集合里面的数据，未必是最新的数据。因为线程2、线程3、线程4四个线程都修改了CopyOnWriteArrayList里面的数据，但是线程1拿到的还是最老的那个Object[] array，新添加进去的数据并没有，所以线程1读取的内容未必准确。不过这些数据虽然对于线程1是不一致的，但是对于之后的线程一定是一致的，它们拿到的Object[] array一定是三个线程都操作完毕之后的Object array[]，这就是最终一致。最终一致对于分布式系统也非常重要，它通过容忍一定时间的数据不一致，提升整个分布式系统的可用性与分区容错性。当然，最终一致并不是任何场景都适用的，像火车站售票这种系统用户对于数据的实时性要求非常非常高，就必须做成强一致性的。 最后总结一点，随着CopyOnWriteArrayList中元素的增加，CopyOnWriteArrayList的修改代价将越来越昂贵，因此，CopyOnWriteArrayList适用于读操作远多于修改操作的并发场景中。 感谢 http://www.cnblogs.com/xrq730/p/5020760.html http://blog.csdn.net/u013124587/article/details/52863533 https://www.cnblogs.com/leesf456/p/5547853.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F2.LinkedList%2F</url>
    <content type="text"><![CDATA[提到ArrayList，就会比较与LinkedList的区别。本文来看看LinkedList的核心原理。 如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。 一、LinkedList属性 123456//链表的节点个数.transient int size = 0;//Pointer to first node.transient Node&lt;E&gt; first;//Pointer to last node.transient Node&lt;E&gt; last; 二、Node的结构 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next;//后置指针 Node&lt;E&gt; prev;//前置指针 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 三、添加元素 3.1 LinkedList表头添加一个元素 当向表头插入一个节点时，很显然当前节点的前驱一定为 null，而后继结点是 first 指针指向的节点，当然还要修改 first 指针指向新的头节点。除此之外，原来的头节点变成了第二个节点，所以还要修改原来头节点的前驱指针，使它指向表头节点，源码的实现如下： 1234567891011121314151617public void addFirst(E e) &#123; linkFirst(e);&#125;private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); //新节点作为新的first节点 first = newNode; if (f == null) last = newNode;//初始就是个空LinkedList的话，last指向当前新节点 else f.prev = newNode;//初始值不为空，将其前置指针指向新节点 size++; modCount++;&#125; 3.2 LinkedList表尾添加一个元素 当向表尾插入一个节点时，很显然当前节点的后继一定为 null，而前驱结点是 last 指针指向的节点，然后还要修改 last 指针指向新的尾节点。此外，还要修改原来尾节点的后继指针，使它指向新的尾节点，源码的实现如下： 123456789101112131415161718public void addLast(E e) &#123; linkLast(e);&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); //新节点作为新的last节点 last = newNode; //如果原来有尾节点，则更新原来节点的后继指针，否则更新头指针 if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 3.3 LinkedList在指定节点前添加一个元素 12345678910111213141516171819202122232425262728293031323334public void add(int index, E element) &#123; //判断数组是否越界 checkPositionIndex(index); if (index == size) linkLast(element);//直接插在最后一个 else linkBefore(element, node(index));//在index节点之前插入&#125;private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; //找到索引位置的前面一个元素pred final Node&lt;E&gt; pred = succ.prev; //新节点，前置指针指向pred,后置指针指向索引处元素 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); //修改索引出元素的前置指针为新节点 succ.prev = newNode; if (pred == null) first = newNode;//说明是插在表头 else pred.next = newNode;//说明是插在非表头位置，修改pred后置指针为新指针 size++; modCount++;&#125; 可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。 四、删除元素 删除操作与添加操作大同小异，例如删除指定节点的过程如下图所示，需要把当前节点的前驱节点的后继修改为当前节点的后继，以及当前节点的后继结点的前驱修改为当前节点的前驱。 就不赘述了。 五、获取元素 12345678910111213141516171819202122//获取指定索引对应的元素public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;//寻找元素的方向是根据index在表中的位置决定的Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123;//索引小于表长的一半，从表头开始往后找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123;//索引大于表长的一半，从表尾往前开始找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 上述代码，利用了双向链表的特性，如果index离链表头比较近，就从节点头部遍历。否则就从节点尾部开始遍历。使用空间（双向链表）来换取时间。 node()会以O(n/2)的性能去获取一个结点 如果索引值大于链表大小的一半，那么将从尾结点开始遍历 这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 总结 1、理论上无容量限制，只受虚拟机自身限制影响，所以没有扩容方法。 2、和ArrayList一样，LinkedList也是是未同步的，多线程并发读写时需要外部同步，如果不外部同步，那么可以使用Collections.synchronizedList方法对LinkedList的实例进行一次封装。 3、和ArrayList一样，LinkedList也对存储的元素无限制，允许null元素。 4、顺序插入速度ArrayList会比较快，因为ArrayList是基于数组实现的，数组是事先new好的，只要往指定位置塞一个数据就好了；LinkedList则不同，每次顺序插入的时候LinkedList将new一个对象出来，如果对象比较大，那么new的时间势必会长一点，再加上一些引用赋值的操作，所以顺序插入LinkedList必然慢于ArrayList 5、基于上一点，因为LinkedList里面不仅维护了待插入的元素，还维护了Entry的前置Entry和后继Entry，如果一个LinkedList中的Entry非常多，那么LinkedList将比ArrayList更耗费一些内存 6、数据遍历的速度，看最后一部分，这里就不细讲了，结论是：使用各自遍历效率最高的方式，ArrayList的遍历效率会比LinkedList的遍历效率高一些 7、有些说法认为LinkedList做插入和删除更快，这种说法其实是不准确的： LinkedList做插入、删除的时候，慢在寻址，快在只需要改变前后Entry的引用地址 ArrayList做插入、删除的时候，慢在数组元素的批量copy，快在寻址 所以，如果待插入、删除的元素是在数据结构的前半段尤其是非常靠前的位置的时候，LinkedList的效率将大大快过ArrayList，因为ArrayList将批量copy大量的元素；越往后，对于LinkedList来说，因为它是双向链表，所以在第2个元素后面插入一个数据和在倒数第2个元素后面插入一个元素在效率上基本没有差别，但是ArrayList由于要批量copy的元素越来越少，操作速度必然追上乃至超过LinkedList。 从这个分析看出，如果你十分确定你插入、删除的元素是在前半段，那么就使用LinkedList；如果你十分确定你删除、删除的元素在比较靠后的位置，那么可以考虑使用ArrayList。如果你不能确定你要做的插入、删除是在哪儿呢？那还是建议你使用LinkedList吧，因为一来LinkedList整体插入、删除的执行效率比较稳定，没有ArrayList这种越往后越快的情况；二来插入元素的时候，弄得不好ArrayList就要进行一次扩容，记住，ArrayList底层数组扩容是一个既消耗时间又消耗空间的操作. 8、ArrayList使用最普通的for循环遍历，LinkedList使用foreach循环比较快.注意到ArrayList是实现了RandomAccess接口而LinkedList则没有实现这个接口.关于RandomAccess这个接口的作用，看一下JDK API上的说法： 9、如果使用普通for循环遍历LinkedList，在大数据量的情况下，其遍历速度将慢得令人发指 整理自： 1、http://www.cnblogs.com/xrq730/p/5005347.html 2、http://blog.csdn.net/u013124587/article/details/52837848 3、http://blog.csdn.net/u011392897/article/details/57115818 4、http://blog.csdn.net/fighterandknight/article/details/61476335]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList/Vector]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F1.ArrayList%E5%92%8CVector%2F</url>
    <content type="text"><![CDATA[面试中，关于java的一些容器，ArrayList是最简单也是最常问的，尤其是里面的扩容机制。 ArrayList 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 构造函数为： 123456789101112131415161718//用初始容量作为参数的构造方法public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //初始容量大于0，实例化数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //初始容量等于0，赋予空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125;//无参的构造方法public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 从构造方法中我们可以看见，默认情况下，elementData是一个大小为0的空数组，当我们指定了初始大小的时候，elementData的初始大小就变成了我们所指定的初始大小了。 ArrayList相当于动态数据，其中最重要的两个属性分别是: elementData 数组，以及 size 大小。 在调用 add() 方法的时候： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话： 12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码: 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 也是一个数组复制的过程，ArrayList每次扩容都是扩1.5倍，然后调用Arrays类的copyOf方法，把元素重新拷贝到一个新的数组中去。 由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。 序列化 由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 从实现中可以看出 ArrayList 只序列化了被使用的数据。 Vector Vector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 Vector比ArrayList多了一个属性： 1protected int capacityIncrement; 这个属性是在扩容的时候用到的，它表示每次扩容只扩capacityIncrement个空间就足够了。该属性可以通过构造方法给它赋值。先来看一下构造方法： 123456789101112131415public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 从构造方法中，我们可以看出Vector的默认大小也是10，而且它在初始化的时候就已经创建了数组了，这点跟ArrayList不一样。再来看一下grow方法： 12345678910private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从grow方法中我们可以发现，newCapacity默认情况下是两倍的oldCapacity，而当指定了capacityIncrement的值之后，newCapacity变成了oldCapacity+capacityIncrement。 以下是 add() 方法： 123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据: 12345678910111213public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年的最后一天，对商城项目的架构做个改造]]></title>
    <url>%2F2019%2F01%2F20%2F2018%E5%B9%B4%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%A4%A9%EF%BC%8C%E5%AF%B9%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%9E%B6%E6%9E%84%E5%81%9A%E4%B8%AA%E6%94%B9%E9%80%A0%2F</url>
    <content type="text"><![CDATA[一直以来都是学习慕课的实战视频，虽然也跟着做出了一些东西，但是思路都是别人提供好的，脱离了老师，我一直在问自己一个问题：能不能独立地按照自己的思路做出一些东西来？ 前方图片高能…更有几十兆gif演示动画，图片全部存放于七牛云上。 在去年，即2017年年底，我在慕课上学习了这两门课程： 第一期项目实现了比较简单的电商业务，整合SSM，并且部署到云端。 第二期实现了tomcat集群，配合redis实现分布式session，还有一些定时任务、redis分布式锁、maven环境隔离的一些东西，还涉及很多spring和springmvc的有用的机巧。 整体感觉是：一期实现业务，二期对于一期的提高不是太大，跟分布式无太大关系，仅仅实现了单点登陆和分布式session存储而已。 个人感觉下一期的课程应该是springCloud的分布式改造，进行服务拆分和治理。所以，在整合这两个课程的基础上用springCloud进行微服务治理。 项目详细描述 项目源码地址：https://github.com/sunweiguo/MMall 整体效果演示： 下面贴个小一点的gif: 部分页面截图： 经过一遍遍测试，商城还是存在一些无伤大雅的bug，但是主要还是锻炼自己的能力嘛！ 下订单的时候，报错：商品不存在或库存不足，是因为我模拟的秒杀，所以商品的库存要提前预置于redis中，后端管理系统的商品管理页面有预置库存的按钮。 新增商品的时候，对于上传图片，需要耐心等待一会，需要等待FTP服务器上传完毕，给一个返回信息(Map，是图片的文件名)才能真正显示（对于富文本中的图片上传，在上传之后需要等待一会，时间与小图上传差不多，否则直接保存不报错，但是前端看不到图，因为还没上传完毕,url还没回传回来） 普通注册的账号，没有管理员权限，所以不能登陆后台管理系统。 后来做了eureka集群，但是配置文件还是只指向其中一个eureka 在学习的视频中，一期只是实现业务功能，单体架构，一个tomcat。二期对其做了集群，并且解决了集群模式下session存储问题，实现了比较简单的单点登陆功能。架构如下： 对于上面的架构来说，只是做了一些集群进行优化，随着业务的发展，用户越来越多，用户服务等其他服务必然要拆分出来独立成为一个服务，这样做的好处是，一方面一个团队负责一个服务可以提高开发效率，另外，对于扩展性也是非常有利的，但是也是有缺点的，会带来很多的复杂性，尤其是引入了分布式事务，所以不能为了分布式而分布式，而是针对不同的业务场景而采用合适的架构。 微服务的实现，主要有两种，国内是阿里系的以dubbo+zookeeper为核心的一套服务治理和发现生态。另一个则是大名鼎鼎的spring cloud栈。 spring cloud并不是像spring是一个框架，他是解决微服务的一种方案，是由各种优秀开源组件共同配合而实现的微服务治理架构。下面的图是我构思的项目结构图： 最前面是Nginx，这里就作为一个静态资源映射和负载均衡，nginx中有几个配置文件，分别为www.oursnail.cn.conf，这个主要是对zuul网关地址做一个负载均衡，指向网关所在的服务器，并且找到前台页面所在位置对页面进行渲染。admin.oursnail.cn.conf，这个主要是配置后端以及后端的页面文件；img.oursnail.cn.conf是对图片服务器地址进行映射。 然后是zuul网关，这里主要是用来限流、鉴权以及路由转发。 再后面就是我们的应用服务器啦。对服务器进行了服务追踪(sleuth)，实现了动态刷新配置(spring cloud config+bus)等功能。以http restful的方式进行通信(openFeign),构建起以eureka为注册中心的分布式架构。 每个服务都是基于springboot打造，结合mybatis持久层操作的框架，完成基本的业务需求。springboot基于spring，特点是快速启动、内置tomcat以及无xml配置。将很多东西封装起来，引入pom就可以直接使用，比如springMVC就基本上引入starter-web即可。 由于资源的原因，只有三台最低配的服务器，所以本来想做的基于ES的全文检索服务没有做，也没有分库分表。 至于定时任务以及Hystrix服务熔断和降级，比较简单，就不做了。 项目的接口文档详见wiki：https://github.com/sunweiguo/spring-cloud-for-snailmall/wiki 项目的数据库表设计：snailmall.sql 下面详细介绍每个模块实现的大体思路（仅供参考，毕竟应届生，真实项目没做过）： 用户模块 关于用户模块，核心的功能是登陆。再核心是如何验证以及如何存储用户信息。这里采取的方案为： 对于用户注册，我这里就是用户名（昵称），那么如何保证不重复呢（高并发）？这里还是用了分布式锁来保证的。 对于未登陆章台下用户修改密码，逻辑为： 购物车模块 订单模块 针对这些问题，我想说一下我的思路。 对于幂等性，这里产生幂等性的主要原因在于MQ的重传机制，可能第一个消息久久没有发出去，然后重新发送一条，结果第一条消息突然又好了，那么就会重复发两跳，对于用户来说，只下一次单，但是服务器下了多次订单。网上解决这个问题的思路是创建一张表，如果是重复的订单号，就不可能创建多次了。还有一种可能方案是用分布式锁对该订单号锁住一段时间，由于只是锁住订单号，所以不影响性能，在这一段时间内是不可以再放同一个订单号的请求进来。 对于MQ消息不丢失，只能是订阅模式了。消息发出去之后，消费端给MQ回复一个接收到的信息，MQ本次消费成功，给订阅者一个回复。 对于全局唯一ID生成，这里用的是雪花算法，具体介绍可以看我的笔记 对于分布式事务，比较复杂，这里其实并没有真正处理，对于数据库扣减库存和数据库插入订单，他们在不同的数据库，廖师兄比较倾向的方式是： 这一切的基础还是需要有一个可靠的消息服务，确保消息要能送达。 针对redis预减库存存在的并发问题，这里的思路是用lua+redis，在预减之前判断库存是否够，这两个操作要在一个原子操作里面才行，lua恰好可以实现原子性、顺序性地操作。 支付模块 这里对接的是支付宝-扫码支付，用到是支付宝沙箱环境。支付的扫码支付详细流程在这里聊一聊哈。 商户前台将商品参数发送至商户后台，商户后台生成内部订单号并用于请求支付平台创建预下单，支付平台创建完预订单后将订单二维码信息返还给商户，此时用户即可扫取二维码进行付款操作。 内部订单号：这是相对于支付宝平台而言的，这个订单号是我们商城自己生成的，对于我们商城来说是内部订单号，但是对于支付宝来说是外部订单号。 将一系列的数据按照支付宝的要求发送给支付宝平台，包括商品信息，生成的验签sign，公钥；支付宝去将sign解密，进行商品的各种信息校验。校验通过，同步返回二维码串。 支付业务流程图： 在获取支付的二维码串之后，用工具包将其转换未二维码展示给用户扫码。 用户扫码后，会收到第一次支付宝的回调，展示要支付的金额，商品信息等。 用户输入密码成功后，正常情况会收到支付宝的第二次回调，即支付成功信息。 但是也可能会由于网络等原因，迟迟收不到支付宝的回调，这个时候就需要主动发起轮询去查看支付状态。 在支付成功之后，接收回调的接口要记得返回success告诉支付宝我已经收到你的回调了，不要再重复发给我了。接收回调的接口也要做好去除重复消息的逻辑。 这个流程是多么地简单而理所当然！ 对应于代码层面，其实就是两个接口，一个是用户点击去支付按钮，此时发起预下单，展示付款二维码，另一个是接收支付宝回调： 预下单： 接收支付宝支付状态回调： 项目进展 [x] 2018/12/31 完成了聚合工程的创建、Eureka服务注册中心、spring cloud config+gitHub+spring cloud bus（rabbitMQ）实现配置自动刷新–v1.0 [x] 2018/12/31 将Eureka注册中心(单机)和配置中心部署到服务器上，这比较固定，所以先部署上去，以后本地就直接用这两个即可，对配置进行了一点点修改 [ ] 2018/12/31 关于配置的自动刷新，用postman发送post请求是可以的，但是用github webhook不行，不知道是不是这个版本的问题 [x] 2018/12/31 用户模块的逻辑实现,首先增加了一些pom文件的支持，整合mybatis，测试数据库都通过，下面就可以真正去实现业务代码了 [x] 2019/1/1 完成用户注册、登陆、校验用户名邮箱有效性、查看登陆用户信息、根据用户名去拿到对应的问题、校验答案是否正确、重置密码这个几个接口，在注册这个接口，增加一个ZK分布式锁来防止在高并发场景下出现用户名或邮箱重复问题 [x] 2019/1/2 上午完成门户用户模块所有接口–v2.0 [x] 2019/1/2 下午完成品类管理模块，关于繁琐的获取用户并且鉴权工作，这里先放每个接口里面处理，后面放到网关中去实现–v3.0 [x] 2019/1/3 上午引入网关服务，将后台重复的权限校验统一放到网关中去做，并且加了限流，解决了一下跨域问题。–v4.0 [x] 2019/1/3 下午和晚上完成门户和后台的商品管理模块所有的接口功能，除了上传文件的两个接口没有测试以外，其他接口都进行了简单的测试，其中还用Feign去调用了品类服务接口–v5.0 [x] 2019/1/3 初步把购物车模块和模块引入，通过基础测试，后面在此基础上直接开发代码即可，明天下午看《大黄蜂》，晚上师门聚餐吃火锅，明天早上赶一赶吧，今天任务结束！ [x] 2019/1/4 整理了接口文档，并且画了一下购物车模块的流程图以及订单服务的流程图，针对订单服务中，记录了需要一些注意的问题，尽可能地完善，提高可用性和性能。并且完成购物车模块的controller层。 [x] 2019/1/5 完成购物车模块，并且进行了简单的测试，这里进行了两处改造，一个是判断了一下是否需要判断库存；另一个是商品信息从redis中取，取不出来则调用商品服务初始化值 [x] 2019/1/5 收货地址管理模块，这个模块就是个增删改查，没啥东西写，这里就不加缓存了。 [x] 2019/1/6 完成了后台订单管理模块并且进行了测试，调用收货地址服务时，发现收货地址服务无法读取到cookie，通过这个方法(https://blog.csdn.net/WYA1993/article/details/84304243) 暂时解决了问题 [x] 2019/1/6 预置所有商品库存到redis中；预置所有商品到redis中；大概确定好订单服务思路：预减库存（redis判断库存）—对userID增加分布式锁防止用户重复提交订单–MQ异步下订单 [x] 2019/1/6 新增全局唯一ID生成服务，雪花算法实现 [x] 2019/1/7 完善订单服务-这一块涉及跨库操作，并且不停地调用其他服务，脑子都快晕了，这里采取的策略是：用到购物车的时候，去调用购物车服务获取；产品详情从redis中获取。首先将商品以及商品库存全部缓存到redis中，然后用户下单，先从redis中判断库存，够则减，判断 和扣减放在lua脚本中原子执行，然后MQ异步出去生成订单（生成订单主表和订单详情表放在一个本地事务中），这两步操作成功之后，再用MQ去异步删除购物车。MQ消费不成功则重试。 对于扣减库存这一步，想法是用定时任务，定时与redis中进行同步。这里是模拟了秒杀场景，预减库存+MQ异步，提交订单–&gt;redis判断并且减库存–&gt;调用cart-service获取购物车–&gt;MQ异步(userId,shippingId)生成订单主表和详情表–&gt;上面都成功，则MQ异步(userId) 去清除购物车，库存用定时任务去同步(未做)，理想的做法是：MQ异步扣减库存，订单服务订阅扣减库存消息，一旦库存扣减成功，则进行订单生成。 [x] 2019/1/8 继续完善订单接口，完成支付服务，就直接放在订单服务里面了，因为与订单逻辑紧密，就放在一起了。 [x] 2019/1/8 使用了一下swagger，发现代码侵入比较强，每一个接口上面都要手动打上响应的注解 [x] 2019/1/8 关于hystrix熔断与降级，可以引入hystrix的依赖，用@HystrixCommand注解来控制超时时间、服务降级以及服务熔断。也可以直接再@FeignClient接口中指定服务降级的类，这里不演示了，因为设置比如超时时间，我还要重新测试，写起来很简单，测起来有点儿麻烦 [x] 2019/1/9 服务跟踪，服务端是直接用的线程的，只需要下载：wget -O zipkin.jar ‘https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec’，然后nohup java -jar zipkin.jar &gt; zipkin.server.out &amp; ，开放9411端口，打开浏览器http://ip:9411看到页面即可。 客户端只需要添加相应依赖和配置文件即可。用客户端测试，发现死活不出现我的请求，经过搜索，发现需要增加spring.zipkin.sender.type= web这个配置项才行. [x] 2019/1/10 初步把项目部署到服务器上，进行测试，bug多多，修改中… [x] 2019/1/10 改了一天的bug，其中网关的超时时间以及feign的超时时间都要改大一点，否则会超时报错。最终成功，花了三台服务器，部署了11个服务。后面把部署过程写一下。 [x] 2019/1/11 将注册中心做成集群，因为早上一起来，注册中心挂了？？？ [ ] 2019/1/11 docker部署(商城第四期的改造目标:容器化+容器编排)，本期改造结束。 [x] 2019/1/11 完善readme文档 [x] 2019/1/12 两次发现redis数据被莫名其妙清空，我确定不是缓存到期，为了安全起见，设置了redis的密码，明天看缓存数据还在不在。 [x] 2019/1/14 redis数据没有再丢失，修复用户更新信息的bug 项目启动 安装redis、zookeeper、mysql、jdk、nginx以及rabbitMQ。 对代码进行maven-package操作。打包成jar包。将其放到服务器上： 执行nohup java -jar snailmall-user-service.8081 &gt; user-service.out &amp;后台启动即可。 补充：针对配置刷新，修改了github信息，用postman请求http://xxxxx:8079/actuator/bus-refresh 触发更新。 本改造是基于快乐慕商城一期和快乐慕商城二期的基础上进行改造。所以需要在其业务基础上改造会比较顺手。关于微服务，尤其是电商中的一些处理手段，很多思路都是学习于码吗在线中分布式电商项目。再加上慕课网廖师兄的spring cloud微服务实践。 前台项目 只要阅读readme文档即可。代码仓库为：https://github.com/sunweiguo/snailmall-front 学习不仅要有输入，更要有自己的输出，实践是提升的捷径！]]></content>
      <tags>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-进程与线程]]></title>
    <url>%2F2019%2F01%2F19%2F%E9%9D%A2%E8%AF%95-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[都是操作系统管理的对象，比较容易混淆，但是又是两样完全不同的东西，因此区别很多。从他们区别也可以发散出很多关于操作系统比较重要的知识。所以面试比较常问。 1. 进程到底是什么？ 翻了一下书：《操作系统概念》第三章中提及了进程的概念。他是这样说的： 进程是执行中的程序，这是一种非正式说法。进程不只是程序代码（代码块），进程还包含了当前活动，通过程序计数器的值和处理器寄存器的内容来表示。此外，进程还包含进程堆栈段（包括临时数据，如函数参数、返回地址和局部变量）和数据段（包括全局变量）。进程还可能包含堆，是在进程运行期间动态分配的内存。 程序本身不是进程。程序只是被动实体，如存储在磁盘上包含一系列指令的文件内容。而进程是活动实体，它有一个程序计数器来表示下一个要执行的命令和相关资源集合。当一个可执行文件被装入内存时，一个程序才能成为进程。 总结一下进程是什么，是我个人理解：它是一个活动实体，运行在内存上。然后它占用很多独立的资源，比如：内存资源、程序运行肯定涉及CPU计算、占用的端口资源（公共的）、文件资源（公共的）、网络资源（公共的）等等等。要想执行这个进程，首先要有一个可执行文件，有了这个可执行文件，还要有相应的执行需要的资源。所以将可执行文件、当前进程的上下文、内存等资源结合起来，才是一个真正的进程。 那么，我们就可以理解一句话：进程是资源分配的基本单位。 进程中的内存空间（虽然空间大小都一样，下文会说明）是独立的，否则就会出现一种情况：修改自己程序中的某个指针就可以指向其他程序中的地址，然后拿到里面的数据，岂不是很恐怖的场景？ 如上图，进程中包含了线程。操作系统可能会运行几百个进程，进程中也可能有几个到几百个线程在运行。 文件和网络句柄是所有进程共享的，多个进程可以去打开同一个文件，去抢占同一个网络端口。 图中还有个内存。这个内存不是我们经常说的内存条，即物理内存，而是虚拟内存，是进程独立的，大小与实际物理内存无关。 2. 寻址空间 比如8086只有20根地址线，那么它的寻址空间就是1MB，我们就说8086能支持1MB的物理内存，及时我们安装了128M的内存条在板子上，我们也只能说8086拥有1MB的物理内存空间。 以前叫卖的32位的机子，32位是指寻址空间为2的32次方。32位的386以上CPU就可以支持最大4GB的物理内存空间了。 3. 为什么会有虚拟内存和物理内存的区别 正在运行的一个进程，他所需的内存是有可能大于内存条容量之和的，比如你的内存条是256M，你的程序却要创建一个2G的数据区，那么不是所有数据都能一起加载到内存（物理内存）中，势必有一部分数据要放到其他介质中（比如硬盘），待进程需要访问那部分数据时，在通过调度进入物理内存。 所以，虚拟内存是进程运行时所有内存空间的总和，并且可能有一部分不在物理内存中，而物理内存就是我们平时所了解的内存条。 关键的是不要把虚拟内存跟真实的插在主板上的内存条相挂钩，虚拟内存它是“虚拟的”不存在，假的啦，它只是内存管理的一种抽象！ 4. 虚拟内存地址和物理内存地址是如何映射呢 假设你的计算机是32位，那么它的地址总线是32位的，也就是它可以寻址0 ~ 0xFFFFFFFF（4G）的地址空间，但如果你的计算机只有256M的物理内存0x~0x0FFFFFFF（256M），同时你的进程产生了一个不在这256M地址空间中的地址，那么计算机该如何处理呢？回答这个问题前，先说明计算机的内存分页机制。 计算机会对虚拟内存地址空间（32位为4G）分页产生页（page），对物理内存地址空间（假设256M）分页产生页帧（page frame），这个页和页帧的大小是一样大的，所以呢，在这里，虚拟内存页的个数势必要大于物理内存页帧的个数。 在计算机上有一个页表（page table），就是映射虚拟内存页到物理内存页的，更确切的说是页号到页帧号的映射，而且是一对一的映射。但是问题来了，虚拟内存页的个数 &gt; 物理内存页帧的个数，岂不是有些虚拟内存页的地址永远没有对应的物理内存地址空间？ 不是的，操作系统是这样处理的。操作系统有个页面失效（page fault）功能。操作系统找到一个最少使用的页帧，让他失效，并把它写入磁盘，随后从磁盘中把把需要访问的数据所在的页放到最少使用的页帧中，并修改页表中的映射（即修改页号指向当前页帧），这样就保证所有的页都有被调度的可能了。这就是处理虚拟内存地址到物理内存的步骤。 至于里面如何实现的细节，我没有过多去探究。 5. 什么是虚拟内存地址和物理内存地址 虚拟内存地址由页号和偏移量组成。页号就是上面所说的。偏移量就是我上面说的页（或者页帧）的大小，即这个页（或者页帧）到底能存多少数据。 举个例子，有一个虚拟地址它的页号是4，偏移量是20，那么他的寻址过程是这样的：首先到页表中找到页号4对应的页帧号（比如为8），如果找不到对应的页桢，则用失效机制调入页。如果存在，把页帧号和偏移量传给MMU（CPU的内存管理单元）组成一个物理上真正存在的地址，接着就是访问物理内存中的数据了。 6. 线程里面有什么 写到这里，好像还与本标题无关，即进程和线程到底是什么关系和区别等。但是我们要知道，面试或者学习一个知识点，不是为了学习这个区别而学习， 我们应该学习为什么有进程和线程，有了进程还需要线程吗？有了线程还要进程吗？你说进程是资源分配的单位，分配的是什么资源呢？进程中的内存是咋管理的呢？虚拟内存和物理内存是什么？什么是虚拟内存地址和物理内存地址？等等等，所以面试是千变万化的，重要的是我们尽可能地多问自己几个为什么，然后从为什么开始去逐个击破，形成一个体系。 说说这个栈，我们知道，执行程序从主程序入口进入开始，可能会调用很多的函数，那么这些函数的参数和返回地址都会被压入栈中，包括这些函数中定义的临时局部变量都会压入栈中，随着函数的执行完毕，再逐层地弹出栈，回到主函数运行的地方，再继续执行。 PC(program counter)，就是程序计数器，指向的下一条指令执行的地址。 由此可见，操作系统运行的其实是一个一个的线程，而进程只是一个隔离资源的容器。 上面说到，PC是指向下一条指令执行的地址。而这些指令是放在内存中的。 我们的计算机大多数是存储程序型的。就是说数据和程序是同时存储在同一片内存里的。 所以我们经常会听到一个漏洞叫做“缓冲区溢出”：比如有一个地方让用户输入用户名，但是黑客输入很长很长的字符串进去，那么很有可能就会超出存放这个用户名的一片缓冲区，而直接侵入到存放程序的地方，那么黑客就可以植入程序去执行。解决方案就是限制输入的用户名长度，不要超过缓冲区大小。 还有一块是TLS(thread local storage)，我们知道进程有自己独立的内存，那么我们的线程能不能也有一小块属于自己的内存区域呢？ 这个东西，其实很简单，就是说，比如new一个对象，往往是在堆中开辟空间的，但是现在的情况是：在一个函数内，new出来一个对象，这个对象不引用外部对象，也不会被外部引用，是纯粹属于这个函数段，可以理解为这个对象是属于这个函数的局部临时变量。 此时，new这个对象就不需要再去堆中开辟空间了，因为一方面不需要共享，另一方面是在堆中开辟是比较慢的，并且可能有很多函数，这种局部对象零零总总加起来还是很多的，在堆中开辟会浪费空间。 所以，能不能在栈中就可以new出这个对象，反正用完就扔。TLS可以是现在这个。栈中直接new多方便多快，因为不需要走垃圾回收机制，还避免了线程安全问题。可以去搜索：栈上分配和逃逸分析 7. 线程VS进程 到这里，就清晰了很多。我们也可以多多少少理解他们的区别。 可以做个简单的比喻，便于记忆：进程=火车，线程=车厢 线程在进程下行进（单纯的车厢无法运行） 一个进程可以包含多个线程（一辆火车可以有多个车厢） 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘） 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易） 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源） 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢） 进程可以拓展到多机，进程最多适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上） 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－“互斥锁” 进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量” 再补充几句。 线程是调度的基本单位，进程是资源分配的基本单位 进程间没有共享内存，所以交互要通过TCP/IP端口的等方式来实现。线程间由于有共享内存，所以交互比较方便。 线程占用很多资源，而线程只需要分配栈和PC即可。 8. 针对虚拟内存和物理内存的总结 每个进程都有自己独立的4G(32位系统下)内存空间，各个进程的内存空间具有类似的结构 一个新进程建立的时候，将会建立起自己的内存空间，此进程的数据，代码等从磁盘拷贝到自己的进程空间（建立一个进程，就要把磁盘上的程序文件拷贝到进程对应的内存中去，对于一个程序对应的多个进程这种情况，浪费内存！），哪些数据在哪里，都由进程控制表中的task_struct记录 每个进程的4G内存空间只是虚拟内存空间，每次访问内存空间的某个地址，都需要把地址翻译为实际物理内存地址 所有进程共享同一物理内存，每个进程只把自己目前需要的虚拟内存空间映射并存储到物理内存上。 进程要知道哪些内存地址上的数据在物理内存上，哪些不在，还有在物理内存上的哪里，需要用页表来记录 页表的每一个表项分两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址（如果在的话） 当进程访问某个虚拟地址，去看页表，如果发现对应的数据不在物理内存中，则缺页异常 缺页异常的处理过程，就是把进程需要的数据从磁盘上拷贝到物理内存中，如果内存已经满了，没有空地方了，那就找一个页覆盖，当然如果被覆盖的页曾经被修改过，需要将此页写回磁盘 9. 关于进程和线程更深的认识 关于为什么要分进程和线程，先抛出结论： 进程process：进程就是时间总和=执行环境切换时间+程序执行时间------&gt;CPU加载执行环境-&gt;CPU执行程序-&gt;CPU保存执行环境 线程thread：线程也是时间总和=执行环境切换时间（共享进程的）+程序模块执行时间------&gt;CPU加载执行环境（共享进程的）-&gt;CPU执行程序摸块-&gt;CPU保存执行环境（共享进程的） 进程和线程都是描述CPU工作的时间段，线程是更细小的时间段。 那么，如果CPU时间片临幸本进程，那么这个进程在恢复执行环境之后，执行里面的若干线程就不需要再不停地切换执行环境了，所以说，线程相比于进程是比较轻量的。 在CPU看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。。 进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文 线程是什么呢？进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成：程序A得到CPU =》CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。 这里a，b，c的执行是共享了A的上下文，CPU在执行的时候没有进行上下文切换的。这里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境、更为细小的CPU时间段。 进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同。 整理自： https://www.zhihu.com/question/25532384 https://blog.csdn.net/moshenglv/article/details/52242153 https://blog.csdn.net/u012861978/article/details/53048077]]></content>
      <tags>
        <tag>操作系统相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步一步理解HTTPS]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F7.%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3HTTPS%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第七篇文章。HTTPS（SSL/TLS）的加密机制是前端后端ios安卓等都应了解的基本问题。也是面试经常问的点。 一、为什么需要加密？ 小时候看谍战片，情报发过来了之后，用一个小本本进行翻译，然后解密出情报。加密就是防止明文被别人看到甚至篡改嘛！ 回到互联网，因为http的内容是明文传输的，明文数据会经过中间代理服务器、路由器、wifi热点、通信服务运营商等多个物理节点，如果信息在传输过程中被劫持，传输的内容就完全暴露了，他还可以篡改传输的信息且不被双方察觉，这就是中间人攻击。所以我们才需要对信息进行加密。最简单容易理解的就是对称加密 。 二、什么是对称加密？ 小明写个求爱信给小红，小明担心小红的妈妈看到这封信的内容，他灵机一动，对信加个密，并确定好我用这个密钥加密的，小红收到之后也用这个密钥解密才行。 但是呢，这里有个麻烦的地方就是，小明和小红不在一个学校，这个钥匙呢，不方便直接送到手里。所以呢，小明得想办法把这个钥匙寄一个送给小红，好吧，就用最贵的顺丰吧！ 就是有一个密钥，它可以对一段内容加密，加密后只能用它才能解密看到原本的内容，和我们日常生活中用的钥匙作用差不多。 三、用对称加密可行吗？ 顺丰快递到了，结果小红不在家，小红的妈妈收到了，一看是个男同学寄的，怎么能忍住，赶紧打开，以看是一把钥匙，作为程序猿，妈妈得意一笑：哼哼，还能逃过我的眼睛？我赶紧复制一把藏着，我倒要看看他后面要寄啥来，还要加密？！ 果然小红的妈妈等到了来自小明寄过来的情书，解密一看，实锤早恋。 同样地，小明这边也非常危险，快递员刚出发，就被小明的妈妈拦截了，拿到了这个钥匙，那小明还没寄出的信已经被妈妈看光了。 所以问题的根本就是，这把钥匙要传输，传输就可能被截取。 回到互联网，如果通信双方都各自持有同一个密钥，且没有别人知道，这两方的通信安全当然是可以被保证的（除非密钥被破解）。 然而最大的问题就是这个密钥怎么让传输的双方知晓，同时不被别人知道。 如果由服务器生成一个密钥并传输给浏览器，那这个传输过程中密钥被别人劫持弄到手了怎么办？ 换种思路？试想一下，如果浏览器内部就预存了网站A的密钥，且可以确保除了浏览器和网站A，不会有任何外人知道该密钥，那理论上用对称加密是可以的，这样浏览器只要预存好世界上所有HTTPS网站的密钥就行啦！这么做显然不现实。 怎么办？所以我们就需要神奇的非对称加密。 四、什么是非对称加密？ 有两把密钥，通常一把叫做公钥、一把叫做私钥。 用公钥加密的内容必须用私钥才能解开，同样，私钥加密的内容只有公钥能解开。 五、用非对称加密可行吗？ 公钥呢，还是要通过快递员送给小红的。OK，假设小红要回信，写好了用公钥加密，小红的妈妈因为拿不到私钥，看不到信的内容。 OK，但是反过来呢？小明用私钥加密传给小红，那么小红的妈妈可就能解密了（因为公钥可能会被小红的妈妈拿到）。 回到互联网，服务器先把公钥直接明文传输给浏览器，之后浏览器向服务器传数据前都先用这个公钥加密好再传，这条数据的安全似乎可以保障了！因为只有服务器有相应的私钥能解开这条数据。 然而由服务器到浏览器的这条路怎么保障安全？ 如果服务器用它的的私钥加密数据传给浏览器，那么浏览器用公钥可以解密它，而这个公钥是一开始通过明文传输给浏览器的，这个公钥被谁劫持到的话，他也能用该公钥解密服务器传来的信息了。 所以目前似乎只能保证由浏览器向服务器传输数据时的安全性（其实仍有漏洞，下文会说）。 六、改良的非对称加密方案，似乎可以？ 小明和小红年纪不大，但是很聪明，针对这个情况，还是迅速升级加密方法。他们想到既然一组公钥私钥不够，那两组呢？ OK，小明和小红各造了一对。下面就是互相交换公钥。那么就变成： 下面就好办啦，小明写信用公钥B加密，那么信的内容只有小红能破解，因为小红是随身携带私钥B。相反，小红用公钥A对信加密，这样只有小明能破解，因为小明也是随身携带私钥A。好像很安全啦！除了下面提到的漏洞，唯一的缺点可能是：小红得花半天时间才能解密完这封信，有点受不了。 回到互联网。请看下面的过程： 某网站拥有用于非对称加密的公钥A、私钥A；浏览器拥有用于非对称加密的公钥B、私钥B。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器把公钥B明文传输给服务器。 之后浏览器向服务器传输的所有东西都用公钥A加密，服务器收到后用私钥A解密。由于只有服务器拥有这个私钥A可以解密，所以能保证这条数据的安全。 服务器向浏览器传输的所有东西都用公钥B加密，浏览器收到后用私钥B解密。同上也可以保证这条数据的安全。 的确可以！抛开这里面仍有的漏洞不谈（下文会讲），HTTPS的加密却没使用这种方案，为什么？最主要的原因是非对称加密算法非常耗时，特别是加密解密一些较大数据的时候有些力不从心。 七、非对称加密+对称加密？ 小明也知道，这个信很长，用非对称加密，太慢！办法也有，没有必要对那么长的信加密，我只要保证这个真正解密的钥匙不被别人拿到就行，那么他灵机一动想到这个方法： 小明和小红利用非对称加密对钥匙加密，姑且认为是这个钥匙被放在了一个盒子里，这个盒子也被锁起来了，只有小红或者小明才能打开盒子，再用钥匙去解密。 这个真正用于对称加密解密的钥匙别人就拿不到啦！ 自从用了这个方案，感觉又安全，解密又快，感情又深温了呢！ 回到互联网，步骤如下： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都用密钥X加密解密。 HTTPS的基本思想就是基于这个。但是这个方案也存在上面一直在说的漏洞。 八、中间人攻击 像妈妈这样级别的程序猿可能是那他们两没办法啦，但是呢，校区有个看门的大爷，以前是个黑客，也不知道咋回事，明明才50岁，但是看起来像80岁，头上光溜溜的，冬天冷呢。整天在那胡言乱语：docker牛逼啊，spring cloud牛逼啊，这个开源软件XXX写的真好，跟周围的老大爷老大妈根本谈不到一起去。 他也是闲的蛋疼，非要掺和，因为据说他以前单身30年，苦逼敲代码，不知道谈恋爱是啥滋味，姑且认为他好奇心重吧。 在小明第一次寄公钥A的时候，大爷出手了，截取下来。换成自己做的公钥B。然后送给小红。 小红哪里会知道这公钥被掉包了呢，所以直接就用了，按照正常步骤，小红想了一个随机字符串，这次就叫xiaomingwoxuanni吧，OK，用这个公钥B对这个字符串加个密，这个字符串就被锁进了用大爷公钥B锁的盒子里。 老大爷在门口守着呢，一看到小红寄东西了，又偷偷地截取下来，用自己的私钥B来解密这个盒子。轻易地拿到了里面的字符串，OK，怕小明察觉，再用小明寄来的公钥A加密传给小明，这样双方都不知道他们的钥匙已经被大爷给获取了。 小明和小红之间的信就用xiaomingwoxuanni这个钥匙进行对称加密和对称解密，完全不知道有个大爷就天天拿着这个字符串去解密信件，看的不亦乐乎，甚至还偷偷改几个字呢。 回到互联网。中间人的确无法得到浏览器生成的密钥B，这个密钥本身被公钥A加密了，只有服务器才有私钥A解开拿到它呀！然而中间人却完全不需要拿到密钥A就能干坏事了。请看： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器向网站服务器请求，服务器把公钥A明文给传输浏览器。 中间人劫持到公钥A，保存下来，把数据包中的公钥A替换成自己伪造的公钥B（它当然也拥有公钥B对应的私钥B）。 浏览器随机生成一个用于对称加密的密钥X，用公钥B（浏览器不知道公钥被替换了）加密后传给服务器。 中间人劫持后用私钥B解密得到密钥X，再用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样在双方都不会发现异常的情况下，中间人得到了密钥B。根本原因是浏览器无法确认自己收到的公钥是不是网站自己的。只要解决了这个公钥一定是这个网站发来的，那么基本就OK了 九、如何证明浏览器收到的公钥一定是该网站的公钥？ 现实生活中，如果想证明某身份证号一定是小明的，怎么办？看身份证。这里政府机构起到了“公信”的作用，身份证是由它颁发的，它本身的权威可以对一个人的身份信息作出证明。互联网中能不能搞这么个公信机构呢？给网站颁发一个“身份证”？ 十、数字证书 网站在使用HTTPS前，需要向“CA机构”申请颁发一份数字证书，即SSL证书，数字证书里有证书持有者、证书持有者的公钥等信息，服务器把证书传输给浏览器，浏览器从证书里取公钥就行了，证书就如身份证一样，可以证明“该公钥对应该网站”。然而这里又有一个显而易见的问题了，证书本身的传输过程中，如何防止被篡改？即如何证明证书本身的真实性？身份证有一些防伪技术，数字证书怎么防伪呢？解决这个问题我们就基本接近胜利了！ SSL证书内容： 证书的发布机构CA 证书的有效期 公钥 证书所有者 签名 十一、如何放防止数字证书被篡改？ 我们把证书内容生成一份“签名”，比对证书内容和签名是否一致就能察觉是否被篡改。这种技术就叫数字签名。 提到数字签名，其实原理很简单啦，就是比如我要传输一句话叫：“你给我转100块钱，我的账号是123456，转完了告诉我一声。”，如果不做任何处理，被刚才的老大爷截取了，他偷偷地改一下内容“你给我转200块钱，我的账号是654321，不要告诉任何人，尤其是你嫂子。” 是不是太坏了，弄不好被抓，大爷可不敢做大的，只敢骗个喝酒钱。 那么怎么防止大爷这种猥琐技术又高的人篡改呢？数字签名排上用场啦！ 以后再传消息就是“你给我转100块钱，我的账号是123456，转完了告诉我一声。”+“！……@&amp;@%#……！￥@￥！@%……#￥！%……”,后面那一串东西就是数字签名，简单来说，就是想办法对前面的内容进行非对称加密（这样别人根本不知道你加密的私钥是什么，也就伪装不了签名了）。传过去之后，我要对其进行解密，与传过来的明文一一对比参数，看有没有被改动过。一旦发现哪里不对应，说明已经被篡改了。 “CA机构”制作签名的过程： CA拥有非对称加密的私钥和公钥。 CA对证书明文信息进行hash。 对hash后的值用私钥加密，得到数字签名。 明文和数字签名共同组成了数字证书，这样一份数字证书就可以颁发给网站了。网站把这个数字证书传给浏览器。 那浏览器拿到服务器传来的数字证书后，如何验证它是不是真的？（有没有被篡改、掉包） 浏览器验证过程： 拿到证书，得到明文T，数字签名S。 用CA机构的公钥对S解密（由于是浏览器信任的机构，所以浏览器保有它的公钥。详情见下文），得到S’。 用证书里说明的hash算法对明文T进行hash得到T’。 比较S’是否等于T’，等于则表明证书可信。 为什么这样可以证明证书可信呢？我们来仔细想一下。 十二、中间人有可能篡改该证书吗？ 老大爷就算有天大的能耐，也拿不到加密的私钥，那么只是单纯地篡改明文，只会造成校验不通过。 回到互联网，假设中间人篡改了证书的原文，由于他没有CA机构的私钥，所以无法得到此时加密后签名，无法相应地篡改签名。 浏览器收到该证书后会发现原文和签名解密后的值不一致，则说明证书已被篡改，证书不可信，从而终止向服务器传输信息，防止信息泄露给中间人。 十三、中间人有可能把证书掉包吗？ 假设有另一个网站B也拿到了CA机构认证的证书，它想搞垮网站A，想劫持网站A的信息。于是它成为中间人拦截到了A传给浏览器的证书，然后替换成自己的证书，传给浏览器，之后浏览器就会错误地拿到B的证书里的公钥了，会导致上文提到的漏洞。 其实这并不会发生，因为证书里包含了网站A的信息，包括域名，浏览器把证书里的域名与自己请求的域名比对一下就知道有没有被掉包了。 总结：因为一个网站域名对应一个证书，你的证书根其他人的证书肯定是不一样的，那么你就算拿到了其他人的证书再掉包成自己的，也没用，毕竟浏览器那边只要看一下是不是我要查看的域名。 十四、为什么制作数字签名时需要hash一次？ 最显然的是性能问题，前面我们已经说了非对称加密效率较差，证书信息一般较长，比较耗时。而hash后得到的是固定长度的信息（比如用md5算法hash后可以得到固定的128位的值），这样加密解密就会快很多。 十五、怎么证明CA机构的公钥是可信的？ 让我们回想一下数字证书到底是干啥的？没错，为了证明某公钥是可信的，即“该公钥是否对应该网站/机构等”，那这个CA机构的公钥是不是也可以用数字证书来证明？没错，操作系统、浏览器本身会预装一些它们信任的根证书，如果其中有该CA机构的根证书，那就可以拿到它对应的可信公钥了。 实际上证书之间的认证也可以不止一层，可以A信任B，B信任C，以此类推，我们把它叫做信任链或数字证书链，也就是一连串的数字证书，由根证书为起点，透过层层信任，使终端实体证书的持有者可以获得转授的信任，以证明身份。 另外，不知你们是否遇到过网站访问不了、提示要安装证书的情况？这里安装的就是根证书。说明浏览器不认给这个网站颁发证书的机构，那么没有该机构的根证书，你就得手动下载安装（风险自己承担XD）。安装该机构的根证书后，你就有了它的公钥，就可以用它验证服务器发来的证书是否可信了。 也就是说，公钥是从证书中获取的。证书是网站从机构那边申请来的，证书+签名传给浏览器。只要校验通过，那么公钥必然没有被篡改过，并且一定是这个网站传来的，那么解决了我们最核心的问题：确定公钥是我们指定的网站传来的。 既然公钥是正确的，那么小红就会用正确的公钥对随机字符串加密，中间不会出现篡改。 十六、HTTPS必须在每次请求中都要先在SSL/TLS层进行握手传输密钥吗？ 这也是我当时的困惑之一，显然每次请求都经历一次密钥传输过程非常耗时，那怎么达到只传输一次呢？用session就行。 服务器会为每个浏览器（或客户端软件）维护一个session ID，在TSL握手阶段传给浏览器，浏览器生成好密钥传给服务器后，服务器会把该密钥存到相应的session ID下，之后浏览器每次请求都会携带session ID，服务器会根据session ID找到相应的密钥并进行解密加密操作，这样就不必要每次重新制作、传输密钥了！ 十七、HTTPS原理 下面再来看看HTTPS原理就特别简单啦！ HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：可以理解为HTTP+SSL/TLS， 即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL，用于安全的 HTTP 数据传输。 1234HTTPSSL/TLSTCPIP 我们只要知道，在SSL层里面可以完成校验和密钥的传输。 理解了上面，这个图也就没啥好解释的了。 整理自：https://zhuanlan.zhihu.com/p/43789231]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP基础知识提炼]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F6.HTTP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%8F%90%E7%82%BC%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第六篇文章。这里简单记录一些关于HTTP的基本概念，比较基础。下面的内容是对《图解HTTP》提炼的再提炼，主要原因是很多重要的东西前面已经详细说过了，还有一些东西知道即可，用到再去查，作为一个后端攻城狮，也没有必要了解那么琐碎。 HTTP是不保存状态的协议 HTTP是无状态协议。自身不对请求和响应之间通信状态进行保存（即不做持久化处理）。 HTTP之所以设计得如此简单，是为了更快地处理大量事物，确保协议的可伸缩性。 HTTP/1.1 随时无状态协议，但可通过 Cookie 技术保存状态。 告知服务器意图的HTTP方法 GET：获取资源 POST：传输实体主体 PUT：传输文件 HEAD：获得报文首部，与GET方法一样，只是不返回报文主体内容。用于确认URI的有效性及资源更新的日期时间等。 DELETE：删除文件，与PUT相反（响应返回204 No Content）。 OPTIONS：询问支持的方法，查询针对请求URI指定的资源支持的方法（Allow:GET、POST、HEAD、OPTIONS）。 TRACE：追踪路径 CONNECT：要求用隧道协议连接代理（主要使用SSL（Secure Sockets Layer，安全套接层）和TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输）。 URI、URL 官方解释都是什么乱起八糟的东西。各种博客也是跟着抄，这两者到底是什么关系和意义？ 统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来。 对应于实际例子就是：每个人都有身份证，这个身份证号码就对应这个人。比如张三的身份证号码为123456，那么我只要知道123456就可以找到这个人。 那什么是URL呢？从名字看是：统一资源定位器。 如果做类比，URL就是：动物住址协议://地球/中国/浙江省/杭州市/西湖区/某大学/14号宿舍楼/525号寝/张三.人 我们通过这个详细的地址也可以找到张三这个人。 那么他们俩到底是什么关系呢？ URI是以某种规则唯一地标识资源，手段不限，比如身份证号。当然了，地址可以唯一标识，那么也属于URI的一种手段。所以说URL是URI的子集。 回到Web上，假设所有的Html文档都有唯一的编号，记作html:xxxxx，xxxxx是一串数字，即Html文档的身份证号码，这个能唯一标识一个Html文档，那么这个号码就是一个URI。 而URL则通过描述是哪个主机上哪个路径上的文件来唯一确定一个资源，也就是定位的方式来实现的URI。 对于现在网址我更倾向于叫它URL，毕竟它提供了资源的位置信息，如果有一天网址通过号码来标识变成了http://741236985.html，那感觉叫成URI更为合适。 HTTP请求报文 返回结果的HTTP状态码 状态码的职责是当客户端向服务器端发送请求时，描述返回的请求结果。 状态码如200 OK，以3为数字和原因短语组成。 数字中的第一位定义了响应类别，后两位无分类。响应类别有以下五种： 类别 原因短语 1XX Informational(信息性状态码) 2XX Success（成功状态码） 3XX Redirection（重定向状态码） 4XX Client Error（客户端错误状态码） 5XX Server Error（服务器错误状态码） ⭐2XX 成功 200 OK：请求被正常处理 204 No Content：一般在只需从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。 206 Partial Content：客户端进行范围请求 ⭐3XX 重定向 301 Moved Permanently：永久重定向。表示请求的资源已被分配了新的URI，以后应使用资源现在所指的URI。 也就是说，如果已经把资源对应的URI保存为书签了，这时应该按Location首部字段提示的URI重新保存。 302 Found：临时性重定向。表示请求的资源已被分配了新的URI，希望用户（本次）能使用新的URI访问。 和301 Moved Permanently状态码相似，但302状态码代表的资源不是被永久移动，只是临时性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。比如，用户把URI保存成书签，但不会像301状态码出现时那样去更新书签，而是仍旧保留返回302状态码的页面对应的URI（在Chrome中，还是会保存为重定向后的URI，不解）。 303 See Other：表示由于请求对应的资源存在着另一个URI，应使用GET方法定向获取请求的资源。这与302类似，但303明确表示客户端应当采用GET方法获取资源。 304 Not Modified：该状态码表示客户端发送附带条件的请求（指采用GET方法的请求报文中包含If-Match,If-Modified-Since，If-None-March，If-Range，If-Unmodified-Since中任一首部。）时，服务器端允许请求访问资源，但因发生请求为满足条件的情况后，直接返回304（服务器端资源未改变，可直接使用客户端未过期的缓存）。304状态码返回时，不包含任何响应的主体部分。 304虽被划分在3XX类别，但是和重定向没有关系。 307 Temporary Redirect：临时重定向。与302有相同含义。307遵守浏览器标准，不会从POST变成GET。 ⭐4XX 客户端错误 4XX的响应结果表明客户端是发生错误的原因所在。 400 Bad Request：表示请求报文中存在语法错误。 401 Unauthorized：表示发送的请求需要有通过HTTP认证（BASIC认证、DIGEST认证）的认证信息。 403 Forbidden：表明对请求资源的访问被服务器拒绝了。服务器端可在实体的主体部分对原因进行描述（可选） 404 Not Found：表明服务器上无法找到请求的资源。除此之外，也可以在服务器端拒绝请求且不想说明理由时时用。 ⭐5XX 服务器错误 5XX的响应结果表明服务器本身发生错误。 500 Interval Server Error：表明服务器端在执行请求时发生了错误。也有可能是Web应用存在的bug或某些临时的故障。 503 Service Unavailable：表明服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。如果事先得知解除以上状况需要的时间，最好写入Retry-After首部字段再返回给客户端。 HTTP的瓶颈 一条连接上只可发送一个请求（前面讲到，持久化可保持TCP连接状态，但仍完成一次请求/响应后才能进行下一次请求/响应，而管线化方式可让一个TCP连接并行发送多个请求。） 请求只能从客户端开始。客户端不可以接收除响应以外的指令 请求/响应首部未经压缩就发送。首部信息越多延迟越大 发送冗长(重复)的首部。每次互相发送相同的首部造成的浪费较多 SPDY以会话层的形式加入，控制对数据的流动，但还是采用HTTP建立通信连接。因此，可照常使用HTTP的GET和POST等方法、Cookie以及HTTP报文等。 使用 SPDY后，HTTP协议额外获得以下功能。 多路复用流：通过单一的TCP连接，可以无限制处理多个HTTP请求。所有请求的处理都在一条TCP连接上完成，因此TCP的处理效率得到提高。 赋予请求优先级：SPDY不仅可以无限制地并发处理请求，还可以给请求逐个分配优先级顺序。这样主要是为了在发送多个请求时，解决因带宽低而导致响应变慢的问题。 压缩HTTP首部：压缩HTTP请求和响应的首部。 推送功能：支持服务器主动向客户端推送数据的功能。 服务器提示功能：服务器可以主动提示客户端请求所需的资源。由于在客户端发现资源之前就可以获知资源的存在，因此在资源已缓存等情况下，可以避免发送不必要的请求。 WebSocket 利用Ajax和Comet技术进行通信可以提升Web的浏览速度。但问题在于通信若使用HTTP协议，就无法彻底解决瓶颈问题。 WebSocket技术主要是为了解决Ajax和Comet里XMLHttpRequst附带的缺陷所引起的问题。 一旦Web服务器与客户端之间建立起WebSocket协议的通信连接，之后所有的通信都依靠这个专用协议进行。通信过程中可互相发送JSON、XML、HTML或图片等任意格式的数据。 WebSocket的主要特点： 推送功能：支持由服务器向客户端推送数据。 减少通信量：和HTTP相比，不但每次连接时的总开销减少，而且由于WebSocket的首部信息很小，通信量也相应较少。 为了实现WebSocket通信，在HTTP连接建立之后，需要完成一次“握手”的步骤。 握手·请求：为了实现WebSocket通信，需要用到HTTP的Upgrade首部字段，告知服务器通信协议发生改变，以达到握手的目的。 握手·响应：对于之前的请求，返回状态码101 Switching Protocols 的响应。 成功握手确立WebSocket连接后，通信时不再使用HTTP的数据帧，而采用WebSocket独立的数据帧。 由于是建立在HTTP基础上的协议，因此连接的发起方仍是客户端，而一旦确立WebSocket通信连接，不论服务器端还是客户端，任意一方都可直接向对方发送报文。 整理自：https://github.com/JChehe/blog/blob/master/posts/《图解HTTP》读书笔记.md]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手和四次挥手]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F5.TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第五篇文章。面试讲到TCP，那么基本都会问三次握手和四次挥手的过程，以及比如对于握手，为什么是三次，而不是两次或者四次，本章详细探讨其中的门道。 1.复习 首先针对http协议，我们有必要复习一下最重要的东西。 HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 在HTTP 1.0以0.9版本中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。 通常的做法是即使不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道 客户端“在线”。 若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 2.SOCKET原理 2.1 套接字（socket）概念 初次接触这个名词：“套接字”，说实话，心里是蒙蔽的，这是啥玩意，但是可以去搜索一下什么是套接管： 我们可以看出来，两个管子可能直接连的话连不起来，那么可以通过中间一个东西连接起来。 那么，现在就好理解了，两个程序要通信，需要知道对方的一些信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 它是什么呢？它是网络通信过程中端点的抽象表示，这个抽象里面就包含了网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 那为什么一定要用它呢？ 在同一台计算机上，TCP协议与UDP协议可以同时使用相同的port而互不干扰。 操作系统根据套接字地址，可以决定应该将数据送达特定的进程或线程。这就像是电话系统中，以电话号码加上分机号码，来决定通话对象一般。 因为我们电脑上可能会跑很多的应用程序，TCP协议端口需要为这些同时运行的程序提供并发服务，或者说，传输层需要为应用层的多个进程提供通信服务，每个进程起一个TCP连接，那么这多个TCP连接可能是通过同一个 TCP协议端口传输数据。 如何区别哪个进程对应哪个TCP连接呢？ 许多计算机操作系统为应用程序与TCP／IP协议交互提供了套接字(Socket)接口。应 用层可以和传输层通过Socket接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。 2.2 建立socket连接 建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。 套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。 客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发 给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 2.3 SOCKET连接与TCP连接 创建Socket连接时，可以指定使用的传输层协议，Socket可以支持不同的传输层协议（TCP或UDP），当使用TCP协议进行连接时，该Socket连接就是一个TCP连接。 3.TCP基本字段 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 针对协议中的字段，我们只需要了解一下：ACK、SYN、序号这三个部分。 ACK : 确认 TCP协议规定，只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 SYN ： 在连接建立时用来同步序号。 当SYN=1而ACK=0时，表明这是一个连接请求报文。 对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此, SYN置1就表示这是一个连接请求或连接接受报文。 FIN 即终结的意思， 用来释放一个连接。 当 FIN = 1 时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接。 4.三次握手(重要，细读) 首先，TCP作为一种可靠传输控制协议，其核心思想：既要保证数据可靠传输，又要提高传输的效率，而用三次恰恰可以满足以上两方面的需求！ 然后，要明确TCP连接握手，握的是啥？ 答案：通信双方数据原点的序列号！ 我们在上面一篇文章知道，消息的完整是靠给每个消息包搞一个编号，依次地ACK确认。确认机制是累计的，意味着 X 序列号之前(不包括 X) 包都是被确认接收到的。 TCP可靠传输的精髓：TCP连接的一方A，由操作系统动态随机选取一个32位长的序列号（Initial Sequence Number）。 假设A的初始序列号为1000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，1001，1002，1003…，并把自己的初始序列号ISN告诉B。 让B有一个思想准备，什么样编号的数据是合法的，什么编号是非法的，比如编号900就是非法的，同时B还可以对A每一个编号的字节数据进行确认。 如果A收到B确认编号为2001，则意味着字节编号为1001-2000，共1000个字节已经安全到达。 同理B也是类似的操作，假设B的初始序列号ISN为2000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，2001，2002，2003…，并把自己的初始序列号ISN告诉A，以便A可以确认B发送的每一个字节。如果B收到A确认编号为4001，则意味着字节编号为2001-4000，共2000个字节已经安全到达。 好了，在理解了握手的本质之后，下面就可以总结上面图的握手过程了。 对于A与B的握手过程，可以总结为： A 发送同步信号SYN + A's Initial sequence number（丢失会A会重传） B 确认收到A的同步信号，并记录 A's ISN 到本地，命名 B's ACK sequence number B发送同步信号SYN + B's Initial sequence number （丢失B会周期性超时重传，直到收到A的确认） A确认收到B的同步信号，并记录 B's ISN 到本地，命名 A's ACK sequence number 很显然2和3 这两个步骤可以合并，只需要三次握手，可以提高连接的速度与效率。 这里就会引出一个问题，两次不行吗？ A 发送同步信号SYN + A’s Initial sequence number B发送同步信号SYN + B’s Initial sequence number + B’s ACK sequence number 这里有一个问题，A与B就A的初始序列号达成了一致，但是B无法知道A是否已经接收到自己的同步信号，如果这个同步信号丢失了，A和B就B的初始序列号将无法达成一致。 所以A必须再给B一个确认，以确认A已经接收到B的同步信号。 如果A发给B的确认丢了，该如何？ A会超时重传这个ACK吗？不会！TCP不会为没有数据的ACK超时重传。 那该如何是好？B如果没有收到A的ACK，会超时重传自己的SYN同步信号，一直到收到A的ACK为止。 写到这里，其实我们已经明白了，握手其实就是各自确认对方的序列号。因为后面的数据编号就会以此为基础，从而保证后续数据的可靠性。 谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 但是有的人指出，其实这只是表象，或者说并不是三次握手的设计初衷，我表示认同，这个防止已失效的连接请求报文段应该只是附加的一些好处，而不应该是解释为什么是三次握手的原因。 TCP初始阶段为什么是三次握手，原因总结如下： 为了实现可靠数据传输， TCP 协议的通信双方， 都必须维护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。 三次握手的过程即是通信双方相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤，如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认。 5.四次挥手 当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了； 但是，这个时候主机1还是可以接受来自主机2的数据； 当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的； 当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了； 主机1告诉主机2知道了，主机2收到这个确认之后就立马关闭自己。 主机1等待2MSL之后也关闭了自己。 针对最后一条消息，即主机1发送ack后，主机2接收到此消息，即认为双方达成了同步：双方都知道连接可以释放了，此时B可以安全地释放此TCP连接所占用的内存资源、端口号。 所以被动关闭的B无需任何wait time，直接释放资源。 但是主机1并不知道主机2是否接到自己的ACK，主机1是这么想的： 如果主机2没有收到自己的ACK，主机2会超时重传FiN，那么主机1再次接到重传的FIN，会再次发送ACK 如果主机2收到自己的ACK，也不会再发任何消息，包括ACK 无论是情况1还是2，A都需要等待，要取这两种情况等待时间的最大值，以应对最坏的情况发生，这个最坏情况是： 主机2没有收到主机1的ACK，那么超时之后主机2会重传FIN，也就是说，要浪费一个主机1发出ACK的最大存活时间(MSL)+FIN消息的最大存活时间(MSL) 不可能时间再多了，这个已经针对最糟糕的状况。 等待2MSL时间，A就可以放心地释放TCP占用的资源、端口号，此时可以使用该端口号连接任何服务器。 在等待的时间内，主机2可以重试多次，因为2MSL时间为240秒，超时重传只有0.5秒，1秒，2秒，，16秒。 当主机2重试次数达到上限，主机2会reset连接。 那么为什么是2MSL我们已经了解了，但是为什么要等这个时间呢？ 如果不等，释放的端口可能会重连刚断开的服务器端口，这样依然存活在网络里的老的TCP报文可能与新TCP连接报文冲突，造成数据冲突，为避免此种情况，需要耐心等待网络老的TCP连接的活跃报文全部死翘翘，2MSL时间可以满足这个需求。 整理自： https://www.zhihu.com/question/24853633 https://www.zhihu.com/question/67013338 https://github.com/jawil/blog/issues/14 https://www.jianshu.com/p/9968b16b607e]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议入门]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F4.TCP%E5%8D%8F%E8%AE%AE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第四篇文章。首先要明确：信道本身不可靠（丢包、重复包、出错、乱序），不安全。所以引出了七层或五层模型来保证。因此，任何一个东西的提出都是为了解决某个问题的。学习计算机，从他的历史出发，理解为什么会有不断低迭代，因为是为了解决某个痛点问题。比如HTTP的发展，为什么在HTTP1.0基础上还要提出HTTP1.1，为什么还要提出HTTP2.0，我们学习了他的发展历史之后就会明白了。同样，下面再说一说为什么要有TCP协议，TCP到底解决了什么问题。 一、回顾 首先简单回顾一下。 1.1 物理层 物理层是相当于物理连接，将0101以电信号的形式传输出去。 1.2 数据链路层 数据链路层，有一个叫做以太网协议，规定了电子信号是如何组成数据包的，这个协议的头里面，包含了自身的网卡信息，还有目的地的网卡信息（一般我们可以知道对方的IP，IP可以通过DNS解析到，然后根据ARP协议将IP转换为MAC地址）。那么，如果在同一个局域网内，我们就可以通过广播的方式找到对应MAC地址的主机。—即以太网协议解决了子网内部的点对点通信。 1.3 网络层 但是呢，以太网协议不能解决多个局域网通信，每个局域网之间不是互通的，那么以太网这种广播的方式不可用，就算可用，网络那么大，通过广播进行找，是一个可怕的场景。那么，IP协议可以连接多个局域网，简单来说，IP 协议定义了一套自己的地址规则，称为 IP 地址。物理器件，比如说路由器，就是基于IP协议，里面保存一套地址指路牌，想去哪个局域网，可以通过这个牌子来找，然后逐步路由到目标局域网，最后就可以找到那台主机了。IP层就是对应了网络层。 1.4 传输层 那么，此时解决了多个局域网路由问题，也解决了局域网内寻址问题，即我这台主机已经可以找到那台主机了，下面还有什么事情需要做呢？显然，找到主机还不行啊，比如我用微信发一条消息，我发到你主机了，但是你主机上的微信不知道这条消息发给他了，这里说的就是端口，信息要发到这个端口上，监听这个端口的程序才会收到消息。 1.5 应用层 OK，最上层的应用层，就是最贴近用户的，他的一系列协议只是为了让两台主机会互相都理解而已。 二、问题和解决 2.1 存在的问题 在明白了计算机网络为什么要这几层模型之后，我们再回到一开始，如何保证安全、可靠、完整地传输信息呢？ 很显然，上面提到的，只是保证信息能找到对方主机和端口，但是这个信息中途被拦截了、甚至被篡改了、信息延迟了（几分钟或者几个小时，或者几个世纪）、网络不通或者挂了，信息自己可不会告诉你他挂了或者要迟到一会，如果没有一个协议来保障可靠性，那么我这条消息发出去，能不能到、能不能及时到、能不能完整到、能不能不被篡改到等这些问题将会造成灾难，网络传输也就没有了意义。 计算机的前辈们，为我们提供了一系列的措施来尽可能保证信息能正确送达。 2.2 数据校验 首先在数据链路层，可以通过各种校验，比如奇偶校验等手段来判断数据包传的是否正确。 2.3 数据可靠性 好了，解决了数据是否正确之后，但是还不能保证线路是可靠的，加入某个包没发出去或者发错了，应该有一个出错重传机制，保证信息传输的可靠性。这就引出了TCP协议。 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 下面来看看TCP是如何解决丢包、重复包、出错、乱序等不可靠的一些问题的。 三、滑动窗口协议的提出 这就引出了TCP中最终的一个东西：滑动窗口协议。 3.1 朴素的方法来确保可靠性 先从简单的角度出发，自己想一下，如何保证不丢包、不乱序。 按照顺序，发送一个确认一个，显然，吞吐量非常低。那么，一次性发几个包，然后一起确认呢？ 3.2 改进方案 那么就引出第二个问题，我一次性发几个包合适呢？就这引出了滑动窗口。 四、数据包编号和重传机制 4.1 数据包编号 在说明滑动窗口原理之前，必须要说一下TCP数据包的编号SEQ。 我们知道，由于以太网数据包大小限制，所以每个包承载的数据是有限的，如果发一个很大的包，必然是要拆分的。 发送的时候，TCP 协议为每个包编号（sequence number，简称 SEQ），以便接收的一方按照顺序还原。万一发生丢包，也可以知道丢失的是哪一个包。 第一个包的编号是一个随机数。为了便于理解，这里就把它称为1号包。假定这个包的负载长度是100字节，那么可以推算出下一个包的编号应该是101。这就是说，每个数据包都可以得到两个编号：自身的编号，以及下一个包的编号。接收方由此知道，应该按照什么顺序将它们还原成原始文件。 4.2 数据重传机制 TCP协议就是根据这些编号来重新还原文件的。并且接收端保证顺序性返回ACK确认，比如有两个包发过去，为1号和2号，2号接收成功，但是发现1号包还没接收到，所以2号的ACK是不会发回去的，这个时候，如果在重传时间内收到1号了，那么就把这两个包的ACK都返回回去，如果超时了，就重传1号包。知道1号包接收成功，后续的才会返回ACK。 具体是如何做到的呢？ 前面说过，每一个数据包都带有下一个数据包的编号。如果下一个数据包没有收到，那么 ACK 的编号就不会发生变化。 举例来说，现在收到了4号包，但是没有收到5号包。ACK 就会记录，期待收到5号包。过了一段时间，5号包收到了，那么下一轮 ACK 会更新编号。如果5号包还是没收到，但是收到了6号包或7号包，那么 ACK 里面的编号不会变化，总是显示5号包。这会导致大量重复内容的 ACK。 如果发送方发现收到三个连续的重复 ACK，或者超时了还没有收到任何 ACK，就会确认丢包，即5号包遗失了，从而再次发送这个包。通过这种机制，TCP 保证了不会有数据包丢失。 （图片说明：Host B 没有收到100号数据包，会连续发出相同的 ACK，触发 Host A 重发100号数据包。） 五、慢启动 下面再来说说慢启动。 服务器发送数据包，当然越快越好，最好一次性全发出去。但是，发得太快，就有可能丢包。带宽小、路由器过热、缓存溢出等许多因素都会导致丢包。线路不好的话，发得越快，丢得越多。 最理想的状态是，在线路允许的情况下，达到最高速率。但是我们怎么知道，对方线路的理想速率是多少呢？答案就是慢慢试。 TCP 协议为了做到效率与可靠性的统一，设计了一个慢启动（slow start）机制。开始的时候，发送得较慢，然后根据丢包的情况，调整速率：如果不丢包，就加快发送速度；如果丢包，就降低发送速度。 Linux 内核里面设定了（常量TCP_INIT_CWND），刚开始通信的时候，发送方一次性发送10个数据包，即&quot;发送窗口&quot;的大小为10。然后停下来，等待接收方的确认，再继续发送。 默认情况下，接收方每收到两个 TCP 数据包，就要发送一个确认消息。&quot;确认&quot;的英语是 acknowledgement，所以这个确认消息就简称 ACK。 ACK 携带两个信息： 期待要收到下一个数据包的编号 接收方的接收窗口的剩余容量 发送方有了这两个信息，再加上自己已经发出的数据包的最新编号，就会推测出接收方大概的接收速度，从而降低或增加发送速率。这被称为&quot;发送窗口&quot;，这个窗口的大小是可变的。 我们可以知道，发送发和接收方都维护了一个缓冲区，可以理解为窗口。根据接收速度可以调整发送速度，逐渐达到这条线路最高的传输速率。 ok，下面就可以研究一下滑动窗口了。 六、滑动窗口原理 正常情况下： （如图，123表示已经正常发送并且收到了ACK确认。4567属于已发送但是还没有收到ACK。8910表示待发送。这个窗口当前长度为7.正常情况下，4号包收到ACK，那么窗口就会右移一格。） 但是往往会出现一些问题，比如5号包迟迟收不到ACK，在接收端可能是没有收到5号包，但是可能会收到6号包甚至是7、8号包，那么此时只能等待5号包，如果5号包顺利到达了，那么就把5678号包的ACK都发给发送端，那么发送端滑动窗口向右右移四格。如果迟迟收不到5号包，只能重传。 以上就是关于TCP中比较重要的出错重传、编号、慢启动以及滑动窗口。这些保证了数据传输的可靠性。 整理自：http://www.ruanyifeng.com/blog/2017/06/tcp-protocol.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http的前世今生]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F3.http%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第三篇文章。作为一个后端攻城狮，每天打交道最多的就是HTTP协议，在如今火热的微服务实现方案中，除了阿里的dubbo，就是spring cloud，而spring cloud目前适用的服务间通信方式也就是基于HTTP 的 restful接口来实现。并且作为浏览器上用的最多的协议，无论是前端、后端还是测试都应该去熟悉它，软件的发展是循序渐进的，每次的迭代升级都是为了解决上一版本的痛点，HTTP协议的发展也是如此，本章着重讲解HTTP的前世今生，让我们更加了解HTTP。 HTTP 是基于 TCP/IP 协议的应用层协议。它不涉及数据包（packet）传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。 一、HTTP/0.9 1.1 简介 这是第一个定稿的HTTP协议。 内容非常简单，只有一个命令GET 没有HEADER等描述数据的信息 服务器发送完毕，就关闭TCP连接（一个HTTP请求在一个TCP连接中完成） 1.2 请求格式 比如发起一个GET请求： GET /index.html 上面命令表示，TCP 连接（connection）建立后，客户端向服务器请求（request）网页index.html。 1.3 响应格式 协议规定，服务器只能回应HTML格式的字符串，不能回应别的格式。 123&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 服务器发送完毕，就关闭TCP连接。 二、HTTP/1.0 2.1 简介 跟现在比较普遍适用的1.1版本已经相差不多。 增加很多命令，比如POST、HEAD等命令 增加status code 和 header 多字符集支持、多部分发送、权限、缓存等 首先，任何格式的内容都可以发送。这使得互联网不仅可以传输文字，还能传输图像、视频、二进制文件。这为互联网的大发展奠定了基础。 其次，除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。 再次，HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。 其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 2.2 请求格式 下面是一个1.0版的HTTP请求的例子。 123GET / HTTP/1.0User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: */* 可以看到，这个格式与0.9版有很大变化。 第一行是请求命令，必须在尾部添加协议版本（HTTP/1.0）。后面就是多行头信息，描述客户端的情况。 客户端请求的时候，可以使用Accept字段声明自己可以接受哪些数据格式。上面代码中，客户端声明自己可以接受任何格式的数据。 2.3 响应格式 服务器的回应如下： 12345678910HTTP/1.0 200 OK Content-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 回应的格式是&quot;头信息 + 一个空行（\r\n） + 数据&quot;。其中，第一行是&quot;协议版本 + 状态码（status code） + 状态描述&quot;。 2.4 Content-Type 字段 关于字符的编码，1.0版规定，头信息必须是 ASCII 码，后面的数据可以是任何格式。因此，服务器回应的时候，必须告诉客户端，数据是什么格式，这就是Content-Type字段的作用。 2.5 缺点 每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。 为了解决这个问题，有些浏览器在请求时，用了一个非标准的Connection字段。 Connection: keep-alive 这个字段要求服务器不要关闭TCP连接，以便其他请求复用。服务器同样回应这个字段。 Connection: keep-alive 一个可以复用的TCP连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段，不同实现的行为可能不一致，因此不是根本的解决办法。 三、HTTP/1.1 3.1 持久连接和管道机制 持久连接（以前的版本中，一个HTTP请求就创建一个TCP连接，请求返回之后就关闭TCP连接，然而建立一次TCP连接的过程是比较耗时的，效率会比较低，现在建立一个TCP连接后，后面的HTTP请求都可以复用这个TCP连接，即允许了在同一个连接里面发送多个请求，会提高效率） pipeline（解决了同一个TCP连接中客户端可以发送多个HTTP请求，但是对于服务端来说，对于进来的请求要按照顺序进行内容的返回，如果前一个请求处理时间长，而后一个请求处理时间端，即便后面一个请求已经处理完毕了，也要等待前一个请求处理完毕返回他才可以返回结果，这种串行的方式比较慢） 在1.1版本以前，每次HTTP请求，都会重新建立一次TCP连接，服务器响应后，就立刻关闭。众所周知，建立TCP连接的新建成本很高，因为需要三次握手，并且有着慢启动的特性导致发送速度较慢。而1.1版本添加的持久连接功能可以让一次TCP连接中发送多条HTTP请求，值得一提的是默认是，控制持久连接的Connection字段默认值是keep-alive，也就是说是默认打开持久连接，如果想要关闭，只需将该字段的值改为close。 Connection: close 而管道化则赋予了客户端在一个TCP连接中连续发送多个请求的能力，而不需要等到前一个请求响应，这大大提高了效率。值得一提的是，虽然客户端可以连续发送多个请求，但是服务器返回依然是按照发送的顺序返回。（强调的是request不需要等待上一个request的response，其实发送的request还是有顺序的，服务端按照这个顺序接收，依次返回响应） HTTP/1.1允许多个http请求通过一个套接字同时被输出 ，而不用等待相应的响应。然后请求者就会等待各自的响应，这些响应是按照之前请求的顺序依次到达。（me：所有请求保持一个FIFO的队列，一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出；同时，服务器端返回这些请求的响应时也是按照FIFO的顺序）。管道化的表现可以大大提高页面加载的速度，尤其是在高延迟连接中。 3.2 Content-Length 字段 一个TCP连接现在可以传送多个回应，势必就要有一种机制，区分数据包是属于哪一个回应的。这就是Content-length字段的作用，声明本次回应的数据长度。 Content-Length: 3495 上面代码告诉浏览器，本次回应的长度是3495个字节，后面的字节就属于下一个回应了。 在1.0版中，Content-Length字段不是必需的，因为浏览器发现服务器关闭了TCP连接，就表明收到的数据包已经全了。 3.3 分块传输编码 对于一些很耗时的动态操作来说，这意味着，服务器要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用&quot;流模式&quot;（stream）取代&quot;缓存模式&quot;（buffer）。 因此，1.1版规定可以不使用Content-Length字段，而使用&quot;分块传输编码&quot;（chunked transfer encoding）。只要请求或回应的头信息有Transfer-Encoding字段，就表明回应将由数量未定的数据块组成。 Transfer-Encoding: chunked 每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后是一个大小为0的块，就表示本次回应的数据发送完了。下面是一个例子。 1234567891011121314151617HTTP/1.1 200 OKContent-Type: text/plainTransfer-Encoding: chunked25This is the data in the first chunk1Cand this is the second one3con8sequence0 3.3 其他功能 1.1版还新增了许多动词方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。 另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名。 Host: www.example.com 有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础。 3.4 缺点 虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为&quot;队头堵塞&quot;（Head-of-line blocking）。 四、SPDY 协议 2009年，谷歌公开了自行研发的 SPDY 协议，主要解决 HTTP/1.1 效率不高的问题。 这个协议在Chrome浏览器上证明可行以后，就被当作 HTTP/2 的基础，主要特性都在 HTTP/2 之中得到继承。 五、HTTP/2 5.1 二进制协议 HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。 HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为&quot;帧&quot;（frame）：头信息帧和数据帧。 二进制协议的一个好处是，可以定义额外的帧。HTTP/2 定义了近十种帧，为将来的高级应用打好了基础。如果使用文本实现这种功能，解析数据将会变得非常麻烦，二进制解析则方便得多。 5.2 多工 HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了&quot;队头堵塞&quot;。 举例来说，在一个TCP连接里面，服务器同时收到了A请求和B请求，于是先回应A请求，结果发现处理过程非常耗时，于是就发送A请求已经处理好的部分， 接着回应B请求，完成后，再发送A请求剩下的部分。 这样双向的、实时的通信，就叫做多工（Multiplexing）。 5.3 数据流 因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。 5.4 头信息压缩 HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如Cookie和User Agent，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。 HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息使用gzip或compress压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。 5.5 服务器推送 HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。 常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析HTML源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。 整理自：http://www.ruanyifeng.com/blog/2016/08/http.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从上到下看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F2.%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第二篇文章。经过上一篇文章的详细介绍，我们了解了一个数据是如何从物理层一步一步到达应用层的，那么本章从上而下的角度来看看一条请求时如何从浏览器传递到服务器并且返回的。 这个过程看的就是用户从浏览器输入一条url之后，是如何发送过去的。 1.上一篇文章的小结 我们已经知道，网络通信就是交换数据包。电脑A向电脑B发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样： 发送这个包，需要知道两个地址： 对方的MAC地址 对方的IP地址 有了这两个地址，数据包才能准确送到接收者手中。但是，前面说过，MAC地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的MAC地址，必须通过网关（gateway）转发。 上图中，1号电脑要向4号电脑发送一个数据包。它先判断4号电脑是否在同一个子网络，结果发现不是（后文介绍判断方法），于是就把这个数据包发到网关A。网关A通过路由协议，发现4号电脑位于子网络B，又把数据包发给网关B，网关B再转发到4号电脑。 1号电脑把数据包发到网关A，必须知道网关A的MAC地址。 发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的MAC地址。 有了一台新电脑之后，要想上网，一种方式是自己配置静态IP： 很多人都没有进行过这个配置，因为一般情况下我们根本不需要这样。但是有的时候也会用到，比如我在电信实习的时候，他们每一个网口旁边都贴着这四个参数，你联网必须要适用他提供的一系列地址才行。其实经过上面的学习，我们已经知道，通信的时候，需要知道对方的IP（ARP知道对方的MAC地址）、子网掩码（确定所在的子网）、默认网关（不在一个子网，要通过网关取转发、路由）、DNS服务器（解析域名为IP地址）。 但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用&quot;动态IP地址上网&quot;。 2.DHCP协议 所谓&quot;动态IP地址&quot;，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。 这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做&quot;DHCP服务器&quot;。新的计算机加入网络，必须向&quot;DHCP服务器&quot;发送一个&quot;DHCP请求&quot;数据包，申请IP地址和相关的网络参数。 前面说过，如果两台计算机在同一个子网络，必须知道对方的MAC地址和IP地址，才能发送数据包。但是，新加入的计算机不知道DHCP服务器的两个地址，怎么发送数据包呢？ DHCP协议做了一些巧妙的规定。 首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的： （1）最前面的&quot;以太网标头&quot;，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 （2）后面的&quot;IP标头&quot;，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。 （3）最后的&quot;UDP标头&quot;，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。 因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。 当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道&quot;这个包是发给我的&quot;，而其他计算机就可以丢弃这个包。 接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个&quot;DHCP响应&quot;数据包。 这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。 动态拿到最核心的四个参数：自己的IP地址、子网掩码、网关地址、DNS服务器，就可以联网了。 3.访问google的过程 我们假定，经过上一节的步骤，用户设置好了自己的网络参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。 这意味着，浏览器要向Google发送一个网页请求的数据包。 3.1 DNS协议 我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。 DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。 然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 3.2 子网掩码 接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。 已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。 3.3 应用层协议 浏览网页用的是HTTP协议，它的整个数据包构造是这样的： HTTP部分的内容，类似于下面这样： 123456789 GET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) ...... Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: ... ... 我们假定这个部分的长度为4960字节，它会被嵌在TCP数据包之中。 3.4 TCP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 3.5 IP协议 然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。 IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 3.6 以太网协议 最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 3.7 服务器响应 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。 根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的&quot;HTTP请求&quot;，接着做出&quot;HTTP响应&quot;，再用TCP协议发回来。 本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 整理自：http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从下到上看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F1.%E4%BB%8E%E4%B8%8B%E5%88%B0%E4%B8%8A%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第一篇文章。要想了解HTTP协议，必然要从最基本的计算机网络知识开始入手。本篇文章从下到上具体介绍五层经典模型，极速入门计算机网络。 经典五层模型 下面我们先来了解一下各层做的事情！ 1.物理层 电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。 这就叫做&quot;物理层&quot;，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 2.数据链路层 单纯的0和1没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？ 这就是&quot;链接层&quot;的功能，它在&quot;实体层&quot;的上方，确定了0和1的分组方式。 2.1 以太网协议 早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做&quot;以太网&quot;（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做&quot;帧&quot;（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。 &quot;标头&quot;包含数据包的一些说明项，比如发送者、接受者、数据类型等等；&quot;数据&quot;则是数据包的具体内容。 &quot;标头&quot;的长度，固定为18字节。&quot;数据&quot;的长度，最短为46字节，最长为1500字节。因此，整个&quot;帧&quot;最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。 2.2 MAC地址 上面提到，以太网数据包的&quot;标头&quot;，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？ 以太网规定，连入网络的所有设备，都必须具有&quot;网卡&quot;接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。 每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。 前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 2.3 广播 定义地址只是第一步，后面还有更多的步骤。 首先，一块网卡怎么会知道另一块网卡的MAC地址？ 回答是有一种ARP协议，可以解决这个问题。下面介绍ARP。 其次，就算有了MAC地址，系统怎样才能把数据包准确送到接收方？ 回答是以太网采用了一种很&quot;原始&quot;的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 上图中，1号计算机向2号计算机发送一个数据包，同一个子网络的3号、4号、5号计算机都会收到这个包。它们读取这个包的&quot;标头&quot;，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做&quot;广播&quot;（broadcasting）。 有了数据包的定义、网卡的MAC地址、广播的发送方式，&quot;链接层&quot;就可以在多台计算机之间传送数据了。 3.网络层 以太网协议，依靠MAC地址发送数据。理论上，单单依靠MAC地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。 但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一&quot;包&quot;，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。 互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。 因此，必须找到一种方法，能够区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用&quot;路由&quot;方式发送。（&quot;路由&quot;的意思，就是指如何向不同的子网络分发数据包，这是一个很大的主题，本文不涉及。）遗憾的是，MAC地址本身无法做到这一点。它只与厂商有关，与所处网络无关。 这就导致了&quot;网络层&quot;的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做&quot;网络地址&quot;，简称&quot;网址&quot;。 于是，&quot;网络层&quot;出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。 3.1 IP协议 规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。 目前，广泛采用的是IP协议第四版，简称IPv4。这个版本规定，网络地址由32个二进制位组成。 习惯上，我们用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。 互联网上的每一台计算机，都会分配到一个IP地址。这个地址分成两个部分，前一部分代表网络，后一部分代表主机。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。处于同一个子网络的电脑，它们IP地址的网络部分必定是相同的，也就是说172.16.254.2应该与172.16.254.1处在同一个子网络。 但是，问题在于单单从IP地址，我们无法判断网络部分。还是以172.16.254.1为例，它的网络部分，到底是前24位，还是前16位，甚至前28位，从IP地址上是看不出来的。 那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数&quot;子网掩码&quot;（subnet mask）。 所谓&quot;子网掩码&quot;，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.254.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。 知道&quot;子网掩码&quot;，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 比如，已知IP地址172.16.254.1和172.16.254.233的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，结果都是172.16.254.0，因此它们在同一个子网络。 总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。 3.2 IP数据包 根据IP协议发送的数据，就叫做IP数据包。不难想象，其中必定包括IP地址信息。 但是前面说过，以太网数据包只包含MAC地址，并没有IP地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？ 回答是不需要，我们可以把IP数据包直接放进以太网数据包的&quot;数据&quot;部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。 具体来说，IP数据包也分为&quot;标头&quot;和&quot;数据&quot;两个部分。&quot;标头&quot;部分主要包括版本、长度、IP地址等信息，&quot;数据&quot;部分则是IP数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。 IP数据包的&quot;标头&quot;部分的长度为20个字节，整个数据包的总长度最大为65,535字节。因此，理论上，一个IP数据包的&quot;数据&quot;部分，最长为65,515字节。前面说过，以太网数据包的&quot;数据&quot;部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。 3.3 ARP协议 关于&quot;网络层&quot;，还有最后一点需要说明。 因为IP数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的MAC地址，另一个是对方的IP地址。通常情况下，对方的IP地址是已知的，但是我们不知道它的MAC地址。 所以，我们需要一种机制，能够从IP地址得到MAC地址。 这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的&quot;网关&quot;（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个&quot;广播&quot;地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 4. 传输层 有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。 接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？ 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做&quot;端口&quot;（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 &quot;端口&quot;是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。 &quot;传输层&quot;的功能，就是建立&quot;端口到端口&quot;的通信。相比之下，“网络层&quot;的功能是建立&quot;主机到主机&quot;的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做&quot;套接字”（socket）。有了它，就可以进行网络应用程序开发了。 4.1 UDP协议 现在，我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。 UDP数据包，也是由&quot;标头&quot;和&quot;数据&quot;两部分组成。 &quot;标头&quot;部分主要定义了发出端口和接收端口，&quot;数据&quot;部分就是具体的内容。然后，把整个UDP数据包放入IP数据包的&quot;数据&quot;部分，而前面说过，IP数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样： UDP数据包非常简单，&quot;标头&quot;部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 4.2 TCP协议 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 为了解决这个问题，提高网络可靠性，TCP协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的UDP协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 因此，TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。 TCP数据包和UDP数据包一样，都是内嵌在IP数据包的&quot;数据&quot;部分。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 关于TCP细节以后再探讨。 5. 应用层 应用程序收到&quot;传输层&quot;的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。 &quot;应用层&quot;的作用，就是规定应用程序的数据格式。 举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了&quot;应用层&quot;。 这是最高的一层，直接面对用户。它的数据就放在TCP数据包的&quot;数据&quot;部分。因此，现在的以太网的数据包就变成下面这样。 *注：UDP头为8个字节，TCP头为20个字节 整理于：http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
</search>
