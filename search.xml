<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java多线程之传参和返回值处理]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2Fjava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E4%BC%A0%E5%8F%82%E5%92%8C%E8%BF%94%E5%9B%9E%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第二篇，对于传参和返回值的问题，是面试中关于多线程这一块问得比较多的问题了，这里进行详细的说明。 一、如何给run()方法传参 我们知道多线程是通过star()方法让线程处于准备就绪状态，而实际运行的业务逻辑是放在run()方法体中的，但是run()方法是没有参数的方法，实际的业务场景中，我们可能需要向方法体中传递参数，实现的方式主要有三种： 构造函数传参，这个在上一篇文章中已经演示了。 成员变量传参，这个就是依靠set方法。 回调函数传参，这个稍微特殊一点。这里说明一下。 上面的两种向线程中传递数据的方法是最常用的。但这两种方法都是main方法中主动将数据传入线程类的。这对于线程来说，是被动接收这些数据的。 然而，在有些应用中需要在线程运行的过程中动态地获取数据，如在下面代码的run方法中产生了3个随机数，然后通过Work类的process方法求这三个随机数的和，并通过Data类的value将结果返回。从这个例子可以看出，在返回value之前，必须要得到三个随机数。也就是说，这个 value是无法事先就传入线程类的。 123456789101112131415161718192021222324252627282930313233343536373839class Data &#123; public int value = 0 ;&#125;class Work &#123; public void process(Data data, Integer[] numbers) &#123; for ( int n : numbers) &#123; data.value += n; &#125; &#125;&#125;public class MyThread3 extends Thread &#123; private Work work; public MyThread3(Work work) &#123; this .work = work; &#125; public void run() &#123; //1.随机生成3个数放进数组中 java.util.Random random = new java.util.Random(); Data data = new Data(); int n1 = random.nextInt( 1000 ); int n2 = random.nextInt( 2000 ); int n3 = random.nextInt( 3000 ); Integer[] numbers = new Integer[3]; numbers[0] = n1; numbers[1] = n2; numbers[2] = n3; //调用函数去计算这三个数之和，计算的结果存在Data实例中的value属性中 //这里process相当于回调函数，我调用这个函数，给我一个计算结果 work.process(data, numbers); System.out.println(String.valueOf(n1) + "+" + String.valueOf(n2) + "+" + String.valueOf(n3) + "=" + data.value); &#125; public static void main(String[] args) &#123; Thread t = new MyThread3( new Work()); t.start(); &#125;&#125; 其中一次的执行结果为： 1707+678+173=1558 在上面代码中的process方法被称为回调函数。从本质上说，回调函数就是事件函数。在Windows API中常使用回调函数和调用API的程序之间进行数据交互。因此，调用回调函数的过程就是最原始的引发事件的过程。在这个例子中调用了process方法来获得数据也就相当于在run方法中引发了一个事件。 二、如何处理线程返回值 由于线程相当于一个异步的处理函数，想要获取它的结果就不能像传统的获取它的return的值那么简单了，主要问题就在于它什么时候能处理好是不知道的，需要一定的机制去等待它处理好了再去获取它的处理结果。方式一般有三种。 2.1 主线程等待法 这个方法是最简单也是最容易想到的处理方式。下面搞个实例来看看大概是如何操作的。 首先写一个类，写这个的含义是，假如主线程不等待，将会一口气执行到最后一行，此时子线程可能还没执行完。就会出现打印空。 那么我们的主线程如何获取到子线程中赋予的值呢？一种方式就是死等，不停地轮询看你的值是否已经计算好了，一旦计算好就可以拿到这个值。类似于以下： 其实这就是自旋，即CPU停在这里等待，不能干其他事情，这必然会大大浪费CPU资源，所以虽然这种方式实现起来非常简单，但是不适合用。另外的缺点就是代码臃肿，比如我要等待的值不止一个，有多个，那是不是要写多个while循环来等待呢？此外，我们大多时候根本不知道这个子线程到底要执行多久，因为我们这里是每隔100毫秒轮询一次，那假如这个值在这100毫秒内值已经有了，那么是不能立即获取的。 针对以上不能精准控制的缺点，这里便有了第二种方法。 Join方法 Thread类中的join方法可以阻塞当前线程以等待子线程处理完毕。 在这里，由于是在主线程中调用的join，所以阻塞主线程，让子线程执行完毕再继续执行。 这种方法更简单，但是存在多个子线程的情况下，做到灵活以及精准控制是做不到的。 Callable接口实现 JAVA提供了有返回值的任务，即实现了Callable接口的任务，执行这个任务之后可以获取一个叫做Futrue的对象，通过get()就可以获取Callable任务返回的内容。 具体是如何获取返回的内容呢？有两种方式，一个是通过FutureTask这个类来获取，一个是通过线程池获取。 对于第一种方式，我们通过例子来理解。 先新建一个实现了Callable接口的任务： 把Callable任务放进FutureTask中，这个FutureTask再放进Thread中去执行： 发现我们的程序并没有显示地等待，FutureTask的get()方法完成了等待和获取返回值。下面来看看Future的继承关系： 我们发现，FutureTask实质上都是Runnable接口的实例，只是它还是Futrue接口的实例，所以不仅可以作为一个线程任务被执行，还可以接受一个Callable接口去接受它的返回值。因此是一个升级版的Runnable实例。 说完了FutureTask的实现方式，下面再来看看另一种方式，即线程池来实现。关于线程池，后文还会详细介绍，这里只是简单先运用一下。 达到了一样的效果。我们来分析分析。 我们发现，其实两种方式的根本就是Future这个接口，第一种是直接用了FutureTask这个类来手动实现，即不仅需要它接收一个Callable任务，还需要将其作为一个线程任务去手动执行。而第二种方式就比较简单了，有了线程池，我直接把Callable任务扔线程池去submit，就可以得到一个可以获取返回值的Future类型对象，就可以根据这个对象获取到值了。 所以两种方式本质上是一样一样的。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程基本知识梳理]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2F%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第一篇，主要说明基本概念，这是面试中最基本的要会的东西，如果这些都回答不了，基本上就没有机会了，本文从源码稍微深入一点去探讨常见的基本概念。本文并不会从最最最最基本的知识开始说起，将不费笔墨直击要害，所以需要一点多线程的基本知识才行，这也符合本博客的宗旨，即知识点再次提炼和升级。 一、进程和线程的区别 这一块详见 面试-进程与线程 里面的内容，相信已经够用了。 二、start()和run()方法的区别 以一个小例子入手，在主函数中尝试新建一个线程，并且以t.run()的形式去调用，从结果可以看出，java默认开启主线程来执行，当我们用t.run()去执行的时候，只是相当于简单的函数调用，因为从打印结果可以看出都是main进程，那么，实质上并没有新建一个子线程。 （注意，不是一调用就会去执行，而是说这个线程处于就绪状态，将有资格获得CPU的临幸，关于线程状态，后文会再次详细说明，关于start之后处于就绪状态这一点默认读者是清楚的，下面表述可能不会太顾及说明这一点）： 那么，从表象上我们已经知道，run只是简单的函数调用，start才会真正地开启一个新线程来执行，下面从源码层面来看看start()的基本实现方式。 说明一下，本源码是基于JDK1.8，我们看到它的核心实现是一个native方法，IDEA上已经看不了，只好去看看openJDK了。 直接打开网址： Thread.c 我们可以看到： 我们看到很多关于线程的方法，但是这里是看不到具体的实现的，我们看到上面引入了jvm.h的库，所以实现应该是在jvm相关的代码中，直接点开： jvm.cpp 可以看到如下： emmm，虽然不大看得懂，但是我们确实看到了start()会调用虚拟机去创建一个新的线程，最终再去调用run方法去执行。所以流程如下： 最终总结： 调用start()方法会创建一个新的子线程并启动 run()方法只是thread的一个普通方法的调用 三、Thread和Runnable是什么关系 还是老规矩，先来翻翻源码： 我们可以看到，Thread是一个class，而Runnable是一个interface，而Runnable中只有一个抽象方法就是run(). 那么，我们上面说到，新建一个线程是要靠start()来实现的，那么Runnable是如何来新建一个线程呢？它不是只有一个run()方法吗？ 此时再来看Thread类，它里面有大量的方法，就包含了run()和start()方法，它还有一个重要的构造函数为: 1public Thread(Runnable target) &#123;...&#125; 就是说，传入Runable接口实例，再调用Thread的start()方法创建子线程，再来调用重写的run()方法就可以了。下面举个例子。 先说说用Thread的方式来创建一个子线程类： 这也从侧面证明了，线程是交替执行的，但是因为属于同一个进程，共享同一个地址和资源，所以不需要进行切换，极大提高了CPU执行效率。 下面再来看看Runnable接口是怎么实现多线程的： 总结一下他们俩： Thread是一个类，Runnable是一个接口，前者实现后者 Thread有start方法，结合run()可以实现多线程，但是Runnable没有start()方法，所以要通过Thread()来实现，所以，两种方式最终都是通过Thread的start()来实现run()的多线程特性 由于JAVA是单一继承的，所以推荐多使用Runnable接口]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实例说明类加载过程]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E5%AE%9E%E4%BE%8B%E8%AF%B4%E6%98%8E%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十三篇文章，本文从一个简单程序入手，将前面所学串联起来，详细来看看类加载的过程到底是什么样子的。 零、Java虚拟机启动、加载类过程分析 下面我将定义一个非常简单的java程序并运行它，来逐步分析java虚拟机启动的过程。 12345678910111213package org.luanlouis.jvm.load; import sun.security.pkcs11.P11Util; public class Main&#123; public static void main(String[] args) &#123; System.out.println("Hello,World!"); ClassLoader loader = P11Util.class.getClassLoader(); System.out.println(loader); &#125; &#125; 在windows命令行下输入： java org.luanlouis.jvm.load.Main 当输入上述的命令时： windows开始运行{JRE_HOME}/bin/java.exe程序，java.exe 程序将完成以下步骤： 1.根据JVM内存配置要求，为JVM申请特定大小的内存空间； 2.创建一个引导类加载器实例，初步加载系统类到内存方法区区域中； 3.创建JVM 启动器实例 Launcher,并取得类加载器ClassLoader； 4.使用上述获取的ClassLoader实例加载我们定义的 org.luanlouis.jvm.load.Main类； 5.加载完成时候JVM会执行Main类的main方法入口，执行Main类的main方法； 6.结束，java程序运行结束，JVM销毁。 下面逐一分析一下这几个步骤。 一、根据JVM内存配置要求，为JVM申请特定大小的内存空间 JVM内存按照功能上的划分，可以粗略地划分为方法区(Method Area) 和堆(Heap),而所有的类的定义信息都会被加载到方法区中。 二、创建一个引导类加载器实例，初步加载系统类到内存方法区区域中 JVM申请好内存空间后，JVM会创建一个引导类加载器（Bootstrap Classloader）实例，引导类加载器是使用C++语言实现的，负责加载JVM虚拟机运行时所需的基本系统级别的类，如java.lang.String, java.lang.Object等等。 引导类加载器(Bootstrap Classloader)会读取 {JRE_HOME}/lib 下的jar包和配置，然后将这些系统类加载到方法区内。 本例中，引导类加载器是用 {JRE_HOME}/lib加载类的，不过，你也可以使用参数 -Xbootclasspath 或 系统变量sun.boot.class.path来指定的目录来加载类。 一般而言，{JRE_HOME}/lib下存放着JVM正常工作所需要的系统类，如下表所示： 文件名 描述 rt.jar 运行环境包，rt即runtime，J2SE 的类定义都在这个包内 charsets.jar 字符集支持包 jce.jar 是一组包，它们提供用于加密、密钥生成和协商以及 Message Authentication Code（MAC） jsse.jar 安全套接字拓展包Java™ Secure Socket Extension classlist 该文件内表示是引导类加载器应该加载的类的清单 net.properties JVM 网络配置信息 引导类加载器(Bootstrap ClassLoader） 加载系统类后，JVM内存会呈现如下格局： 引导类加载器将类信息加载到方法区中，以特定方式组织，对于某一个特定的类而言，在方法区中它应该有 运行时常量池、类型信息、字段信息、方法信息、类加载器的引用，对应class实例的引用等信息。 类加载器的引用,由于这些类是由引导类加载器(Bootstrap Classloader)进行加载的，而 引导类加载器是由C++语言实现的，所以是无法访问的，故而该引用为NULL 对应class实例的引用， 类加载器在加载类信息放到方法区中后，会创建一个对应的Class 类型的实例放到堆(Heap)中, 作为开发人员访问方法区中类定义的入口和切入点。 三、创建JVM 启动器实例 Launcher,并取得类加载器ClassLoader 上述步骤完成，JVM基本运行环境就准备就绪了。接着，我们要让JVM工作起来了：运行我们定义的程序 org.luanlouis,jvm.load.Main。 此时，JVM虚拟机调用已经加载在方法区的类sun.misc.Launcher 的静态方法getLauncher(), 获取sun.misc.Launcher 实例： 1234//获取Java启动器 sun.misc.Launcher launcher = sun.misc.Launcher.getLauncher(); //获取类加载器ClassLoader用来加载class到内存来 ClassLoader classLoader = launcher.getClassLoader(); sun.misc.Launcher 使用了单例模式设计，保证一个JVM虚拟机内只有一个sun.misc.Launcher实例。 在Launcher的内部，其定义了两个类加载器(ClassLoader),分别是sun.misc.Launcher.ExtClassLoader和sun.misc.Launcher.AppClassLoader，这两个类加载器分别被称为拓展类加载器(Extension ClassLoader) 和 应用类加载器(Application ClassLoader).如下图所示： 四、使用类加载器ClassLoader加载Main类 通过 launcher.getClassLoader()方法返回AppClassLoader实例，接着就是AppClassLoader加载 org.luanlouis.jvm.load.Main类的时候了。 12lassLoader classloader = launcher.getClassLoader();//取得AppClassLoader类 classLoader.loadClass("org.luanlouis.jvm.load.Main");//加载自定义类 上述定义的org.luanlouis.jvm.load.Main类被编译成org.luanlouis.jvm.load.Main class二进制文件，这个class文件中有一个叫常量池(Constant Pool)的结构体来存储该class的常量信息。常量池中有CONSTANT_CLASS_INFO类型的常量，表示该class中声明了要用到那些类： 当AppClassLoader要加载 org.luanlouis.jvm.load.Main类时，会去查看该类的定义，发现它内部声明使用了其它的类： sun.security.pkcs11.P11Util、java.lang.Object、java.lang.System、java.io.PrintStream、java.lang.Class；org.luanlouis.jvm.load.Main类要想正常工作，首先要能够保证这些其内部声明的类加载成功。所以AppClassLoader要先将这些类加载到内存中。（注：为了理解方便，这里没有考虑懒加载的情况，事实上的JVM加载类过程比这复杂的多） 加载顺序： 加载java.lang.Object、java.lang.System、java.io.PrintStream、java,lang.Class AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载； 而ExtClassLoader发现不是其加载范围，其返回null； AppClassLoader发现父类加载器ExtClassLoader无法加载， 则会查询这些类是否已经被BootstrapClassLoader加载过， 结果表明这些类已经被BootstrapClassLoader加载过， 则无需重复加载，直接返回对应的Class&lt;T&gt;实例； 加载sun.security.pkcs11.P11Util 此在{JRE_HOME}/lib/ext/sunpkcs11.jar包内，属于ExtClassLoader负责加载的范畴。 AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载； 而ExtClassLoader发现其正好属于加载范围，故ExtClassLoader负责将其加载到内存中。 ExtClassLoader在加载sun.security.pkcs11.P11Util时也分析这个类内都使用了哪些类， 并将这些类先加载内存后，才开始加载sun.security.pkcs11.P11Util， 加载成功后直接返回对应的Class&lt;sun.security.pkcs11.P11Util&gt;实例； 加载org.luanlouis.jvm.load.Main AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载； 而ExtClassLoader发现不是其加载范围，其返回null； AppClassLoader发现父类加载器ExtClassLoader无法加载， 则会查询这些类是否已经被BootstrapClassLoader加载过。 而结果表明BootstrapClassLoader 没有加载过它， 这时候AppClassLoader只能自己动手负责将其加载到内存中， 然后返回对应的Class&lt;org.luanlouis.jvm.load.Main&gt;实例引用； 以上三步骤都成功，才表示classLoader.loadClass(&quot;org.luanlouis.jvm.load.Main&quot;)完成，上述操作完成后，JVM内存方法区的格局会如下所示： 如上图所示： JVM方法区的类信息区是按照类加载器进行划分的，每个类加载器会维护自己加载类信息； 某个类加载器在加载相应的类时，会相应地在JVM内存堆（Heap）中创建一个对应的Class&lt;T&gt;，用来表示访问该类信息的入口 五、使用Main类的main方法作为程序入口运行程序 就是去执行指令，过程与Java如何执行一个最简单的程序类似。 六、方法执行完毕，JVM销毁，释放内存 对于本程序，主程序执行完毕，释放主函数所在的栈帧，释放堆中的内存。 七、再来回顾回顾java类加载器相关的概念吧 本处的内容为再次简单说明，具体见双亲委派模型。类加载器(Class Loader)：顾名思义，指的是可以加载类的工具。JVM自身定义了三个类加载器：引导类加载器(Bootstrap Class Loader)、拓展类加载器(Extension Class Loader )、应用加载器(Application Class Loader)。当然，我们有时候也会自己定义一些类加载器来满足自身的需要。 引导类加载器(Bootstrap Class Loader): 该类加载器使JVM使用C++/C底层代码实现的加载器，用以加载JVM运行时所需要的系统类，这些系统类在{JRE_HOME}/lib目录下。由于类加载器是使用平台相关的底层C++/C语言实现的， 所以该加载器不能被Java代码访问到。但是，我们可以查询某个类是否被引导类加载器加载过。我们经常使用的系统类如：java.lang.String,java.lang.Object,java.lang*… 这些都被放在 {JRE_HOME}/lib/rt.jar包内， 当JVM系统启动的时候，引导类加载器会将其加载到 JVM内存的方法区中。 拓展类加载器(Extension Class Loader): 该加载器是用于加载 java 的拓展类 ，拓展类一般会放在{JRE_HOME}/lib/ext/ 目录下，用来提供除了系统类之外的额外功能。拓展类加载器是是整个JVM加载器的Java代码可以访问到的类加载器的最顶端，即是超级父加载器，拓展类加载器是没有父类加载器的。（注意，其实引导类加载器不能算是扩展类加载器的父类，我们从源码中可以看出来的） 应用类加载器(Applocatoin Class Loader): 该类加载器是用于加载用户代码，是用户代码的入口。我经常执行指令 java xxx.x.xxx.x.x.XClass , 实际上，JVM就是使用的AppClassLoader加载 xxx.x.xxx.x.x.XClass 类的。 用户自定义类加载器（Customized Class Loader）：用户可以自己定义类加载器来加载类。所有的类加载器都要继承java.lang.ClassLoader类。 关于双亲委派模型，就不再赘述了。 八、线程上下文加载器 Java 任何一段代码的执行，都有对应的线程上下文。如果我们在代码中，想看当前是哪一个线程在执行当前代码，我们经常是使用如下方法： 1Thread thread = Thread.currentThread();//返回对当当前运行线程的引用 相应地，我们可以为当前的线程指定类加载器。在上述的例子中， 当执行 java org.luanlouis.jvm.load.Main 的时候，JVM会创建一个Main线程，而创建应用类加载器AppClassLoader的时候，会将AppClassLoader设置成Main线程的上下文类加载器： 123456789101112131415161718public Launcher() &#123; Launcher.ExtClassLoader var1; try &#123; var1 = Launcher.ExtClassLoader.getExtClassLoader(); &#125; catch (IOException var10) &#123; throw new InternalError("Could not create extension class loader", var10); &#125; try &#123; this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); &#125; catch (IOException var9) &#123; throw new InternalError("Could not create application class loader", var9); &#125; //将AppClassLoader设置成当前线程的上下文加载器 Thread.currentThread().setContextClassLoader(this.loader); //....... &#125; 线程上下文类加载器是从线程的角度来看待类的加载，为每一个线程绑定一个类加载器，可以将类的加载从单纯的 双亲加载模型解放出来，进而实现特定的加载需求。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态分派和动态分派]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E9%9D%99%E6%80%81%E5%88%86%E6%B4%BE%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E6%B4%BE%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十二篇文章，本章说明静态分派和动态分派的原理。 这里所谓的分派指的是在Java中对方法的调用。Java中有三大特性：封装、继承和多态。分派是多态性的体现，Java虚拟机底层提供了我们开发中“重写”和“重载”的底层实现。其中重载属于静态分派，而重写则是动态分派的过程。除了使用分派的方式对方法进行调用之外，还可以使用解析调用，解析调用是在编译期间就已经确定了，在类装载的解析阶段就会把符号引用转化为直接引用，不会延迟到运行期间再去完成。而分派调用则既可以是静态的也可以是动态（就是这里的静态分派和动态分派）的。 方法解析 对于方法的调用，虚拟机提供了四条方法调用的字节码指令，分别是： invokestatic: 调用静态方法 invokespecial: 调用构造方法，私有方法，父类方法 invokevirtual: 调用虚方法 invokeinterface: 调用接口方法 其中，1和2都可以在类加载阶段确定方法的唯一版本，因此，在类加载阶段就可以把符号引用解析为直接引用，在调用时刻直接找到方法代码块的内存地址进行执行（编译时已经找到了，并且存在方法调用的入口）；3和4则是在运行期间动态绑定方法的直接引用。 invokestatic指令和invokespecial指令调用的方法称为非虚方法，注意，final修饰的方法也属于虚方法。 静态分派 静态分派只会涉及重载，而重载是在编译期间确定的，那么静态分派自然是一个静态的过程（因为还没有涉及到Java虚拟机）。静态分派的最直接的解释是在重载的时候是通过参数的静态类型而不是实际类型作为判断依据的。比如创建一个类O，在O中创建了静态类内部类A，O中又有两个静态类内部类B、C继承了这个静态内部类A，那么实际上当编写如下的代码： 123456789101112131415161718192021public class O&#123; static class A&#123;&#125; static class B extends A&#123;&#125; static class C extends A&#123;&#125; public void a(A a)&#123; System.out.println("A method"); &#125; public void a(B b)&#123; System.out.println("B method"); &#125; public void a(C c)&#123; System.out.println("C method"); &#125; public static void main(String[] args)&#123; O o = new O(); A b = new B(); A c = new C(); o.a(b); o.a(c); &#125;&#125; 运行的结果是打印出连个“A method”。原因在于静态类型的变化仅仅在使用时发生，变量本身的类型不会发生变化。 比如我们这里中A b = new B();虽然在创建的时候是B的对象，但是当调用o.a(b)的时候才发现是A的对象，所以会输出“A method”。**也就是说在发生重载的时候，Java虚拟机是通过参数的静态类型而不是实际参数类型作为判断依据的。**因此，在编译阶段，Javac编译器选择了a(A a)这个重载方法。 虽然编译器能够在编译阶段确定方法的版本，但是很多情况下重载的版本不是唯一的，在这种模糊的情况下，编译器会选择一个更合适的版本。例如，重载的方法中，参数列表除了参数类型不一样，其他都一样，例接收的参数有char\int\long等，传入参数‘a’，则会调用需要char类型参数的方法，去掉需要char类型参数的方法，则会调用需要int类型参数的方法。这时发生了一次自动类型转换。同样，去掉需要int类型参数的方法，则会调用需要long类型参数的方法。这里再次发生类型转换，会按照char-&gt;int-&gt;long-&gt;float-&gt;double转换类型。 动态分派 动态分派与重写(Override)有着很密切的关联。如下代码： 12345678910111213141516171819202122232425262728package com.xtayfjpk.jvm.chapter8; public class DynamicDispatch &#123; static abstract class Human &#123; protected abstract void sayHello(); &#125; static class Man extends Human &#123; @Override protected void sayHello() &#123; System.out.println("man say hello"); &#125; &#125; static class Woman extends Human &#123; @Override protected void sayHello() &#123; System.out.println("woman say hello"); &#125; &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); man.sayHello(); woman.sayHello(); man = new Woman(); man.sayHello(); &#125; &#125; 这里显然不可能是根据静态类型来决定的，因为静态类型都是Human的两个变量man和woman在调用sayHello()方法时执行了不同的行为，并且变量man在两次调用中执行了不同的方法。 导致这个现象的原是是这两个变量的实际类型不同。那么Java虚拟机是如何根据实际类型来分派方法执行版本的呢，我们使用javap命令输出这段代码的字节码，结果如下： 1234567891011121314151617181920212223public static void main(java.lang.String[]); flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: new #16 // class com/xtayfjpk/jvm/chapter8/DynamicDispatch$Man 3: dup 4: invokespecial #18 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Man."&lt;init&gt;":()V 7: astore_1 8: new #19 // class com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman 11: dup 12: invokespecial #21 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman."&lt;init&gt;":()V 15: astore_2 16: aload_1 17: invokevirtual #22 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Human.sayHello:()V 20: aload_2 21: invokevirtual #22 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Human.sayHello:()V 24: new #19 // class com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman 27: dup 28: invokespecial #21 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman."&lt;init&gt;":()V 31: astore_1 32: aload_1 33: invokevirtual #22 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Human.sayHello:()V 36: return 0-15行的字节码是准备动作，作用是建立man和woman的内存空间，调用Man和Woman类的实例构造器，将这两个实例的引用存放在第1和第2个局部变量表Slot之中，这个动作对应了代码中这两句： 12Human man = new Man(); Human woman = new Woman(); 接下来的第16-21行是关键部分，第16和第20两行分别把刚刚创建的两个对象的引用压到栈顶，这两个对象是将执行的sayHello()方法的所有者，称为接收者(Receiver)。 第17和第21两行是方法调用指令，单从字节码的角度来看，这两条调用指令无论是指令(都是invokevirtual)还是参数(都是常量池中Human.sayHello()的符号引用)都完全一样，但是这两条指令最终执行的目标方法并不相同，其原因需要从invokevirutal指令的多态查找过程开始说起，invokevirtual指令的运行时解析过程大致分为以下步骤： 找到操作数栈顶的第一个元素所指向的对象实际类型，记作C。 如果在类型C中找到与常量中描述符和简单名称都相同的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找结束；不通过则返回java.lang.IllegalAccessError错误。 否则，按照继承关系从下往上依次对C的各个父类进行第2步的搜索与校验过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstractMethodError错误。 由于invokevirtual指令执行的第一步就是在运行期确定接收者的实际类型，所以两次调用中的invokevirtual指令把常量池中的类方法符号引用解析到了不同的直接引用上，这个过程就是Java语言中方法重写的本质。我们把这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派。 单分派与多分派 方法的接收者与方法的参数统称为方法的宗量。根据分派基于多少种宗量，可以将分派划分为单分派与多分派两种。单分派是根据一个宗量来对目标方法进行选择，多分派则是根据多于一个宗量对目标方法进行选择。 在编译期的静态分派过程选择目标方法的依据有两点：一是静态类型；二是方法参数，所以Java语言的静态分派属于多分派类型。在运行阶段虚拟机的动态分派过程只能接收者的实际类型一个宗量作为目标方法选择依据，所以Java语言的动态分派属于单分派类型。所以Java语言是一门静态多分派，动态单分派语言。 JVM实现动态分派 动态分派在Java中被大量使用，使用频率及其高，如果在每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话就可能影响到执行效率，因此JVM在类的方法区中建立虚方法表（virtual method table）来提高性能。 ⭐⭐⭐每个类中都有一个虚方法表，表中存放着各个方法的实际入口。如果某个方法在子类中没有被重写，那子类的虚方法表中该方法的地址入口和父类该方法的地址入口一样，即子类的方法入口指向父类的方法入口。如果子类重写父类的方法，那么子类的虚方法表中该方法的实际入口将会被替换为指向子类实现版本的入口地址。 那么虚方法表什么时候被创建？虚方法表会在类加载的连接阶段被创建并开始初始化，类的变量初始值准备完成之后，JVM会把该类的方法表也初始化完毕。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类的初始化过程]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E7%B1%BB%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十一篇文章，其实在前面的文章中已经说到了类加载机制，但是为了本文的完整性，前面一部分还是重复地放在这里，后面会着重说明初始化过程。 1. 类加载过程 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)7个阶段。其中准备、验证、解析3个部分统称为连接（Linking）。如图所示： 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。以下陈述的内容都以HotSpot为基准。 2. 加载 在加载阶段（可以参考java.lang.ClassLoader的loadClass()方法），虚拟机需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流（并没有指明要从一个Class文件中获取，可以从其他渠道，譬如：网络、动态生成、数据库等）； 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构； 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口； 加载阶段和连接阶段（Linking）的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 3. 验证 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以魔术0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 4. 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值，假设一个类变量的定义为： 1public static int value=123; 那变量value在准备阶段过后的初始值为0而不是123.因为这时候尚未开始执行任何java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行。 至于“特殊情况”是指：public static final int value=123，即当类字段的字段属性是ConstantValue时，会在准备阶段初始化为指定的值，所以标注为final之后，value的值在准备阶段初始化为123而非0. 5. 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 6. 初始化 类初始化阶段是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的java程序代码。在准备阶段，变量已经赋过一次系统要求的初始值，而在初始化阶段，则根据程序猿通过程序制定的主观计划去初始化类变量和其他资源，或者说：初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程. &lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块static{}中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。如下： 123456789public class Test&#123; static &#123; i=0; System.out.println(i);//这句编译器会报错：Cannot reference a field before it is defined（非法向前应用） &#125; static int i=1;&#125; 那么去掉报错的那句，改成下面： 1234567891011121314public class Test&#123; static &#123; i=0;// System.out.println(i); &#125; static int i=1; public static void main(String args[]) &#123; System.out.println(i); &#125;&#125; 输出结果是什么呢？当然是1啦~在准备阶段我们知道i=0，然后类初始化阶段按照顺序执行，首先执行static块中的i=0,接着执行static赋值操作i=1,最后在main方法中获取i的值为1。 &lt;clinit&gt;()方法与实例构造器&lt;init&gt;()方法不同，它不需要显示地调用父类构造器，虚拟机会保证在子类&lt;cinit&gt;()方法执行之前，父类的&lt;clinit&gt;()方法已经执行完毕. ⭐由于父类的&lt;clinit&gt;()方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。 &lt;clinit&gt;()方法对于类或者接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生产&lt;clinit&gt;()方法。 虚拟机会保证一个类的&lt;clinit&gt;()方法在多线程环境中被正确的加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;()方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;()方法完毕。如果在一个类的&lt;clinit&gt;()方法中有耗时很长的操作，就可能造成多个线程阻塞，在实际应用中这种阻塞往往是隐藏的。 虚拟机规范严格规定了有且只有5中情况（jdk1.7）必须对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到 new , getstatic , putstatic , invokestatic 这些字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：①使用new关键字实例化对象的时候、②读取或设置一个类的静态字段（被final修饰、已在编译器把结果放入常量池的静态字段除外）的时候，以及③调用一个类的静态方法的时候。 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 下面说明三种被动引用(除了上面提到的五种情况外，所有引用类的方法都不会触发初始化，成为被动引用)。 第一种：通过子类引用父类的静态字段，不会导致子类初始化。 1234567public class SuperClass &#123; static &#123; System.out.println("superclass static init"); &#125; public static int value = 123;&#125; 12345public class SubClass extends SuperClass&#123; static&#123; System.out.println("SubClass static init"); &#125;&#125; 12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); &#125;&#125; 结果是： 12superclass static init123 说明：对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 第二种：通过数组定义来引用类，不会触发此类的初始化 12345678910package chapter12;//SuperClass复用上面个代码public class NotInitialization&#123; public static void main(String[] args) &#123; SuperClass[] sca = new SuperClass[10]; &#125;&#125; 运行结果：（无） 说明：从结果来看，显然没有触发类chapter12.SuperClass的初始化阶段，但是这段代码触发了另一个名叫 &quot;[Lchapter12.SuperClass&quot;的类的初始化阶段。这显然不是一个合法的类名称，他是由虚拟机自动生成的、直接继承于java.lang.Object的子类，创建动作由字节码制定newarray触发。 这个类代表了一个元素类型为chapter12.SuperClass的一维数组，数组中应有的属性和方法(用于可直接使用的只有被修饰为public的length属性和clone()方法)都实现在这个类里。Java语言中对数组的访问比C/C++相对安全是因为这个类封装了数组元素的访问方法，而C/C++ 直接翻译为对数组指针的移动。 第三种：常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化 12345678910111213141516public class ConstClass&#123; static &#123; System.out.println("ConstClass init!"); &#125; public static final String HELLOWORLD = "hello world";&#125;public class NotInitialization&#123; public static void main(String[] args) &#123; System.out.println(ConstClass.HELLOWORLD); &#125;&#125; 运行结果：hello world 说明：上述代码虽然在java源码中引用了ConstClass类中的常量hello world，但是其实在编译阶段通过常量传播优化，已经将此常量值hello world存储到了NotInitialization的常量池中，以后NotInitialization对常量ConstClass.HELLOWORLD的引用实际上都被转化为NotInitialization对自身常量池的引用了。 7. 接口的加载 接口的加载过程与类加载过程有一些不同，针对接口需要做一些特殊说明： 接口也有初始化过程，而接口中不能使用static{}语句块，但编译器仍然会为接口生成&quot;&lt;clinit()&gt;&quot;类构造器，用于初始化接口中所定义的成员变量。 接口与类真正所区别的是前面讲述的5种“有且仅有”情况的第三种：当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个借口在初始化时，并不要求其父接口全部都已经完成了初始化，只有在真正用到父接口时（如引用接口中定义的常量）才会初始化。 8. 例子巩固 1234567public class SSClass&#123; static &#123; System.out.println("SSClass"); &#125;&#125; 1234567891011121314public class SuperClass extends SSClass&#123; static &#123; System.out.println("SuperClass init!"); &#125; public static int value = 123; public SuperClass() &#123; System.out.println("init SuperClass"); &#125;&#125; 1234567891011121314public class SubClass extends SuperClass&#123; static &#123; System.out.println("SubClass init"); &#125; static int a; public SubClass() &#123; System.out.println("init SubClass"); &#125;&#125; 1234567public class NotInitialization&#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); &#125;&#125; 运行结果： 123SSClassSuperClass init!123 说明：对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 9.总结java执行顺序 举例立刻明白： 123456789101112131415public class Children extends Parent&#123; public Children() &#123; System.out.println("Children构造函数"); &#125; &#123; System.out.println("Children普通代码块"); &#125; static &#123; System.out.println("Children静态代码块"); &#125; public static void main(String[] args) &#123; Children children = new Children(); &#125;&#125; 1234567891011public class Parent &#123; public Parent() &#123; System.out.println("Parent构造函数"); &#125; &#123; System.out.println("Parent普通代码块"); &#125; static &#123; System.out.println("Parent静态代码块"); &#125;&#125; 执行结果： 123456Parent静态代码块Children静态代码块Parent普通代码块Parent构造函数Children普通代码块Children构造函数 总结： 123456父类静态块自身静态块父类块父类构造器自身块自身构造器 10. 总结java赋值顺序 举例立刻明白： 12345678910111213141516public class Parent &#123; public String flag = "父类成员变量赋值"; public Parent() &#123; System.out.println(); System.out.println("父类构造器---&gt;" + flag); flag = "父类构造器赋值"; System.out.println("父类构造器---&gt;" + flag); &#125; &#123; System.out.println("父类代码块---&gt;" + flag); flag = "父类代码块赋值"; System.out.println("父类代码块---&gt;" + flag); &#125;&#125; 123456789101112131415161718192021222324252627public class Children extends Parent&#123; public String flag = "成员变量赋值"; public Children() &#123; System.out.println(); System.out.println("子类构造器---&gt;" + flag); flag = "子类构造器赋值"; System.out.println("子类构造器---&gt;" + flag); &#125; &#123; System.out.println(); System.out.println("子类代码快---&gt;" + flag); flag = "子类代码块赋值"; System.out.println("子类代码块---&gt;" + flag); &#125; public void setFlag()&#123; System.out.println(); System.out.println("子类方法---&gt;" + flag); &#125; public static void main(String[] args) &#123; Children children = new Children(); children.setFlag(); &#125;&#125; 运行结果： 12345678910111213父类代码块---&gt;父类成员变量赋值父类代码块---&gt;父类代码块赋值父类构造器---&gt;父类代码块赋值父类构造器---&gt;父类构造器赋值子类代码快---&gt;成员变量赋值子类代码块---&gt;子类代码块赋值子类构造器---&gt;子类代码块赋值子类构造器---&gt;子类构造器赋值子类方法---&gt;子类构造器赋值 总结： 12345678910父类的静态变量赋值自身的静态变量赋值父类成员变量赋值父类块赋值父类构造器赋值自身成员变量赋值自身块赋值自身构造器赋值]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读5-Class文件中的方法表集合--method方法在class文件中是怎样组织的]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB5-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%E8%A1%A8%E9%9B%86%E5%90%88--method%E6%96%B9%E6%B3%95%E5%9C%A8class%E6%96%87%E4%BB%B6%E4%B8%AD%E6%98%AF%E6%80%8E%E6%A0%B7%E7%BB%84%E7%BB%87%E7%9A%84%2F</url>
    <content type="text"><![CDATA[继续讲class文件中的方法表集合。 1. 方法表集合概述 方法表集合是指由若干个方法表（method_info）组成的集合。对于在类中定义的若干个经过JVM编译成class文件后，会将相应的method方法信息组织到一个叫做方法表集合的结构中，字段表集合是一个类数组结构，如下图所示： 2. method方法的描述-方法表集合在class文件中的位置 method方法的描述-方法表集合紧跟在字段表集合的后面，如下图所示： 3. 一个类中的method方法应该包含哪些信息？----method_info结构体的定义 对于一个方法的表示，我们根据我们可以概括的信息如下所示： 实际上JVM还会对method方法的描述添加其他信息，我们将在后面详细讨论。如上图中的method_info结构体的定义，该结构体的定义跟描述field字段 的field_info结构体的结构几乎完全一致,如下图所示。 方法表的结构体由：访问标志(access_flags)、名称索引(name_index)、描述索引(descriptor_index)、属性表(attribute_info)集合组成。 访问标志(access_flags)： method_info结构体最前面的两个字节表示的访问标志（access_flags），记录这这个方法的作用域、静态or非静态、可变性、是否可同步、是否本地方法、是否抽象等信息，实际上不止这些信息，我们后面会详细介绍访问标志这两个字节的每一位具体表示什么意思。 名称索引(name_index)： 紧跟在访问标志（access_flags）后面的两个字节称为名称索引，这两个字节中的值指向了常量池中的某一个常量池项，这个方法的名称以UTF-8格式的字符串存储在这个常量池项中。如public void methodName(),很显然，“methodName”则表示着这个方法的名称，那么在常量池中会有一个CONSTANT_Utf8_info格式的常量池项，里面存储着“methodName”字符串，而mehodName()方法的方法表中的名称索引则指向了这个常量池项。 描述索引(descriptor_index)： 描述索引表示的是这个方法的特征或者说是签名，一个方法会有若干个参数和返回值，而若干个参数的数据类型和返回值的数据类型构成了这个方法的描述，其基本格式为： (参数数据类型描述列表)返回值数据类型 。我们将在后面继续讨论。 属性表(attribute_info)集合： 这个属性表集合非常重要，方法的实现被JVM编译成JVM的机器码指令，机器码指令就存放在一个Code类型的属性表中；如果方法声明要抛出异常，那么异常信息会在一个Exceptions类型的属性表中予以展现。Code类型的属性表可以说是非常复杂的内容，也是本文最难的地方。 4. 访问标志(access_flags)—记录着method方法的访问信息 访问标志（access_flags）共占有2 个字节，分为 16 位，这 16位 表示的含义如下所示： 举例：某个类中定义了如下方法： 12public static synchronized final void greeting()&#123; &#125; greeting()方法的修饰符有：public、static、synchronized、final 这几个修饰符修饰，那么相对应地，greeting()方法的访问标志中的ACC_PUBLIC、ACC_STATIC、ACC_SYNCHRONIZED、ACC_FINAL标志位都应该是1，即： 从上图中可以看出访问标志的值应该是二进制00000000 00111001,即十六进制0x0039。我们将在文章的最后一个例子中证实这点。 5. 名称索引和描述符索引----一个方法的签名 紧接着访问标志（access_flags）后面的两个字节，叫做名称索引(name_index)，这两个字节中的值是指向了常量池中某个常量池项的索引，该常量池项表示这这个方法名称的字符串。 方法描述符索引(descrptor_index)是紧跟在名称索引后面的两个字节，这两个字节中的值跟名称索引中的值性质一样，都是指向了常量池中的某个常量池项。这两个字节中的指向的常量池项，是表示了方法描述符的字符串。 所谓的方法描述符，实质上就是指用一个什么样的字符串来描述一个方法，方法描述符的组成如下图所示： 举例：对于如下定义的的greeting()方法，我们来看一下对应的method_info结构体中的名称索引和描述符索引信息是怎样组织的。 12public static synchronized final void greeting()&#123; &#125; 如下图所示,method_info结构体的名称索引中存储了一个索引值x，指向了常量池中的第x项，第 x项表示的是字符串&quot;greeting&quot;,即表示该方法名称是&quot;greeting&quot;；描述符索引中的y 值指向了常量池的第y项，该项表示字符串&quot;()V&quot;，即表示该方法没有参数，返回值是void类型。 6. 属性表集合–记录方法的机器指令和抛出异常等信息 属性表集合记录了某个方法的一些属性信息，这些信息包括： 这个方法的代码实现，即方法的可执行的机器指令 这个方法声明的要抛出的异常信息 这个方法是否被@deprecated注解表示 这个方法是否是编译器自动生成的 属性表（attribute_info）结构体的一般结构如下所示： 修正：属性长度为4个字节。 6.1 Code类型的属性表–method方法中的机器指令的信息 Code类型的属性表(attribute_info)可以说是class文件中最为重要的部分，因为它包含的是JVM可以运行的机器码指令，JVM能够运行这个类，就是从这个属性中取出机器码的。除了要执行的机器码，它还包含了一些其他信息，如下所示： Code属性表的组成部分： 机器指令----code： 目前的JVM使用一个字节表示机器操作码，即对JVM底层而言，它能表示的机器操作码不多于2的 8 次方，即 256个。class文件中的机器指令部分是class文件中最重要的部分，并且非常复杂，本文的重点不止介绍它 异常处理跳转信息—exception_table： 如果代码中出现了try{}catch{}块，那么try{}块内的机器指令的地址范围记录下来，并且记录对应的catch{}块中的起始机器指令地址，当运行时在try块中有异常抛出的话，JVM会将catch{}块对应懂得其实机器指令地址传递给PC寄存器，从而实现指令跳转； Java源码行号和机器指令的对应关系—LineNumberTable属性表： 编译器在将java源码编译成class文件时，会将源码中的语句行号跟编译好的机器指令关联起来，这样的class文件加载到内存中并运行时，如果抛出异常，JVM可以根据这个对应关系，抛出异常信息，告诉我们我们的源码的多少行有问题，方便我们定位问题。这个信息不是运行时必不可少的信息，但是默认情况下，编译器会生成这一项信息，如果你项取消这一信息，你可以使用-g:none 或-g:lines来取消或者要求设置这一项信息。如果使用了-g:none来生成class文件，class文件中将不会有LineNumberTable属性表，造成的影响就是 将来如果代码报错，将无法定位错误信息报错的行，并且如果项调试代码，将不能在此类中打断点（因为没有指定行号。） 局部变量表描述信息----LocalVariableTable属性表： 局部变量表信息会记录栈帧局部变量表中的变量和java源码中定义的变量之间的关系，这个信息不是运行时必须的属性，默认情况下不会生成到class文件中。你可以根据javac指令的-g:none或者-g:vars选项来取消或者设置这一项信息。 它有什么作用呢？ 当我们使用IDE进行开发时，最喜欢的莫过于它们的代码提示功能了。如果在项目中引用到了第三方的jar包，而第三方的包中的class文件中有无LocalVariableTable属性表的区别如下所示： Code属性表结构体的解释： attribute_name_index,属性名称索引，占有2个字节，其内的值指向了常量池中的某一项，该项表示字符串“Code”; attribute_length,属性长度，占有 4个字节，其内的值表示后面有多少个字节是属于此Code属性表的； max_stack,操作数栈深度的最大值，占有 2 个字节，在方法执行的任意时刻，操作数栈都不应该超过这个值，虚拟机的运行的时候，会根据这个值来设置该方法对应的栈帧(Stack Frame)中的操作数栈的深度； max_locals,最大局部变量数目，占有 2个字节，其内的值表示局部变量表所需要的存储空间大小； code_length,机器指令长度，占有 4 个字节，表示跟在其后的多少个字节表示的是机器指令； code,机器指令区域，该区域占有的字节数目由 code_length中的值决定。JVM最底层的要执行的机器指令就存储在这里； exception_table_length,显式异常表长度，占有2个字节，如果在方法代码中出现了try{} catch()形式的结构，该值不会为空，紧跟其后会跟着若干个exception_table结构体，以表示异常捕获情况； exception_table，显式异常表，占有8 个字节，start_pc,end_pc,handler_pc中的值都表示的是PC计数器中的指令地址。exception_table表示的意思是：如果字节码从第start_pc行到第end_pc行之间出现了catch_type所描述的异常类型，那么将跳转到handler_pc行继续处理。 attribute_count,属性计数器，占有 2 个字节，表示Code属性表的其他属性的数目 attribute_info,表示Code属性表具有的属性表，它主要分为两个类型的属性表：“LineNumberTable”类型和“LocalVariableTable”类型。 “LineNumberTable”类型的属性表记录着Java源码和机器指令之间的对应关系 “LocalVariableTable”类型的属性表记录着局部变量描述 举例： 如下定义Simple类，使用javac -g:none Simple.java 编译出Simple.class 文件，并使用javap -v Simple &gt; Simple.txt 查看反编译的信息，然后看Simple.class文件中的方法表集合是怎样组织的： 12345public class Simple &#123; public static synchronized final void greeting()&#123; int a = 10; &#125; &#125; 1. Simple.class文件组织信息如下所示： 如上所示，方法表集合使用了蓝色线段圈了起来。 请注意：方法表集合的头两个字节，即方法表计数器（method_count）的值是0x0002，它表示该类中有2 个方法。细心的读者会注意到，我们的Simple.java中就定义了一个greeting()方法，为什么class文件中会显示有两个方法呢？？ JVM为没有显式定义实例化构造方法的类，自动生成默认的实例化构造方法&quot;()&quot; 除了实例化构造方法，JVM还会在特殊的情况下生成一个叫类构造方法&quot;()&quot;。如果我们在类中使用到了static修饰的代码块，那么，JVM会在class文件中生成一个“()”构造方法。关于它们的具体细节，我将在后续的文章中详细讨论，在这里就不展开了。 Simple.class 中的() 方法: 解释： 方法访问标志(access_flags)： 占有 2个字节，值为0x0001,即标志位的第 16 位为 1，所以该()方法的修饰符是：ACC_PUBLIC; 名称索引(name_index)： 占有 2 个字节，值为 0x0004，指向常量池的第 4项，该项表示字符串“”，即该方法的名称是“”; 描述符索引(descriptor_index): 占有 2 个字节，值为0x0005,指向常量池的第 5 项，该项表示字符串“()V”，即表示该方法不带参数，并且无返回值（构造函数确实也没有返回值）； 属性计数器（attribute_count): 占有 2 个字节，值为0x0001,表示该方法表中含有一个属性表，后面会紧跟着一个属性表； 属性表的名称索引(attribute_name_index)：占有 2 个字节，值为0x0006,指向常量池中的第6 项，该项表示字符串“Code”，表示这个属性表是Code类型的属性表； 属性长度（attribute_length）：占有4个字节，值为0x0000 0011，即十进制的 17，表明后续的 17 个字节可以表示这个Code属性表的属性信息； 操作数栈的最大深度（max_stack）：占有2个字节，值为0x0001,表示栈帧中操作数栈的最大深度是1； 局部变量表的最大容量（max_variable）：占有2个字节，值为0x0001, JVM在调用该方法时，根据这个值设置栈帧中的局部变量表的大小； 机器指令数目(code_length)：占有4个字节，值为0x0000 0005,表示后续的5 个字节 0x2A 、0xB7、 0x00、0x01、0xB1表示机器指令; 机器指令集(code[code_length])：这里共有 5个字节，值为0x2A 、0xB7、 0x00、0x01、0xB1； 显式异常表集合（exception_table_count）： 占有2 个字节，值为0x0000,表示方法中没有需要处理的异常信息； Code属性表的属性表集合（attribute_count）： 占有2 个字节，值为0x0000，表示它没有其他的属性表集合，因为我们使用了-g:none 禁止编译器生成Code属性表的 LineNumberTable 和LocalVariableTable; B. Simple.class 中的greeting() 方法: 解释： 方法访问标志(access_flags)： 占有 2个字节，值为 0x0039 ,即二进制的00000000 00111001,即标志位的第11、12、13、16位为1，根据上面讲的方法标志位的表示，可以得到该greeting()方法的修饰符有：ACC_SYNCHRONIZED、ACC_FINAL、ACC_STATIC、ACC_PUBLIC; 名称索引(name_index)： 占有 2 个字节，值为 0x0007，指向常量池的第 7 项，该项表示字符串“greeting”，即该方法的名称是“greeting”; 描述符索引(descriptor_index): 占有 2 个字节，值为0x0005,指向常量池的第 5 项，该项表示字符串“()V”，即表示该方法不带参数，并且无返回值； 属性计数器（attribute_count): 占有 2 个字节，值为0x0001,表示该方法表中含有一个属性表，后面会紧跟着一个属性表； 属性表的名称索引(attribute_name_index)：占有 2 个字节，值为0x0006,指向常量池中的第6 项，该项表示字符串“Code”，表示这个属性表是Code类型的属性表； 属性长度（attribute_length）：占有4个字节，值为0x0000 0010，即十进制的16，表明后续的16个字节可以表示这个Code属性表的属性信息； 操作数栈的最大深度（max_stack）：占有2个字节，值为0x0001,表示栈帧中操作数栈的最大深度是1； 局部变量表的最大容量（max_variable）：占有2个字节，值为0x0001, JVM在调用该方法时，根据这个值设置栈帧中的局部变量表的大小； 机器指令数目(code_length)：占有4 个字节，值为0x0000 0004,表示后续的4个字节0x10、 0x0A、 0x3B、0xB1的是表示机器指令; 机器指令集(code[code_length])：这里共有4 个字节，值为0x10、 0x0A、 0x3B、0xB1 ； 显式异常表集合（exception_table_count）： 占有2 个字节，值为0x0000,表示方法中没有需要处理的异常信息； Code属性表的属性表集合（attribute_count）： 占有2 个字节，值为0x0000，表示它没有其他的属性表集合，因为我们使用了-g:none 禁止编译器生成Code属性表的 LineNumberTable 和LocalVariableTable; 6.2 Exceptions类型的属性表----method方法声明的要抛出的异常信息 有些方法在定义的时候，会声明该方法会抛出什么类型的异常，如下定义一个Interface接口，它声明了sayHello()方法，抛出Exception异常： 123public interface Interface &#123; public void sayHello() throws Exception; &#125; 现在让我们看一下Exceptions类型的属性表(attribute_info)结构体是怎样组织的： 如上图所示，Exceptions类型的属性表(attribute_info)结构体由一下元素组成： 属性名称索引(attribute_name_index)：占有 2个字节，其中的值指向了常量池中的表示&quot;Exceptions&quot;字符串的常量池项； 属性长度(attribute_length)：它比较特殊，占有4个字节，它的值表示跟在其后面多少个字节表示异常信息； 异常数量(number_of_exceptions)：占有2 个字节，它的值表示方法声明抛出了多少个异常，即表示跟在其后有多少个异常名称索引； 异常名称索引(exceptions_index_table)：占有2个字节，它的值指向了常量池中的某一项，该项是一个CONSTANT_Class_info类型的项，表示这个异常的完全限定名称； Exceptions类型的属性表的长度计算 如果某个方法定义中，没有声明抛出异常，那么，表示该方法的方法表(method_info)结构体中的属性表集合中不会有Exceptions类型的属性表；换句话说，如果方法声明了要抛出的异常，方法表(method_info)结构体中的属性表集合中必然会有Exceptions类型的属性表，并且该属性表中的异常数量不小于1。 我们假设异常数量中的值为 N，那么后面的异常名称索引的数量就为N，它们总共占有的字节数为N*2，而异常数量占有2个字节，那么将有下面的这个关系式： 属性长度(attribute_length)中的值= 2 + 2*异常数量(number_of_exceptions)中的值 Exceptions类型的属性表（attribute_info）的长度=2+4+属性长度(attribute_length)中的值 举例： 将上面定义的Interface接口类编译成class文件，然后我们查看Interface.class文件，找出方法表集合所在位置和相应的数据，并辅助javap -v Inerface 查看 由于sayHello()方法是在的Interface接口类中声明的，它没有被实现，所以它对应的方法表(method_info)结构体中的属性表集合中没有Code类型的属性表。 方法计数器（methods_count）中的值为0x0001，表明其后的方法表(method_info)就一个,即我们就定义了一个方法，其后会紧跟着一个方法表(method_info)结构体； 方法的访问标志（access_flags）的值是0x0401，二进制是00000100 00000001,第6位和第16位是1，对应上面的标志位信息，可以得出它的访问标志符有：ACC_ABSTRACT、ACC_PUBLIC。细心的读者可能会发现，在上面声明的sayHello()方法中并没有声明为abstract类型啊。确实如此，这是因为编译器对于接口内声明的方法自动加上ACC_ABSTRACT标志。 名称索引（name_index）中的值为0x0005，0x0005指向了常量池的第5项，第五项表示的字符串为“sayHello”，即表示的方法名称是sayHello 描述符索引(descriptor_index)中的值为0x0006,0x0006指向了常量池中的第6项，第6项表示的字符串为“()V” 表示这个方法的无入参，返回值为void类型 属性表计数器(attribute_count)中的值为0x0001,表示后面的属性表的个数就1个，后面紧跟着一个attribute_info结构体； 属性表（attribute_info）中的属性名称索引(attribute_name_index)中的值为0x0007，0x0007指向了常量池中的第7 项，第 7项指向字符串“Exceptions”，即表示该属性表表示的异常信息； 属性长度（attribute_length）中的值为：0x00000004,即后续的4个字节将会被解析成属性值； 异常数量（number_of_exceptions）中的值为0x0001,表示这个方法声明抛出的异常个数是1个； 异常名称索引(exception_index_table)中的值为0x0008,指向了常量池中的第8项，第8项表示的是CONSTANT_Class_info类型的常量池项，表示“java/lang/Exception”，即表示此方法抛出了java.lang.Exception异常。 7. IDE代码提示功能实现的基本原理 每个IDE都提供了代码提示功能，它们实现的基本原理其实就是IDE针对它们项目下的包中所有的class文件进行建模，解析出它们的方法信息，当我们一定的条件时，IDE会自动地将合适条件的方法列表展示给开发者，供开发者使用。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读4-Class文件中的字段表集合--field字段在class文件中是怎样组织的]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB4-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E5%AD%97%E6%AE%B5%E8%A1%A8%E9%9B%86%E5%90%88--field%E5%AD%97%E6%AE%B5%E5%9C%A8class%E6%96%87%E4%BB%B6%E4%B8%AD%E6%98%AF%E6%80%8E%E6%A0%B7%E7%BB%84%E7%BB%87%E7%9A%84%2F</url>
    <content type="text"><![CDATA[继续讲class文件中的字段表集合。 1. 字段表集合概述 字段表集合是指由若干个字段表（field_info）组成的集合。对于在类中定义的若干个字段，经过JVM编译成class文件后，会将相应的字段信息组织到一个叫做字段表集合的结构中，字段表集合是一个类数组结构，如下图所示： 注意：这里所讲的字段是指在类中定义的静态或者非静态的变量，而不是在类中的方法内定义的变量。请注意区别。 比如，如果某个类中定义了5个字段，那么，JVM在编译此类的时候，会生成5个字段表（field_info）信息,然后将字段表集合中的字段计数器的值设置成5，将5个字段表信息依次放置到字段计数器的后面。 2. 字段表集合在class文件中的位置 字段表集合紧跟在class文件的接口索引集合结构的后面，如下图所示： 3. Java中的一个Field字段应该包含那些信息？------字段表field_info结构体的定义 针对上述的字段表示，JVM虚拟机规范规定了field_info结构体来描述字段，其表示信息如下： 下面我将一一讲解FIeld_info的组成元素：访问标志（access_flags）、名称索引（name_index）、描述索引（descriptor_index）、属性表集合 4. field字段的访问标志 如上图所示定义的field_info结构体，field字段的访问标志(access_flags)占有两个字节，它能够表述的信息如下所示： 举例：如果我们在某个类中有定义field域：private static String str;，那么在访问标志上，第15位ACC_PRIVATE和第13位ACC_STATIC标志位都应该为1。field域str的访问标志信息应该是如下所示： 5. 字段的数据类型表示和字段名称表示 class文件对数据类型的表示如下图所示： field字段名称，我们定义了一个形如private static String str的field字段，其中&quot;str&quot;就是这个字段的名称。 class文件将字段名称和field字段的数据类型表示作为字符串存储在常量池中。在field_info结构体中，紧接着访问标志的，就是字段名称索引和字段描述符索引，它们分别占有两个字节，其内部存储的是指向了常量池中的某个常量池项的索引，对应的常量池项中存储的字符串，分别表示该字段的名称和字段描述符。 6. 属性表集合-----静态field字段的初始化 在定义field字段的过程中，我们有时候会很自然地对field字段直接赋值，如下所示： 12public static final int MAX=100; public int count=0; 对于虚拟机而言，上述的两个field字段赋值的时机是不同的： 对于非静态（即无static修饰）的field字段的赋值将会出现在实例构造方法()中 对于静态的field字段，有两个选择：1、在静态构造方法()中进行；2 、使用ConstantValue属性进行赋值 Sun javac编译器对于静态field字段的初始化赋值策略： 如果使用final和static同时修饰一个field字段，并且这个字段是基本类型或者String类型的，那么编译器在编译这个字段的时候，会在对应的field_info结构体中增加一个ConstantValue类型的结构体，在赋值的时候使用这个ConstantValue进行赋值； 如果该field字段并没有被final修饰，或者不是基本类型或者String类型，那么将在类构造方法()中赋值。 对于上述的public static final init MAX=100： javac编译器在编译此field字段构建field_info结构体时，除了访问标志、名称索引、描述符索引外，会增加一个ConstantValue类型的属性表。 7. 实例解析 定义如下一个简单的Simple类，然后通过查看Simple.class文件内容并结合javap -v Simple 生成的常量池内容，分析str field字段的结构： 1234public class Simple &#123; private transient static final String str ="This is a test"; &#125; 字段计数器中的值为0x0001,表示这个类就定义了一个field字段 字段的访问标志是0x009A,二进制是00000000 10011010，即第9、12、13、15位标志位为1，这个字段的标志符有：ACC_TRANSIENT、ACC_FINAL、ACC_STATIC、ACC_PRIVATE; 名称索引中的值为0x0005,指向了常量池中的第5项，为“str”,表明这个field字段的名称是str； 描述索引中的值为0x0006,指向了常量池中的第6项，为&quot;Ljava/lang/String;&quot;，表明这个field字段的数据类型是java.lang.String类型； 5.属性表计数器中的值为0x0001,表明field_info还有一个属性表； 6.属性表名称索引中的值为0x0007,指向常量池中的第7项，为“ConstantValue”,表明这个属性表的名称是ConstantValue，即属性表的类型是ConstantValue类型的； 7.属性长度中的值为0x0002，因为此属性表是ConstantValue类型，它的值固定为2； 8.常量值索引 中的值为0x0008,指向了常量池中的第8项，为CONSTANT_String_info类型的项，表示“This is a test” 的常量。在对此field赋值时，会使用此常量对field赋值。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读3-Class文件中的访问标志、类索引、父类索引、接口索引集合]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB3-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E8%AE%BF%E9%97%AE%E6%A0%87%E5%BF%97%E3%80%81%E7%B1%BB%E7%B4%A2%E5%BC%95%E3%80%81%E7%88%B6%E7%B1%BB%E7%B4%A2%E5%BC%95%E3%80%81%E6%8E%A5%E5%8F%A3%E7%B4%A2%E5%BC%95%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[讲完了class文件中的常量池，我们就相当于克服了class文件中最麻烦的模块了。现在，我们来看一下class文件中紧接着常量池后面的几个东西：访问标志、类索引、父类索引、接口索引集合。 1. 访问标志、类索引、父类索引、接口索引集合 在class文件中的位置 2. 访问标志(access_flags)能够表示什么？ 访问标志（access_flags）紧接着常量池后，占有两个字节，总共16位，如下图所示： 当JVM在编译某个类或者接口的源代码时，JVM会解析出这个类或者接口的访问标志信息，然后，将这些标志设置到访问标志（access_flags）这16个位上。JVM会考虑如下设置如下访问表示信息： a. 类或接口 我们知道，每个定义的类或者接口都会生成class文件（这里也包括内部类，在某个类中定义的静态内部类也会单独生成一个class文件）。 对于定义的类，JVM在将其编译成class文件时，会将class文件的访问标志的第11位设置为1 。第11位叫做ACC_SUPER标志位； 对于定义的接口，JVM在将其编译成class文件时，会将class文件的访问标志的第8位 设置为 1 。第8位叫做ACC_INTERFACE标志位； b. 访问权限：public类型和包package类型。 如果类或者接口被声明为public类型的，那么，JVM将其编译成class文件时，会将class文件的访问标志的第16位设置为1 。第16位叫做ACC_PUBLIC标志符； c. 类是否为抽象类型的，即我们定义的类有没有被abstract关键字修饰，即我们定义的类是否为抽象类。 1public abstract class MyClass&#123;......&#125; 定义某个类时，JVM将它编译成class文件的时候，会将class文件的访问标志的第7位设置为1 。第7位叫做ACC_ABSTRACT标志位。 另外值得注意的是，对于定义的接口，JVM在编译接口的时候也会对class文件的访问标志上的ACC_ABSTRACT标志位设置为 1； d. 该类是否被声明了final类型,即表示该类不能被继承。 此时JVM会在编译class文件的过程中，会将class文件的访问标志的第12位设置为 1 。第12位叫做ACC_FINAL标志位； e.是否是JVM通过java源代码文件编译而成的 如果我们这个class文件不是JVM通过java源代码文件编译而成的，而是用户自己通过class文件的组织规则生成的，那么，一般会对class文件的访问标志第4位设置为 1 。通过JVM编译源代码产生的class文件此标志位为 0，第4位叫做ACC_SYNTHETIC标志位； f. 枚举类 对于定义的枚举类如：public enum EnumTest{…}，JVM也会对此枚举类编译成class文件，这时，对于这样的class文件，JVM会对访问标志第2位设置为 1 ，以表示它是枚举类。第2位叫做ACC_ENUM标志位； g. 注解类 对于定义的注解类如：public @interface{…},JVM会对此注解类编译成class文件，对于这样的class文件，JVM会将访问标志第3位设置为1，以表示这是个注解类，第3位叫做ACC_ANNOTATION标志位。 当JVM确定了上述标志位的值后，就可以确定访问标志（access_flags）的值了。实际上JVM上述标志会根据上述确定的标志位的值，对这些标志位的值取或，便得到了访问标志（access_flags）。如下图所示: 举例 定义一个最简单的类Simple.java，使用编译器编译成class文件，然后观察class文件中的访问标志的值，以及使用javap -v Simple 查看访问标志。 123public class Simple &#123; &#125; 使用UltraEdit查看编译成的class文件，如下图所示： 上述的图中黄色部分表示的是常量池部分,常量池后面紧跟着就是访问标志，它的十六进制值为0x0021,二进制的值为：00000000 00100001，由二进制的1的位数可以得出第11、16位为1，分别对应ACC_SUPER标志位和ACC_PUBLIC标志位。验证一下: 3. 类索引(this_class)是什么？ 我们知道一般情况下一个Java类源文件经过JVM编译会生成一个class文件，也有可能一个Java类源文件中定义了其他类或者内部类，这样编译出来的class文件就不止一个，但每一个class文件表示某一个类，至于这个class表示哪一个类，便可以通过 类索引 这个数据项来确定。JVM通过类的完全限定名确定是某一个类。 类索引的作用，就是为了指出class文件所描述的这个类叫什么名字。 类索引紧接着访问标志的后面，占有两个字节，在这两个字节中存储的值是一个指向常量池的一个索引，该索引指向的是CONSTANT_Class_info常量池项. 以上面定义的Simple.class 为例，如下图所示，查看他的类索引在什么位置和取什么值。 由上可知，它的类索引值为0x0001,那么，它指向了常量池中的第一个常量池项，那我们再看一下常量池中的信息。使用javap -v Simple,常量池中有以下信息： 可以看到常量池中的第一项是CONSTANT_Class_info项，它表示一个&quot;com/louis/jvm/Simple&quot;的类名。即类索引是告诉我们这个class文件所表示的是哪一个类。 4. 父类索引(super_class)是什么？ Java支持单继承模式，除了java.lang.Object 类除外，每一个类都会有且只有一个父类。class文件中紧接着类索引(this_class)之后的两个字节区域表示父类索引，跟类索引一样，父类索引这两个字节中的值指向了常量池中的某个常量池项CONSTANT_Class_info，表示该class表示的类是继承自哪一个类。 5. 接口索引集合(interfaces)是什么？ 一个类可以不实现任何接口，也可以实现很多个接口，为了表示当前类实现的接口信息，class文件使用了如下结构体描述某个类的接口实现信息: 由于类实现的接口数目不确定，所以接口索引集合的描述的前部分叫做接口计数器（interfaces_count），接口计数器占用两个字节，其中的值表示着这个类实现了多少个接口，紧跟着接口计数器的部分就是接口索引部分了，每一个接口索引占有两个字节，接口计数器的值代表着后面跟着的接口索引的个数。接口索引和类索引和父类索引一样，其内的值存储的是指向了常量池中的常量池项的索引，表示着这个接口的完全限定名。 举例： 定义一个Worker接口，然后类Programmer实现这个Worker接口，然后我们观察Programmer的接口索引集合是怎样表示的。 12345public interface Worker&#123; public void work(); &#125; 1234567public class Programmer implements Worker &#123; @Override public void work() &#123; System.out.println("I'm Programmer,Just coding...."); &#125; &#125;]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读2-Class文件中的常量池]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB2-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E5%B8%B8%E9%87%8F%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[上一节Class类文件结构大致地介绍了class文件的组织结构，接下来，我们将深入每一个结构，来详细了解它们。这一章节呢，我们就来扒一扒class文件中非常重要 的一个数据区域------常量池。它在JVM虚拟机中扮演了非常重要的地位。 本篇内容来自于java虚拟机原理图解，自己一边理解一边进行复制整理得此文章，也是看了很多遍，逐渐地好像懂了常量池怎么玩的，所以一定要坚持，读不懂多读几遍一定可以读懂的。 本篇文章内容过多，这里将目录列举在此。 常量池是什么 常量池在class文件的什么位置？ 常量池里面是怎么组织的？ 常量池项 (cp_info) 的结构是什么？ 常量池能够表示哪些信息？ int和float数据类型的常量在常量池中是怎样表示和存储的？ long和 double数据类型的常量在常量池中是怎样表示和存储的？ String类型的字符串常量在常量池中是怎样表示和存储的？ 类文件中定义的类名和类中使用到的类在常量池中是怎样被组织和存储的？ 类中引用到的field字段在常量池中是怎样描述的？ 类中引用到的method方法在常量池中是怎样描述的？ 类中引用到某个接口中定义的method方法在常量池中是怎样描述的？ 更好地支持动态语言所增加的三项 1. 常量池是什么 可以理解为class文件之中的资源仓库，它是class文件结构中与其他项目关联最多的数据类型，也是占用class文件空间最大的数据项目之一，同时它还是class文件中第一个出现表类型的数据项目． 由于常量池的数量是不固定的，所以在常量池入口需要放置一项u2（即２个字节）类型的数据，代表常量池容量计数值（constant-pool-count）(从１开始，将０表示不引用任何常量). 常量池中主要存放两大类常量：字面量（Literal）和符号引用(Synbolic Reference)． 字面量：比较接近于Java语言层面的常量概念，如文本字符串，声明为final的常量值. 符号引用：包括如下三类常量： 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 2. 常量池在class文件的什么位置？ 3. 常量池的里面是怎么组织的？ 常量池的组织很简单，前端的两个字节占有的位置叫做常量池计数器(constant_pool_count)，它记录着常量池的组成元素 常量池项(cp_info) 的个数。紧接着会排列着constant_pool_count-1个常量池项(cp_info)。如下图所示： 4. 常量池项 (cp_info) 的结构是什么？ 每个常量池项(cp_info) 都会对应记录着class文件中的某种类型的字面量。让我们先来了解一下常量池项(cp_info)的结构吧： JVM虚拟机规定了不同的tag值和不同类型的字面量对应关系如下： 所以根据cp_info中的tag 不同的值，可以将cp_info 更细化为以下结构体： 现在让我们看一下细化了的常量池的结构会是类似下图所示的样子： 5. 常量池能够表示那些信息？ 6. int和float数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Integer_info, CONSTANT_Float_info) Java语言规范规定了 int类型和Float 类型的数据类型占用 4 个字节的空间。那么存在于class字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将 int和Float类型的常量分别使用CONSTANT_Integer_info和 Constant_float_info表示，他们的结构如下所示： 举例：建下面的类 IntAndFloatTest.java，在这个类中，我们声明了五个变量，但是取值就两种int类型的10 和Float类型的11f. 123456789public class IntAndFloatTest &#123; private final int a = 10; private final int b = 10; private float c = 11f; private float d = 11f; private float e = 11f; &#125; 然后用编译器编译成IntAndFloatTest.class字节码文件，我们通过javap -v IntAndFloatTest 指令来看一下其常量池中的信息，可以看到虽然我们在代码中写了两次10 和三次11f，但是常量池中，就只有一个常量10 和一个常量11f,如下图所示: 从结果上可以看到常量池第#8 个常量池项(cp_info) 就是CONSTANT_Integer_info,值为10；第#23个常量池项(cp_info) 就是CONSTANT_Float_info,值为11f。 代码中所有用到 int 类型 10 的地方，会使用指向常量池的指针值#8 定位到第#8 个常量池项(cp_info)，即值为 10的结构体 CONSTANT_Integer_info，而用到float类型的11f时，也会指向常量池的指针值#23来定位到第#23个常量池项(cp_info) 即值为11f的结构体CONSTANT_Float_info。如下图所示： 7. long和 double数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Long_info、CONSTANT_Double_info ) Java语言规范规定了 long 类型和 double类型的数据类型占用8 个字节的空间。那么存在于class 字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将long和double类型的常量分别使用CONSTANT_Long_info和Constant_Double_info表示，他们的结构如下所示： 代码中所有用到 long 类型-6076574518398440533L 的地方，会使用指向常量池的指针值#18 定位到第 #18 个常量池项(cp_info)，即值为-6076574518398440533L 的结构体CONSTANT_Long_info，而用到double类型的10.1234567890D时，也会指向常量池的指针值#26 来定位到第 #26 个常量池项(cp_info) 即值为10.1234567890D的结构体CONSTANT_Double_info。如下图所示： 8. String类型的字符串常量在常量池中是怎样表示和存储的？（CONSTANT_String_info、CONSTANT_Utf8_info） 对于字符串而言，JVM会将字符串类型的字面量以UTF-8 编码格式存储到在class字节码文件中。这么说可能有点摸不着北，我们先从直观的Java源码中中出现的用双引号&quot;&quot; 括起来的字符串来看，在编译器编译的时候，都会将这些字符串转换成CONSTANT_String_info结构体，然后放置于常量池中。其结构如下所示： 如上图所示的结构体，CONSTANT_String_info结构体中的string_index的值指向了CONSTANT_Utf8_info结构体，而字符串的utf-8编码数据就在这个结构体之中。如下图所示： 请看一例，定义一个简单的StringTest.java类，然后在这个类里加一个&quot;JVM原理&quot; 字符串，然后，我们来看看它在class文件中是怎样组织的。 123456public class StringTest &#123; private String s1 = "JVM原理"; private String s2 = "JVM原理"; private String s3 = "JVM原理"; private String s4 = "JVM原理"; &#125; 在上面的图中，我们可以看到CONSTANT_String_info结构体位于常量池的第#15个索引位置。而存放&quot;Java虚拟机原理&quot; 字符串的 UTF-8编码格式的字节数组被放到CONSTANT_Utf8_info结构体中，该结构体位于常量池的第#16个索引位置。上面的图只是看了个轮廓，让我们再深入地看一下它们的组织吧。请看下图： 9. 类文件中定义的类名和类中使用到的类在常量池中是怎样被组织和存储的？(CONSTANT_Class_info) JVM会将某个Java 类中所有使用到了的类的完全限定名 以二进制形式的完全限定名 封装成CONSTANT_Class_info结构体中，然后将其放置到常量池里。CONSTANT_Class_info 的tag值为 7 。其结构如下： 类的完全限定名和二进制形式的完全限定名 在某个Java源码中，我们会使用很多个类，比如我们定义了一个 ClassTest的类，并把它放到com.louis.jvm 包下，则 ClassTest类的完全限定名为com.louis.jvm.ClassTest，将JVM编译器将类编译成class文件后，此完全限定名在class文件中，是以二进制形式的完全限定名存储的，即它会把完全限定符的&quot;.“换成”/&quot; ，即在class文件中存储的 ClassTest类的完全限定名称是&quot;com/louis/jvm/ClassTest&quot;。因为这种形式的完全限定名是放在了class二进制形式的字节码文件中，所以就称之为 二进制形式的完全限定名。 举例，我们定义一个很简单的ClassTest类，来看一下常量池是怎么对类的完全限定名进行存储的。 123public class ClassTest &#123; private Date date =new Date(); &#125; 如上图所示，在ClassTest.class文件的常量池中，共有 3 个CONSTANT_Class_info结构体，分别表示ClassTest 中用到的Class信息。 我们就看其中一个表示com/jvm.ClassTest的CONSTANT_Class_info 结构体。它在常量池中的位置是#1，它的name_index值为#2，它指向了常量池的第2 个常量池项，如下所示: 注意： 对于某个类而言，其class文件中至少要有两个CONSTANT_Class_info常量池项，用来表示自己的类信息和其父类信息。(除了java.lang.Object类除外，其他的任何类都会默认继承自java.lang.Object）如果类声明实现了某些接口，那么接口的信息也会生成对应的CONSTANT_Class_info常量池项。 除此之外，如果在类中使用到了其他的类，只有真正使用到了相应的类，JDK编译器才会将类的信息组成CONSTANT_Class_info常量池项放置到常量池中。如下： 12345678910import java.util.Date; public class Other&#123; private Date date; public Other() &#123; Date da; &#125; &#125; 上述的Other的类，在JDK将其编译成class文件时，常量池中并没有java.util.Date对应的CONSTANT_Class_info常量池项，为什么呢? 在Other类中虽然定义了Date类型的两个变量date、da，但是JDK编译的时候，认为你只是声明了“Ljava/util/Date”类型的变量，并没有实际使用到Ljava/util/Date类。将类信息放置到常量池中的目的，是为了在后续的代码中有可能会反复用到它。很显然，JDK在编译Other类的时候，会解析到Date类有没有用到，发现该类在代码中就没有用到过，所以就认为没有必要将它的信息放置到常量池中了。 将上述的Other类改写一下，仅使用new Date()，如下所示： 12345678import java.util.Date; public class Other&#123; public Other() &#123; new Date(); &#125; &#125; 总结： 对于某个类或接口而言，其自身、父类和继承或实现的接口的信息会被直接组装成CONSTANT_Class_info常量池项放置到常量池中； 类中或接口中使用到了其他的类，只有在类中实际使用到了该类时，该类的信息才会在常量池中有对应的CONSTANT_Class_info常量池项； 类中或接口中仅仅定义某种类型的变量，JDK只会将变量的类型描述信息以UTF-8字符串组成CONSTANT_Utf8_info常量池项放置到常量池中，上面在类中的private Date date;JDK编译器只会将表示date的数据类型的“Ljava/util/Date”字符串放置到常量池中。 10. 类中引用到的field字段在常量池中是怎样描述的？(CONSTANT_Fieldref_info, CONSTANT_Name_Type_info) 一般而言，我们在定义类的过程中会定义一些 field 字段，然后会在这个类的其他地方（如方法中）使用到它。有可能我们在类的方法中只使用field字段一次，也有可能我们会在类定义的方法中使用它很多很多次。 举一个简单的例子，我们定一个叫Person的简单java bean，它有name和age两个field字段，如下所示： 1234567891011121314151617181920public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 在上面定义的类中，我们在Person类中的一系列方法里，多次引用到namefield字段 和agefield字段，对于JVM编译器而言，name和age只是一个符号而已，并且它在由于它可能会在此类中重复出现多次，所以JVM把它当作常量来看待，将name和age以field字段常量的形式保存到常量池中。 将它name和age封装成 CONSTANT_Fieldref_info 常量池项，放到常量池中，在类中引用到它的地方，直接放置一个指向field字段所在常量池的索引。 上面的Person类，使用javap -v Person指令，查看class文件的信息，你会看到，在Person类中引用到age和namefield字段的地方，都是指向了常量池中age和namefield字段对应的常量池项中。表示field字段的常量池项叫做CONSTANT_Fieldref_info。 怎样描述某一个field字段的引用？ 实例解析： 现在，让我们来看一下Person类中定义的namefield字段在常量池中的表示。通过使用javap -v Person会查看到如下的常量池信息： 请读者看上图中namefield字段的数据类型，它在#6个常量池项，以UTF-8编码格式的字符串“Ljava/lang/String;” 表示，这表示着这个field 字段是java.lang.String 类型的。关于field字段的数据类型，class文件中存储的方式和我们在源码中声明的有些不一样。请看下图的对应关系： 注意： 如果我们在类中定义了field 字段，但是没有在类中的其他地方用到这些字段，它是不会被编译器放到常量池中的。 只有在类中的其他地方引用到了，才会将他放到常量池中。 11. 类中引用到的method方法在常量池中是怎样描述的？(CONSTANT_Methodref_info, CONSTANT_Name_Type_info) 1. 举例 还是以Person类为例。在Person类中，我们定义了setName(String name)、getName()、setAge(int age)、getAge()这些方法： 123456789101112131415161718192021public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 虽然我们定义了方法，但是这些方法没有在类总的其他地方被用到（即没有在类中其他的方法中引用到），所以它们的方法引用信息并不会放到常量中。 现在我们在类中加一个方法 getInfo()，调用了getName()和getAge() 方法： 1234public String getInfo() &#123; return getName()+"\t"+getAge(); &#125; 这时候JVM编译器会将getName()和getAge()方法的引用信息包装成CONSTANT_Methodref_info结构体放入到常量池之中。 这里的方法调用的方式牵涉到Java非常重要的一个术语和机制，叫动态绑定。这个动态绑定问题以后在单独谈谈。 2. 表示一个方法引用 3. 方法描述符的组成 4. getName() 方法引用在常量池中的表示 12. 类中引用到某个接口中定义的method方法在常量池中是怎样描述的？(CONSTANT_InterfaceMethodref_info, CONSTANT_Name_Type_info) 当我们在某个类中使用到了某个接口中的方法，JVM会将用到的接口中的方法信息方知道这个类的常量池中。 比如我们定义了一个Worker接口，和一个Boss类，在Boss类中调用了Worker接口中的方法，这时候在Boss类的常量池中会有Worker接口的方法的引用表示。 12345public interface Worker&#123; public void work(); &#125; 12345678public class Boss &#123; public void makeMoney(Worker worker) &#123; worker.work(); &#125; &#125; 如上图所示，在Boss类的makeMoney()方法中调用了Worker接口的work()方法，机器指令是通过invokeinterface指令完成的，invokeinterface指令后面的操作数，是指向了Boss常量池中Worker接口的work()方法描述，表示的意思就是：“我要调用Worker接口的work()方法”。 Worker接口的work()方法引用信息，JVM会使用CONSTANT_InterfaceMethodref_info结构体来描述，CONSTANT_InterfaceMethodref_info定义如下： CONSTANT_InterfaceMethodref_info结构体和上面介绍的CONSTANT_Methodref_info 结构体很基本上相同，它们的不同点只有： CONSTANT_InterfaceMethodref_info 的tag 值为11，而CONSTANT_Methodref_info的tag值为10； CONSTANT_InterfaceMethodref_info 描述的是接口中定义的方法，而CONSTANT_Methodref_info描述的是实例类中的方法； 其他的基本与上面一个一毛一样。参照上面个理解即可。 13. CONSTANT_MethodType_info，CONSTANT_MethodHandle_info，CONSTANT_InvokeDynamic_info 这三项主要是为了让Java语言支持动态语言特性而在Java 7 版本中新增的三个常量池项，只会在极其特别的情况能用到它，在class文件中几乎不会生成这三个常量池项。 其实我花了一些时间来研究这三项，并且想通过各种方式生成这三项，不过没有成功，最后搞的还是迷迷糊糊的。从我了解到的信息来看，Java 7对动态语言的支持很笨拙，并且当前没有什么应用价值，然后就对着三项的研究先放一放了。）]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读1-Class类文件结构]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB-Class%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[总体概览一下Class文件是什么以及有什么。 整体感知 class文件是一种8位字节的二进制流文件， 各个数据项按顺序紧密的从前向后排列， 相邻的项之间没有间隙， 这样可以使得class文件非常紧凑， 体积轻巧， 可以被JVM快速的加载至内存， 并且占据较少的内存空间。 我们的Java源文件， 在被编译之后， 每个类（或者接口）都单独占据一个class文件， 并且类中的所有信息都会在class文件中有相应的描述， 由于class文件很灵活， 它甚至比Java源文件有着更强的描述能力。 Class文件格式 换成表格的形式： 类型 名称 数量 u4 magic 1 u2 minor_version 1 u2 major_version 1 u2 constant_pool_count 1 cp_info constant_pool constant_pool_count - 1 u2 access_flags 1 u2 this_class 1 u2 super_class 1 u2 interfaces_count 1 u2 interfaces interfaces_count u2 fields_count 1 field_info fields fields_count u2 methods_count 1 method_info methods methods_count u2 attribute_count 1 attribute_info attributes attributes_count NO1. 魔数(magic) 所有的由Java编译器编译而成的class文件的前4个字节都是“0xCAFEBABE” 它的作用在于： 当JVM在尝试加载某个文件到内存中来的时候，会首先判断此class文件有没有JVM认为可以接受的“签名”，即JVM会首先读取文件的前4个字节，判断该4个字节是否是“0xCAFEBABE”，如果是，则JVM会认为可以将此文件当作class文件来加载并使用。 NO2.版本号(minor_version,major_version) 主版本号和次版本号在class文件中各占两个字节，副版本号占用第5、6两个字节，而主版本号则占用第7，8两个字节。JDK1.0的主版本号为45，以后的每个新主版本都会在原先版本的基础上加1。若现在使用的是JDK1.7编译出来的class文件，则相应的主版本号应该是51,对应的7，8个字节的十六进制的值应该是 0x33。 JVM在加载class文件的时候，会读取出主版本号，然后比较这个class文件的主版本号和JVM本身的版本号，如果JVM本身的版本号小于class文件的版本号，JVM会认为加载不了这个class文件，会抛出我们经常见到的&quot;java.lang.UnsupportedClassVersionError: Bad version number in .class file &quot; Error错误；反之，JVM会认为可以加载此class文件，继续加载此class文件。 NO3.常量池计数器(constant_pool_count) 常量池是class文件中非常重要的结构，它描述着整个class文件的字面量信息。 常量池是由一组constant_pool结构体数组组成的，而数组的大小则由常量池计数器指定。常量池计数器constant_pool_count 的值等于constant_pool表中的成员数+ 1。constant_pool表的索引值只有在大于 0 且小于constant_pool_count时(即1~(constant_pool_count-1))才会被认为是有效的。 这个容量计数是从1而不是从0开始的，如果常量池容量为十六进制数0x0016，即十进制22，这就代表着常量池中有21个常量，索引值范围为1-21。在Class文件格式规范制定时，设计者将第0项常量空出来是有特殊考虑的，用于在特定情况下表达“不引用任何一个常量池项目”。 NO4.常量池数据区(constant_pool[contstant_pool_count-1]) 常量池，constant_pool是一种表结构,它包含 Class 文件结构及其子结构中引用的所有字符串常量、 类或接口名、字段名和其它常量。 常量池中的每一项都具备相同的格式特征——第一个字节作为类型标记用于识别该项是哪种类型的常量，称为 “tag byte” 。常量池的索引范围是 1 至constant_pool_count−1。常量池的具体细节我们会稍后讨论。 NO6.访问标志(access_flags) 访问标志，access_flags 是一种掩码标志，用于表示某个类或者接口的访问权限及基础属性。 NO7.类索引(this_class) 类索引，this_class的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类或接口。 NO8.父类索引(super_class) 父类索引，对于类来说，super_class 的值必须为 0 或者是对constant_pool 表中项目的一个有效索引值。如果它的值不为 0，那 constant_pool 表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类的直接父类。当前类的直接父类，以及它所有间接父类的access_flag 中都不能带有ACC_FINAL 标记。对于接口来说，它的Class文件的super_class项的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为代表 java.lang.Object 的 CONSTANT_Class_info 类型常量 。如果 Class 文件的 super_class的值为 0，那这个Class文件只可能是定义的是java.lang.Object类，只有它是唯一没有父类的类。 NO9.接口计数器(interfaces_count) 接口计数器，interfaces_count的值表示当前类或接口的直接父接口数量。 NO10.接口信息数据区(interfaces[interfaces_count]) 接口表，interfaces[]数组中的每个成员的值必须是一个对constant_pool表中项目的一个有效索引值， 它的长度为 interfaces_count。每个成员 interfaces[i] 必须为 CONSTANT_Class_info类型常量，其中 0 ≤ i &lt;interfaces_count。在interfaces[]数组中，成员所表示的接口顺序和对应的源代码中给定的接口顺序（从左至右）一样，即interfaces[0]对应的是源代码中最左边的接口。 NO11.字段计数器(fields_count) 字段计数器，fields_count的值表示当前 Class 文件 fields[]数组的成员个数。 fields[]数组中每一项都是一个field_info结构的数据项，它用于表示该类或接口声明的类字段或者实例字段。 NO12.字段信息数据区(fields[fields_count]) 字段表，fields[]数组中的每个成员都必须是一个fields_info结构的数据项，用于表示当前类或接口中某个字段的完整描述。 fields[]数组描述当前类或接口声明的所有字段，但不包括从父类或父接口继承的部分。 NO13.方法计数器(methods_count) 方法计数器， methods_count的值表示当前Class 文件 methods[]数组的成员个数。Methods[]数组中每一项都是一个 method_info 结构的数据项。 NO14.方法信息数据区(methods[methods_count]) 方法表，methods[] 数组中的每个成员都必须是一个 method_info 结构的数据项，用于表示当前类或接口中某个方法的完整描述。如果某个method_info 结构的access_flags 项既没有设置 ACC_NATIVE 标志也没有设置ACC_ABSTRACT 标志，那么它所对应的方法体就应当可以被 Java 虚拟机直接从当前类加载，而不需要引用其它类。 method_info结构可以表示类和接口中定义的所有方法，包括实例方法、类方法、实例初始化方法方法和类或接口初始化方法方法 。methods[]数组只描述当前类或接口中声明的方法，不包括从父类或父接口继承的方法。 NO15.属性计数器(attributes_count) 属性计数器，attributes_count的值表示当前 Class 文件attributes表的成员个数。attributes表中每一项都是一个attribute_info 结构的数据项。 NO16.属性信息数据区(attributes[attributes_count]) 属性表，attributes 表的每个项的值必须是attribute_info结构。 在Java 7 规范里，Class文件结构中的attributes表的项包括下列定义的属性： InnerClasses 、 EnclosingMethod 、 Synthetic 、Signature、SourceFile，SourceDebugExtension 、Deprecated、RuntimeVisibleAnnotations 、RuntimeInvisibleAnnotations以及BootstrapMethods属性。 对于支持 Class 文件格式版本号为 49.0 或更高的 Java 虚拟机实现，必须正确识别并读取attributes表中的Signature、RuntimeVisibleAnnotations和RuntimeInvisibleAnnotations属性。对于支持Class文件格式版本号为 51.0 或更高的 Java 虚拟机实现，必须正确识别并读取 attributes表中的BootstrapMethods属性。Java 7 规范 要求任一 Java 虚拟机实现可以自动忽略 Class 文件的 attributes表中的若干 （甚至全部） 它不可识别的属性项。任何本规范未定义的属性不能影响Class文件的语义，只能提供附加的描述信息 。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存分配和回收策略]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十篇文章，本章对内存分配和垃圾回收的细节再次详细说明一下，并且说明一下逃逸分析/栈上分配以及TLAB两种方式的概念和原理。 1. 对象优先在Eden分配 前面文章曾介绍HotSpot虚拟机新生代内存布局及算法: （1）、将新生代内存分为一块较大的Eden空间和两块较小的Survivor空间； （2）、每次使用Eden和其中一块Survivor； （3）、当回收时，将Eden和使用中的Survivor中还存活的对象一次性复制到另外一块Survivor； （4）、而后清理掉Eden和使用过的Survivor空间； （5）、后面就使用Eden和复制到的那一块Survivor空间，重复步骤3； 默认Eden：Survivor=8:1，即每次可以使用90%的空间，只有一块Survivor的空间被浪费； 大多数情况下，对象在新生代Eden区中分配； 当Eden区没有足够空间进行分配时，JVM将发起一次Minor GC（新生代GC）； Minor GC时，如果发现存活的对象无法全部放入Survivor空间，只好通过分配担保机制提前转移到老年代。 2. 大对象直接进入老年代 大对象指需要大量连续内存空间的Java对象，如，很长的字符串、数组； 经常出现大对象容易导致内存还有不少空间就提前触发GC,以获取足够的连续空间来存放它们，所以应该尽量避免使用创建大对象； -XX:PretenureSizeThreshold： 可以设置这个阈值，大于这个参数值的对象直接在老年代分配； 默认为0（无效），且只对Serail和ParNew两款收集器有效； 如果需要使用该参数，可考虑ParNew+CMS组合。 3. 长期存活的对象将进入老年代 JVM给每个对象定义一个对象年龄计数器，其计算流程如下： 在Eden中分配的对象，经Minor GC后还存活，就复制移动到Survivor区，年龄为1； 而后每经一次Minor GC后还存活，在Survivor区复制移动一次，年龄就增加1岁； 如果年龄达到一定程度，就晋升到老年代中； -XX:MaxTenuringThreshold： 设置新生代对象晋升老年代的年龄阈值，默认为15； 4. 动态对象年龄判定 JVM为更好适应不同程序，不是永远要求等到MaxTenuringThreshold中设置的年龄； 如果在Survivor空间中相同年龄的所有对象大小总和大于Survivor空间的一半，大于或等于该年龄的对象就可以直接进入老年代 5. 空间分配担保 在前面曾简单介绍过分配担保： 当Survivor空间不够用时，需要依赖其他内存（老年代）进行分配担保（Handle Promotion）； 分配担保的流程如下： 在发生Minor GC前，JVM先检查老年代最大可用的连续空间是否大于新生所有对象空间； 如果大于，那可以确保Minor GC是安全的； 如果不大于，则JVM查看HandlePromotionFailure值是否允许担保失败； 如果允许，就继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小； 如果大于，将尝试进行一次Minor GC，但这是有风险的； 如果小于或HandlePromotionFailure值不允许冒险，那这些也要改为进行一次Full GC； 尝试Minor GC的风险–担保失败： 因为尝试Minor GC前面，无法知道存活的对象大小，所以使用历次晋升到老年代对象的平均大小作为经验值； 假如尝试的Minor GC最终存活的对象远远高于经验值的话，会导致担保失败（Handle Promotion Failure）； 失败后只有重新发起一次Full GC，这绕了一个大圈，代价较高； 但一般还是要开启HandlePromotionFailure，避免Full GC过于频繁，而且担保失败概率还是比较低的； JDK6-u24后，JVM代码中已经不再使用HandlePromotionFailure参数了； 规则变为： ⭐⭐⭐只要老年代最大可用的连续空间大于新生所有对象空间或历次晋升到老年代对象的平均大小，就会进行Minor GC；否则进行Full GC； ⭐⭐⭐即老年代最大可用的连续空间小于新生所有对象空间时，不再检查HandelPromotionFailure，而直接检查历次晋升到老年代对象的平均大小； 6. 逃逸分析 般认为new出来的对象都是被分配在堆上，但是这个结论不是那么的绝对，通过对Java对象分配的过程分析，可以知道有两个地方会导致Java中new出来的对象并不一定分配在所认为的堆上。这两个点分别是Java中的逃逸分析和TLAB（Thread Local Allocation Buffer）。 6.1 什么是栈上分配？ 栈上分配主要是指在Java程序的执行过程中，在方法体中声明的变量以及创建的对象，将直接从该线程所使用的栈中分配空间。 一般而言，创建对象都是从堆中来分配的，这里是指在栈上来分配空间给新创建的对象。 6.2 什么是逃逸？ 逃逸是指在某个方法之内创建的对象，除了在方法体之内被引用之外，还在方法体之外被其它变量引用到； 这样带来的后果是在该方法执行完毕之后，该方法中创建的对象将无法被GC回收，由于其被其它变量引用。 正常的方法调用中，方法体中创建的对象将在执行完毕之后，将回收其中创建的对象；而此时由于无法回收，即成为逃逸。 123456789101112static V global_v; public void a_method()&#123; V v=b_method(); c_method(); &#125; public V b_method()&#123; V v=new V(); return v; &#125; public void c_method()&#123; global_v=new V(); &#125; 其中b_method方法内部生成的V对象的引用被返回给a_method方法内的变量v，c_method方法内生成的V对象被赋给了全局变量global_v。这两种场景都发生了（引用）逃逸。 6.3 逃逸分析 在JDK 6之后支持对象的栈上分析和逃逸分析，在JDK7中完全支持栈上分配对象。其是否打开逃逸分析依赖于以下JVM的设置： -XX:+DoEscapeAnalysis 6.4 栈上分配与逃逸分析的关系 进行逃逸分析之后，产生的后果是所有的对象都将由栈上分配，而非从JVM内存模型中的堆来分配。 6.5 逃逸分析／栈上分配的优劣分析 JVM在Server模式下的逃逸分析可以分析出某个对象是否永远只在某个方法、线程的范围内，并没有“逃逸”出这个范围，逃逸分析的一个结果就是对于某些未逃逸对象可以直接在栈上分配，由于该对象一定是局部的，所以栈上分配不会有问题。 消除同步。 线程同步的代价是相当高的，同步的后果是降低并发性和性能。逃逸分析可以判断出某个对象是否始终只被一个线程访问，如果只被一个线程访问，那么对该对象的同步操作就可以转化成没有同步保护的操作，这样就能大大提高并发程度和性能。 矢量替代。 逃逸分析方法如果发现对象的内存存储结构不需要连续进行的话，就可以将对象的部分甚至全部都保存在CPU寄存器内，这样能大大提高访问速度。 劣势： 栈上分配受限于栈的空间大小，一般自我迭代类的需求以及大的对象空间需求操作，将导致栈的内存溢出；故只适用于一定范围之内的内存范围请求。 6.6 测试 123456789101112131415161718192021222324class EscapeAnalysis &#123; private static class Foo &#123; private int x; private static int counter; //会发生逃逸 public Foo() &#123; x = (++counter); &#125; &#125; public static void main(String[] args) &#123; //开始时间 long start = System.nanoTime(); for (int i = 0; i &lt; 1000 * 1000 * 10; ++i) &#123; Foo foo = new Foo(); &#125; //结束时间 long end = System.nanoTime(); System.out.println("Time cost is " + (end - start)); &#125;&#125; 未开启逃逸分析设置为： -server -verbose:gc 在未开启逃逸分析的状况下运行情况如下： 12345678910[GC 5376K-&gt;427K(63872K), 0.0006051 secs] [GC 5803K-&gt;427K(63872K), 0.0003928 secs] [GC 5803K-&gt;427K(63872K), 0.0003639 secs] [GC 5803K-&gt;427K(69248K), 0.0003770 secs] [GC 11179K-&gt;427K(69248K), 0.0003987 secs] [GC 11179K-&gt;427K(79552K), 0.0003817 secs] [GC 21931K-&gt;399K(79552K), 0.0004342 secs] [GC 21903K-&gt;399K(101120K), 0.0002175 secs] [GC 43343K-&gt;399K(101184K), 0.0001421 secs] Time cost is 58514571 开启逃逸分析设置为： -server -verbose:gc -XX:+DoEscapeAnalysis 开启逃逸分析的状况下，运行情况如下： Time cost is 10031306 未开启逃逸分析时，运行上述代码，JVM执行了GC操作，而在开启逃逸分析情况下，JVM并没有执行GC操作。同时，操作时间上，开启逃逸分析的程序运行时间是未开启逃逸分析时间的1/5。 7. 再来聊聊TLAB JVM在内存新生代Eden Space中开辟了一小块线程私有的区域，称作TLAB（Thread-local allocation buffer）。默认设定为占用Eden Space的1%。在Java程序中很多对象都是小对象且用过即丢，它们不存在线程共享也适合被快速GC，所以对于小对象通常JVM会优先分配在TLAB上，并且TLAB上的分配由于是线程私有所以没有锁开销。因此在实践中分配多个小对象的效率通常比分配一个大对象的效率要高。 也就是说，Java中每个线程都会有自己的缓冲区称作TLAB（Thread-local allocation buffer），每个TLAB都只有一个线程可以操作，TLAB结合bump-the-pointer技术可以实现快速的对象分配，而不需要任何的锁进行同步，也就是说，在对象分配的时候不用锁住整个堆，而只需要在自己的缓冲区分配即可。 8. 对象内存分配过程再升级 编译器通过逃逸分析，确定对象是在栈上分配还是在堆上分配。如果是在堆上分配，则进入选项2. 如果tlab_top + size &lt;= tlab_end，则在在TLAB上直接分配对象并增加tlab_top 的值，如果现有的TLAB不足以存放当前对象则3. 重新申请一个TLAB，并再次尝试存放当前对象。如果放不下，则4. 在Eden区加锁（这个区是多线程共享的），如果eden_top + size &lt;= eden_end则将对象存放在Eden区，增加eden_top 的值，如果Eden区不足以存放，则5. 执行一次Young GC（minor collection）。 经过Young GC之后，如果Eden区任然不足以存放当前对象，则直接分配到老年代。 老年代还是不足，则触发Full GC，再不足就OOM错误]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集器介绍]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第九篇文章，主要介绍七种比较经典的垃圾收集器的实现原理。 垃圾收集器 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 1. Serial 收集器 Serial 翻译为串行，垃圾收集和用户程序不能同时执行，这意味着在执行垃圾收集的时候需要停顿用户程序。除了 CMS 和 G1 之外，其它收集器都是以串行的方式执行。CMS 和 G1 可以使得垃圾收集和用户程序同时执行，被称为并发执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 模式下的默认新生代收集器，因为在用户的桌面应用场景下，分配给虚拟机管理的内存一般来说不会很大。Serial 收集器收集几十兆甚至一两百兆的新生代停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿是可以接受的。 2. ParNew 收集器 它是 Serial 收集器的多线程版本。 是 Server模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作。 默认开始的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数。 3. Parallel Scavenge 收集器 与 ParNew 一样是并行的多线程收集器。 其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 -XX:MaxGCPauseMillis 参数以及直接设置吞吐量大小的 -XX:GCTimeRatio 参数（值为大于 0 且小于 100 的整数）。缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 还提供了一个参数 -XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为 GC 自适应的调节策略（GC Ergonomics）。 4. Serial Old 收集器 是 Serial 收集器的老年代版本，也是给 Client 模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器 CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。 特点：并发收集、低停顿。并发指的是用户线程和 GC 线程同时运行。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。可以使用 -XX:CMSInitiatingOccupancyFraction 来改变触发 CMS 收集器工作的内存占用百分，如果这个值设置的太大，导致预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器 G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 Java 堆被分为新生代、老年代和永久代，其它收集器进行收集的范围都是整个新生代或者老生代，而 G1 可以直接对新生代和永久代一起回收。 G1 把新生代和老年代划分成多个大小相等的独立区域（Region），新生代和永久代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 8. 比较 收集器 串行/并行/并发 新生代/老年代 收集算法 目标 适用场景 Serial 串行 新生代 复制 响应速度优先 单 CPU 环境下的 Client 模式 Serial Old 串行 老年代 标记-整理 响应速度优先 单 CPU 环境下的 Client 模式、CMS 的后备预案 ParNew 串行 + 并行 新生代 复制算法 响应速度优先 多 CPU 环境时在 Server 模式下与 CMS 配合 Parallel Scavenge 串行 + 并行 新生代 复制算法 吞吐量优先 在后台运算而不需要太多交互的任务 Parallel Old 串行 + 并行 老年代 标记-整理 吞吐量优先 在后台运算而不需要太多交互的任务 CMS 并行 + 并发 老年代 标记-清除 响应速度优先 集中在互联网站或 B/S 系统服务端上的 Java 应用 G1 并行 + 并发 新生代 + 老年代 标记-整理 + 复制算法 响应速度优先 面向服务端应用，将来替换 CMS]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC相关]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2FGC%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第八篇文章，我们知道，JVM为我们管理垃圾对象实现自动回收，让我们不需要太关心内存释放问题，一定程度上减少了内存溢出的错误。这一切的背后是如何实现的呢？ 一、垃圾标记算法 1.1 引用计数法 算法思想 给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值加一；当引用失效时，计数器☞减一；任何时候计数器为0的对象是不可能再被使用的。 主要缺陷 无法解决对象间相互循环引用的问题。 举个例子 12345678910111213141516171819202122232425public class Test &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; private byte[] bigSize = new byte[2 * _1MB]; public static void testGC() &#123; Test objA = new Test();//count=1 Test objB = new Test();//count=1 objA.instance = objB;//count=2 objB.instance = objA;//count=2 objA = null;//count=1 objB = null;//count=1 System.gc(); &#125; public static void main(String[] args) &#123; testGC(); &#125;&#125; 输入参数 -verbose:gc -XX:+PrintGCDetails 结果 1234567891011[GC (System.gc()) [PSYoungGen: 6063K-&gt;600K(37888K)] 6063K-&gt;608K(123904K), 0.0037131 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 600K-&gt;0K(37888K)] [ParOldGen: 8K-&gt;529K(86016K)] 608K-&gt;529K(123904K), [Metaspace: 2595K-&gt;2595K(1056768K)], 0.0062705 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] Heap PSYoungGen total 37888K, used 328K [0x00000000d6100000, 0x00000000d8b00000, 0x0000000100000000) eden space 32768K, 1% used [0x00000000d6100000,0x00000000d6152030,0x00000000d8100000) from space 5120K, 0% used [0x00000000d8100000,0x00000000d8100000,0x00000000d8600000) to space 5120K, 0% used [0x00000000d8600000,0x00000000d8600000,0x00000000d8b00000) ParOldGen total 86016K, used 529K [0x0000000082200000, 0x0000000087600000, 0x00000000d6100000) object space 86016K, 0% used [0x0000000082200000,0x0000000082284778,0x0000000087600000) Metaspace used 2601K, capacity 4486K, committed 4864K, reserved 1056768K class space used 288K, capacity 386K, committed 512K, reserved 1048576K 分析 日志中6063K-&gt;600K(37888K)，从原来的6M内存变成了600k，表明对象已被回收，从而表明JVM没有使用引用计数算法。Java中使用了可达性分析算法来来判定对象是否存活。 1.2 可达性分析算法 这个算法的基本思路就是通过一系列的称谓GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所有走过的路径为引用链，当一个对象到GC Roots没有任何引用链时，则证明此对象时不可用的，下面看一下例子： 上面的这张图，对象object5、object6、object7虽然互相没有关联，但是它们到GC Roots是不可达的，所以它们将会被判定为是可回收的对象 注：Java语言中，可作为GC Roots的对象包括下面几种： 虚拟机栈(栈帧中的本地变量表)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI(即一般说的Native方法)引用的对象 活跃线程引用的对象 二、Java中的引用类型 从JDK1.2之后，Java对引用的概念进行了扩充，将引用分为强引用，软引用，弱引用，虚引用，这四种引用的强度一次逐渐减弱 强引用就是指在程序代码之中普遍存在的，类似 Object obj = new Object() 这类的引用，只要强引用还存在，垃圾回收器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非需要的对象，对于软引用关联着的对象，在系统将要发生内存异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存异常 弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前，当垃圾收集器工作时，无论当前内存释放足够，都会回收掉只被弱引用关联的对象 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例，对一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 三、两次标记 《深入理解java虚拟机》原文： 在java根搜索算法中判断对象的可达性，对于不可达的对象，也并不一定是必须清理。这个时候有一个缓刑期，真正的判断一个对象死亡，至少要经过俩次标记过程： 如果对象在进行根搜索后发现没有与GC roots相关联的引用链，那他将会第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法，当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这俩种情况都视为“没有必要执行”。 即当一个对象重写了finalize()方法的时候，这个对象被判定为有必要执行finalize()方法，那么这个对象被放置在F-Queue队列之中，并在稍后由一条由虚拟机自动建立的、低优先级的Finalizer线程去执行。这里所谓的执行是指虚拟机会出发这个方法，但不承诺会等待它运行结束。这样做的原因：如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（极端的情况下），将可能会导致F-Queue队列中的其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己----只要重新与引用链上的任何建立关联即可，那么在第二次标记时它将会被移出“即将回收”的集合；如果对象这时候没有逃脱，就会被回收。 3.1 finalize的工作原理 一旦垃圾收集器准备好释放对象占用的存储空间，它首先调用finalize()，而且只有在下一次垃圾收集过程中，才会真正回收对象的内存.所以如果使用finalize()，就可以在垃圾收集期间进行一些重要的清除或清扫工作. 3.2 finalize()在什么时候被调用? 所有对象被Garbage Collection时自动调用,比如运行System.gc()的时候. 程序退出时为每个对象调用一次finalize方法。 显式的调用finalize方法 这个方法的用途就是：在该对象被回收之前，该对象的finalize()方法会被调用。这里的回收之前指的就是被标记之后，问题就出在这里，有没有一种情况就是原本一个对象开始不再上一章所讲的“关系网”（引用链）中，但是当开发者重写了finalize()后，并且将该对象重新加入到了“关系网”中，也就是说该对象对我们还有用，不应该被回收，但是已经被标记啦，怎么办呢？ 针对这个问题，虚拟机的做法是进行两次标记，即第一次标记不在“关系网”中的对象，并且要判断该对象有没有实现finalize()方法了，如果没有实现就直接判断该对象可回收。如果实现了就会先放在一个队列中，并由虚拟机建立的一个低优先级的线程去执行它。 随后就会进行第二次的小规模标记，如果对象还没有逃脱，在这次被标记的对象就会真正的被回收了。 四、垃圾收集算法 4.1 标记-清除算法 最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经基本介绍过了。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-清除算法的执行过程如图： 4.2 复制算法 为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。 复制算法的执行过程如图： 4.3 标记-整理算法 复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，“标记-整理”算法的示意图如图 4.4 分代收集算法 当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收 五、新生代和老年代 5.1 新生代 新生代分为三个区域，一个Eden区和两个Survivor区，它们之间的比例为（8：1：1），这个比例也是可以修改的。通常情况下，对象主要分配在新生代的Eden区上，少数情况下也可能会直接分配在老年代中。 Java虚拟机每次使用新生代中的Eden和其中一块Survivor（From），在经过一次MinorGC后，将Eden和Survivor中还存活的对象一次性地复制到另一块Survivor空间上（这里使用的复制算法进行GC），最后清理掉Eden和刚才用过的Survivor（From）空间。将此时在Survivor空间存活下来的对象的年龄设置为1，以后这些对象每在Survivor区熬过一次GC，它们的年龄就加1，当对象年龄达到某个年龄（默认值为15）时，就会把它们移到老年代中。 在新生代中进行GC时，有可能遇到另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。 总结： 1、Minor GC是发生在新生代中的垃圾收集，采用的复制算法； 2、新生代中每次使用的空间不超过90%，主要用来存放新生的对象； 3、Minor GC每次收集后Eden区和一块Survivor区都被清空； 5.1 老年代 老年代里面存放都是生命周期长的对象，对于一些较大的对象（即需要分配一块较大的连续内存空间），是直接存入老年代的，还有很多从新生代的Survivor区域中熬过来的对象。 老年代中使用的是Full GC，Full GC所采用的是标记-清除或者标记-整理算法。老年代中的Full GC不像Minor GC操作那么频繁，并且进行一次Full GC所需要的时间要比Minor GC的时间长。 5.2 触发Full GC的条件 老年代空间不足 JDK8以前的永久代空间不足，现在永久代已经被元数据区代替 CMS GC时出现promotion failed，concurrent mode failure(下面文章讲到CMS垃圾收集器的时候会说明) minor GC晋升到老年代的平均大小大于老年代的剩余空间 调用System.gc()提醒JVM回收一下，只是提醒 5.3 对象如何晋升到老年代 一般有如下情况会晋升： 经历一定minor次数依然存活的对象 survivor区中存放不下的对象 新生成的大对象 5.4 常用的调优参数 5.5 内存申请过程 A. JVM会试图为相关Java对象在Eden中初始化一块内存区域 B. 当Eden空间足够时，内存申请结束。否则到下一步 C. JVM试图释放在Eden中所有不活跃的对象（Minor GC）, 释放后若Eden空间仍然不足以放入新对象，则试图将部分Eden中活跃对象放入Survivor区 D. 当Survivor区空间不够时或者某些对象熬的时间比较长，则Survivor区这些对象会被移到Old区 E. 当Old区空间不够时，JVM会在Old区进行完全的垃圾收集（Full GC） F. 完全垃圾收集后，若Survivor及Old区仍然无法存放从Eden复制过来的部分对象，导致JVM无法在Eden区为新对象创建内存区域，则出现out of memory错误.]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA内存模型常问面试题]]></title>
    <url>%2F2019%2F02%2F08%2FJVM%2FJAVA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%B8%B8%E9%97%AE%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第七篇文章，介绍一些面试比较常问的问题。 一、JVM三大性能调优参数-Xms -Xmx -Xss的含义 一般我们可以传入以下参数去调整堆和内存所占的大小： 1java -Xms128m -Xmx128m -Xss256k -jar xxx.jar -Xms ：堆的初始值 -Xmx ：堆能达到的最大值 -Xss ：规定了每个线程虚拟机栈的大小 二、JAVA内存模型中堆和栈的区别 首先来了解一下几种不同的内存分配策略： 静态存储：编译时确定每个数据目标在运行时的存储空间需求，比如static声明的静态变量，这里的数据一般都放在方法区，java8中这个区域叫做元数据区，用的时物理内存，并且之前合在一起的字符串常量池也被移到了堆区，详情见上一篇文章。 栈式存储：数据去需求在编译时未知，运行时模块入口前确定，比如基本数据类型，都是在运行的时候，才知道数据(字面量)到底是什么，对于JVM，一个方法内的执行，局部变量表和操作数栈的大小时确定的，即引用变量和栈空间大小是编译器确定的，至于字面量等运行时才能确定。 堆式存储：编译时或运行时模块入口都无法确定，动态分配，比如可变长度串、对象实例 下面来看看栈和堆的联系： 引用对象或者数组时，栈里定义变量保存堆中目标的首地址。 下面来看看栈和堆的区别： 管理方式：栈自动释放，堆需要GC 空间大小：栈比堆小 碎片相关：栈产生的碎片远小于堆 分配方式：栈支持静态和动态分配，而堆仅支持动态分配 效率：栈的效率比堆高 简单总结：栈比较小，随着方法执行完毕自动释放，栈数据结构简单，所以操作也简单高效。堆放各种对象实例和数组，必定要比较大的空间，那么需要GC来回收不需要的数据，效率低并且碎片也比较多，由于堆的操作比较复杂，所以数据结构也复杂，效率低。 三、元空间、堆、线程独占部分间的联系 先来看一个最简单的程序： 我们分别从元空间、堆、以及线程独占的部分来看看分别存储了啥： 学到这里，对于这些东西已经不需要解释了。针对JVM内存模型的知识在这里就串联起来了。了解到这里，对内存模型这一块基本的知识已经差不多了。 四、再来说说字符串 之前在java字符串核心一网打尽文章中，其实是对于JDK8这个版本的字符串特性进行详细的解读，其中也介绍了intern这个方法的含义和用法，由于JDK6和JDK6+关于intern是不一样的，这里对比一下。 对于JDK8： 12345678910111213public static void main(String[] args) throws ClassNotFoundException &#123; //第一种情况 String str1 = new String("a"); str1.intern(); String str2 = "a"; System.out.println(str1 == str2); //第二种情况 String str3 = new String("a") + new String("a"); str3.intern(); String str4 = "aa"; System.out.println(str3 == str4);&#125; 输出结果为： 12falsetrue 但是在JDK6中执行结果为： 12falsefalse 这个问题困扰了我很久，由于之前基础不是太扎实，所以直接就跳过了这个问题，在面试的时候几乎也不会太深究，但是一直成为我心里的坎。今天要把他解决掉。在说明这个问题之前，需要说明一下JVM有三种常量池： 4.1 三种常量池 Class文件中的常量池 这里面主要存放两大类常量：字面量和符号引用，符号引用包含三类常量： 类和接口的全限定名(Full Qualified Name) 字段的名称和描述符(Descriptor) 方法的名称和描述符 这个用javap看一下就能明白，这里只涉及字符串就不谈其他的了。简单地说，用双引号引起来的字符串字面量都会进这里面。 1String str2 = "a"; 这里的str2就是符号引用，a就是字面量。 运行时常量池 方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池(Constant Pool Table)，存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池。 全局字符串 HotSpot VM里，记录interned string的一个全局表叫做StringTable，它本质上就是个HashSet&lt;String&gt;。这是个纯运行时的结构，而且是惰性（lazy）维护的。 注意，它里面存放的是引用。 所以，一般我们说一个字符串进入了全局的字符串常量池其实是说在这个StringTable中保存了对它的引用，反之，如果说没有在其中就是说StringTable中没有对它的引用。 4.2 字面量进入字符串常量池的时机 先给出一个结论：就HotSpot VM的实现来说，加载类的时候，那些字符串字面量会进入到当前类的运行时常量池，不会进入全局的字符串常量池（即在StringTable中并没有相应的引用，在堆中也没有对应的对象产生） 那么加载类的过程发生的是什么呢？ R大的一篇文章： 在类加载阶段， JVM会在堆中创建 对应这些 class文件常量池中的 字符串对象实例 并在字符串常量池中驻留其引用。具体在resolve阶段执行。这些常量全局共享。 这里说的比较笼统，没错，是resolve阶段，但是并不是大家想的那样，立即就创建对象并且在字符串常量池中驻留了引用。 JVM规范里明确指定resolve阶段可以是lazy的。 所以，类加载的时候，必定要做的东西是，将class文件中字面量和符号引用放入运行时常量池中，而JVM规范里Class文件的常量池项的类型，有两种东西：CONSTANT_Utf8和CONSTANT_String。后者是String常量的类型，但它并不直接持有String常量的内容，而是只持有一个index，这个index所指定的另一个常量池项必须是一个CONSTANT_Utf8类型的常量，这里才真正持有字符串的内容。 CONSTANT_Utf8会在类加载的过程中就全部创建出来，而CONSTANT_String则是lazy resolve的，例如说在第一次引用该项的ldc指令被第一次执行到的时候才会resolve。 4.3 ldc指令是什么东西？ 简单地说，它用于将int、float或String型常量值从常量池中推送至栈顶 以下面代码为例： 12345public class Abc &#123; public static void main(String[] args) &#123; String a = "AA"; &#125; &#125; 查看其编译后的Class文件如下： 根据上面说的，在类加载阶段，这个 resolve 阶段（ constant pool resolution）是lazy的。换句话说并没有真正的对象，字符串常量池里自然也没有。执行ldc指令就是触发这个lazy resolution动作的条件。 ldc字节码在这里的执行语义是：到当前类的运行时常量池去查找该index对应的项,即上面说的CONSTANT_String指向的index，如果该项尚未resolve则resolve之，并返回resolve后的内容。 在遇到String类型常量时，resolve的过程如果发现StringTable已经有了内容匹配的java.lang.String的引用，则直接返回这个引用，反之，如果StringTable里尚未有内容匹配的String实例的引用，则会在Java堆里创建一个对应内容的String对象，然后在StringTable记录下这个引用，并返回这个引用出去。 这里很重要，昭示了一个重要问题：String a = &quot;AA&quot;;这一句执行完，要看字符串常量池中是否已经存在，不存在的话是要在堆中先创建对象的，然后把堆地址给全局的字符串常量池。 理解到这，有些问题就可以解决了，这里先不回答最上面的问题，先来看看下面的例子。注意运行环境是JDK8： 123456789class NewTest0 &#123; public static String s1="static"; // 第一句 public static void main(String[] args) &#123; String s2 = new String("he")+new String("llo"); //第二句 s2.intern(); // 第三句 String s3="hello"; //第四句 System.out.println(s2 == s3);//第五句，输出是true。 &#125;&#125; &quot;static&quot; &quot;he&quot; &quot;llo&quot; &quot;hello&quot;都会进入Class的常量池， 按照上面说的，类加载阶段由于resolve 阶段是lazy的，所以是不会创建实例，更不会驻留字符串常量池了。 但是要注意这个“static”和其他三个不一样，它是静态的，在类加载阶段中的初始化阶段，会为静态变量指定初始值，也就是要把“static”赋值给s1，这个赋值操作要怎么搞啊，先ldc指令把它放到栈顶，然后用putstatic指令完成赋值。注意，ldc指令，根据上面说的，会创建&quot;static&quot;字符串对象，并且会保存一个指向它的引用到字符串常量池。 运行main方法后，首先是第二句，一样的，要先用ldc把&quot;he&quot;和&quot;llo&quot;送到栈顶，换句话说，会创建他俩的对象（注意，在堆中开辟本体所占的空间，还没到new的那一步），并且会保存引用到字符串常量池中（把本地在堆中空间地址传给字符串常量池）；然后有个＋号对吧，内部是创建了一个StringBuilder对象，一路append，最后调用StringBuilder对象的toString方法得到一个String对象（内容是hello，注意这个toString方法会new一个String对象），并把它赋值给s2（s2指向的是new出来的新对象，是新的一块内存空间）。 注意，此时还没有把hello的引用放入字符串常量池。然后是第三句，intern方法一看，字符串常量池里面没有，它会把上面的这个hello对象的引用保存到字符串常量池，然后返回这个引用，但是这个返回值我们并没有使用变量去接收，所以没用。 第四句，字符串常量池里面已经有了，直接用嘛。所以s2和s3都是s2的指向的地址。 再来看个例子： 123456789101112public static void main(String[] args) &#123; // ① String s1=new String("he")+new String("llo"); String s2=new String("h")+new String("ello"); // ② String s3=s1.intern(); // ③ String s4=s2.intern(); // ④ System.out.println(s1==s3); System.out.println(s1==s4);&#125; 首先是将一些符号引用和字面量从class文件的常量池中撞到运行时常量池。然后运行main方法，先看第一句，会创建&quot;he&quot;和&quot;llo&quot;对象，并放入字符串常量池，然后会创建一个&quot;hello&quot;对象，没有放入字符串常量池，s1指向这个&quot;hello&quot;对象。 第二句，创建&quot;h&quot;和&quot;ello&quot;对象，并放入字符串常量池，然后会创建一个&quot;hello&quot;对象，没有放入字符串常量池，s2指向这个&quot;hello&quot;对象。 第三句，字符串常量池里面还没有，于是会把s1指向的String对象的引用放入字符串常量池（换句话说，放入池中的引用和s1指向了同一个对象），然后会把这个引用返回给了s3，所以s3==s1是true。 第四句，字符串常量池里面已经有了，直接将它返回给了s4，所以s4==s1是true。 此时，回到一开始： 123456789101112131415161718192021public static void main(String[] args) throws ClassNotFoundException &#123; //第一种情况 //1 String str1 = new String("a"); //2 str1.intern(); //3 String str2 = "a"; //4 System.out.println(str1 == str2); //第二种情况 //5 String str3 = new String("a") + new String("a"); //6 str3.intern(); //7 String str4 = "aa"; //8 System.out.println(str3 == str4);&#125; 在jdk1.6及以前，调用intern() 如果常量池中不存在值相等的字符串时，jvm会复制一个字符串到创量池中，并返回常量池中的字符串。 而在jdk1.7及以后，调用intern() 如果常量池中不存在值相等的字符串时，jvm只是在常量池记录当前字符串的引用，并返回当前字符串的引用。 所以在JDK6情况下，都是返回false，原因是：第一种情况下，执行第一句，看到有个字符串&quot;a&quot;，那么首先是创建&quot;a&quot;本体对象，并且把副本放入字符串常量池中。执行第二句，发现字符串常量池中已经存在，则不放了。执行第三句， s2指向的是字符串常量池中的&quot;a&quot;，这个字符串常量池&quot;a&quot;所在的地址，肯定与堆中的新new出来的不一样。所以返回false。 第二种情况，第一句相当于： 其实相当于: 123String s1 = new String("a");String s2 = new String("a");String str3 = (new StringBuilder()).apend(s1).apend(s2).toString(); 会先在堆中创建两个对象&quot;a&quot;，拷贝一个副本到字符串常量池中，此时&quot;a&quot;已经存在于字符串常量池中了。然后拼接生成一个新的对象&quot;aa&quot;在堆中，这种拼接出来的&quot;aa&quot;此时是不会把副本放进字符串常量池的，因为字符串常量池只保存已确定的字面量，这种拼接的属于运行完成才能确定，所以字符串常量池中没有，直到执行第6句，才会尝试把&quot;aa&quot;副本放入字符串常量池，但是还是跟上面一样，一个指向堆，一个指向字符串常量池，肯定不相等。 在JDK6+情况下，第一个返回false，第二个返回true。原因是：第一种情况下，执行第一句，首先是创建&quot;a&quot;本体对象，并且把引用放进字符串常量池中，然后new，开辟新的地址空间，此时str1指向的是new出来的空间的引用。执行第二句，尝试将str1的引用放入字符串常量池，但是池中已经存在了，所以不能放，所以一个指向堆，一个是本体对象的引用，不一样，所以为false。第二种情况，&quot;a&quot;跟上面一样，在堆中开辟，然后引用放入字符串常量池中，后面拼接成&quot;aa&quot;，此时只是在堆中开辟空间，下面执行intern尝试把它的引用传给字符串常量池，由于字符串常量池中没有，所以就放进去了。此时字符串常量池中的引用与&quot;aa&quot;对象实际的堆地址是一样的，所以为true. 这边有一个事实：在执行String s1 = new String(&quot;a&quot;)的new之前，JVM先看到有一个字符串&quot;a&quot;，则会先看看字符串常量池中是否有这个&quot;a&quot;，有则直接返回字符串常量池引用，没有则给它开辟空间，并且把这个空间的引用传给字符串常量池。 整理自：木女孩的回答]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内连接和外连接]]></title>
    <url>%2F2019%2F02%2F05%2Fmysql%2F%E5%86%85%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%A4%96%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[在电信实习的时候，天天有人来面试，问的问题我都听的蛮清楚的，比如内连接和外连接的区别，好像很简单，但还是说的不好，这里总结一下。 12345 A表 B表id name id name 1 a 1 b 2 b 3 c4 c 内连接 内连接就是左表和右表相同的数据: 1select * from A inner join B on A.id=B.id 结果： 12id name id name 1 a 1 b 左外连接 左外连接就是以左表为准，去匹配右表，左表有多少条数据，结果就是多少条数据 1select * from A left join B on A.id=B.id 1234id name id name 1 a 1 b 2 b null null4 c null null 右外连接 右外连接就是与左外连接反之，以右表为准，去匹配左表，右表有多少条数据，结果就是多少条数据 1select * from A right join B on A.id=B.id 123id name id name 1 a 1 b null null 3 c 交叉连接 交叉连接不带 WHERE 子句，它返回被连接的两个表所有数据行的笛卡尔积，返回到 结果集合中的数据行数等于第一个表中符合查询条件的数据行数乘以第二个表中符合查 询条件的数据行数。 1select * from A join B 1234567id name id name1 a 1 b1 a 3 c2 b 1 b2 b 3 c4 c 1 b4 c 3 c 内连接和外连接的区别 内连接只列出两张表共同匹配的数据行，而外连接的结果集中不仅包含符合连接条件的数据行，还包括左表(左外连接或左连接)或右表(右外连接或右连接)中的所有数据行。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA内存模型-线程共享]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2FJAVA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-%E7%BA%BF%E7%A8%8B%E5%85%B1%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第六篇文章，介绍线程共享区域。 一、内存模型–JAVA堆 java堆一般是java虚拟机所管理的内存中最大的一块。 java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。 堆上存放对象实例和数组。 java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 二、内存模型–方法区 方法区和堆一样，是各个线程共享的内存区域。 它用于存储已被虚拟机加载的类信息、常量、静态变量、及时编译器编译后的代码等数据。 其中，类信息包含类的版本、字段、接口、方法 八、PermGen与Metaspace 其实，方法区可以理解为一个规范，jdk6的具体实现是PermGen,而后来的版本具体实现是Metaspace。它们有一定的区别。 在 HotSpot JVM 中，永久代中用于存放类和方法的元数据以及常量池，比如Class和Method。每当一个类初次被加载的时候，它的元数据都会放到永久代中。 永久代是有大小限制的，它用的是JVM内存，即与堆内存等价的no heap区域，因此如果加载的类太多，很有可能导致永久代内存溢出，即万恶的 java.lang.OutOfMemoryError: PermGen ，为此我们不得不对虚拟机做调优。 由于 PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。 根据上面的各种原因，PermGen 最终被移除，方法区移至 Metaspace，字符串常量移至 Java Heap。Metaspace并不在虚拟机中，而是使用本地内存,十分方便管理，不会出现永久带内存溢出问题，垃圾回收的时候这个单独区域方便处理。 三、运行时常量池 是方法区的一部分。 类文件中除了类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法的运行时常量池中存放。 这里尤其值得注意的是字符串的创建，会被扔到字符串常量池中。如果是new，那么还是在堆重创建的。当然，运行时也可以产生新的常量放入池中，比如讲new出来的字符串用intern()方法便可以在运行时将其放到常量池中。 举例 123456789101112public static void main(String[] args) &#123; String str1 = "hello"; String str2 = "hello"; System.out.println(str1 == str2); //true String str3 = new String("hello"); System.out.println(str1 == str3); //false System.out.println(str1 == str3.intern()); //true &#125; 说明 对于直接声明的内容相同的字符串，对于str2来说是不需要重新分配地址的，因为str1的hello这个常量已经存在于常量池中了。所以他们两个其实是一个东西。 对于new出来的str3，是不会直接扔到常量池中的，他是在堆中分配，地址不一样，所以显然是false。 String类的intern()方法，使得运行时将堆中产生的对象放入常量池中，所以是true。 这里我在java字符串核心一网打尽中已经详细说明了，不再赘述。 四、对象探秘 4.1 对象的创建过程 类加载检查：检查该对象的类是否已经被加载、解析、初始化过，如果没有则先进行类加载操作。 分配内存：如果内存规整使用“指针碰撞”分配，否则一般使用“空闲列表”分配，具体看垃圾回收器是否带有整理（Compact）空闲内存功能。 初始化：将内存区初始化置零，不包含对象头，这一步保证了对象的实例字段在java代码中可以不赋初值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 对象头设置：这个对象是哪个类的实例、如何找到类的元数据信息、哈希码、GC分代年龄信息等即为对象头 对象的方法：即按照程序员的意愿进行初始化 4.2 对象的内存布局 对象头 一部分称为Mark Word，存储对象自身运行时的数据，包含哈希码、GC分代年龄、锁状态标志等等。 采用压缩存储，压缩到虚拟机位数（32位/64位）。由于对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word被设计为一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。 另一部分为类型指针，指向它的类元数据，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息不一定要经过对象本身。 如果对象是一个java数组，那么在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通java对象的元数据信息确定java对象的大小，但是从数组的元数据中却无法确定数组的大小。 实例数据 实例数据部分是对象真正存储的有效信息，也是在程序中定义的各种类型的字段内容。 无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。 从分配策略中可以看出，相同宽度的字段总是分配在一起，在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前。 对齐填充 非必需，只有前两者加起来非8的倍数时才会有。 因为HotSpot VM 的自动内存管理系统要求对象起始地址必须是8字节的整数倍，也就是说，对象的大小必须是8字节的整数倍。不对齐的时候，需要通过它来填充对齐。 九、对象的访问定位 通过句柄访问 通过句柄访问对象：当java虚拟机GC移动堆对象时，并不需要修改reference，只需修改句柄对象的实例数据指针。 通过直接指针访问 通过直接指针访问对象：加快了对象访问速度，比间接访问少一次对象实例数据的访问，HotSpot则采用的这种访问方式。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA内存模型-线程私有]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2FJAVA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-%E7%BA%BF%E7%A8%8B%E7%A7%81%E6%9C%89%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第五篇文章，JVM的内存模型一般是面试必问的点，因为对JVM内存模型有所了解，才会有可能知道调优手段。本篇文章首先介绍线程私有的一些区域。 一、从整体看JVM运行时内存模型 下面详细说说各个部分的作用。 二、内存模型–程序计数器 占用内存小：是一块较小的内存空间，当前线程所执行的字节码的行号指示器。 PC作用：字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 线程独立：为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们成这类内存区域为“线程私有”的内存。 native方法：如果线程正在执行的是一个java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空(undefined). 无内存溢出异常：此内存区域是唯一一个在java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 三、内存模型–JAVA虚拟机栈 线程私有，生命周期与线程相同。 虚拟机栈描述的是Java方法的内存模型：每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程。 四、栈帧 我们口中常常提到的栈与堆，其中栈就是现在讲的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 局部变量表存放了编译期可知的各种基本数据类型(boolean,byte,char,short,int,float,long,double),对象引用(它不等同于对象本身，可能是一个指向对象地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)、returnAddress类型(指向了一条字节码指令的地址) 其中64位长度的long和double类型的数据会占用2个局部变量空间，其余的数据类型只占用1个。 局部变量表所需的内存空间在编译期完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 如果线程请求的栈深度大于虚拟机所允许的深度，比如递归层数过多，将抛出StackOverflowError异常；如果虚拟机可以动态扩展，即虚拟机栈申请过多，扩展时却无法申请到足够的内存，就会抛出OutOfMemoryError异常。 五、内存模型–本地方法栈 本地方法栈与虚拟机栈所发挥的作用是非常相似的，他们之间的区别不过是虚拟机栈尾虚拟机执行java方法(也就是字节码)服务，而本地方法栈则为虚拟机用到的Native方法服务。 Sun HotSpot虚拟机直接将本地方法栈和虚拟机栈合二为一。 与虚拟机栈一样会抛出StackOverflowError异常或者OutOfMemoryError异常。 什么是native方法？ 简单地讲，一个Native Method就是一个java调用非java代码的接口。一个Native Method是这样一个java的方法：该方法的实现由非java语言实现，比如C。这个特征并非java所特有，很多其它的编程语言都有这一机制，比如在C＋＋中，你可以用extern “C”告知C＋＋编译器去调用一个C的函数。 下一篇来看看线程共享的区域。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细谈loadClass]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2F%E7%BB%86%E8%B0%88loadClass%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第四篇文章，对于获取Class对象，其实我们不知不觉中已经接触过两种了，一种就是loadClass，一种就是反射中的forName，它们到底有什么区别呢？其实涉及了类加载过程的区别。下面好好来探讨一下。 一、问题的提出 对于之前的 测试代码： 12345678public class Test &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; MyClassLoader myClassLoader = new MyClassLoader("C:\\Users\\swg\\Desktop\\","myClassLoader"); Class c = myClassLoader.loadClass("Robot"); System.out.println(c.getClassLoader()); c.newInstance(); &#125;&#125; 不知道大家有没有疑惑，我们这里是用了loadClass(name)来加载对应的Class对象的，最后还需要进行newInstance()。那么为什么要调用newInstance()才行呢？ 1.1 new的方式构建对象实例 下面要进行相应的测试。对于Robot.java: 首先用new的方式： 显示结果为： 1hello , i am a robot! 1.2 loadClass来获取Class对象 如果仅仅这样写，显示结果仅仅为： 1sun.misc.Launcher$AppClassLoader@18b4aac2 也就是说，并不会触发static静态块的执行，也就是说这个类根本就没有初始化。 1.3 forName来获取Class对象 显示结果为： 1hello , i am a robot! 触发了静态块的执行。 二、类加载过程 要想说明上面区别产生的原因，这里必须要介绍一个从未使用过的类加载的过程。 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)7个阶段。其中准备、验证、解析3个部分统称为连接（Linking）。如图所示： 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。 2.1 加载 在加载阶段（可以参考java.lang.ClassLoader的loadClass()方法），虚拟机需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流（并没有指明要从一个Class文件中获取，可以从其他渠道，譬如：网络、动态生成、数据库等）； 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构； 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口； 加载阶段和连接阶段（Linking）的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 2.2 验证 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以魔术0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 2.3 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值，假设一个类变量的定义为： 1public static int value=123; 那变量value在准备阶段过后的初始值为0而不是123.因为这时候尚未开始执行任何java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行。 至于“特殊情况”是指：public static final int value=123，即当类字段的字段属性是ConstantValue时，会在准备阶段初始化为指定的值，所以标注为final之后，value的值在准备阶段初始化为123而非0. 2.4 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 对于这里说的：将符号引用替换为直接引用。很多人包括我第一次看到的时候感觉莫名其妙，教材上也是直接用这些专用名词，给我们的学习带来了极大的困扰。这里还是要解释一下。 比如以下代码： 123public static void main(String[] args) &#123; String s = "abc";&#125; s是符号引用，而abc是字面量。 此时，知道了什么是符号引用就好办了，因为符号引用一般都是放在栈中的，这个玩意肯定是依赖于实际的东西，相当于一个指针，多以我们程序需要将其解析成这个实际东西所在的真正的地址。所以，一旦解析了，那么内存中必然实际存在了这个对象，即拥有实际的物理地址了。 2.5 初始化 类初始化阶段是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的java程序代码。在准备阶段，变量已经赋过一次系统要求的初始值，而在初始化阶段，则根据程序猿通过程序制定的主观计划去初始化类变量和其他资源，或者说：初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程. 三、new、loadClass、forName 正常情况下，我们一般构建对象实例是通过new的方式，new是隐式构建对象实例，不需要newInstance()，并且可以用带参数的构造器来生成对象实例； 对于new，我们有点基础的，是知道，已经一直来到了最后初始化完成的这一步，生成了可以直接使用的对象实例。由于篇幅不宜太长，不想展开讲new的过程发生了什么，这里先贴个我觉得讲的不错的链接：https://www.jianshu.com/p/ebaa1a03c594 然而loadClass(name)这种显示调用的方式，我们可以看到，只有加载的功能，而没有后续连接以及初始化的过程。 所以loadClass(name)需要进行newInstance()才能生成对应的对象实例，并且这个newInstance()方法不支持参数调用，要想实现输入参数生成实例对象，需要通过反射获取构造器对象传入参数再生成对象实例。 这里也就解释了为什么要newInstance()，因为不这样，loadClass(name)只是加载，并没有后续过程，也就是说这个类根本就没有动它，仅仅是加载进来而已。从代码层面调用loadClass()的时候，我们可以看到一个之前故意忽视的东西： 这个resolve默认是传入false的，那么进来看看这个resolveClass()方法： 再下去是native方法，不必关心，我们只看方法的注释即可，写的是链接指定的类，就是上面的连接过程。我们由上面知道，如果这个方法能执行，那么就会触发验证、准备、解析这三个过程，而准备阶段是会去执行静态方法或静态块，类变量会被进行初始化，即分配内存，但是仅仅赋初值即可。 所以，loadClass(name)有一种懒加载的思想在里面，要用了再去进行初始化，而不是一开始就初始化好。 既然已经知道了new和loadClass的区别了，下面再来看看Class.forName(),聪明的读者估计已经可以猜到了，没错，根据实验的结果来看，它至少要进行到连接完，实质它也完成了初始化，即已经到达第三步： 总结一下：loadClass仅仅是第一步的加载，而forName和new都是已经初始化好了。 存在的原因 所谓存在即合理，forName的用法，最常见的莫过于用于加载数据库驱动这，我们这里实验一下，首先引入相关的依赖： 经典写法来啦： 点进去看看： 我们这个时候发现，里面是一个static方法，也就是说，我们要立即创建驱动。所以这个时候必须用forname方法啦！ 那么对于loadClass，其实上面已经提及了，就是懒加载，这个思想再spring中是到处可见的，bean只是加载，但是步进行初始化，等用的时候再去初始化，提高性能。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双亲委派模型]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第三篇文章，谈到JVM类加载机制，双亲委派模型是绕不开的话题，名字看好像是个高大上、深不可测的玩意，其实逐步揭开面纱之后很简单。下面我们就来揭揭看。 回顾类加载器 上一节简单说明了类加载器的作用，只说到一个核心功能是加载class文件。但是，绝对没有这么简单，神书《深入理解Java虚拟机》第二版对类加载器的说明： 代码编译的结果从本地机器码转变成字节码，是存储格式的一小步，却是编程语言发展的一大步。 Java虚拟机把描述类的数据从Class文件加载进内存，并对数据进行校验，转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 虚拟机设计团队把类加载阶段中的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这动作的代码模块成为“类加载器”。 类加载器虽然只用于实现类的加载动作，但它在Java程序中起到的作用却远远不限于类加载阶段。对于任意一个类，都需要由加载他的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类命名空间。这句话可以表达的更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来自同一个Class文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这个两个类就必定不相等。 对于上面进行一些说明： 注意，加载之后要将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构（方法区就是用来存放已被加载的类信息，常量，静态变量，编译后的代码的运行时内存区域） 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。这个Class对象并没有规定是在Java堆内存中，它比较特殊，虽为对象，但存放在方法区中。 这样，就可以使用这个类了。 还有，关于相等，只有在满足如下三个类“相等”判定条件，才能判定两个类相等。 两个类来自同一个Class文件 两个类是由同一个虚拟机加载 两个类是由同一个类加载器加载 什么是双亲委派模型 我们上一节已经知道了有四种类加载器，它们的实际关系为： 从这个图来看，是一个继承的关系，是这样吗？我们用代码来看看是不是真的是这样。 代码还是用上一篇文章自定义类加载器来测试： 结果是： 从这个结果就很容易看出，层级关系是与上图所述的一样。那么，这个层级关系其实就是我们下面要说的双亲委派模型的结构。 这里还想补充一点：就是为什么最后一个是null，即bootstrap为什么显示null，其实是因为它是用C++实现的，不是java语言实现的，所以与其他几个都有区别，这里根据就调用不到，所以显示null。如果非要看bootstrap里面大概如何实现的，需要去看看opjdk的代码。 结合代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; //1.加锁 synchronized (getClassLoadingLock(name)) &#123; //2.首先看看当前类加载器是否已经加载过，没有则委派给父亲查询 Class&lt;?&gt; c = findLoadedClass(name); //3.如果当前类加载器没有加载过，进来 if (c == null) &#123; long t0 = System.nanoTime(); try &#123; //4.看是否有父类加载器，有则进来 if (parent != null) &#123; //5.父类加载器看看是否已经加载过 //注意，这里是各递归函数，如果由下至上查询都没有加载过，则从上至下尝试去加载 c = parent.loadClass(name, false); &#125; else &#123; //进到这个，是来看看bootstrap类加载器是否加载过，没有加载过则加载 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; //6.如果所有类加载器都没有加载过，则开始尝试从上而下逐级去加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //去加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; //一开始是false if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 其实很简单，就是先一级一级往上查询是否已经加载过，加载过直接返回即可；一直查询到bootstrap类加载器，都没有加载过，那么就从bootstrap类加载器开始一级一级向下到他们的扫描范围内尝试加载这个class文件，知道自定义类加载(如果有的话)，没有则返回找不到。 说一下代码的实现思路。代码使用递归实现的，先一级一级找父亲，即一级一级向上入栈，某一个查到了就返回，每一层递归停留在c = parent.loadClass(name, false);；都查不到，再一级一级出栈去执行，那么就从c = findBootstrapClassOrNull(name);后面的代码继续执行，那么显然就是执行if (c == null) {...}尝试去加载。 为什么要用双亲委派模型 为什么需要双亲委派模型呢？假设没有双亲委派模型，试想一个场景： 黑客自定义一个java.lang.String类，该String类具有系统的String类一样的功能，只 是在某个函数稍作修改。比如equals函数，这个函数经常使用，如果在这这个函数中， 黑客加入一些“病毒代码”。并且通过自定义类加载器加入到JVM中。此时，如果没有双亲 委派模型，那么JVM就可能误以为黑客自定义的java.lang.String类是系统的String类， 导致“病毒代码”被执行。 而有了双亲委派模型，黑客自定义的java.lang.String类永远都不会被加载进内存。因为首先是最顶端的类加载器加载系统的java.lang.String类，最终自定义的类加载器无法加载java.lang.String类。 或许你会想，我在自定义的类加载器里面强制加载自定义的java.lang.String类，不去通过调用父加载器不就好了吗?确实，这样是可行。但是，在JVM中，判断一个对象是否是某个类型时，如果该对象的实际类型与待比较的类型的类加载器不同，那么会返回false。 举个简单例子： ClassLoader1、ClassLoader2都加载java.lang.String类，对应Class1、Class2对象。 那么Class1对象不属于ClassLoad2对象加载的java.lang.String类型。 委托机制的意义：防止内存中出现多份同样的字节码 比如两个类A和类B都要加载System类： 如果不用委托而是自己加载自己的，那么类A就会加载一份System字节码，然后类B又会加载一份System字节码，这样内存中就出现了两份System字节码。 如果使用委托机制，会递归的向父类查找，也就是首选用Bootstrap尝试加载，如果找不到再向下。这里的System就能在Bootstrap中找到然后加载，如果此时类B也要加载System，也从Bootstrap开始，此时Bootstrap发现已经加载过了System那么直接返回内存中的System即可而不需要重新加载，这样内存中就只有一份System的字节码了。 一个面试题 能不能自己写个类叫java.lang.System？ 显然是不可以的，可能方案是自己搞一个这个类放在特殊目录，用自定义类加载器去加载，然而系统自身的类加载器会先去加载使用，下次再用的时候，是先逐级向上查询是否已经加载过，根本没有机会让自定义类加载器去加载。 所以，如果非要用，那么必定是要破坏双亲委派模型了，那么又回到为什么要用双亲委派模型的问题上了，所以，为了自己写一个java.lang.System而破坏双亲委派模型，我只能说，脑子秀逗了。所以不要搞这些东西，包名或类名写的不一样即可。 一个问题 那么为什么不能用一个加载器去一个目录加载所有呢？还要分这么多的类加载器，不是麻烦么？ 其实，这个问题也是比较可笑的，毕竟每个层级的功能是不一样的，比如bootstrap是加载最核心的文件，没有它，都玩不起来。而自定义的呢？是比较特殊的需求，需要的时候才用到。对于这种有个性化的要求，一套代码来实现，显然是不合理的。 比如这个回答是根据加载的方式来思考的： 每一个类加载器都是为了去在不同的情景下去加载类。比如，你可以从联网服务器上加载一个class文件，也可以从远程web服务器下载二进制类。这么设计是因为我们需要类加载器提供一致的接口，这样客户端就可以加载类但是却不用管类加载器到底是怎么实现的。启动类加载器能够加载JVM_HOME/lib 下的类，但如果我们需要在其他的情况下加载类呢？简单来说，加载类的方法有无数种，我们需要一个灵活的加载器系统去在特定的情况下按照我们的想法来加载类。 还有一个回答是说更方便地对特定类进行优化： 虽然 对java 虚拟机没有研究过，java 为什么不能 一个加载器 加载全部的类 很明显， 实现起来也可以 但是需要 的 代码 更多，也更难 为各种类进行 优化，为了更简单的抽象 我在明确知道 该类是启动类的情况下，我就会 为该类 进行优化。 如果是自定义类，可能就 不会进行 此类优化。 在明确 目的的情况下， 专用代码 比 通用代码 更简单，也更有效。 总之，就是为了清晰和方便，这也是我们在进行软件设计的时候最基本的要求，即不能写死代码，影响扩展性；层次结构也不能写的太乱，影响后续的优化。 至此，双亲委派模型就讲完了。我们也清晰地知道了其设计思想和好处。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈ClassLoader]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2F%E6%B5%85%E8%B0%88ClassLoader%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第二篇文章，上一篇文章初步提到了class文件，以及一个最简单程序执行的指令含义，我们提到，是由JAVA虚拟机先加载这些编译好的class文件，然后再去根据解析出来的指令去转换为具体平台上的机器指令执行，但是加载这个class文件时如何加载的呢？其实就涉及比较重要的东西：ClassLoader 有一个基本认识，从编译到实例化对象的过程可以概括为以下三个阶段： 编译器将xxx.java源文件编译为xxx.class字节码文件 ClassLoader将字节码转换为JVM种的Class&lt;xxx&gt;对象 JVM利用Class&lt;xxx&gt;对象实例化为xxx对象 一、JVM系统结构 ClassLoader：依据特定格式，加载class文件到内存 Execution Engine：对命令进行解析 Native Interface：融合不同开发语言的原生库为Java所用 Runtime Data Area：JVM内存空间结构模型 首先通过ClassLoader加载符合条件的字节码文件到内存中，然后通过Execution Engine解析字节码指令，交由操作系统去执行。 二、什么是ClassLoader ClassLoader在java中有着非常重要的作用，它主要工作在Class装载的加载阶段，其主要作用是从系统外部获得Class二进制数据流。他是JAVA的核心组件，所有的Class都是由ClassLoader进行加载的，ClassLoader负责通过将Class文件里的二进制数据流装载进系统，然后交给JAVA虚拟机进行连接、初始化等操作。 简而言之，就是加载字节码文件。 我们翻开ClassLoader源码看看： 1public abstract class ClassLoader &#123;...&#125; 它是一个抽象类，下面我们再来说具体的实现类。 里面比较重要的是loadClass()方法： 123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125; 就是根据name来加载字节码文件，返回Class实例，加载不到则抛出ClassNotFoundException异常。 三、ClassLoader的种类 启动类加载器（Bootstrap ClassLoader）：由C++语言实现（针对HotSpot）,加载核心库java.*。 扩展类加载器（Extension ClassLoader）：Java编写，加载扩展库javax.* 它扫描的是哪个路径呢？ 我们看到，它负责将 &lt;JAVA_HOME &gt;/lib/ext或者由系统变量-Djava.ext.dir指定位置中的类库 加载到内存中。 应用程序类加载器（Application ClassLoader）：Java编写，加载程序所在目录 它负责将 用户类路径(java -classpath或-Djava.class.path变量所指的目录，即当前类所在路径及其引用的第三方类库的路径，看截图的最后一行，显示的是当前项目路径。 自定义ClassLoader：自定义 四、如何自定义ClassLoader 要自己实现一个ClassLoader，其核心涉及两个方法： 123456789protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name);&#125;protected final Class&lt;?&gt; defineClass(byte[] b, int off, int len) throws ClassFormatError&#123; return defineClass(null, b, off, len, null);&#125; 首先想一下为什么是这两个类？ 其实答案在loadClass()这个方法里面。如果已经熟悉双亲委派模型的同学，都会知道加载Class对象是先委派给父亲，看父亲是否已经加载，如果没有加载过，则从最顶层父亲开始逐层往下进行加载，这一块详细在下一篇文章中解释，我们先走马观花看看这个的核心方法长啥样： 123456789101112131415161718192021222324252627282930313233343536373839protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; //首先看看当前类加载器是否已经加载过，没有则委派给父亲查询 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //注意，这里是各递归函数，如果由下至上查询都没有加载过，则从上至下尝试去加载 c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; //如果所有类加载器都没有加载过，则开始尝试从上而下逐级去加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //去加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 如果我们不去重写findClass(name)方法，默认是直接抛出找不到的异常，所以我们要对这个方法进行重写。 由于字节码文件是一堆二进制流，所以需要一个方法来根据这个二进制流来定义成一个类，即defineClass()这个方法来实现这个功能。 说的比较抽象，下面来真正实践一把！ 五、实践自定义ClassLoader 首先写一个类：Robot.java 12345public class Robot &#123; static &#123; System.out.println("hello , i am a robot!"); &#125;&#125; 在对Robot.java用javac编译之后形成Robot.class文件，就要删除本项目下的这个Robot.java文件，要不然就会被AppClassLoader类加载先加载了，而无法再被我们的自定义类加载器再去加载。这个Robot.class文件我就直接放到桌面去了。路径为C:/Users/swg/Desktop/. 然后定义一个自定义的ClassLoader，按照上面的理论，只要重写findClass就可以指定到某个地方获取class字节码文件，此时获取的是二进制流文件，转换为字节数组，最后借用defineClass获取真正的Class对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MyClassLoader extends ClassLoader&#123; //执行加载的class文件的路径 private String path; //自定义类加载器的名字 private String classLoaderName; MyClassLoader(String path,String classLoaderName)&#123; this.path = path; this.classLoaderName = classLoaderName; &#125; //用于寻找类文件 @Override protected Class findClass(String name)&#123; byte[] b = loadClassData(name); return defineClass(name,b,0,b.length); &#125; //用于加载类文件 private byte[] loadClassData(String name) &#123; name = path + name + ".class"; InputStream in = null; ByteArrayOutputStream out = null; try&#123; in = new FileInputStream(new File(name)); out = new ByteArrayOutputStream(); int i=0; while ((i = in.read()) != -1)&#123; out.write(i); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; try &#123; in.close(); out.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; return out.toByteArray(); &#125;&#125; 最后测试一下能不能用自定义类加载器去加载到Robot对应的Class对象： 12345678public class Test &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; MyClassLoader myClassLoader = new MyClassLoader("C:\\Users\\swg\\Desktop\\","myClassLoader"); Class c = myClassLoader.loadClass("Robot"); System.out.println(c.getClassLoader()); c.newInstance(); &#125;&#125; 打印结果： 12MyClassLoader@677327b6hello , i am a robot! 好了，学习了关于ClassLoader的分类以及如何自定义ClassLoader，我们知道了类加载器的基本实现，上面谈到了一个重要方法是loadClass，这就涉及了类加载器的双亲委派模型。下一节从代码层面好好来说说这个，其实很简单。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年展望]]></title>
    <url>%2F2019%2F02%2F05%2Fsuibi%2F2019%E5%B9%B4%E5%B1%95%E6%9C%9B%2F</url>
    <content type="text"><![CDATA[今天是大年初一，算是真正步入2019年了。保存几张老家门口拍的照片，以作纪念。 老家越来越冷清，越来越萧条。 今年四月底顺利毕业的话，就真的工作了，在南京，本科+研究生读了七年书，加上小时候每年暑假都来南京玩，对南京的熟悉的程度远远大于家乡盐城。 所以希望可以努力，在南京能扎下根，然后一家人全搬过去。 在找工作方面，找的并不理想，可能还在不好的几个offer之中选择了更不好的一个，心里甚是难受。不过，凡事都没有绝对，往好处多想想就好了，关键还是靠自己努力。 在2018年，算是学习java的进阶之年，自己学习了很多新的技术，也好好地夯实了基础，把以前很多模糊的问题搞清楚了，确实，基础真的太重要的，光学时髦的框架，可以写写简单的CRUD应用，是远远不够的，我觉得程序员的目标是可以造出大家都认可并且乐于使用的开源作品，没有好的基础，便是天方夜谭，也只能永远做一个普通的码农。 所以，我给自己定一个三年的目标，三年以后，无论是搞java还是搞大数据还是其他，我希望能达到中高级水平，在这个行业方向上有较好的基础和较深的认识。为后续更高的发展打下坚实的基础。 我希望，github真正能成为我出发的地方，并且能够走很远。犹记得，14年在大神室友的推荐下，注册了github，但是真正使用还是从去年开始吧。所以甚是惭愧，如果早一点上路，虽然追赶不上大神的脚步（已经进了google），但是进个二线比如京东、美团等都是轻而易举吧，但是谈这些确实是废话，没有人有假如。 除了对未来三年的一些初步想法之外，我还是希望我与家人都有一个健健康康的身体，所以工作以后身体的锻炼是必不可少的，八块腹肌是不指望了，至少爬几层楼不用喘吧。 一切还是视实际情况而定，但是终身学习的信念要埋藏在心里，这个时代以及未来的时代，选择了这一行，掉头发是注定的了，但是如果仅仅以掉头发的代价，可以让家人舒舒服服，健健康康，开开心心的话，那给我剃个光头也无妨了。 废话不多说了，我要继续完成本笔记的JVM部分了，不忘初心，套用喜剧之王的台词：努力，奋斗！]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解java反射机制]]></title>
    <url>%2F2019%2F02%2F04%2Fjava-basic%2F%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3java%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[反射机制这一块也是面试经常会被问到的，我从反射的基本概念到反射的一些面试题出发，好好理一理反射的知识。 1. 什么是反射 标准定义：JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用方法的功能成为反射机制。 注意几个关键字：运行状态中，动态获取。 2. Class对象和实例对象 想要理解反射首先需要知道Class这个类，它的全称是java.lang.Class类。java是面向对象的语言，讲究万物皆对象，即使强大到一个类，它依然是另一个类（Class类）的对象，换句话说，普通类是Class类的对象，即Class是所有类的类（There is a class named Class）。 我们知道java世界是运行在JVM之上的，我们编写的类代码，在经过编译器编译之后，会为每个类生成对应的.class文件，这个就是JVM可以加载执行的字节码。 运行时期间，当我们需要实例化任何一个类时，JVM会首先尝试看看在内存中是否有这个类，如果有，那么会直接创建类实例；如果没有，那么就会根据类名去加载这个类，当加载一个类，或者当加载器(class loader)的defineClass()被JVM调用，便会为这个类产生一个Class对象（一个Class类的实例），用来表达这个类，该类的所有实例都共同拥有着这个Class对象，而且是唯一的。 也就是说，加载.class文件之后会生成一个对应的Class对象。下面说说如何获取这个Class对象。 3. 取得Class对象的三种方式 我们假设有这么一个类叫MyClass： 1public class MyClass &#123; &#125; 第一种方式：通过“类名.class”的方式取得 1Class classInstance= MyClass.class; 例如： 123Class clazz = Car.class;Class cls1 = int.class;Class cls2 = String.class; 第二种方式：通过类创建的实例对象的getClass方法取得 12MyClass myClass = new MyClass();Class classInstance = myClass.getClass(); 第三种方式：通过Class类的静态方法forName方法取得（参数是带包名的完整的类名） 12345try &#123; Class classInstance = Class.forName("mypackage.MyClass");&#125; catch (ClassNotFoundException e) &#123; e.printStackTrace();&#125; 上面三种方法取得的对象都是相同的，所以效果上等价。 classInstance是类类型，通过类类型可以得到一个类的属性和方法等参数，这是反射的基础。 4. 利用反射API全面分析类的信息——方法，成员变量，构造器 反射的一大作用是用于分析类的结构，或者说用于分析和这个类有关的所有信息。而这些信息就是类的基本的组成： 方法，成员变量和构造器。 在java种万物皆对象，一个类中的方法，成员变量和构造器也分别对应着一个对象 每个方法都对应有一个保存和该方法有关信息的Method对象， 这个对象所属的类是java.lang.reflect.Method; 每个成员变量都对应有一个保存和该变量有关信息的Field对象，这个对象所属的类是 java.lang.reflect.Field 每个构造器都对应有一个保存和该构造器有关信息的Constructor对象，这个对象所属的类是java.lang.reflect.Constructor 假设c是一个类的Class对象： 通过 c.getDeclaredMethods()可取得这个类中所有声明方法对应的Method对象组成的数组 通过 c.getDeclaredFields()可取得这个类中所有声明的成员变量对应的Field对象组成的数组 通过 c.getConstructors(); 可取得这个类中所有构造函数所对应的Constructor对象所组成的数组 1234567Method [] methods = c.getDeclaredMethods(); // 获取方法对象列表 Field [] fields = c.getDeclaredFields(); // 获取成员变量对象列表Constructor [] constructors = c.getConstructors(); // 获取构造函数对象列表xxx.getName()就可以打印出对应的名字了。 5. 更多的反射api getMethods和getDeclaredMethods方法 getMethods取得的method对应的方法包括从父类中继承的那一部分，而 getDeclaredMethods取得的method对应的方法不包括从父类中继承的那一部分 一个普通的类，他们的基类都是Object，那么如果用getMethods，遍历得到的结果，会发现Object中的基础方法名都会被打印出来。 诸如wait(),equals(),toString(),getClass(), notify(),notifyAll(),hashCode()等等。 通过method.getReturnType()获取方法返回值对应的Class对象 12Class returnClass = method.getReturnType(); // 获取方法返回值对应的Class对象String returnName = returnClass.getName(); //获取返回值所属类的类名——也即返回值类型 通过method.getParameterTypes()获取方法各参数的Class对象组成的数组 12345Class [] paramsClasses = method.getParameterTypes();for (Class pc: paramsClasses) &#123; String paramStr = pc.getName(); // 获取当前参数类型 paramsStr+=paramStr + " ";&#125; 获取成员变量类型对应的的Class对象 123Field field = c.getDeclaredField("name"); // 取得名称为name的field对象field.setAccessible(true); // 这一步很重要！！！设置为true才能访问私有成员变量name的值！String nameValue = (String) field.get(obj); // 获取obj中name成员变量的值 通过getType方法读取成员变量类型的Class对象 12Field field = class1.getDeclaredField(number");System.out.print(field.getType().getName()); 因为java权限的原因，直接读取私有成员变量的值是非法的（加了field.setAccessible(true)后就可以了），但仍可以直接读取私有成员变量的类型 利用反射API分析类中构造器信息 123public class MyClass &#123; public MyClass(int a, String str)&#123;&#125;&#125; 12345678910111213public static void printContructorsMessage (Object obj) &#123;Class c = obj.getClass(); // 取得obj所属类对应的Class对象Constructor [] constructors = c.getDeclaredConstructors();for (Constructor constructor : constructors) &#123; Class [] paramsClasses = constructor.getParameterTypes(); String paramsStr = ""; for (Class pc : paramsClasses) &#123; String paramStr = pc.getName(); paramsStr+=paramStr + " "; &#125; System.out.println("构造函数的所有参数的类型列表：" + paramsStr);&#125;&#125; 运行结果： 1构造函数的所有参数的类型列表：int java.lang.String 6. 利用反射动态加载类，并用该类创建实例对象 我们用普通的方式使用一个类的时候，类是静态加载的 ，而使用Class.forName(“XXX”)这种方式，则属于动态加载一个类 静态加载的类在编译的时候就能确定该类是否存在，但动态加载一个类的时候却无法在编译阶段确定是否存在该类，而是在运行时候才能够确定是否有这个类，所以要捕捉可能发生的异常. Class对象有一个newInstance方法，我们可以用它来创建实例对象 12Class classInstance = Class.forName("mypackage.MyClass");MyClass myClass = (MyClass) classInstance.newInstance(); 7. 总结 反射为我们提供了全面的分析类信息的能力，例如类的方法，成员变量和构造器等的相关信息，反射能够让我们很方便的获取这些信息， 而实现这个获取过程的关键是取得类的Class对象，然后根据Class对象取得相应的Method对象，Field对象和Constructor对象，再分别根据各自的API取得信息。 反射还为我们提供动态加载类的能力 API中getDeclaredXXX和getXXX的区别在于前者只获取本类声明的XXX（如成员变量或方法），而不获取超类中继承的XXX， 后者都可以获取 API中， getXXXs（注意后面的s）返回的是一个数组， 而对应的 getXXX（“键”）按键获取一个值（这个时候因为可能报已检查异常所以要用try*catch语句包裹） 私有成员变量是不能直接获取到值的！因为java本身的保护机制，允许你取得私有成员变量的类型，但是不允许直接获取值，所以要对对应的field对象调用field.setAccessible(true) 放开权限 8. 面试 什么是反射 反射是一种能够在程序运行时动态访问、修改某个类中任意属性（状态）和方法（行为）的机制 反射到底有什么具体的用处 操作因访问权限限制的属性和方法； 实现自定义注解； 动态加载第三方jar包，解决android开发中方法数不能超过65536个的问题； 按需加载类，节省编译和初始化APK的时间； 反射的原理是什么 当我们编写完一个Java项目之后，每个java文件都会被编译成一个.class文件，这些Class对象承载了这个类的所有信息，包括父类、接口、构造函数、方法、属性等，这些class文件在程序运行时会被ClassLoader加载到虚拟机中。当一个类被加载以后，Java虚拟机就会在内存中自动产生一个Class对象。我们通过new的形式创建对象实际上就是通过这些Class来创建，只是这个过程对于我们是透明的而已。 反射的工作原理就是借助Class.java、Constructor.java、 Method.java、Field.java这四个类在程序运行时动态访问和修改任何类的行为和状态。 如何获取Class对象 Class的forName()方法的返回值就是Class类型，也就是动态导入类的Class对象的引用 1public static Class&lt;?&gt; forName(String className) throws ClassNotFoundException 每个类都会有一个名称为Class的静态属性，通过它也是可以获取到Class对象 1Class&lt;Student&gt; clazz = Student.class; Object类中有一个名为getClass的成员方法，它返回的是对象的运行时类的Class对象。因为Object类是所有类的父类，所以，所有的对象都可以使用该方法得到它运行时类的Class对象 12Student stu = new Student();Class&lt;Student&gt; clazz = stu.getClass(); 反射的特点 优点 灵活、自由度高：不受类的访问权限限制，想对类做啥就做啥 缺点 性能问题 通过反射访问、修改类的属性和方法时会远慢于直接操作，但性能问题的严重程度取决于在程序中是如何使用反射的。如果使用得很少，不是很频繁，性能将不会是什么问题； 安全性问题 反射可以随意访问和修改类的所有状态和行为，破坏了类的封装性，如果不熟悉被反射类的实现原理，随意修改可能导致潜在的逻辑问题； 如何提高反射性能 java应用反射的时候，性能往往是java程序员担心的地方，那么在大量运用反射的时候，性能的微弱提升，对这个系统而言都是如旱地逢甘霖。 setAccessible(true),可以防止安全性检查（做这个很费时） 做缓存，把要经常访问的元数据信息放入内存中，class.forName 太耗时 getMethods() 等方法尽量少用，尽量调用getMethod(name)指定方法的名称，减少遍历次数 java面试中面试官让你讲讲反射，应该从何讲起？ 先讲反射机制，反射就是程序运行期间JVM会对任意一个类洞悉它的属性和方法，对任意一个对象都能够访问它的属性和方法。依靠此机制，可以动态的创建一个类的对象和调用对象的方法。 其次就是反射相关的API，只讲一些常用的，比如获取一个Class对象。Class.forName(完整类名)。通过Class对象获取类的构造方法，class.getConstructor。根据Class对象获取类的方法，getMethod和getMethods。使用Class对象创建一个对象，class.newInstance等。 最后可以说一下反射的优点和缺点，优点就是增加灵活性，可以在运行时动态获取对象实例。缺点是反射的效率很低，而且会破坏封装，通过反射可以访问类的私有方法，不安全。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java如何执行一个最简单的程序]]></title>
    <url>%2F2019%2F02%2F03%2FJVM%2FJava%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E4%B8%80%E4%B8%AA%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第一篇文章，需要之前对JVM有一定了解的基础。我们都知道，JAVA号称：一次编译多处运行。这就离不开字节码文件和虚拟机啦！那么，虚拟机到底是如何去执行一个简单的程序的呢？理解了这个，我们就可以理解java时如何做到平台无关的了。下面我们来分析分析。 首先，写一个最简单的程序： 123456789public class Main &#123; public static void main(String[] args) &#123; int i=1,j=5; i++; ++j; System.out.println(i); System.out.println(j); &#125;&#125; 运行之后的结果想必就一目了然，我们就通过这个程序来分析分析到底是怎么执行这个程序额的。 首先呢，java程序的执行经历编译，编译成系统能识别的文件，这里的系统对应java语言就是JVM，即JAVA虚拟机。JVM在识别之后，再去与我们真正的操作系统进行交互和处理。 所以，我们要执行一个.java程序，必须要先进行编译。初学者都会学习一个指令叫做javac： 我们会发现路径下面就会多一个.class文件，这就是编译之后的文件。直接点开： 123456789101112131415161718//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//public class Main &#123; public Main() &#123; &#125; public static void main(String[] var0) &#123; byte var1 = 1; byte var2 = 5; int var3 = var1 + 1; int var4 = var2 + 1; System.out.println(var3); System.out.println(var4); &#125;&#125; 我们看第一行注释，说的是编译后的文件已经自动被IDEA反编译了，所以我们还能看得懂。真正的文件是： 12345漱壕 4          &lt;init&gt; ()V Code LineNumberTable main ([Ljava/lang/String;)V SourceFile Main.java         Main java/lang/Object java/lang/System out Ljava/io/PrintStream; java/io/PrintStream println (I)V !            *? ?       E   &lt;=??? ? ? ? ? 我们可以看到，其实是一堆乱码，根本看不懂。而在执行的时候，class文件是一种8位字节的二进制流文件。放在sublime中可以看到二进制文件（以16进制显示，在JAVA虚拟机中将来了解这各文件的含义，我们可以看到第一个单词是cafe babe，表明这是一个class字节码文件）： 那么我们想看看.class中的信息，还是需要反编译，这个时候可以用javap指令来做。如果我们对其不熟悉，可以先执行javap -help来了解了解。 12345678910111213141516171819用法: javap &lt;options&gt; &lt;classes&gt;其中, 可能的选项包括: -help --help -? 输出此用法消息 -version 版本信息 -v -verbose 输出附加信息 -l 输出行号和本地变量表 -public 仅显示公共类和成员 -protected 显示受保护的/公共类和成员 -package 显示程序包/受保护的/公共类 和成员 (默认) -p -private 显示所有类和成员 -c 对代码进行反汇编 -s 输出内部类型签名 -sysinfo 显示正在处理的类的 系统信息 (路径, 大小, 日期, MD5 散列) -constants 显示最终常量 -classpath &lt;path&gt; 指定查找用户类文件的位置 -cp &lt;path&gt; 指定查找用户类文件的位置 -bootclasspath &lt;path&gt; 覆盖引导类文件的位置 我们注意到，有一个-c是进行反汇编，那么就用它试试: 123456789101112131415161718192021222324252627E:\JavaBasic\src&gt;javap -c Main.classCompiled from &quot;Main.java&quot;public class Main &#123; public Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: iconst_1 1: istore_1 2: iconst_5 3: istore_2 4: iinc 1, 1 7: iinc 2, 1 10: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 13: iload_1 14: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 17: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 20: iload_2 21: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 24: return&#125; 那么这反汇编出来的东西是什么呢？这是一连串的指令，其实这些是加载class文件时真正执行的java虚拟机指令。 我们来看看它的含义吧！ 仔细看看，其实发现并不神秘，一个函数的执行是一个入栈出栈的过程。ok，大体了解了字节码文件是什么以及里面的指令含义之后，我们对java如何执行它已经大体清楚了。下面执行一下： 那么如何运行呢？ 其实这是废话，初学java其实是java Main运行的： 1234E:\JavaBasic\src&gt;java Main26 这个时候，class文件可以移植到任何平台上去，比如直接上传到linux上，只要JDK或者JRE环境类似即可，就可以直接运行了，不需要编译，也不需要关心是什么系统。这就做到了一次编译到处运行。 下面总结一下： Java源码首先被编译成字节码，再由不同平台的JVM进行解析，JAVA语言在不同平台上运行时不需要进行重新编译，JAVA虚拟机在执行字节码的时候，把字节码转换为具体平台上的机器指令，然后各种操作系统就可以正确识别了。这就是JAVA如何执行代码和平台无关性的原因。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Redis一些重要的面试点]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2F%E5%85%B3%E4%BA%8ERedis%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9A%84%E9%9D%A2%E8%AF%95%E7%82%B9%2F</url>
    <content type="text"><![CDATA[这里整理一些面试题目，很多已经在前面的文章中详细说明了，这里算是总结一下。也补充了一点新的知识。 Redis有哪些数据结构？ 字符串 String、字典 Hash 、列表 List 、集合 Set 、有序集合 SortedSet。 如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。 String类型的底层数据结构 Redis 是一个键值对数据库, 数据库的值可以是字符串、集合、列表等多种类型的对象， 而数据库的键则总是字符串对象。 对于那些包含字符串值的字符串对象来说， 每个字符串对象都包含一个 sds 值。 “包含字符串值的字符串对象”，这种说法初听上去可能会有点奇怪， 但是在 Redis 中， 一个字符串对象除了可以保存字符串值之外， 还可以保存 long 类型的值， 所以为了严谨起见， 这里需要强调一下： 当字符串对象保存的是字符串时， 它包含的才是 sds 值， 否则的话， 它就是一个 long 类型的值。 举个例子， 以下命令创建了一个新的数据库键值对， 这个键值对的键和值都是字符串对象， 它们都包含一个 sds 值： 12345redis&gt; SET book &quot;Mastering C++ in 21 days&quot;OKredis&gt; GET book&quot;Mastering C++ in 21 days&quot; 目前来说， 只要记住这个事实即可： 在 Redis 中， 客户端传入服务器的协议内容、 aof 缓存、 返回给客户端的回复， 等等， 这些重要的内容都是由 sds 类型来保存的。 在 C 语言中，字符串可以用一个 \0 结尾的 char 数组来表示。 比如说， hello world 在 C 语言中就可以表示为 &quot;hello world\0&quot; 。 这种简单的字符串表示，在大多数情况下都能满足要求，但是，它并不能高效地支持长度计算和追加（append）这两种操作： 每次计算字符串长度（strlen(s)）的复杂度为 θ(N) 。 对字符串进行 N 次追加，必定需要对字符串进行 N 次内存重分配（realloc）。 在 Redis 内部， 字符串的追加和长度计算很常见， 而 APPEND 和 STRLEN 更是这两种操作，在 Redis 命令中的直接映射， 这两个简单的操作不应该成为性能的瓶颈。 另外， Redis 除了处理字符串之外， 还需要处理单纯的字节数组， 以及服务器协议等内容， 所以为了方便起见， Redis 的字符串表示还应该是二进制安全的： 程序不应对字符串里面保存的数据做任何假设， 数据可以是以 \0 结尾的 C 字符串， 也可以是单纯的字节数组， 或者其他格式的数据。 考虑到这两个原因， Redis 使用 sds 类型替换了 C 语言的默认字符串表示： sds 既可高效地实现追加和长度计算， 同时是二进制安全的。 在前面的内容中， 我们一直将 sds 作为一种抽象数据结构来说明， 实际上， 它的实现由以下两部分组成： 1234567891011121314typedef char *sds;struct sdshdr &#123; // buf 已占用长度 int len; // buf 剩余可用长度 int free; // 实际保存字符串数据的地方 char buf[];&#125;; 其中，类型 sds 是 char * 的别名（alias），而结构 sdshdr 则保存了 len 、 free 和 buf 三个属性。 作为例子，以下是新创建的，同样保存 hello world 字符串的 sdshdr 结构： 12345struct sdshdr &#123; len = 11; free = 0; buf = &quot;hello world\0&quot;; // buf 的实际长度为 len + 1&#125;; 通过 len 属性， sdshdr 可以实现复杂度为 θ(1) 的长度计算操作。 另一方面， 通过对 buf 分配一些额外的空间， 并使用 free 记录未使用空间的大小， sdshdr 可以让执行追加操作所需的内存重分配次数大大减少。 为了易于理解，我们用一个 Redis 执行实例作为例子，解释一下，当执行以下代码时， Redis 内部发生了什么： 12345678redis&gt; SET msg "hello world"OKredis&gt; APPEND msg " again!"(integer) 18redis&gt; GET msg"hello world again!" 首先， SET 命令创建并保存 hello world 到一个 sdshdr 中，这个 sdshdr 的值如下： 12345struct sdshdr &#123; len = 11; free = 0; buf = "hello world\0";&#125; 当执行 APPEND 命令时，相应的 sdshdr 被更新，字符串 &quot; again!&quot; 会被追加到原来的 “hello world” 之后： 12345struct sdshdr &#123; len = 18; free = 18; buf = "hello world again!\0 "; // 空白的地方为预分配空间，共 18 + 18 + 1 个字节&#125; 在这个例子中， 保存 “hello world again!” 共需要 18 + 1 个字节， 但程序却为我们分配了 18 + 18 + 1 = 37 个字节 —— 这样一来， 如果将来再次对同一个 sdshdr 进行追加操作， 只要追加内容的长度不超过 free 属性的值， 那么就不需要对 buf 进行内存重分配。 这种分配策略会浪费内存吗？ 执行过 APPEND 命令的字符串会带有额外的预分配空间， 这些预分配空间不会被释放， 除非该字符串所对应的键被删除， 或者等到关闭 Redis 之后， 再次启动时重新载入的字符串对象将不会有预分配空间。 因为执行 APPEND 命令的字符串键数量通常并不多， 占用内存的体积通常也不大， 所以这一般并不算什么问题。 另一方面， 如果执行 APPEND 操作的键很多， 而字符串的体积又很大的话， 那可能就需要修改 Redis 服务器， 让它定时释放一些字符串键的预分配空间， 从而更有效地使用内存。 当然， sds 也对操作的正确实现提出了要求 —— 所有处理 sdshdr 的函数，都必须正确地更新 len 和 free 属性，否则就会造成 bug 。 更多参见：简单动态字符串 从海量数据中查询某一固定前缀的key 使用keys指令可以扫出指定模式的key列表。 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 基于游标的迭代器，需要使用上一次游标延续之前的迭代过程。游标为0的时候代表开始或结束。 1234#模式scan cursor match pattern count#示例scan 0 match k* count 10 Redis做异步队列 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。 如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 如果有大量的key需要设置同一时间过期，一般需要注意什么？ 如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。 Redis如何做持久化的？ bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要aof来配合使用。在redis实例重启时，会使用bgsave持久化文件重新构建内存，再使用aof重放近期的操作指令来实现完整恢复重启之前的状态。 对方追问那如果突然机器掉电会怎样？取决于aof日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。 对方追问bgsave的原理是什么？你给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行bgsave操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 Pipeline有什么好处，为什么要用pipeline？ 可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 Redis的同步机制了解么？ Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 是否使用过Redis集群，集群的原理是什么？ Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 整理自： 天下无难试之Redis面试题刁难大全]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种主流缓存框架介绍]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2F%E5%87%A0%E7%A7%8D%E4%B8%BB%E6%B5%81%E7%BC%93%E5%AD%98%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第十二篇文章。本文对Guava Cache,Memcache以及redis进行简单介绍和对比。 缓存特征 缓存都会涉及：命中率、最大元素、清空策略(FIFO,LFU,LRU,过期时间，随机) 影响缓存命中率因素 业务场景和业务需求：适合读多写少的场景 缓存的设计(粒度和策略)：缓存粒度越小，命中率越高 缓存容量(经常用LRU)和基础设施(是否可扩展，避免缓存失效-一致性hash算法和几点冗余) 缓存分类 本地缓存：编程实现（成员变量、局部变量、静态变量）、Guava Cache 分布式缓存：Memcache，Redis 本地缓存：各应用之间无法很好地共享，与应用本身耦合过紧；而分布式缓存，本身就是独立的应用，各独立应用之间共享缓存。 Guava Cache 设计思想类似于jdk1.7中的ConcurrentHashMap，也是用多个segments的细粒度锁，在保证线程安全的同时，支持高并发场景的需求。 下面数据存储就是以键值对的形式存储，另外，需要处理缓存过期、动态加载等算法逻辑，所以需要一些额外的信息来实现这些操作。 主要实现的功能有：自动将节点加入到缓存结构中，当缓存的数据超过设置的最大值时，用LRU算法来移除。他具备根据节点上次被访问或者写入的时间来计算他的过期机制。 memcache memcache简单认识 memcache是一个高性能的分布式的内存对象缓存系统，它在内存里维护一个统一的巨大的hash表。能用来缓存各种格式的数据，包括图像、视频、文件以及数据库检索等结果. memcache是以守护程序方式运行于一个或多个服务器中，随时会接收客户的连接和操作。 存在memcache中的对象实际放置在内存中，这也是memcache如此高效的原因。 本身是不提供分布式的解决方案的。分布式是在客户端实现的，通过客户端的路由来处理达到分布式的目的。 应用服务器每次在存储某个key和value的时候，通过某种算法把key映射到某台服务器上。 一致性hash算法 客户端实现分布式：一致性hash算法，这个算法已经详细介绍过了。 memcache一些特性 Memcached单进程在32位系统中最大使用内存为2G，若在64位系统则没有限制,这是由于32位系统限制单进程最多可使用2G内存,要使用更多内存，可以分多个端口开启多个Memcached进程。 32 位寻址空间只有 4GB 大小，于是 32 位应用程序进程最大只能用到 4GB 的内存。然而，除了应用程序本身要用内存，操作系统内核也需要使用。应用程序使用的内存空间分为用户空间和内核空间，每个 32 位程序的用户空间可独享前 2GB 空间（指针值为正数），而内核空间为所有进程共享 2GB 空间（指针值为负数）。所以，32 位应用程序实际能够访问的内存地址空间最多只有 2GB。 最大30天的数据过期时间，设置为永久也会在这个时间过期。最长键长为250字节，大于该长度无法存储。最大同时连接数是200; memcache是一种无阻塞的socket通信方式服务，基于libevent库，犹豫无阻塞通信，对内存读写速度非常快。 不适用memcached的业务场景？ 缓存对象的大小大于1MB 虚拟主机不让运行memcached服务 key的长度大于250字符 需要持久化 不能够遍历memcached中所有的item？ 这个操作的速度相对缓慢且阻塞其他的操作 memcache如何分配内存？ 这张图片里面涉及了slab_class、slab、page、chunk四个概念，它们之间的关系是： MemCache将内存空间分为一组slab 每个slab下又有若干个page，每个page默认是1M，如果一个slab占用100M内存的话，那么这个slab下应该有100个page 每个page里面包含一组chunk，chunk是真正存放数据的地方，同一个slab里面的chunk的大小是固定的 有相同大小chunk的slab被组织在一起，称为slab_class 那么是具体如何分配的呢？ MemCache中的value过来存放的地方是由value的大小决定的，value总是会被存放到与chunk大小最接近的一个slab中，比如slab[1]的chunk大小为80字节、slab[2]的chunk大小为100字节、slab[3]的chunk大小为128字节（相邻slab内的chunk基本以1.25为比例进行增长，MemCache启动时可以用-f指定这个比例），那么过来一个88字节的value，这个value将被放到2号slab中。 放slab的时候，首先slab要申请内存，申请内存是以page为单位的，所以在放入第一个数据的时候，无论大小为多少，都会有1M大小的page被分配给该slab。申请到page后，slab会将这个page的内存按chunk的大小进行切分，这样就变成了一个chunk数组，最后从这个chunk数组中选择一个用于存储数据。 如果这个slab中没有chunk可以分配了怎么办，如果MemCache启动没有追加-M（禁止LRU，这种情况下内存不够会报Out Of Memory错误），那么MemCache会把这个slab中最近最少使用的chunk中的数据清理掉，然后放上最新的数据。 MemCache的内存分配chunk里面会有内存浪费，88字节的value分配在128字节（紧接着大的用）的chunk中，就损失了30字节，但是这也避免了管理内存碎片的问题 MemCache的LRU算法不是针对全局的，是针对slab的 该可以理解为什么MemCache存放的value大小是限制的，因为一个新数据过来，slab会先以page为单位申请一块内存，申请的内存最多就只有1M，所以value大小自然不能大于1M了 最后再总结一下memcache MemCache中可以保存的item数据量是没有限制的，只要内存足够 MemCache单进程在32位机中最大使用内存为2G，64位机则没有限制 Key最大为250个字节，超过该长度无法存储 单个item最大数据是1MB，超过1MB的数据不予存储 MemCache服务端是不安全的，比如已知某个MemCache节点，可以直接telnet过去，并通过flush_all让已经存在的键值对立即失效 不能够遍历MemCache中所有的item，因为这个操作的速度相对缓慢且会阻塞其他的操作 MemCache的高性能源自于两阶段哈希结构：第一阶段在客户端，通过Hash算法根据Key值算出一个节点；第二阶段在服务端，通过一个内部的Hash算法，查找真正的item并返回给客户端。从实现的角度看，MemCache是一个非阻塞的、基于事件的服务器程序 MemCache设置添加某一个Key值的时候，传入expire为0表示这个Key值永久有效，这个Key值也会在30天之后失效 redis redis特点 支持数据持久化，可以将内存中的数据保存到磁盘。 支持更多的数据结构 支持数据备份 性能极高，读可以达到11万次每秒；写达到8万1千次每秒 redis所有操作都是原子性，并且支持几个操作一起的原子性 支持发布-订阅功能 redis适用场景 取最新n个数据、排行榜 精准过期时间 计数器 唯一性检查 实时系统、垃圾系统、缓存等 redis VS memcache 当提到redis就问memcache，当提到memcache就提到redis，说明这两者用的都十分广泛，redis号称“强化版memcached”，他们之间的区别到底是啥呢？ 基本命令 memcache支持的命令很少，因为他只支持String的操作，通讯协议包括文本格式和二进制格式，用于满足简单网络客户端工具（如telnet）和对性能要求更高的客户端的不同需求；redis操作类似，只是数据结构更复杂以支持更多的特性，如发布订阅、消息队列等。redis的客户端-服务器通讯协议完全采用文本格式(Redis Cluster服务端节点之间通讯采用二进制格式)。 事务 redis通过multi / watch / exec等命令可以支持事务的概念，原子性的执行一批命令; memcache:即使在多线程模式，所有的命令都是原子的；命令序列不是原子的。在并发的情况下，您也可能覆写了一个被其他进程set的item。memcached 1.2.5以及更高版本，提供了gets和cas命令，它们可以解决上面的问题。如果您使用gets命令查询某个key的item，memcached会给您返回该item当前值的唯一标识。如果您覆写了这个item并想把它写回到memcached中，您可以通过cas命令把那个唯一标识一起发送给 memcached。如果该item存放在memcached中的唯一标识与您提供的一致，您的写操作将会成功。如果另一个进程在这期间也修改了这个 item，那么该item存放在memcached中的唯一标识将会改变，您的写操作就会失败。 数据备份，有效性，持久化等 memcached不保证存储的数据的有效性，slab内部基于LRU也会自动淘汰旧数据;memcached也不做数据的持久化工作; redis可以以master-slave的方式配置服务器，slave节点对数据进行replica备份，slave节点也可以充当read only的节点分担数据读取的工作;redis内建支持两种持久化方案，snapshot快照和AOF增量Log方式。 性能 memcached自身并不主动定期检查和标记哪些数据需要被淘汰，只有当再次读取相关数据时才检查时间戳，或者当内存不够使用需要主动淘汰数据时进一步检查LRU数据。 redis为了减少大量小数据CMD操作的网络通讯时间开销 RTT (Round Trip Time)，支持pipeline和script技术。 集群 memcached的服务器端互相完全独立，客户端通常通过对键值应用hash算法决定数据的分区，为了减少服务器的增减对hash结果的影响，导致大面积的缓存失效，多数客户端实现了一致性hash算法。 redis3.0已经支持服务端集群了。 性能对比 由于redis只使用单核，而memcached可以使用多核，所以平均每一个核上redis在存储小数据时比memcached性能更高。而在100k以上的数据中，memcached性能要高于redis，虽然redis最近也在存储大数据的性能上进行优化，但是比起memcached，还是稍有逊色 内存使用效率 使用简单的key-value存储的话，memcached的内存利用率更高，而如果redis采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于memcached。另外，memcached使用预分配的内存池的方式，带来一定程度的空间浪费 并且在内存仍然有很大空间时，新的数据也可能会被剔除，而redis使用现场申请内存的方式来存储数据，不会剔除任何非临时数据 redis更适合作为存储而不是cache。 redis支持服务器端的数据操作 redis相比memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，通常在memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。在redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果需要缓存能够支持更复杂的结构和操作，那么redis会是不错的选择 何时应该使用memcache: 首先就是对小型静态数据进行缓存处理，最具代表性的例子就是HTML代码片段。这是因为memcached在处理元数据时所消耗的内存资源相对更少. 在以前，redis3.0版本之前，memcached在横向扩展方面也比redis更具优势。由于其在设计上的思路倾向以及相对更为简单的功能设置，memcached在实现扩展时的难度比redis低得多。 何时应该使用redis： 其他场景都可以用redis来替换。 相比于武断的LRU(即最低近期使用量)算法，redis允许用户更为精准地进行细化控制，利用六种不同回收策略确切提高缓存资源的实际利用率。redis还采用更为复杂的内存管理与回收对象备选方案。 memcached将键名限制在250字节，值也被限制在不超过1MB，且只适用于普通字符串。redis则将键名与值的最大上限各自设定为512MB，且支持二进制格式。 它所保存的数据具备透明化特性，也就是说服务器能够直接对这些数据进行操作. redis还提供可选而且能够具体调整的数据持久性方案 redis能够提供复制功能。复制功能旨在帮助缓存体系实现高可用性配置方案，从而在遭遇故障的情况下继续为应用程序提供不间断的缓存服务。 使用redis的正确姿势： 要进行master-slave配置，出现服务故障时可以支持切换。 在master侧禁用数据持久化，只需在slave上配置数据持久化。 物理内存+虚拟内存不足，这个时候dump一直死着，时间久了机器挂掉。这个情况就是灾难。 当redis物理内存使用超过内存总容量的3/5时就会开始比较危险了，就开始做swap,内存碎片大。 当达到最大内存时，会清空带有过期时间的key，即使key未到过期时间。 redis与DB同步写的问题，先写DB，后写redis，因为写内存基本上没有问题。]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis事务]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2FRedis%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第十一篇文章。详细探讨redis事务的用法和原理。 redis 事务是一组命令的集合，至少是两个或两个以上的命令，redis 事务保证这些命令被执行时中间不会被任何其他操作打断。 事务基本认识 当客户端处于非事务状态下时， 所有发送给服务器端的命令都会立即被服务器执行。 但是， 当客户端进入事务状态之后， 服务器在收到来自客户端的命令时， 不会立即执行命令， 而是将这些命令全部放进一个事务队列里， 然后返回 QUEUED ， 表示命令已入队。 事务执行 前面说到， 当客户端进入事务状态之后， 客户端发送的命令就会被放进事务队列里。 但其实并不是所有的命令都会被放进事务队列， 其中的例外就是 EXEC 、 DISCARD 、 MULTI 和 WATCH 这四个命令 —— 当这四个命令从客户端发送到服务器时， 它们会像客户端处于非事务状态一样， 直接被服务器执行： 如果客户端正处于事务状态， 那么当 EXEC 命令执行时， 服务器根据客户端所保存的事务队列， 以先进先出（FIFO）的方式执行事务队列中的命令： 最先入队的命令最先执行， 而最后入队的命令最后执行。 事务基本命令介绍 除了 EXEC 之外， 服务器在客户端处于事务状态时， 不加入到事务队列而直接执行的另外三个命令是 DISCARD 、 MULTI 和 WATCH 。 DISCARD 命令用于取消一个事务， 它清空客户端的整个事务队列， 然后将客户端从事务状态调整回非事务状态， 最后返回字符串 OK 给客户端， 说明事务已被取消。 Redis 的事务是不可嵌套的， 当客户端已经处于事务状态， 而客户端又再向服务器发送 MULTI 时， 服务器只是简单地向客户端发送一个错误， 然后继续等待其他命令的入队。 MULTI 命令的发送不会造成整个事务失败， 也不会修改事务队列中已有的数据。 WATCH 只能在客户端进入事务状态之前执行， 在事务状态下发送 WATCH 命令会引发一个错误， 但它不会造成整个事务失败， 也不会修改事务队列中已有的数据（和前面处理 MULTI 的情况一样）。 正常情况 123multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//命令入队exec//提交事务（执行命令） 异常情况 1234multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//正常命令入队set key//错误命令，直接报错exec//事务被丢弃，提交失败 例外情况 1234multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//正常命令入队incr key//虽然字符串不能增一，但是不报错，入队exec//自增会失败，但是key被设置成功了，整个事务没有回滚 放弃事务 123multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//正常命令入队discard 乐观锁 乐观锁：每次拿数据的时候都认为别人不会修改该数据，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这条数据，一般使用版本号进行判断，乐观锁使用于读多写少的应用类型，这样可以提高吞吐量。 乐观锁大多情况是根据数据版本号(version)的机制实现的，何为数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库添加一个version字段来实现读取出数据时，将此版本号一起读出，之后更新时，对此版本号加1，此时将提交数据的版本号与数据库表对应记录的当前版本号进行比对，如果提交的数据版本号大于数据库表的当前版本，则予以更新，否则认为是过期数据，不予更新。 A B 读出版本号为1，操作 A操作时，读出版本号也为1，进行某个操作(修改) 执行修改，version+1=2，因为2&gt;1，所以更新 … … 执行修改，version+1=2，发现数据库记录的版本也为2，2=2,更新失败 watch机制 WATCH 命令用于在事务开始之前监视任意数量的键： 当调用 EXEC 命令执行事务时， 如果任意一个被监视的键已经被其他客户端修改了， 那么整个事务不再执行， 直接返回失败。 123456set k1 1 //设置k1值为1watch k1 //监视k1(其他客户端不能修改k1值)set k1 2 //设置k1值为2multi //开始事务set k1 3 //修改k1值为3exex //提交事务，k1值仍为2，因为事务开始之前k1值被修改了 watch机制举例 大家可能知道redis提供了基于incr命令来操作一个整数型数值的原子递增，那么我们假设如果redis没有这个incr命令，我们该怎么实现这个incr的操作呢？ 正常情况下我们想要对一个整形数值做修改是这么做的(伪代码实现)： 123val = GET mykeyval = val + 1SET mykey $val 但是上述的代码会出现一个问题,因为上面吧正常的一个incr(原子递增操作)分为了两部分,那么在多线程(分布式)环境中，这个操作就有可能不再具有原子性了。 研究过java的juc包的人应该都知道cas，那么redis也提供了这样的一个机制，就是利用watch命令来实现的。 具体做法如下: 123456WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。 由于WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以在一般的情况下我们需要在EXEC执行失败后重新执行整个函数。 执行EXEC命令后会取消对所有键的监控，如果不想执行事务中的命令也可以使用UNWATCH命令来取消监控。 watch机制原理 WATCH 命令的实现 在每个代表数据库的 redis.h/redisDb 结构类型中， 都保存了一个 watched_keys 字典， 字典的键是这个数据库被监视的键， 而字典的值则是一个链表， 链表中保存了所有监视这个键的客户端。 比如说，以下字典就展示了一个 watched_keys 字典的例子： 其中， 键 key1 正在被 client2 、 client5 和 client1 三个客户端监视， 其他一些键也分别被其他别的客户端监视着。 WATCH 命令的作用， 就是将当前客户端和要监视的键在 watched_keys 中进行关联。 举个例子， 如果当前客户端为 client10086 ， 那么当客户端执行 WATCH key1 key2 时， 前面展示的 watched_keys 将被修改成这个样子： 通过 watched_keys 字典， 如果程序想检查某个键是否被监视， 那么它只要检查字典中是否存在这个键即可； 如果程序要获取监视某个键的所有客户端， 那么只要取出键的值（一个链表）， 然后对链表进行遍历即可。 WATCH 的触发 在任何对数据库键空间（key space）进行修改的命令成功执行之后 （比如 FLUSHDB 、 SET 、 DEL 、 LPUSH 、 SADD 、 ZREM ，诸如此类）， multi.c/touchWatchedKey 函数都会被调用 —— 它检查数据库的 watched_keys 字典， 看是否有客户端在监视已经被命令修改的键， 如果有的话， 程序将所有监视这个/这些被修改键的客户端的 REDIS_DIRTY_CAS 选项打开： 当客户端发送 EXEC 命令、触发事务执行时， 服务器会对客户端的状态进行检查： 如果客户端的 REDIS_DIRTY_CAS 选项已经被打开，那么说明被客户端监视的键至少有一个已经被修改了，事务的安全性已经被破坏。服务器会放弃执行这个事务，直接向客户端返回空回复，表示事务执行失败。 如果 REDIS_DIRTY_CAS 选项没有被打开，那么说明所有监视键都安全，服务器正式执行事务。 举个例子，假设数据库的 watched_keys 字典如下图所示： 如果某个客户端对 key1 进行了修改（比如执行 DEL key1 ）， 那么所有监视 key1 的客户端， 包括 client2 、 client5 和 client1 的 REDIS_DIRTY_CAS 选项都会被打开， 当客户端 client2 、 client5 和 client1 执行 EXEC 的时候， 它们的事务都会以失败告终。 最后，当一个客户端结束它的事务时，无论事务是成功执行，还是失败， watched_keys 字典中和这个客户端相关的资料都会被清除。 事务的 ACID 性质 Redis 事务保证了其中的一致性（偶尔也有可能不一致）和隔离性，但并不保证原子性和持久性。 原子性（Atomicity） 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 如果一个事务队列中的所有命令都被成功地执行，那么称这个事务执行成功。 另一方面，如果 Redis 服务器进程在执行事务的过程中被停止 —— 比如接到 KILL 信号、宿主机器停机，等等，那么事务执行失败。 当事务失败时，Redis 也不会进行任何的重试或者回滚动作。 一致性（Consistency） Redis 的一致性问题可以分为三部分来讨论：入队错误、执行错误、Redis 进程被终结。 前面两者上面已经讨论过了，这里再重复一下. 入队错误 入队错误一般是错误的命令(不考虑能不能执行，命令本身就是错误的)，带有不正确入队命令的事务不会被执行，也不会影响数据库的一致性； 执行错误 如果命令在事务执行的过程中发生错误，比如说，对一个不同类型的 key 执行了错误的操作， 那么 Redis 只会将错误包含在事务的结果中， 这不会引起事务中断或整个失败，不会影响已执行事务命令的结果，也不会影响后面要执行的事务命令， 所以它对事务的一致性也没有影响。 Redis 进程被终结 如果 Redis 服务器进程在执行事务的过程中被其他进程终结，或者被管理员强制杀死，那么根据 Redis 所使用的持久化模式，可能有以下情况出现： 内存模式：如果 Redis 没有采取任何持久化机制，那么重启之后的数据库总是空白的，所以数据总是一致的。 RDB 模式：在执行事务时，Redis 不会中断事务去执行保存 RDB 的工作，只有在事务执行之后，保存 RDB 的工作才有可能开始。所以当 RDB 模式下的 Redis 服务器进程在事务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到 RDB 文件里。所以显然会造成不一致 AOF 模式：因为保存 AOF 文件的工作在后台线程进行，所以即使是在事务执行的中途，保存 AOF 文件的工作也可以继续进行,如果事务语句未写入到 AOF 文件，那么显然是一致的，因为事务里的操作全部失败；如果事务的部分语句被写入到 AOF 文件，并且 AOF 文件被成功保存，那么不完整的事务执行信息就会遗留在 AOF 文件里，当重启 Redis 时，程序会检测到 AOF 文件并不完整，Redis 会退出，并报告错误。需要使用 redis-check-aof 工具将部分成功的事务命令移除之后，才能再次启动服务器。还原之后的数据总是一致的，而且数据也是最新的（直到事务执行之前为止）。 隔离性（Isolation） Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。 持久性（Durability） 在单纯的内存模式下，事务肯定是不持久的。 在 RDB 模式下，服务器可能在事务执行之后、RDB 文件更新之前的这段时间宕机，所以 RDB 模式下的 Redis 事务也是不持久的。 在 AOF 的“总是 SYNC ”模式下，事务的每条命令在执行成功之后，都会立即调用 fsync 或 fdatasync 将事务数据写入到 AOF 文件。但是，这种保存是由后台线程进行的，主线程不会阻塞直到保存成功，所以从命令执行成功到数据保存到硬盘之间，还是有一段非常小的间隔，服务器也有可能出现问题，所以这种模式下的事务也是不持久的。 都是不持久的。 总结 MULTI 命令的执行标记着事务的开始 当客户端进入事务状态之后， 服务器在收到来自客户端的命令时， 不会立即执行命令， 而是将这些命令全部放进一个事务队列里， 然后返回 QUEUED ， 表示命令已入队 Redis 的事务保证了 ACID 中的一致性（C）（偶尔也有可能不一致）和隔离性（I），但并不保证原子性（A）和持久性（D）。 不加入到事务队列而直接执行的四个命令为：EXEC 、 DISCARD 、 MULTI 和 WATCH DISCARD 命令用于取消一个事务 Redis 的事务是不可嵌套的 WATCH 只能在客户端进入事务状态之前执行 WATCH机制的原理 参考： 事务 redis的事务和watch]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存更新]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2FRedis%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第十篇文章。redis缓存更新策略学习。 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching，我们下面一一来看一下这四种Pattern。这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 先来看看缓存可能存在的一些问题，目的是突出缓存使用策略选择的重要性。 1.缓存穿透 缓存穿透是说访问一个缓存中没有的数据，但是这个数据数据库中也不存在。 解决方案是： 缓存空对象。如果缓存未命中，而数据库中也没有这个对象，则可以缓存一个空对象到缓存。如果使用Redis，这种key需设置一个较短的时间，以防内存浪费。 缓存预测。预测key是否存在。如果缓存的量不大可以使用hash来判断，如果量大可以使用布隆过滤器来做判断。采用布隆，将所有可能存在的数据哈希到一个足够大的BitSet中，不存在的数据将会被拦截掉，从而避免了对存储系统的查询压力。 2.缓存并发 多个客户端同时访问一个没有在cache中的数据，这时每个客户端都会执行从DB加载数据set到缓存，就会造成缓存并发。 缓存预热。提前把所有预期的热数据加到缓存。定位热数据还是比较复杂的事情，需要根据自己的服务访问情况去评估。这个方案只能减轻缓存并发的发生次数不能全部抵制。 缓存加锁。 如果多个客户端访问不存在的缓存时，在执行加载数据并set缓存这个逻辑之前先加锁，只能让一个客户端执行这段逻辑。 3.缓存雪崩 缓存雪崩是缓存服务暂时不能提供服务，导致所有的请求都直接访问DB。 解决方案： 构建高可用的缓存系统。目前常用的缓存系统Redis和Memcache都支持高可用的部署方式，所以部署的时候不防先考虑是否要以高可用的集群方式部署。 限流。Netflix的Hystrix是非常不错的工具，在用缓存时不妨搭配它来使用。 4.Cache Aside Pattern 一种错误的做法是：先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，直到这个缓存失效为止。 Cache Aside Pattern是最常用最常用的pattern了。其具体逻辑如下： 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？ 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 但还是存在问题的。比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。不过，实际上出现的概率可能非常低. 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 5.Read/Write Through Pattern Read Through：读取数据的时候如果当前缓存中没有数据，惯常的操作都是应用程序去DB加载数据，然后加入到缓存中。Read Through与之不同的是我们不需要在应用程序自己加载数据了，缓存层会帮忙做件事。 Write Through：更新数据的时候，如果命中缓存，则先更新缓存然后缓存在负责把数据更新到数据库；如果没有命中缓存则直接更新数据库。 这种方式缓存层直接屏蔽了DB，应用程序只需要更缓存打交道。优点是应用逻辑简单了，而且更高效了；缺点是缓存层的实现相对复杂一些。 6.Write Back Pattern Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道Unix/Linux非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。 另外，Write Back实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的write back会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。 7.实际使用的一些策略 业务方（调用者）更新 传统上，更新缓存都是由业务方来做，也就是由调用者负责更新DB和缓存。 DB中间件监听DB变化，更新缓存 现在有种新的办法就是利用DB中间件监听DB变化（比如阿里的Canal中间件，点评的Puma），从而对缓存进行更新。 这种办法的一个好处就是：把缓存的更新逻辑，和业务逻辑解藕。业务只更新DB，缓存的更新被放在另外一个专门的系统里面。 8.总结 一句话，无论谁先谁后，只要更新缓存和更新DB不是原子的，就可能导致不一致。 总之，只是从实际业务来讲，一般缓存也都是保持“最终一致性“，而不是和DB的强一致性。 并且一般建议先更新DB，再更新缓存，优先保证DB数据正确。 9.一致性问题 上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新Cache成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback.后续再探讨。 参考1：https://coolshell.cn/articles/17416.html 参考2：https://www.jianshu.com/p/3c111e4719b8 参考3：缓存更新策略/缓存穿透/缓存雪崩]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存设计与优化]]></title>
    <url>%2F2019%2F02%2F01%2Fredis%2F%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第九篇文章。介绍redis缓存中 一些重要的问题。 1. 缓存收益和成本 1.1 收益 加速读写 降低后端负载(降低mysql负载) 1.2 成本 数据不一致：缓存层和数据层有时间窗口不一致，和更新策略有关 代码维护成本：多了一层缓存逻辑 运维成本：例如redis cluster 1.3 使用场景 降低后端负载：对于高消耗的SQL：join结果集、分组统计结果；对这些结果进行缓存。 加速请求响应 大量写合并为批量写：如计数器先redis累加再批量写入DB 2. 缓存的更新策略 LRU/LFU/FIFO算法剔除：例如maxmemory-policy FIFO(first in first out) 先进先出策略，最先进入缓存的数据在缓存空间不够的情况下（超出最大元素限制）会被优先被清除掉，以腾出新的空间接受新的数据。策略算法主要比较缓存元素的创建时间。在数据实效性要求场景下可选择该类策略，优先保障最新数据可用。 LFU(less frequently used) 最少使用策略，无论是否过期，根据元素的被使用次数判断，清除使用次数较少的元素释放空间。策略算法主要比较元素的hitCount（命中次数）。在保证高频数据有效性场景下，可选择这类策略。 LRU(least recently used) 最近最少使用策略，无论是否过期，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素释放空间。策略算法主要比较元素最近一次被get使用时间。在热点数据场景下较适用，优先保证热点数据的有效性。 超时剔除：例如expire 主动更新：开发控制生命周期（最终一致性，时间间隔比较短） 低一致性：最大内存和淘汰策略 高一致性：超时剔除和主动更新结合，最大内存和淘汰策略兜底。 3. 缓存粒度控制 3.1 缓存粒度控制三个角度 通用性：全量属性更好(添加删除属性不需要改东西) 占用空间：部分属性更好 代码维护：表面上全量属性更好(添加删除属性不需要改东西) 4. 缓存穿透优化 4.1 定义 大量请求不命中,缓存已经没有存在的意义了： 4.2 产生原因 业务代码自身问题 恶意攻击、爬虫等 4.3 如何发现 业务响应时间 业务本身问题 相关指标：总调用数、缓存层命中数、存储层命中数 4.4 解决方案 方案一：缓存空对象 存在的问题 需要更多的键:恶意攻击、爬虫会有很多乱七八糟的键，当量很大时，会有风险，所以会对这种空对象设置缓存时间控制风险 缓存层和存储层数据“短期”不一致：缓存了空对象，但是当业务恢复了，真实数据又存在于DB中了，那么在这个空对象过期时间内，取到的仍然是空对象，造成短期内数据不一致的问题。解决：可以订阅消息，当恢复正常后接受到消息，然后刷新缓存。 方案二：布隆过滤器拦截 什么是Bloom Filter？ 布隆过滤器（Bloom Filter）是1970年由布隆提出的, “a space-efficient probabilistic data structure”。它实际上是一个很长的二进制矢量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢，上述三种结构的检索时间复杂度分别为O(n),O(log n),O(n/k)。 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。 优点：相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数（O（k））。另外, 散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。布隆过滤器可以表示全集，其它任何数据结构都不能；k和m相同，使用同一组散列函数的两个布隆过滤器的交并差运算可以使用位操作进行。 缺点：但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。另外，一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加1,这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。在降低误算率方面，有不少工作，使得出现了很多布隆过滤器的变种。 检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。 Bloom Filter应用场景？ 用Redis的Bitmap作为位数组构建起来的可扩展的布隆过滤器。 Redis实现的布隆过滤器如何快速有效删除数据？：EXPIRE “bitmap的key值” 0 4.5 解决方案对比 5. 无底洞问题优化 5.1 问题描述 2010年，facebook有了3000个Memcache节点 发现问题：&quot;加&quot;机器性能没能提升，反而下降 5.2 问题原因 当存在的节点异常多的时候，IO的代价已经超过数据传输，上文提到的facebook的节点已经超过3000个，在这种情况下再增加节点已经没法再提高效率了。 5.3 问题解决—优化IO 命令本身的效率：例如sql优化，命令优化 网络次数：减少通信次数 降低接入成本:长连/连接池,NIO等。 IO访问合并:O(n)到O(1)过程:批量接口(mget)，就是上一篇文章中介绍的对于mget的四个方案。 6. 缓存雪崩优化 6.1 什么是缓存雪崩？ 从下图可以很清晰出什么是缓存雪崩：由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。 缓存雪崩的英文原意是 stampeding herd（奔逃的野牛），指的是缓存层宕掉后，流量会像奔逃的野牛一样，打向后端存储。 6.2 如何防止缓存雪崩？ 保证缓存层服务高可用性。 和飞机都有多个引擎一样，如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如前面介绍过的 Redis Sentinel 和 Redis Cluster 都实现了高可用。 依赖隔离组件为后端限流并降级 无论是缓存层还是存储层都会有出错的概率，可以将它们视同为资源。作为并发量较大的系统，假如有一个资源不可用，可能会造成线程全部 hang 在这个资源上，造成整个系统不可用。降级在高并发系统中是非常正常的：比如推荐服务中，如果个性化推荐服务不可用，可以降级补充热点数据，不至于造成前端页面是开天窗。 在实际项目中，我们需要对重要的资源 ( 例如 Redis、 MySQL、 Hbase、外部接口 ) 都进行隔离，让每种资源都单独运行在自己的线程池中，即使个别资源出现了问题，对其他服务没有影响。但是线程池如何管理，比如如何关闭资源池，开启资源池，资源池阀值管理，这些做起来还是相当复杂的，这里推荐一个 Java 依赖隔离工具 Hystrix。超出范围了。不再赘述。 7. 热点key重建优化 7.1 问题 热点key( 例如一个热门的娱乐新闻）+较长的重建时间（可能是一个复杂计算，例如复杂的 SQL、多次 IO、多个依赖等） 就是说在高并发的情况下，某个key在缓存中重建时间太长，以至于高并发下缓存查不到，都去DB进行查询。对于DB压力很大，并且响应时间长。 三个目标：要减少缓存重建次数、数据尽可能一致、减少潜在危险。 两个解决：互斥锁、永远不过期 7.2 互斥锁—setex,setnx 存在问题：有等待时间。 伪代码： (1) 从 Redis 获取数据，如果值不为空，则直接返回值，否则执行 (2.1) 和 (2.2)。 (2) 如果 set(nx 和 ex) 结果为 true，说明此时没有其他线程重建缓存，那么当前线程执行缓存构建逻辑。 (2.2) 如果 setnx(nx 和 ex) 结果为 false，说明此时已经有其他线程正在执行构建缓存的工作，那么当前线程将休息指定时间 ( 例如这里是 50 毫秒，取决于构建缓存的速度 ) 后，重新执行函数，直到获取到数据。 7.3 永远不过期 这里我想了很久到底是什么意思，，，我感觉这是一个场景：保证数据的定期更新。对于热点key,无非是并发特别大并且重建缓存时间比较长，如果直接设置过期时间，那么时间到的时候，巨大的访问量会压迫到数据库上，所以我们实际上，是不给他设置过期时间，但是不设置过期时间，怎么做到定时更新呢？这里的方案是给热点key的val增加一个逻辑过期时间字段，并发访问的时候，判断这个逻辑字段的时间值是否大于当前时间，大于了说明要对缓存进行更新了，那么这个时候，依然让所有线程访问老的缓存，因为缓存并没有设置过期，但是另开一个线程对缓存进行重构。等重构成功，即执行了redis set操作之后，所有的线程就可以访问到重构后的缓存中的新的内容了。不知道我的理解是不是正确。 “永远不过期”包含两层意思： 从缓存层面来看，确实没有设置过期时间，所以不会出现热点 key 过期后产生的问题，也就是“物理”不过期。 从功能层面来看，为每个 value 设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。 2018/6/19 号补充：物理上缓存确实是不过期的，保证所有线程都能访问到，但是有可能是老的数据；逻辑上给 value 增加过期时间，如果当过期时间超过当前时间(每一个线程拿缓存数据的时候都会判断一下，也就是说这里仍然使用互斥锁，其中一个线程发现过期时间超过当前时间了，那么锁住，另开一个线程去完成数据重建)，新开一个线程去构建缓存，构建成功之后，设置新内容到缓存中并且删除老缓存，就完成了热点 key 的重建。 伪代码实现： 两种方案对比]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Cluster理论详解]]></title>
    <url>%2F2019%2F02%2F01%2Fredis%2FRedis-Cluster%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第八篇文章。上一篇我们学习了redis sentinel，知道了它是redis高可用的一种实现方案。但是面对要求很高的场景，一台master是一定不能解决问题的，redis 3.0给我们带来了服务端集群方案，解决了这个问题。 1. 数据分区 集群，那么就会涉及到数据是如何分片的。有两种方式：顺序分区和哈希分区 两者对比： 直接hash取模进行数据分片时，当节点增加，会有很多数据命中不了，需要重新映射。如果大多数数据在增加或者减少节点之后进行迁移的话，对于性能影响是很大的，因为数据迁移，那么缓存中现在是无法命中的，必须去数据库取，是灾难性的行为。 早期的做法就是这样，在客户端hash取余节点个数来进行数据分片。如果非要这样，采取翻倍扩容会稍微好一点，迁移数据量会小一点。不过无论如何，这种方式在大数据量情况下是不可行的。 2. 一致性hash算法 对于上面提到的直接hash取余的方式，会导致大量数据的迁移。那么有没有一种方式，在增加或减少节点时，只有少部分数据迁移呢？ 针对一致性hash算法，已经在简明理解一致性hash算法中详细说明了，不再赘述。 对于redis 3.0之前，客户端可以用这种方式来实现数据分片。在redis 3.0之后，就不需要客户端来实现分片算法了，而是直接给我们提供了服务端集群方案redis cluster. 3. 虚拟槽 redis cluster引入槽的概念，一定要与一致性hash的槽区分！这里每一个槽映射一个数据集。 CRC16(key) &amp; 16383 这里计算结果发送给redis cluster任意一个redis节点，这个redis节点发现他是属于自己管辖范围的，那就将它放进去；不属于他的槽范围的话，由于redis之间是相互通信的，这个节点是知道其他redis节点的槽的信息，那么会告诉他去那个redis节点去看看。 那么就实现了服务端对于槽、节点、数据的管理。 当master节点增加时，即扩容时，对于以上两种方案，都会出现数据迁移，那么只能作为缓存场景使用。但是redis cluster，由于每个节点维护的槽的范围是固定的，当有新加入的节点时，是不会干扰到其他节点的槽的，必须是以前的节点将使用槽的权利分配给你，并且将数据分配给你，这样，新的节点才会真正拥有这些槽和数据。这种实现还处于半自动状态，需要人工介入。-----主要的思想是：槽到集群节点的映射关系要改变，不变的是键到槽的映射关系. Redis集群，要保证16384个槽对应的node都正常工作，如果某个node发生故障，那它负责的slots也就失效，整个集群将不能工作。为了增加集群的可访问性，官方推荐的方案是将node配置成主从结构，即一个master主节点，挂n个slave从节点。这时，如果主节点失效，Redis Cluster会根据选举算法从slave节点中选择一个上升为主节点，整个集群继续对外提供服务。 4. 某个Master又怎么知道某个槽自己是不是拥有呢？ Master节点维护着一个16384/8字节的位序列，Master节点用bit来标识对于某个槽自己是否拥有。比如对于编号为1的槽，Master只要判断序列的第二位（索引从0开始）是不是为1即可。 如上面的序列，表示当前Master拥有编号为1，134的槽。集群同时还维护着槽到集群节点的映射，是由长度为16384类型为节点的数组实现的，槽编号为数组的下标，数组内容为集群节点，这样就可以很快地通过槽编号找到负责这个槽的节点。位序列这个结构很精巧，即不浪费存储空间，操作起来又很便捷。 具体参照：http://blog.jobbole.com/103258/ ,还提到了slot迁移的一些细节。 5. redis节点之间如何通信的？ gossip协议：节点之间彼此不断通信交换信息，一段时间后所有节点都会知道集群完整的信息。 节点与节点之间通过二进制协议进行通信。 客户端和集群节点之间通信和通常一样，通过文本协议进行。 集群节点不会代理查询。 6. 集群伸缩 这里6385为新加入的节点，一开始是没有槽的，所以进行slot的迁移。 集群伸缩：槽和数据在节点之间的移动。 迁移数据的流程图： 迁移key可以用pipeline进行批量的迁移。 对于扩容，原理已经很清晰了，至于具体操作，网上很多。至于缩容，也是先手动完成数据迁移，再关闭redis。 7. 客户端路由 7.1 moved重定向 其中，槽直接命中的话，就直接返回槽编号： 槽不命中，返回带提示信息的异常，客户端需要重新发送一条命令： 对于命令行的实验，用redis-cli去连接集群： redis -c -p 7000:加上-c，表示使用集群模式，帮助我们在第一次不命中的情况下自动跳转到对应的节点上： 如果不加-c的话，会返回moved异常，不会自动跳转： 7.2 ask重定向 在扩容缩容的时候，由于需要遍历这个节点上的所有的key然后进行迁移，是比较慢的，对客户端是一个挑战。因为假设一个场景，客户端访问某个key，节点告诉客户端这个key在源节点，当我们再去源节点访问的时候，却发现key已经迁移到目标节点。 7.3 moved重定向和ask重定向对比 两者都是客户端的重定向 moved：槽已经确定转移 ask:槽还在迁移中 问题：如果节点众多，那么让客户端随机访问节点，那么直接命中的概率只有百分之一，还有就是发生ask异常时（即节点正在迁移时）客户端如何还能高效运转？ 总结一句话就是redis cluster的客户端的实现会更复杂。 8. smart客户端 8.1 追求目标 追求性能，不会使用代理模式，而是直连对应节点。需要对moved异常和ask异常做兼容。也就是说，需要有一个这个语言对应的客户端来高效实现查找等操作。 8.2 smart原理 从集群中选一个可运行节点，使用cluster slots初始化槽和节点映射 将slot与node节点的结果映射到本地，为每个节点创建JedisPool 准备执行命令 第一步中将slot与node节点的对应关系放在了map中，形成一个映射关系；key是通过CRC16算法再取余得到slot，所以key与slot的映射关系也是确定的。我们就可以直接发送命令。只要后面集群没有发生数据迁移，那么就会连接成功。但是如果在连接的时候出现了连接出错，说明这个key已经迁移到其他的node上了。如果发现key不停地迁移，超过5次就报错。 在发生moved异常的时候，则需要刷新缓存，即一开始维护的map。 有一个情况比较全的图： java redis cluster客户端：jedisCluster基本使用–伪代码 jedisCluster内部已经封装好池的借还操作等。 先写一个JedisClusterFactory: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import redis.clients.jedis.HostAndPort;import redis.clients.jedis.JedisCluster;import redis.clients.jedis.JedisPoolConfig;import java.io.IOException;import java.util.HashSet;import java.util.List;import java.util.Set;public class JedisClusterFactory &#123; private JedisCluster jedisCluster; private List&lt;String&gt; hostPortList; //超时时间 private int timeout; public void init()&#123; //这里可以设置相关参数 JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); //从配置文件中读取ip:port的参数放进Set中 Set&lt;HostAndPort&gt; nodeSet = new HashSet&lt;HostAndPort&gt;(); for(String hostPort : hostPortList)&#123; String[] arr = hostPort.split(":"); if(arr.length != 2)&#123; continue; &#125; nodeSet.add(new HostAndPort(arr[0],Integer.parseInt(arr[1]))); &#125; try &#123; jedisCluster = new JedisCluster(nodeSet,timeout,jedisPoolConfig); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void destory()&#123; if(jedisCluster != null)&#123; try &#123; jedisCluster.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public JedisCluster getJedisCluster() &#123; return jedisCluster; &#125; //spring注入hostPortList和timeout public void setHostPortList(List&lt;String&gt; hostPortList) &#123; this.hostPortList = hostPortList; &#125; public void setTimeout(int timeout) &#123; this.timeout = timeout; &#125;&#125; hostPortList 放入spring bean中，spring自动完成注入。 8.3 多节点命令实现 有的时候我们想操作所有节点的数据。如何实现呢？ 8.4 批量操作 mget,mset必须在一个槽。这个条件比较苛刻，一般是不能保证的，那么如何实现批量的操作呢？ Redis Cluster的行为和Redis 的单节点不同，甚至和一个Sentinel 监控的主从模式也不一样。主要原因是集群自动分片，将一个key 映射到16384个槽中的一个，这些槽分布在多个节点上。因此操作多个key 的命令必须保证所有的key 都映射到同一个槽上，避免跨槽执行错误。更进一步说，今后一个单独的集群节点，只服务于一组专用的keys，请求一个命令到一个Server，只能得到该Server 上拥有keys 的对应结果。一个非常简单的例子是执行KEYS命令，当发布该命令到集群环境中的某个节时，只能得到该节点上拥有的keys，而不是集群中所有的keys。所以要得到集群中所有的keys，必须从集群的所有主节点上获取所有的keys。 对于分散在redis集群中不同节点的数据，我们如何比较高效地批量获取数据呢？？？？ 串行mget–原始方案，整一个for循环 串行IO 对key进行RCR16和取余操作得到slot，将slots按照节点进行分批传送： 并行IO hash_tag 不做任何改变的话，hash之后就比较均匀地散在每个节点上： 那么我们能不能像使用单机redis一样，一次IO将所有的key取出来呢？hash-tag提供了这样的功能，如果将上述的key改为如下，也就是用大括号括起来相同的内容，那么这些key就会到指定的一个节点上。 在mget的时候只需要在一台机器上去即可。 对比 方案三比较复杂，一般不用；方案四可能会出现数据倾斜，也不用。方案一在key小的时候可以用；方案二相对来说有一点优势； 为什么说是一点优势呢？pipeline批量处理不应该比串行处理好很多吗？ http://xiezefan.me/2015/12/13/redis_cluster_research_2/ http://trumandu.github.io/2016/05/09/RedisCluster构建批量操作探讨/ 9. 故障转移 9.1 故障发现 通过ping/pong消息实现故障发现：不需要sentinel 分为主观下线和客观下线 主观下线： 客观下线： pfail消息就是主观下线的信息，维护在一个链表中，链表中包含了所有其他节点对其他节点所有的主观信息，是有时间周期的，为了防止很早以前的主观下线信息还残留在这里。对这个链表进行分析，符合条件就尝试客观下线。 9.2 故障恢复 从节点接收到他的主节点客观下线的通知，则进行故障恢复的操作。 资格检查 选取出符合条件的从节点：当从节点和故障主节点的断线时间太长，会被取消资格。 准备选举时间 就是为了保证偏移量大的从节点优先被选举投票 选举投票 替换主节点 这些所有步骤加起来，差不多十几秒左右。最后如果故障节点又恢复功能了，就称为新的Master的slave节点。 10. 常见问题 10.1 集群完整性 cluster-require-full-coverage默认为yes - 要求所有节点都在服务，集群中16384个槽全部可用：保证集群完整性 - 节点故障或者正在故障转移：`(error)CLUSTERDOWN the cluster is down` 但是大多数业务都无法容忍。需要将cluster-require-full-coverage设置为no 10.2 带宽消耗 消息发送频率：节点发现与其他节点最后通信时间超过cluster-node-timeout/2时会直接发送Ping消息 消息数据量：slots槽数组(2k空间)和整个集群1、10的状态数据(10个节点状态数据约10k) 节点部署的机器规模：进去分布的机器越多且每台机器划分的节点数越均匀，则集群内整体的可用带宽越高。 优化：避免“大”集群，：避免多业务使用一个集群，大业务可用多集群；cluster-node-timeout时间设置要注意是带宽和故障转移速度的均衡；尽量均匀分配到多机器上：保证高可用和带宽。 10.3 PubSub广播 问题：publish在集群中每个节点广播：加重带宽。 解决：单独“走”一套redis sentinel。就是针对目标的几个节点构建redis sentinel，在这个里面实现广播。 10.4 数据倾斜 节点和槽分配不均匀 ./redis-trib.rb info ip:port查看节点、槽、键值分布 慎用rebalance命令 不同槽位对应键数量差异较大 CRC16正常情况下比较均匀 可能存在hash_tag cluster countKeysinslot {slot}获取槽对应键值个数 包含bigkey 例如大字符串、几百万的元素的hash、set等 在从节点上执行:redis-cli --bigkeys来查看bigkey情况 优化：优化数据结构 内存相关配置不一致 因为某种情况下，某个节点对hash或者Set这种数据结构进行了单独的优化，而其他节点都没有配置，会出现配置不一致的情况。 10.5 请求倾斜 热点key：重要的key或者bigkey 优化：避免bigkey;热键不使用hash_tag；当一致性不高时，可以用本地缓存+MQ 10.6 读写分离 只读连接：集群模式的从节点不接受任何读写请求 重定向到负责槽的主节点(对从节点进行读，都是重定向到主节点再返回信息) readonly命令可以读：连接级别命令(每次重新连接都要写一次) 上图可以看出，redis cluster 默认slave 也是不能读的，如果要读取，需要执行 readonly，就可以了。 读写分离：更加复杂（成本很高，尽量不要使用） 同样的问题：复制延迟、读取过期数据、从节点故障 修改客户端 10.7 数据迁移 分为离线迁移和在线迁移(唯品会redis-migrate-tool和豌豆荚redis-port)。 官方的方式：只能从单机迁移到集群、不支持在线迁移、不支持断点续传、单线程迁移影响速度 ./redis-trib.rb import --from 源ip:port --copy 目标ip:port 加入在迁移时再往源redis插入几条数据，这几条数据会丢失(丢失一部分) 10.8 集群vs单机 集群也有一定的限制： 分布式redis不一定是好的： 11. 简单总结]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明理解一致性hash算法]]></title>
    <url>%2F2019%2F02%2F01%2Fmiscellany%2F15%E7%AE%80%E6%98%8E%E7%90%86%E8%A7%A3%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在分布式集群中，对机器的添加删除，或者机器故障后自动脱离集群这些操作是分布式集群管理最基本的功能。如果采用常用的hash(object)%N算法，那么在有机器添加或者删除后，很多原有的数据就无法找到了，这样严重的违反了单调性原则。接下来主要讲解一下一致性哈希算法是如何设计的。 环形Hash空间 按照常用的hash算法来将对应的key哈希到一个具有2^32 次方个桶的空间中，即0~(2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图 把数据通过一定的hash算法处理后映射到环上 现在我们将object1、object2、object3、object4四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上。如下图： 1234Hash(object1) = key1；Hash(object2) = key2；Hash(object3) = key3；Hash(object4) = key4； 将机器通过hash算法映射到环上 在采用一致性哈希算法的分布式集群中将新的机器加入，其原理是通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中。 假设现在有NODE1，NODE2，NODE3三台机器，通过Hash算法得到对应的KEY值，映射到环中，其示意图如下： 123Hash(NODE1) = KEY1;Hash(NODE2) = KEY2;Hash(NODE3) = KEY3; 通过上图可以看出对象与机器处于同一哈希空间中，这样按顺时针转动object1存储到了NODE1中，object3存储到了NODE2中，object2、object4存储到了NODE3中。在这样的部署环境中，hash环是不会变更的，因此，通过算出对象的hash值就能快速的定位到对应的机器中，这样就能找到对象真正的存储位置了。 机器的删除与添加 普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。下面来分析一下一致性哈希算法是如何处理的。 节点（机器）的删除 以上面的分布为例，如果NODE2出现故障被删除了，那么按照顺时针迁移的方法，object3将会被迁移到NODE3中，这样仅仅是object3的映射位置发生了变化，其它的对象没有任何的改动。如下图： 节点（机器）的添加 如果往集群中添加一个新的节点NODE4，通过对应的哈希算法得到KEY4，并映射到环中，如下图： 通过按顺时针迁移的规则，那么object2被迁移到了NODE4中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 平衡性 根据上面的图解分析，一致性哈希算法满足了单调性和负载均衡的特性以及一般hash算法的分散性，但这还并不能当做其被广泛应用的原由，因为还缺少了平衡性。下面将分析一致性哈希算法是如何满足平衡性的。 hash算法是不保证平衡的，如上面只部署了NODE1和NODE3的情况（NODE2被删除的图），object1存储到了NODE1中，而object2、object3、object4都存储到了NODE3中，这样就照成了非常不平衡的状态。在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。 “虚拟节点”（ virtual node ）是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。 以上面只部署了NODE1和NODE3的情况（NODE2被删除的图）为例，之前的对象在机器上的分布很不均衡，现在我们以2个副本（复制个数）为例，这样整个hash环中就存在了4个虚拟节点，最后对象映射的关系图如下： 根据上图可知对象的映射关系：object1-&gt;NODE1-1，object2-&gt;NODE1-2，object3-&gt;NODE3-2，object4-&gt;NODE3-1。通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，真正的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图： “虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值： 1Hash(“192.168.1.100”); 引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值为： 12Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2 整理自： 五分钟理解一致性哈希算法]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Sentinel实现高可用读写分离]]></title>
    <url>%2F2019%2F02%2F01%2Fredis%2FRedis-Sentinel%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第七篇文章。Redis Sentinel 是一个分布式系统，你可以在一个架构中运行多个 Sentinel 进程，这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息，并使用投票协议（agreement protocols）来决定是否执行自动故障迁移，以及选择哪个从服务器作为新的主服务器。 虽然 Redis Sentinel 是一个单独的可执行文件 redis-sentinel ，但实际上它只是一个运行在特殊模式下的 Redis 服务器，你可以在启动一个普通 Redis 服务器时通过给定 –sentinel 选项来启动 Redis Sentinel 。 启动方式一：使用sentinel可执行文件 redis-sentinel 程序来启动 Sentinel 系统，命令如下： 1redis-sentinel /path/to/sentinel.conf sentinel只是运行在特殊模式下的redis服务器，你可以用启动redis服务的命令来启动一个运行在 Sentinel 模式下的 Redis 服务器： 1redis-server /path/to/sentinel.conf --sentinel 1. redis sentinel 首先来看看什么是 redis sentinel，中文翻译是redis哨兵。顾名思义，哨兵是站岗监督突发情况的，那么这里具体的功能上很类似： 监控：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移：当一个主服务器不能正常工作时，Sentinel 会开始一次自动故障迁移操作，它会将失效主服务器的其中一个从服务器升级为新的主服务器，并让失效主服务器的其他从服务器改为复制新的主服务器；当客户端试图连接失效的主服务器时，集群也会向客户端返回新主服务器的地址，使得集群可以使用新主服务器代替失效服务器。 其中总结一下故障转移的基本原理： 多个sentinel发现并确认master有问题 选举出一个sentinel作为领导 选出一个可以成为新的master的slave 通知其他的slave称为新的master的slave 通知客户端主从变化 等待老的master复活称为新的master的slave 也支持多个master-slave结构： 2. 安装与配置 配置开启主从节点 配置开启sentinel监控主节点（sentinel是特殊的redis） 实际应该多台机器，但是演示方便，只用一台机器来搭建 详细配置节点 本地安装的结构图： 对于master:redis-7000.conf配置： 12345port 7000daemonize yespidfile /usr/local/redis/data/redis-7000.pidlogfile &quot;7000.log&quot;dir &quot;/usr/local/redis/data&quot; 对于slave:redis-7001和redis-7002配置： 123456port 7001daemonize yespidfile /usr/local/redis/data/redis-7001.pidlogfile &quot;7001.log&quot;dir &quot;/usr/local/redis/data&quot;slaveof 127.0.0.1 7000 启动redis服务： 1redis-server ../config/redis-7000.conf 访问7000端口的master redis: 1redis-cli -p 7000 info replication 显示他有两个从节点： 12345678910# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=7002,state=online,offset=99550,lag=1slave1:ip=127.0.0.1,port=7001,state=online,offset=99816,lag=0master_repl_offset:99816repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:99815 对于sentinel主要配置： master sentinel config: 123456port 26379daemonize yesdir &quot;/usr/local/redis/data&quot;logfile &quot;26379.log&quot;sentinel monitor mymaster 127.0.0.1 7000 2... 启动redis sentinel: 1redis-sentinel ../config/redis-sentinel-26379.conf 访问26379 redis sentinel master: 1redis-cli -p 26379 info sentinel 显示： 123456# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=127.0.0.1:7000,slaves=2,sentinels=3 1查看这六个进程是否都起来了：ps -ef | grep redis 注意，如果上面是配置在虚拟机的话，需要将127.0.0.1改为虚拟机的ip，要不然找不着。 3. 故障转移演练 3.1 java客户端程序 JedisSentinelPool只是一个配置中心，不需要具体连接某个redis，注意它不是代理。 1234567891011121314151617181920212223242526272829303132333435private Logger logger = LoggerFactory.getLogger(AppTest.class);@Testpublic void test4()&#123; //哨兵配置，我们访问redis，就通过sentinel来访问 String masername = "mymaster"; Set&lt;String&gt; sentinels = new HashSet&lt;&gt;(); sentinels.add("10.128.24.176:26379"); sentinels.add("10.128.24.176:26380"); sentinels.add("10.128.24.176:26381"); JedisSentinelPool sentinelPool = new JedisSentinelPool(masername,sentinels); //一个while死循环，每隔一秒往master塞入一个值，并且日志打印 while (true)&#123; Jedis jedis = null; try&#123; jedis = sentinelPool.getResource(); int index = new Random().nextInt(100000); String key = "k-" + index; String value = "v-" + index; jedis.set(key,value); logger.info("&#123;&#125; value is &#123;&#125;",key,jedis.get(key)); TimeUnit.MILLISECONDS.sleep(1000); &#125;catch (Exception e)&#123; logger.error(e.getMessage(),e); &#125;finally &#123; if(jedis != null)&#123; jedis.close(); &#125; &#125; &#125;&#125; maven依赖是： 123456789101112131415161718&lt;!--jedis--&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--slf4j日志接口--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt;&lt;/dependency&gt;&lt;!--logback日志实现--&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; 启动程序，发现是正常写入： 123456789101112131416:16:01.424 [main] INFO com.njupt.swg.AppTest - k-54795 value is v-5479516:16:02.426 [main] INFO com.njupt.swg.AppTest - k-55630 value is v-5563016:16:03.429 [main] INFO com.njupt.swg.AppTest - k-70642 value is v-7064216:16:04.430 [main] INFO com.njupt.swg.AppTest - k-42978 value is v-4297816:16:05.431 [main] INFO com.njupt.swg.AppTest - k-96297 value is v-9629716:16:06.433 [main] INFO com.njupt.swg.AppTest - k-4220 value is v-422016:16:07.435 [main] INFO com.njupt.swg.AppTest - k-34103 value is v-3410316:16:08.436 [main] INFO com.njupt.swg.AppTest - k-9177 value is v-917716:16:09.437 [main] INFO com.njupt.swg.AppTest - k-24389 value is v-2438916:16:10.439 [main] INFO com.njupt.swg.AppTest - k-32325 value is v-3232516:16:11.440 [main] INFO com.njupt.swg.AppTest - k-68538 value is v-6853816:16:12.441 [main] INFO com.njupt.swg.AppTest - k-36233 value is v-3623316:16:13.443 [main] INFO com.njupt.swg.AppTest - k-305 value is v-30516:16:14.444 [main] INFO com.njupt.swg.AppTest - k-59279 value is v-59279 我们将现在的端口为7000的redis master 给kill掉 kill -9 master的pid 我们会发现：客户端报异常，但是在大概十几秒之后，就继续正常塞值了。原因是服务端的哨兵机制的选举matser需要一定的时间。 4. 三个定时任务 4.1 每10秒每个sentinel对master和slave执行Info 发现slave节点 确认主从关系 4.2 每2秒每个sentinel通过master节点的channel交换信息(pub/sub) 通过__sentinel__:hello进行频道交互 交互对节点的“看法”和自身信息 4.3 每1秒每个sentinel对其他sentinel和redis执行ping 心跳监测，失败判定依据 5. 主观下线和客观下线 对于之前的Sentinel配置文件中有两条配置： 监控master redis节点，这里是当超过两个sentinel认为master挂了，则认为master挂了。 sentinel monitor &lt;masterName&gt; &lt;masterIp&gt; &lt;msterPort&gt; &lt;quorum&gt; sentinel monitor mymaster 127.0.0.1 6379 2 这里是每秒sentinel都会去Ping周围的master redis，超过30秒没有任何响应，说明其挂了。 sentinel down-after-milliseconds &lt;masterName&gt; &lt;timeout&gt; sentinel down-after-milliseconds mymaster 300000 5.1 主观下线 主观下线：每个sentinel节点对Redis节点失败的“偏见” 这是一种主观下线。因为在复杂的网络环境下，这个sentinel与这个master不通，但是master与其他的sentinel都是通的呢？所以是一种“偏见” 这是依靠的第三种定时：每秒去ping一下周围的sentinel和redis。对于slave redis,可以使用这个主观下线，因为他不需要进行故障转移。 5.2 客观下线 客观下线：所有sentinel节点对master Redis节点失败“达成共识”（超过quorum个则统一） 这是依靠的第二种定时：每两秒，sentinel之间进行“商量”，传递的消息是:sentinel is-master-down-by-addr 对于master redis的下线，必须要达成共识才可以，因为涉及故障转移，仅仅依靠一个sentinel判断是不够的。 6. 领导者选举 原因：只有一个sentinel节点完成故障转移 选举：通过sentinel is-master-down-by-addr命令都希望成为领导者 每个做主观下线的sentinel节点向其他sentinel节点发送命令，要求将它设置为领导者 收到命令的sentinel节点如果还没有同意过其他semtinel节点发送的命令，那么将同意该请求，否则拒绝 如果该sentinel节点发现自己的票数已经超过sentinel集合半数并且超过quorum，那么它将成为领导者。 如果此过程中多个sentinel节点成为了领导者，那么将等待一段时间重新进行选举 7. 故障转移 从slave节点中选出一个“合适的”节点作为新的master节点 对上述的slave节点执行“slaveof no one”命令使其成为master节点 向剩余的slave节点发送命令，让它们成为新master节点的slave节点，复制规则和parallel-syncs参数一样 更新对原来的master节点配置为slave，并保持着对其“关注”，当恢复后命令他去复制新的master节点 那么，如何选择“合适”的slave节点呢？ 选择slave-priority(slave节点优先级)最高的slave节点，如果存在则返回，不存在则继续。 选择复制偏移量最大的slave节点(复制得最完整)，如果存在则返回，不存在则继续 选择run_id最小的slave节点(最早的节点) 8. 节点下线 主节点下线：sentinel failover &lt;masterName&gt; 从节点下线要注意读写分离问题。 9. 总结与思考 redis sentinel是redis高可用实现方案：故障发现、故障自动转移、配置中心、客户端通知。 redis sentinel从redis2.8版本才正式生产可用，之前版本不可生产用。 尽可能在不同物理机上部署redis sentinel所有节点。 redis sentinel中的sentinel节点个数应该大于等于3且最好是奇数。 redis sentinel中的数据节点和普通数据节点没有区别。每个sentinel节点在本质上还是一个redis实例，只不过和redis数据节点不同的是，其主要作用是监控redis数据节点 客户端初始化时连接的是sentinel节点集合，不再是具体的redis节点，但sentinel只是配置中心不是代理。 redis sentinel通过三个定时任务实现了sentinel节点对于主节点、从节点、其余sentinel节点的监控。 redis sentinel在对节点做失败判定时分为主观下线和客观下线。 看懂redis sentinel故障转移日志对于redis sentinel以及问题排查非常有用。 redis sentinel实现读写分离高可用可以依赖sentinel节点的消息通知，获取redis数据节点的状态变化。 redis sentinel可以实现高可用的读写分离，高可用体现在故障转移，那么实现高可用的基础就是要有从节点，主从节点还实现了读写分离，减少master的压力。但是如果是从节点下线了，sentinel是不会对其进行故障转移的，并且连接从节点的客户端也无法获取到新的可用从节点，而这些问题在Cluster中都得到了有效的解决。 对于性能提高、容量扩展的时候，这种方式是比较复杂的，比较推荐的是使用集群，就是下面讨论的redis cluster!]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数值计算精度丢失问题]]></title>
    <url>%2F2019%2F01%2F31%2Fmiscellany%2F14%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%B2%BE%E5%BA%A6%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[无论在什么业务中，钱是非常重要的东西，对账的时候一定要对的上，不能这边少一分那边多一分。对于数值的计算，尤其是小数，double和double都是禁止使用的。 阿里强制要求存放小数时使用 decimal，禁止使用 float 和 double。 说明：float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。 处理方式可以为：mysql 可以用 decimal ，如果你是用 java， 在商业计算中我们要用 java.math.BigDecimal，注意：如果需要精确计算，非要用String来够造BigDecimal不可！ 那么到底是什么情况？ 一个例子说明 废话不多说，上图： 问题原因 无论是我们本文提到的double，还是float，都是浮点数。 在计算机科学中，浮点（英语：floating point，缩写为FP）是一种对于实数的近似值数值表现法，由一个有效数字（即尾数）加上幂数来表示，通常是乘以某个基数的整数次指数得到。以这种表示法表示的数值，称为浮点数（floating-point number）。 其实我觉得很好理解，我们之前说过，计算机计算加减乘除啊，都是用的加法器，实质都是二进制的加法处理。那么这里就有一个二进制表示的问题。试想，4，2，8之流都是2的幂次方，可以完美用二进制表示，计算当然不会出现问题。对于0，1，3，5之类也都可以用二进制来表示出来，所以，正数肯定是没问题的。 但是对于小数呢？1、0.5、0.25那都是可以转换成二进制的小数，如十进制的0.1，就无法用二进制准确的表示出来。因此只能使用近似值的方式表达。 如果我们尝试着把10进制的0.1转化成二进制，会怎么转呢？ 在十进制中，0.1如何计算出来的呢？ 0.1 = 1 ÷ 10 那么二进制中也是同理： 1 ÷ 1010 我们回到小学的课堂，来列竖式吧： 123456789101112131415 0.000110011... ------------------1010 ) 1 0000 1010 ------ 1100 1010 ---- 10000 1010 ----- 1100 1010 ---- 10 很显然，除不尽，除出了一个无限循环小数：二进制的 0.0001100110011… 那么，如何在计算机中表示这个无限不循环的小数呢？只能考虑按照不同的精度保理不同的位数。 我们知道float是单精度的，double是双精度的。不同的精度，其实就是保留的有效数字位数不同，保留的位数越多，精度越高。 所以，浮点数在Java中是无法精确表示的，因为大部分浮点数转换成二进制是一个无限不循环的小数，只能通过保留精度的方式进行近似表示。 问题的解决 String 构造方法是完全可预知的：写入 newBigDecimal(&quot;0.1&quot;) 将创建一个 BigDecimal，它正好等于预期的 0.1。因此，比较而言，通常建议优先使用String构造方法。 使用BigDecimal(String val)！ 123456789101112131415161718192021222324252627//加法public static BigDecimal add(double v1, double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.add(b2);&#125;//减法public static BigDecimal sub(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.subtract(b2);&#125;//乘法public static BigDecimal mul(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.multiply(b2);&#125;//除法public static BigDecimal div(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.divide(b2,2,BigDecimal.ROUND_HALF_UP);//四舍五入,保留2位小数,应对除不尽的情况&#125; 那么，上面的精度丢失问题就迎刃而解了。但是除不尽怎么办？比如10.0除以这里的3.0，保留小数点后三位有效数字： 那么，每个用户得到的都是3.333元，三个用户加起来是得不到10块钱的。 对于除法，始终会产生除不尽的情况怎么办？有个词叫轧差 什么意思呢？举个简单例子。假如现在需要把10元分成3分，如果是10除以3这么除，会发现为3.33333无穷尽的3。这些数字完全无法在程序或数据库中进行精确的存储。 简单理解就是，当除不尽或需去除小数点的时候，前面的n-1笔（这里n=3）做四舍五入。最后一笔做兜底（总金额减去前面n-1笔之和）。这样保证总金额的不会丢失。 比如10块钱，三个用户分，前面两个用户只能各分到3。333块钱，最后一个用户分到3.334块钱。保证总额不变。 至于原理，有一点点数学化，以后再作探讨吧。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从复制]]></title>
    <url>%2F2019%2F01%2F31%2Fredis%2FRedis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第六篇文章。介绍redis主从复制功能实现原理。作为高可用的基础，了解一下其中的门道是有必要的。 1.单机有什么问题 机器故障 容量瓶颈 QPS瓶颈 2. 主从复制的作用 数据副本 扩展读性能，slave专门用来读 一个master可以有多个slave，一个salve只能有一个master 3. 两种实现方式 方式一：slaveof命令 slaveof masterIp masterPort slaveof no one(不会清除原来同步的数据，而是新的数据不会再同步给他) 方式二：配置 修改某一行的配置：slaveof ip port 从节点只做读操作：slave-read-only yes 对比 命令的优点：不需要重启 命令的缺点：不便于管理 配置的优点：统一配置 配置的缺点：需要重启 一个场景，假如6380是6379的一个从节点，然后将6380执行salveof no one，然后插入一些新的数据；再重新变成6379的从节点，那么里面的新数据会被清除掉。 查看run_id redis-cli -p 6379 info server | grep run 4. 全量复制 全量复制开销 bgsave时间 rdb网络传输时间 从节点清空数据的时间 从节点加载RDB的时间 可能的AOF重写时间 存在的问题 时间开销比较大 如果master和slave之间网络扰动甚至断开，那么master此间更新的数据对于slave是不知道的，最简单的方法就是再进行一次全量复制，但是显然，消耗太大了。 5. 部分复制 6. 开发与运维的问题 读写分离 master只做写操作，slave来做读操作，来分摊流量。但是会有一些问题： 复制数据延迟 读到过期数据 从节点故障 主从配置不一致 例如maxmemory不一致：丢失数据 数据结构优化参数：内存不一致 规避全量复制 第一次全量复制：不可避免—小主节点(maxmemroy不要太大)或者在低峰时进行操作 节点run_id不匹配（主节点重启，那么master的run_id会发生变化，slave发现其run_id变化，会进行全量复制）；我们可以用故障转移，例如哨兵或集群来避免全量复制。 复制积压缓冲区不足(网络中断，部分复制无法满足)，可以增大复制缓冲区配置size，网络增强 规避复制风暴 概念：主节点宕机造成大量的全量复制 单主节点复制风暴：主节点重启，多从节点复制；解决：更换复制拓扑 单机器复制风暴：机器宕机后（该机器全是Mater），大量全量复制。解决：master分散多机器。 说到底，还是需要有一种高可用的实现方式，在master出现故障之后，如何自动实现从slave晋升为master继续使用.而不是一直死守着原来老的master不放，因为老的master啥时候恢复不知道，恢复了可能会造成复制风暴，既然从节点本来是一直与master节点保持尽量的同步的，那么为什么不将数据最新的从节点升级为主节点呢？下一章继续来分析。]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2F2019%2F01%2F31%2Fredis%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第五篇文章。redis处理数据都是在内存中进行，所以速度特别快，同样，它也可以支持持久化，这里注意，并不是说redis要来充当mysql那种角色，其实更多的是为了在崩溃的时候快速恢复以及主从复制这样的功能。redis的持久化主要有两种方式，一种是RDB，一种是AOF，对于他们的原理和区别都是比较重要的面试考察点，需要掌握。 1. 什么是持久化 redis所有数据保持在内存中，对数据的更新将异步地保存到磁盘中。 2. 持久化的方式 快照—mysql dump或者redis rdb 写日志—mysql binlog或者hbase glog或者redis aof 3. RDB 什么是RDB 触发机制三种主要方式 save(同步持久化，会造成redis主线程的阻塞，不推荐使用) save是同步的，当保存的数据量很大时，可能造成redis的阻塞，即客户端访问redis被阻塞。 他的文件策略是：如果存在老的RDB文件，则新的替换老的。复杂度为O(n)。 bgsave(异步，fork一个子进程来进行持久化，不会造成主线程的阻塞) 一般情况下，fork是比较快的，但是也可以会慢，这时会阻塞redis。只要fork不慢，客户端不会被阻塞。 他的文件策略和复杂度与save是一样的。 save和bgsave两者对比： 自动 redis的自动保存的默认配置是： 配置 seconds changes save 900 1 save 300 10 save 60 10000 就是说，在60秒内改变了10000条数据，就自动保存；在300秒内有10条改变才自动保存；900秒内有1一条改变就保存。 RDB总结 RDB是Redis内存到硬盘的快照，用于持久化。 save通常会阻塞redis。 bgsave不会阻塞redis，但是会fork新进程。 save自动配置满足任一就会被执行。 有些触发机制不容忽视。 4. AOF RDB问题 全量数据存入磁盘 O(n)数据的备份，很耗时间；对于bgsave来说，fork()是一个很消耗内存的操作；将数据全写到硬盘，必然对硬盘IO占用很大。 宕机丢失数据多 还有一点是：某个时间点宕机，那么在某个时间段的数据就丢失了。 AOF原理 将对redis的操作追加到aof文件中。当redis宕机之后，使用aof恢复所有的操作继而实现数据的恢复。 AOF三种策略 always everysec redis出现故障，有可能丢失一秒的数据。redis默认方式。 no 三种策略的比较 AOF重写 好处是：减少硬盘占用、减少数据丢失 下面是AOF的bgrewirteaof的过程： 注意：这里的重写并不是上面演示的，将原来的aof文件进行重写，而是根据redis现在的内存数据进行一次回溯。 aof重写流程 也就是说，子进程在执行 AOF 重写时，主进程需要执行以下三个工作： 1.处理命令请求； 2.将写命令追加到现有的 AOF 文件中； 3.将写命令追加到 AOF 重写缓存中。 如此可以保证： 现有的AOF功能继续执行，即使 AOF 重写期间发生停机，也不会有任何数据丢失； 所有对数据库进行修改的命令都会被记录到 AOF 重写缓存中。 当子进程完成对 AOF 文件重写之后，它会向父进程发送一个完成信号，父进程接到该完成信号之后，会调用一个信号处理函数，该函数完成以下工作：(阻塞) 将 AOF 重写缓存中的内容全部写入到新的 AOF 文件中；(现有 AOF 文件、新的 AOF 文件和数据库三者的状态就完全一致了) 对新的 AOF 文件进行改名，覆盖原有的 AOF 文件。(执行完毕后，程序就完成了新旧两个 AOF 文件的替换) 当这个信号处理函数执行完毕之后，主进程就可以继续像往常一样接收命令请求了。在整个 AOF 后台重写过程中，只有最后的“主进程写入命令到AOF缓存”和“对新的 AOF 文件进行改名，覆盖原有的 AOF 文件”这两个步骤会造成主进程阻塞，在其他时候， AOF 后台重写都不会对主进程造成阻塞，这将 AOF 重写对性能造成的影响降到最低。 小结： AOF 重写的目的是轻量地保存数据库状态，整个重写过程基本上不影响 Redis 主进程处理命令请求； AOF在redis宕机的时候最多丢失一秒的数据，比RDB要好一点，并且可读性高，基本上能看得懂 AOF 重写其实是一个有歧义的名字，实际上重写工作是针对数据库的当前值来进行的，重写过程中不会读写、也不适用原来的 AOF 文件； AOF 可以由用户手动触发，也可以由服务器自动触发。 5. 持久化的取舍和选择 RDB和AOF对比 可以看出，世界上没有完美的东西，只有合适的东西。AOF同样存在一些问题：AOF文件的体积通常要大于RDB文件的体积、且恢复速度慢。 RDB最佳策略 “关”：建议关闭，但是后面主从复制功能是需要他的，因为需要主节点执行dbsave，然后将rdb文件传给从节点。所以说，关不是永久关。 “集中管理”：虽然RDB很重，但是对于数据备份是很重要的，按照小时或者天集中地进行备份比较好，因为他的文件很小，利于传输。 “主从，从开”：有时候从节点打开这个功能是比较好的，但是备份太频繁，取决于实际的场景。 AOF最佳策略 “开”：建议打开，如果仅仅是作为一个普通缓存，对于数据要求不是很高，这次数据丢了，下次可以从数据库取(数据库压力不是很大)，这种情况就建议关闭，因为AOF还是有性能开销的。 “everysec” Redis4 Redis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。 RDB和AOF共存的情况下如何恢复数据： 优点： 混合持久化结合了RDB持久化 和 AOF 持久化的优点, 由于绝大部分都是RDB格式，加载速度快，同时结合AOF，增量的数据以AOF方式保存了，数据更少的丢失。 缺点： 兼容性差，一旦开启了混合持久化，在4.0之前版本都不识别该aof文件，同时由于前部分是RDB格式，阅读性较差 策略是： 6. 总结 http://www.ywnds.com/?p=4876]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入Nginx原理]]></title>
    <url>%2F2019%2F01%2F30%2Fmiscellany%2F13%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Nginx%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Nginx是一个高性能的HTTP和反向代理服务器，及电子邮件（IMAP/POP3）代理服务器，同时也是一个非常高效的反向代理、负载平衡中间件。是非常常用的web server.我们需要理解它的原理，才能达到游刃有余的程度。 本篇文章需要对Nginx有基本的使用以及对IO复用模型有一定的了解。文章比较长。 1.正向代理和反向代理 正向代理的工作原理就像一个跳板，比如：我访问不了google.com，但是我能访问一个代理服务器A，A能访问google.com，于是我先连上代理服务器A，告诉他我需要google.com的内容，A就去取回来，然后返回给我。从网站的角度，只在代理服务器来取内容的时候有一次记录，有时候并不知道是用户的请求，也隐藏了用户的资料，这取决于代理告不告诉网站。 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 简单来说： 正向代理是不知道客户端是谁，代理是一个跳板，所有客户端通过这个跳板来访问到对应的内容。 反向代理是不知道服务端是谁，用户的请求被转发到内部的某台服务器去处理。 2.基本的工作流程 用户通过域名发出访问Web服务器的请求，该域名被DNS服务器解析为反向代理服务器的IP地址； 反向代理服务器接受用户的请求； 反向代理服务器在本地缓存中查找请求的内容，找到后直接把内容发送给用户； 如果本地缓存里没有用户所请求的信息内容，反向代理服务器会代替用户向源服务器请求同样的信息内容，并把信息内容发给用户，如果信息内容是缓存的还会把它保存到缓存中。 3.优点 保护了真实的web服务器，保证了web服务器的资源安全 节约了有限的IP地址资源 减少WEB服务器压力，提高响应速度(缓存功能) 请求的统一控制，包括设置权限、过滤规则等 实现负载均衡 区分动态和静态可缓存内容 … 4.使用场景 Nginx作为Http代理、反向代理 Nginx作为负载均衡器 Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 Nginx作为Web缓存 5.Nginx的Master-Worker模式 启动Nginx后，其实就是在80端口启动了Socket服务进行监听，如图所示，Nginx涉及Master进程和Worker进程。 6.Master进程的作用是？ 读取并验证配置文件nginx.conf；管理worker进程； 接收来自外界的信号 向各worker进程发送信号 监控worker进程的运行状态，当worker进程退出后(异常情况下)，会自动重新启动新的worker进程 7.Worker进程的作用是？ 每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 思考：Nginx如何做到热部署？ 所谓热部署，就是配置文件nginx.conf修改后，不需要stop Nginx，不需要中断请求，就能让配置文件生效！（nginx -s reload 重新加载/nginx -t检查配置/nginx -s stop） 通过上文我们已经知道worker进程负责处理具体的请求，那么如果想达到热部署的效果，可以想象： 方案一： 修改配置文件nginx.conf后，主进程master负责推送给woker进程更新配置信息，woker进程收到信息后，更新进程内部的线程信息。（有点volatile的味道） 方案二： 修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx采用的就是方案二来达到热部署的！ 思考：Nginx如何做到高并发下的高效处理？ 上文已经提及Nginx的worker进程个数与CPU绑定、worker进程内部包含一个线程高效回环处理请求，这的确有助于效率，但这是不够的。 作为专业的程序员，我们可以开一下脑洞：BIO/NIO/AIO、异步/同步、阻塞/非阻塞… 要同时处理那么多的请求，要知道，有的请求需要发生IO，可能需要很长时间，如果等着它，就会拖慢worker的处理速度。 Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。 思考：Nginx挂了怎么办？ Nginx既然作为入口网关，很重要，如果出现单点问题，显然是不可接受的。 答案是：Keepalived+Nginx实现高可用。 Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用。（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合） Keepalived+Nginx实现高可用的思路： 第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP） 第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,从而实现Nginx故障切换） 6.nginx.conf 第一：location可以进行正则匹配，应该注意正则的几种形式以及优先级。（这里不展开） 第二：Nginx能够提高速度的其中一个特性就是：动静分离，就是把静态资源放到Nginx上，由Nginx管理，动态请求转发给后端。 第三：我们可以在Nginx下把静态资源、日志文件归属到不同域名下（也即是目录），这样方便管理维护。 第四：Nginx可以进行IP访问控制，有些电商平台，就可以在Nginx这一层，做一下处理，内置一个黑名单模块，那么就不必等请求通过Nginx达到后端在进行拦截，而是直接在Nginx这一层就处理掉。 除了可以映射静态资源，上面已经说了，可以作为一个代理服务器来使用。 所谓反向代理，很简单，其实就是在location这一段配置中的root替换成proxy_pass即可。root说明是静态资源，可以由Nginx进行返回；而proxy_pass说明是动态请求，需要进行转发，比如代理到Tomcat上。 反向代理，上面已经说了，过程是透明的，比如说request -&gt; Nginx -&gt; Tomcat，那么对于Tomcat而言，请求的IP地址就是Nginx的地址，而非真实的request地址，这一点需要注意。不过好在Nginx不仅仅可以反向代理请求，还可以由用户自定义设置HTTP HEADER。 负载均衡【upstream】 上面的反向代理中，我们通过proxy_pass来指定Tomcat的地址，很显然我们只能指定一台Tomcat地址，那么我们如果想指定多台来达到负载均衡呢？ 第一，通过upstream来定义一组Tomcat，并指定负载策略（IPHASH、加权论调、最少连接），健康检查策略（Nginx可以监控这一组Tomcat的状态）等。 第二，将proxy_pass替换成upstream指定的值即可。 负载均衡可能带来的问题？ 负载均衡所带来的明显的问题是，一个请求，可以到A server，也可以到B server，这完全不受我们的控制，当然这也不是什么问题，只是我们得注意的是：用户状态的保存问题，如Session会话信息，不能在保存到服务器上。 7.惊群现象 定义：惊群效应就是当一个fd的事件被触发时，所有等待这个fd的线程或进程都被唤醒。 Nginx的IO通常使用epoll，epoll函数使用了I/O复用模型。与I/O阻塞模型比较，I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。Nginx的epoll工作流程如下： master进程先建好需要listen的socket后，然后再fork出多个woker进程，这样每个work进程都可以去accept这个socket 当一个client连接到来时，所有accept的work进程都会受到通知，但只有一个进程可以accept成功，其它的则会accept失败，Nginx提供了一把共享锁accept_mutex来保证同一时刻只有一个work进程在accept连接，从而解决惊群问题 当一个worker进程accept这个连接后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完成的请求就结束了 8.Nginx架构及工作流程 Nginx真正处理请求业务的是Worker之下的线程。worker进程中有一个ngx_worker_process_cycle()函数，执行无限循环，不断处理收到的来自客户端的请求，并进行处理，直到整个Nginx服务被停止。 当一个 worker 进程在 accept() 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，一个完整的请求。一个请求，完全由 worker 进程来处理，而且只能在一个 worker 进程中处理。 这样做带来的好处： 节省锁带来的开销。每个 worker 进程都是独立的进程，不共享资源，不需要加锁。同时在编程以及问题查上时，也会方便很多。 独立进程，减少风险。采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快重新启动新的 worker 进程。当然，worker 进程的也能发生意外退出。 9.nginx为什么高性能 因为nginx是多进程单线程的代表，多进程模型每个进程/线程只能处理一路IO，那么 Nginx是如何处理多路IO呢？ 如果不使用 IO 多路复用，那么在一个进程中，同时只能处理一个请求，比如执行 accept()，如果没有连接过来，那么程序会阻塞在这里，直到有一个连接过来，才能继续向下执行。 而多路复用，允许我们只在事件发生时才将控制返回给程序，而其他时候内核都挂起进程，随时待命。 核心：Nginx采用的 IO多路复用模型epoll epoll通过在Linux内核中申请一个简易的文件系统（文件系统一般用什么数据结构实现？B+树），其工作流程分为三部分： 调用 int epoll_create(int size)建立一个epoll对象，内核会创建一个eventpoll结构体，用于存放通过epoll_ctl()向epoll对象中添加进来的事件，这些事件都会挂载在红黑树中。 调用 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) 在 epoll 对象中为 fd 注册事件，所有添加到epoll中的事件都会与设备驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个sockfd的回调方法，将sockfd添加到eventpoll 中的双链表 调用 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) 来等待事件的发生，timeout 为 -1 时，该调用会阻塞直到有事件发生 这样，注册好事件之后，只要有 fd 上事件发生，epoll_wait() 就能检测到并返回给用户，用户就能”非阻塞“地进行 I/O 了。 epoll() 中内核则维护一个链表，epoll_wait 直接检查链表是不是空就知道是否有文件描述符准备好了。（epoll 与 select 相比最大的优点是不会随着 sockfd 数目增长而降低效率，使用 select() 时，内核采用轮训的方法来查看是否有fd 准备好，其中的保存 sockfd 的是类似数组的数据结构 fd_set，key 为 fd，value 为 0 或者 1。） 能达到这种效果，是因为在内核实现中 epoll 是根据每个 sockfd 上面的与设备驱动程序建立起来的回调函数实现的。那么，某个 sockfd 上的事件发生时，与它对应的回调函数就会被调用，来把这个 sockfd 加入链表，其他处于“空闲的”状态的则不会。在这点上，epoll 实现了一个”伪”AIO。但是如果绝大部分的 I/O 都是“活跃的”，每个 socket 使用率很高的话，epoll效率不一定比 select 高（可能是要维护队列复杂）。 可以看出，因为一个进程里只有一个线程，所以一个进程同时只能做一件事，但是可以通过不断地切换来“同时”处理多个请求。 例子：Nginx 会注册一个事件：“如果来自一个新客户端的连接请求到来了，再通知我”，此后只有连接请求到来，服务器才会执行 accept() 来接收请求。又比如向上游服务器（比如 PHP-FPM）转发请求，并等待请求返回时，这个处理的 worker 不会在这阻塞，它会在发送完请求后，注册一个事件：“如果缓冲区接收到数据了，告诉我一声，我再将它读进来”，于是进程就空闲下来等待事件发生。 这样，基于 多进程+epoll， Nginx 便能实现高并发。 10.几种负载均衡的算法介绍 轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 weight 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 ip_hash 每个请求按访问ip的hash结果分配，这样每个访客固定访问同一个后端服务器，可以解决session的问题。但是不能解决宕机问题。 前三种是nginx自带的，直接在配置文件中配置即可使用。 fair（第三方） 按后端服务器的相应时间来分配请求，相应时间短的优先分配。 url_hash（第三方） 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 11.基于不同层次的负载均衡 七层就是基于URL等应用层信息的负载均衡； 同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。 换句话说: 二层负载均衡会通过一个虚拟MAC地址接受请求，然后再分配到真是的MAC地址； 三层负载均衡会通过一个虚拟IP地址接收请求，然后再分配到真实的IP地址； 四层通过虚拟的URL或主机名接收请求，然后再分配到真是的服务器。 所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。 七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。 负载均衡器通常称为四层交换机或七层交换机。四层交换机主要分析IP层及TCP/UDP层，实现四层流量负载均衡。七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息。 负载均衡设备也常被称为&quot;四到七层交换机&quot;，那么四层和七层两者到底区别在哪里？ 第一，技术原理上的区别。 所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 第二，应用场景的需求。 七层应用负载的好处，是使得整个网络更&quot;智能化&quot;。例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。 另外一个常常被提到功能就是安全性。 12.总结 理解正向代理和反向代理的概念 nginx的优点和使用场景 master和work两种进程的作用 如何热部署 Nginx单点故障的预防 映射静态文件、反向代理跳转到后端服务器处理的写法 惊群现象 Nginx 采用的是多进程（单线程） &amp; 多路IO复用模型(底层依靠epoll实现) 几种负载均衡的算法 四层的负载均衡和七层的负载均衡]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis为什么快]]></title>
    <url>%2F2019%2F01%2F30%2Fredis%2FRedis%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第四篇文章，本文主要攻克面试题-Redis为什么这么快。这就涉及Redis的线程模型啦。 完全基于内存 Redis是纯内存数据库，相对于读写磁盘，读写内存的速度就不是几倍几十倍了，一般，hash查找可以达到每秒百万次的数量级。 多路复用IO “多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗）。 Redis为什么是单线程的？ 因为CPU不是Redis的瓶颈。Redis的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 为什么 Redis 中要使用 I/O 多路复用这种技术呢？ 首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的。 假设你是一个老师，让30个学生解答一道题目，然后检查学生做的是否正确，你有下面几个选择： 第一种选择：按顺序逐个检查，先检查A，然后是B，之后是C、D。。。这中间如果有一个学生卡主，全班都会被耽误。这种模式就好比，你用循环挨个处理socket，根本不具有并发能力。 第二种选择：你创建30个分身，每个分身检查一个学生的答案是否正确。 这种类似于为每一个用户创建一个进程或者线程处理连接。 第三种选择，你站在讲台上等，谁解答完谁举手。这时C、D举手，表示他们解答问题完毕，你下去依次检查C、D的答案，然后继续回到讲台上等。此时E、A又举手，然后去处理E和A。。。 第三种就是IO复用模型，Linux下的select、poll和epoll就是干这个的。将用户socket对应的fd注册进epoll，然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用非阻塞模式。这样，整个过程只在调用select、poll、epoll这些调用的时候才会阻塞，收发客户消息是不会阻塞的，整个进程或者线程就被充分利用起来，这就是事件驱动，所谓的reactor模式。 所以，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。 这里还涉及一个名词：fd文件描述符。 Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行I/O操作的系统调用都会通过文件描述符。 redis的线程模型？ Redis 服务采用 Reactor 的方式来实现文件事件处理器。 文件事件处理器使用 I/O 多路复用模块同时监听多个 FD，当 accept、read、write 和 close 文件事件产生时，文件事件处理器就会回调 FD 绑定的事件处理器。 虽然整个文件事件处理器是在单线程上运行的，但是通过 I/O 多路复用模块的引入，实现了同时对多个 FD 读写的监控，提高了网络通信模型的性能，同时也可以保证整个 Redis 服务实现的简单。 上面简单理解就是：多个网络连接并发读写redis的时候，先将对应的fd注册到epoll上，I/O多路复用模块会监听这些网络请求的情况，一旦有一个网络连接产生了accept、read、write 和 close 文件事件，I/O多路复用模块就会向文件事件分派器传送那些产生了事件的网络连接。 当然了，上面的文件事件可能会并发产生，这时的策略是，将所有产生事件的套接字（对应上面的网络连接）都入队到一个队列里面， 然后通过这个队列， 以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字： 当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字。再看看下图，与上图使一样的： 文件事件分派器接收 I/O 多路复用程序传来的套接字， 并根据套接字产生的事件的类型， 调用相应的事件处理器。 服务器会为执行不同任务的套接字关联不同的事件处理器， 这些处理器是一个个函数， 它们定义了某个事件发生时， 服务器应该执行的动作。 整个模块使 Redis 能以单进程运行的同时服务成千上万个文件描述符，避免了由于多进程应用的引入导致代码实现复杂度的提升，减少了出错的可能性，单线程还减少线程切换和调度，实现更加简单 最后总结一下，为什么redis比较快大概思路通俗的说就是：Redis是纯内存数据库，读取快，瓶颈在于IO上，如果使用阻塞式IO，因为是单线程的缘故，就会停止等待。所以采用IO多路复用监听文件描述符的状态，将对redis的开关读写换成事件，加入队列进行相应的事件处理，吞吐量比较大。 IO复用模型的选择 因为 Redis 需要在多个平台上运行，同时为了最大化执行的效率与性能，所以会根据编译平台的不同选择不同的 I/O 多路复用函数作为子模块，提供给上层统一的接口； 因为 select 函数是作为 POSIX 标准中的系统调用，在不同版本的操作系统上都会实现，所以将其作为保底方案： Redis 会优先选择时间复杂度为 O(1) 的 I/O 多路复用函数作为底层实现，包括 Solaries 10 中的 evport、Linux 中的 epoll 和 macOS/FreeBSD 中的 kqueue，上述的这些函数都使用了内核内部的结构，并且能够服务几十万的文件描述符。 但是如果当前编译环境没有上述函数，就会选择 select 作为备选方案，由于其在使用时会扫描全部监听的描述符，所以其时间复杂度较差 O(n)，并且只能同时服务 1024 个文件描述符，所以一般并不会以 select 作为第一方案使用。 reids在linux下的安装 Redis对于Linux是官方支持的，安装起来也非常地简单，直接编译源码然后进行安装即可。 这里以centos为例，大概说一下步骤： 下载redis编译工具:yum install gcc和yum install g++ 解压redis.tar.gz文件，进去之后进行编译:make 然后安装：make install PREFIX=/usr/local/redis 安装成功之后进入/usr/local/redis/bin下启动redis ./redis-server]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis其他的功能介绍]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2FRedis%E5%85%B6%E4%BB%96%E7%9A%84%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第三篇文章，本文主要介绍redis一些其他的功能。遇到某些场景的时候可以想到redis是不是可以实现。 一、慢查询日志 1.1 什么是慢查询日志 慢查询日志帮助开发和运维人员定位系统存在的慢操作。慢查询日志就是系统在命令执行前后计算每条命令的执行时间，当超过预设阀值，就将这条命令的相关信息（慢查询ID，发生时间戳，耗时，命令的详细信息）记录下来。 1.2 redis一条命令简单的生命周期 慢查询只会出现在【3.执行命令】这个阶段，即慢查询只记录命令执行时间，并不包括命令排队时间和网络传输时间。 1.3 慢查询配置参数 慢查询的预设阀值 slowlog-log-slower-than slowlog-log-slower-than参数就是预设阀值，单位是微秒,默认值是10000，如果一条命令的执行时间超过10000微妙(10毫秒)，那么它将被记录在慢查询日志中。 如果slowlog-log-slower-than的值是0，则会记录所有命令。 如果slowlog-log-slower-than的值小于0，则任何命令都不会记录日志。 redis的操作一般是微妙级，slowlog-log-slower-than不要设置太大，一般设置为1毫秒。支持动态设置。 慢查询日志的长度slowlog-max-len slowlog-max-len只是说明了慢查询日志最多存储多少条。 Redis使用一个列表来存储慢查询日志，showlog-max-len就是列表的最大长度。 当慢查询日志已经到达列表的最大长度时，又有慢查询日志要进入列表，则最早插入列表的日志将会被移出列表，新日志被插入列表的末尾。 默认是128，但是slowlog-max-len不要设置太小，可以设置为1000以上. 慢查询日志是一个先进先出队列，慢查询较多的情况下，可能会丢失部分慢查询命令，可以定期执行slow get命令将慢查询日志持久化到其他存储中。然后制作可视化界面查询。 二、pipeline 2.1 为什么会出现Pipeline 用普通的get和set，如果同时需要执行大量的命令，那就是等待上一条命令应答后再执行，这中间不仅仅多了RTT（Round Time Trip），而且还频繁的调用系统IO，发送网络请求。 对于多条命令不是有mget和mset吗？确实对于一批的get和set可以用mget和mset，但是它的问题在于如果我们需要同时传输get和hget呢？此时pipeline(流水线)就出现了。 所以流水线解决的问题是N条命令网络通信的减少。 为什么说网络耗费时间大呢？这里给出一个极端的例子。 pipeline与原生M操作的对比。 原生M操作是一个原子操作。 pipeline非原子命令。 当某个命令的执行需要依赖前一个命令的返回结果时，无法使用pipeline。 12mset a “a1” b “b” c “c1” mget a b c mget和mset命令也是为了减少网络连接和传输时间所设置的，其本质和pipeline的应用区别不大，但是在特定场景下只能用pipeline实现，例如： 12get aset b ‘1’ pipeline适合执行这种连续，且无相关性的命令。 2.2 一个demo 搭建一个quickstart的maven工程。过程略。 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 直接再单元测试中进行测试：普通的直接hset 10000条数据： 1234567891011@Testpublic void test1()&#123; Jedis jedis = new Jedis("127.0.0.1",6379); long before = System.currentTimeMillis(); for(int i=0;i&lt;10000;i++)&#123; jedis.hset("hashkey"+i,"filed"+i,"value"+i); &#125; long after = System.currentTimeMillis(); System.out.println("一共耗时: "+(after-before)+"ms");&#125; 运行结果： 一共耗时: 1526ms 但是用pipeline后： 123456789101112131415@Testpublic void test2()&#123; Jedis jedis = new Jedis("127.0.0.1",6379); long before = System.currentTimeMillis(); //分为10次批量发送 for(int i=0;i&lt;10;i++)&#123; Pipeline pipeline = jedis.pipelined(); for(int j=1000*i;j&lt;(i+1)*1000;j++)&#123; pipeline.hset("hashkey:"+j,"field:"+j,"value:"+j); &#125; pipeline.syncAndReturnAll(); &#125; long after = System.currentTimeMillis(); System.out.println("使用pipeline一共耗时: "+(after-before)+"ms");&#125; 运行结果：使用pipeline一共耗时: 139ms 可以预见，对于更多的传输次数，pipeline的优势将越来越明显。但是pipeline每次只能作用在一个redis节点上。 三、发布订阅 3.1 角色 发布者----频道----订阅者 3.2 模型 注意，新订阅的，是不能收到之前的消息的。 订阅者1：subscribe mytopic 订阅者2：subscribe mytopic 订阅者3：subscribe mytopic 发布者：publish mytopic “hello” 缺点是不能保证消息可达，所以还是用专业的消息队列传达比较保障。 与发布订阅模型很类似的是消息队列模型。 只有一个是可以收到消息的。 四、bitMap 4.1 位图是什么 就是通过一个bit位来表示某个元素对应的值或者状态,其中的key就是对应元素本身。我们知道8个bit可以组成一个Byte，所以bitmap本身会极大的节省储存空间。 Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32长度的位数组。 比如我们执行 set hello big 那么这个big其实是这个形态： 执行getbit hello 0 得到0； 执行getbit hello 1 得到1 setbit hello 7 1，那么再get hello 将得到cig 4.2 位图有什么用呢？ 位图除了getbit和setbit之外，还有bitcount key [start end]，就是获取执行范围内的1的个数。 bitop作用是做多个Bitmap的and,or,not,xor操作。 以一个场景为例：日活跃用户 每次用户登录时会执行一次redis.setbit(daily_active_users, user_id, 1) 因为日活跃用户每天都变化，所以需要每天创建一个新的bitmap。我们简单地把日期（年月日）添加到key后面，以后就可以根据年月日这个key找到某天活跃用户。实现了这个功能。 第二个场景：用户签到情况 将那天所代表的网站的上线日作为offset参数， 比如,如果今天是网站上线的第100天,而用户$uid=10001在今天阅览过网站, 那么执行命令SETBIT peter 100 1. 如果明天$uid=10001也继续阅览网站,那么执行命令SETBIT peter 101 1 ,以此类推. 仔细想想，用位图，一天签到一次只要占一个bit，8天才占一个字节。那么一年这个用户签到占的数据是365/8=45.625个字节.如果不用位图实现，保存一条记录将远远大于一个比特吧，那么当用户量很大的时候，差距将会特别大。 五、hyperLogLog 基于HyperLogLog算法：极小空间完成独立数量统计。本质还是字符串。 pfadd key element [element...]:向hyperloglog添加元素 pfcount key [key...]:计算hyperloglog的独立总数 pfmerge destkey sourcekey [sourcekey...]:合并多个hyperloglog api例子 为什么要用hyperLogLog呢 我们上面例子可以看到，他的功能类似于去重，统计出所有不一样元素的个数。 他的优点是：占用内存极小。 缺点也有： 他可能会出错，错误率为0.81%，看你是否能够容忍错误了 不能拿到单条数据 六、geo 存储经纬度、计算两地距离、范围计算等。 提到LBS(Location Based Service)，基于位置的服务。我立即想起Mongodb的GEO实现地理坐标查询等功能，具体介绍为地理位置附近查询的GEOHASH解决方案。 mongodb最大的特点是灵活，因为其数据是以json的格式存储，所以字段随时可以增加或减少；Redis的特点是快，适合单一的，简单的，大量数据的存储；HBase我没有做深入研究，它的特点是大，适合做离线缓存。在处理社交这种关系复杂的数据存储时，依然还是需要用mysql这种关系型数据库，nosql并不能完全替代。 七、总结 首先是慢查询日志，可以定时地持久化，并且用一个可视化页面进行监测。 pipeline解决的是对没有相互依赖的操作的批量执行，减少网络传输和IO时间。但是呢，需要注意一般只能往一个节点放数据，面对集群的时候，就需要采取一些策略了。mset、mget，目前只支持具有相同slot值的key执行批量操作。后文再讲。 可以实现发布订阅模型以及消息队列，但是消息是无状态的，不能保证消息一定送达，所以需要用专业的MQ来实现。 位图，可以实现极小的空间完成对大量用户信息的统计。 地理坐标服务]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构和操作]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2FRedis%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第二篇文章，本文主要介绍redis如何启动，以及基本的键命令和五种基本数据类型的操作。部分图片可能看不清楚，可以拖到新窗口打开。 一、启动方式 我的环境是windows，那么直接进入redis的解压目录中，分别执行redis-server.exe和redis-cli.exe两个可执行的程序。也可以通过cmd启动： 不要直接用crtl+C关闭server，在linux下，直接停掉server的话，会导致数据的丢失。正确的做法是在客户端执行 redis-cli.exe shutdown 还可以指定端口启动：./redis-server.exe --port 6380 那么对应客户端连接也要指定相应 的端口才能连接。关闭服务端也要指定相应的端口才行： -h指定远程redis的ip 通过配置文件启动,可以在下面这个文件中指定端口号： 结合配置文件启动: 还可以设置密码： 那么客户端连接就必须要密码验证了： 二、命令 1、基础命令 info:查看系统信息 select (0-15)，redis一共有16个工作区间，一般默认从0开始，到15. flushdb：清空当前选择的空间 flushall：清空所有 dbsize：当前空间里面key-value键值对的数目 save：人工实现redis的持久化 quit：退出 2、键命令 del key成功返回1，失败返回0. exits key ttl和expire type key 查看key的类型 randomkey: rename oldkey newkey 如果是重命名为已经存在的key呢？ renamenx: 三、redis数据结构 1、String字符串 setex&amp;psetex getrange&amp;getset mset&amp;mget&amp;strlen setnx&amp;msetnx 数值操作 2、hash 3、list 4、set 5、sorted set]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初步认识Redis]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2F%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86Redis%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第一篇文章，主要从感性层面来认识一下这个开发中的宠儿，无论是什么应用，只要有点用户量的，不上redis是不可能的。作为当今最优秀的缓存中间件，没有理由不去深入了解它！ 一、redis是什么 redis很快，官方宣称QPS(每秒查询率)达10万。 Redis是一个开源的使用ANSI C语言编写、支持网络、单进程单线程、可基于内存亦可持久化、一个高性能的key-value数据库。 简而言之，就是一个缓存数据库，基于内存，也可以持久化，速度贼快，几乎所有互联网公司都在使用。 有的初学者可能看到数据库这个字眼，就把他归类于mysql之类，其实不是，mysql是一种关系型数据库，是存在磁盘中的。核心的数据是一定要落地到mysql之类的数据库中的。redis其实使用最多的功能是缓存，既然能存东西，那么必然也有数据库的功能，但是有可能会造成数据的缺失。所以，数据一定是要落入数据库才保险，redis可以作为缓存，缓存热点数据或者只读数据，提高性能并保护数据库。 二、为什么要用redis 好了，我们已经知道它是一个高性能的缓存中间件。那么必然一大功能是作为缓存使用。那为什么要用缓存呢？直接从数据库查不就行了码？ 在实际的业务场景中，用户量一上来，数据库是吃不消的。数据库是性能的一大瓶颈，如果不采取措施，用户的操作将卡在数据库处理这一块，最终可能导致不可用。 那么，此时，加入缓存，比如商城首页有很多很多内容，这些内容不可能经常变化，至少也要两三天吧？所以，可以将这些数据放到redis中，用户进商城之后，数据直接从redis中获取即可。速度极快，提高了用户的体验。 既然是缓存，那么必定会存在数据不一致的情况，所以缓存最适合于读多写少的情况，当然啦，要修改缓存肯定是可以的，但是要注意热点key的问题，比如微博最火的一片新闻，此时有几百万人再看，你却要修改一下，肯定是要注意点什么东西才行的，后续的文章会讲到如何处理热点key修改的问题。 三、Redis与其他key-value存储有什么不同 这里先简单说说，后面会有文章详细比价一下。 多样的数据结构和原子性操作 Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。redis中的单个命令都是原子性的，什么是原子性，就是该命令不可分割。 运行于内存+持久化于磁盘 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。另一个优点是， 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。 同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 注：我们要知道，对于磁盘的顺序访问速度是远大于随机访问的。这种在硬盘上顺序追加效率很高。 四、redis特点 速度快： 基于内存,这是快的最主要原因。 持久化： 可以同步或异步保存到磁盘中 多种数据结构： 除了五种基本数据类型，还支持位图、HyperLogLog，GEO等 支持多种编程语言客户端： java，python，ruby，Lua… 功能丰富： 可以实现发布-订阅，支持事务、Lua脚本 简单： 不依赖与外部库、单线程模型 主从复制： 主服务器同步数据到从服务器，是高可用的基础 高可用、分布式： 高可用：redis-Sentinel(v2.8版本)；分布式：redis-cluster(v3.0版本) 五、redis典型应用场景 缓存系统：这个就不多说了，redis作为高速缓存是其主要存在价值。 计数器：因为是原子操作incr+单线程，作为计数器永远不会出错 消息队列系统：数据结构list可以实现这种生产者-消费者模式的消息队列。 排行榜：有序集合sorted set就可以实现 社交网络：redis与社交网络就是一家，非常方便用set就能实现诸如共同好友这些功能。 六、redis优势 缓存管理：可以在必要时将无效的旧数据从内存中删除，为新数据腾出新的空间 提供更大的灵活性：redis支持多种类型，并且采用key-value 的形式存储，key和value的大小限制都是512Mb,与编码无关，所以数据安全。但是memcached限制key最大为250字节，value为1MB，况且只支持String类型。 redis提供主从复制：实现高可用的cache系统，支持集群中多个服务器之间的数据同步。 数据持久化：redis可以通过两种方式将数据进行持久化，一定程度上规避缓存中的数据不稳定的问题，也可以在重启服务器时最快的恢复缓存中所需的数据，提高了效率的同时减轻了主数据库系统的开销。 与传统的Memcached相比，优势还是很大的，两者的具体对比我会在后续的文章中详细说明。这里注意存在即合理，Memcached也有不可替代的适用场景： 存储一些粒度比较小的静态数据，比如一些html片段，Memcached便是我们更好的选择。相对于redis而言，Memcached的元数据metadata更小些，所以相对来讲对于数据存储管理的性能更高，额外开销更小。 Memcached的特点：Memcached唯一支持的数据类型是String,所以更适合存储只读数据，因为字符串并不会因为额外的处理造成额外的开销。毕竟Memcached每次更新一个对象时，都需要重复执行下面的操作：获取整个字符串-&gt;反序列化为对象-&gt;修改其中的值-&gt;再次序列化该对象-&gt;在缓存中将整个字符串替换为新字符串。这样一来，更新存储数据就会有更高的消耗，可能就不是我们的最佳选择了。 七、总结 只要记住redis三个关键字：快、持久化、高可用和分布式]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地理位置附近查询的GEOHASH解决方案]]></title>
    <url>%2F2019%2F01%2F29%2Fmiscellany%2F12%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%E9%99%84%E8%BF%91%E6%9F%A5%E8%AF%A2%E7%9A%84GEOHASH%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[地理位置附近查询的GEOHASH解决方案 1.需求场景 现今互联网确实从方方面面影响我们的生活。现在我们可以足不出户就能买到我们心仪的衣服，找到附近的美食。当我们点开一个外卖的app就能看到自己附近的餐厅，那我们有没有想过这是怎么实现的呢？ 2.尝试解决 首先我们能想到的就是把所有餐厅的经纬度存下来 然后当用户选择附近餐厅时 我们先获取用户的经纬度，然后到数据库中查出所有的经纬度，依次计算它们和用户间的距离。 最后根据用户输入的距离范围过滤出合适的餐厅，并根据距离做一个升序排列。 这样貌似能查出附近的餐厅，但是餐厅的数量这么多，直接全查出来内存也要爆掉，即使分批处理计算量也十分大。这样用户等待的时间就会特别长。那有什么办法能减少我们的计算量呢？ 其实很简单，我们应该只计算用户关心的那一片数据，而不是计算所有的。例如用户在北京，那完全没必要计算海南，黑龙江，新疆，浙江等其它地区的数据。如果我们能快速定位到北京甚至某个区，那么我们的计算量将大大减少。我们发现这其实就是索引的功能，但是MySQL对这种二维的地理位置的索引支持并不友好（mongodb有直接的地理位置索引），它对一维的像字符串这样的支持很好。那如果我们的数据在MySQL中，有没有什么方法能将我们的二维坐标转换为一种可比较的字符串呢？这就是我们今天要介绍的geohash算法。 3.基本思想 geohash简单来说就是将一个地理坐标转换为一个可比较的字符串的算法。不过生成的字符串表示的是一个矩形的范围，并不是一个点。 比如西二旗地铁附近这一片矩形区域就可以用wx4eyu82这个字符串表示，并且越靠前的编码表示额范围越大，比如中国绝大部分地区可以用w这个字母表示的矩形区域内。像wx4eyu82表示的区域一定在wx4e表示的区域范围内。利用这些特性我们就可以实现附近餐厅的功能了，比如我们希望查看西二旗地铁附近的餐厅就可以这样查询：select * from table where geohash like 'wx4eyu82%'; 这样就可以利用索引，快速查询出相关餐厅的信息了。并且我们还可以用wx4eyu82为key，餐厅信息为value做缓存。 通过上面的介绍我们知道了GeoHash就是一种将经纬度转换成字符串的方法，并且使得在大部分情况下，字符串前缀匹配越多的距离越近. 4.GeoHash算法的步骤 首先我们将经度和纬度都单独转换为一个二进制编码 得到经度和纬度的二进制编码后，我们按照奇数位放纬度，偶数为放经度的规则（我们这里奇数偶数下标是从0开始）将它们合成一个二进制编码 最后我们需要将这个二进制编码转换为base32编码 举例 地球纬度区间是[-90,90]， 北海公园的纬度是39.928167，可以通过下面算法对纬度39.928167进行逼近编码: 区间[-90,90]进行二分为[-90,0),[0,90]，称为左右区间，可以确定39.928167属于右区间[0,90]，给标记为1； 接着将区间[0,90]进行二分为 [0,45),[45,90]，可以确定39.928167属于左区间 [0,45)，给标记为0； 递归上述过程39.928167总是属于某个区间[a,b]。随着每次迭代区间[a,b]总在缩小，并越来越逼近39.928167； 如果给定的纬度x（39.928167）属于左区间，则记录0，如果属于右区间则记录1，这样随着算法的进行会产生一个序列1011100，序列的长度跟给定的区间划分次数有关。 通过上述计算，纬度产生的编码为10111 00011，经度产生的编码为11010 01011。偶数位放经度，奇数位放纬度，把2串编码组合生成新串：11100 11101 00100 01111。 最后使用用0-9、b-z（去掉a, i, l, o）这32个字母进行base32编码，首先将11100 11101 00100 01111转成十进制，对应着28、29、4、15，十进制对应的编码就是wx4g。 5.缺陷-geohash的边界问题 比如红色的点是我们的位置，绿色的两个点分别是附近的两个餐馆，但是在查询的时候会发现距离较远餐馆的GeoHash编码与我们一样（因为在同一个GeoHash区域块上），而较近餐馆的GeoHash编码与我们不一致。 目前比较通行的做法就是我们不仅获取当前我们所在的矩形区域，还获取周围8个矩形块中的点。那么怎样定位周围8个点呢？关键就是需要获取周围8个点的经纬度，那我们已经知道自己的经纬度，只需要用自己的经纬度减去最小划分单位的经纬度就行。因为我们知道经纬度的范围,又知道需要划分的次数，所以很容易就能计算出最小划分单位的经纬度。 6.几种实现geohash方案的对比 6.1支持二维索引的存储数据库：mongodb mongoDB支持二维空间索引,使用空间索引,mongoDB支持一种特殊查询,如某地图网站上可以查找离你最近的咖啡厅,银行等信息。这个使用mongoDB的空间索引结合特殊的查询方法很容易实现。 API直接支持，很方便 支持按照距离排序，并支持分页。支持多条件筛选。 可满足实时性需求。 资源占用大，数据量达到百万级请流量在10w左右查询速度明显下降。 6.2升级Mysql至5.7，支持Geohash MySQL 5.7.5 增加了对GeoHash的支持，提供了一系列geohash的函数，但是其实Mysql并没有提供类似mogodb类型near这样的函数，仅仅提供了一些经纬度转hash、hash取经纬度的一些函数。 优点:函数直接调用，生成目标hash、根据hash获取经纬度。 缺点：不支持范围查询函数，需要自行处理周边8点的问题，需要补充geo的算法 6.3Redis Commands: Geography Edition GEO 特性在 Redis 3.2 版发布， 这个功能可以将用户给定的地理位置信息储存起来， 并对这些信息进行操作，GEO通过如下命令来完成GEO需求. 命令 描述 geoadd 添加一个或多个经纬度地理位置 georadius 获取指定范围内的对象，也可以增加参数withdistance直接算出距离，也可以增加参数descending/ascending 进行距离排序 georadiusbymember 通过指定的对象，获取其周边对象 geoencode 转换为geohash，52-bit，同时返回该区域最小角的geohash,最大角的geohash，及中心点 geodecode 同上逆操作 优点:效率高，API丰富 缺点：3.2版本是否稳定？ 面试的时候，问到geohash算法以及技术选型大概也能说一说了… 本文章借鉴很多优秀文章，七拼八凑而出。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补码的前世今生]]></title>
    <url>%2F2019%2F01%2F29%2Fjava-basic%2F%E8%A1%A5%E7%A0%81%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[计算机如何来保存负数呢？其实只要达到这样的目的：正数负数都有一个唯一标识即可，但是，正如人类用+1和-1来表示可以提高效率一样，也得有一个比较适当的适合我们的计算机识别的一个方式。下面来详细讲解。 问题的由来 下面为了表示方便，先假设存储一个整型数字用4个bit。 举例来说，+2在计算机中表示为二进制的0010，那么-2怎么表示呢？ 很容易想到，可以将一个二进制位（bit）专门规定为符号位，它等于0时就表示正数，等于1时就表示负数。比如，在8位机中，规定每个字节的最高位为符号位。那么，+2就是0010，而-8则是1010。 更多的例子如下： 这就是直接用原码的方式来存储，虽然说这种方式理论上是可行的，毕竟每个数我都唯一标识了。 但是这种方式存在问题，我们希望+1和-1相加为0，但是通过这个方式算出来的是：0001+1001=1010 (-2)。也就是说，按照正常一步头的方式得不到我们想要的结果。 为了解决了“正负相加等于0”的问题，人们发明了反码。 反码 “反码”表示方式是用来处理负数的，符号位置不变，其余位置相反。 此时，我们再来算一下+1和-1相加，变成了0001+1110=1111，刚好反码表示方式中，1111象征-0。 此时，好像是解决了这个问题，但是我们发现，0这个时候有了两种表达：0000和1111。 即在用反码表示的情况下，0竟然可以用两个值来表示，这显然不好吧。毕竟+0和-0就是同一个玩意啊。 这个时候补码闪亮登场。 补码 很简单，在刚才反码的基础上加1。 此时，我们这里假定整形只有4位。那么-0表示为1111+1=10000，显然溢出了，就需要丢弃最高位，变成0000. 此时，神奇地发现，达到了统一，+0和-0都是用0000来表示了。 此时，也满足正负数相加为0的条件。比如+2为0010，-2为1110.此时两者相加为：0010+1110=(0)0000，丢掉最高位就是0000 那么对于普通情况，比如7+(-4)呢?即0111+1100=0011，就是3。OK，大功告成。 补码怎么求 上面已经说的很详细啦，比如-4，就是在4(0100)的基础上取反(1011)再加一(1100). 上面也解释了为什么要用补码。即保证了对称的正负数相加为0并且0只有一种表示方式。 还有一个重要的点就是，我们注意到，7-4其实我们都是转换成7+(-4)，也就是说，在计算机中，减法都是用加法的逻辑实现的。 即：一套加法的电路实现加减法。此外，乘法和除法其实都是加法这套电路实现的。 补码的本质 这里假设存储一个整型用8个bit。 要将正数转成对应的负数，其实只要用0减去这个数就可以了。比如，-8其实就是0-8。 则8的二进制是00001000，-8就可以用下面的式子求出： 123 ００００００００－００００１０００－－－－－－－－－ 因为00000000（被减数）小于0000100（减数），所以不够减。请回忆一下小学算术，如果被减数的某一位小于减数，我们怎么办？很简单，问上一位借1就可以了。 所以，0000000也问上一位借了1，也就是说，被减数其实是100000000，算式也就改写成： 1234１００００００００－００００１０００－－－－－－－－－ １１１１１０００ 进一步观察，可以发现100000000 = 11111111 + 1，所以上面的式子可以拆成两个： 1234567 １１１１１１１１－００００１０００－－－－－－－－－ １１１１０１１１＋０００００００１－－－－－－－－－ １１１１１０００ 通过这一步，我们就从数学上知道了为什么补码是取反加一了。 你看，求任何一个负数，都是0-正数，那么就用借位的思想来，则变成100000000。 100000000则可以分解为11111111+00000001。 此时求负数的过程就就变成11111111-X+1 而先用11111111来减这个正数，这个结果就是对正数取反。 此时再加上另外一个1. 这与我们求补码的过程是一样的，这也解释了为什么要这样求补码。 证明(可不看) 将上面的特例抽象一下，用统一表达式来证明一下。 我们要证明的是，X-Y或X+(-Y)可以用X加上Y的补码完成。 Y的补码等于(11111111-Y)+1。所以，X加上Y补码，就等于： 1X + (11111111-Y) + 1 我们假定这个算式的结果等于Z，即 1Z = X + (11111111-Y) + 1 接下来，分成两种情况讨论。 第一种情况，如果X小于Y，那么Z是一个负数。 由Y的补码等于(11111111-Y)+1，标记为F=(11111111-Y)+1,那么如何根据F逆向求Y呢？ 1Y=1111111-(F-1) OK,因为此时Z是一个负数，那么Z进行补码的逆运算就可以求出它的绝对值，即正数。再加一个符号，两者相等。 1Z = -[11111111-(Z-1)] = -[11111111-(X + (11111111-Y) + 1-1)] = X - Y 第二种情况，如果X大于Y 这意味着Z肯定大于11111111，但是我们规定了这是8位机，最高的第9位是溢出位，必须被舍去，这相当于减去100000000。所以， 1Z = Z - 100000000 = X + (11111111-Y) + 1 - 100000000 = X - Y 这就证明了，在正常的加法规则下，可以利用2的补码得到正数与负数相加的正确结果。换言之，计算机只要部署加法电路和补码电路，就可以完成所有整数的加法。 本文整理自： 关于2的补码 知乎第一条评论]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot使用logback实现日志按天滚动]]></title>
    <url>%2F2019%2F01%2F28%2Fmiscellany%2F11SpringBoot%E4%BD%BF%E7%94%A8logback%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E6%8C%89%E5%A4%A9%E6%BB%9A%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[日志是任何一个系统都必备的东西，日志的重要程度丝毫不亚于代码。而springboot中经常使用的是logback，那么今天我们就来学习一下在springboot下如何配置logback日志。理解了这里的配置，对于任何的日志都是一样的。 需求 日志按天滚动分割 info和error日志输出到不同文件 为什么使用Logback Logback是Log4j的升级版，作者为同一个人，作者不想再去改Log4j，所以写了Logback 使用日志框架的最佳实践是选择一款日志门面+一款日志实现，这里选择Slf4j+Logback,Slf4j作者也是Logback的作者 SpringBoot从1.4版本开始，内置的日志框架就是Logback Logback在SpringBoot中配置方式一 可以直接在applicatin.properties或者application.yml中配置 以在application.yml中配置为例 123456logging: pattern: console: "%d - %msg%n" file: /var/log/tomcat/sell.log level: com.imooc.LoggerTest: debug 可以发现，这种配置方式简单，但能实现的功能也很局限，只能 定制输出格式 输出文件的路径 指定某个包下的日志级别 如果需要完成我们的需求，这就得用第二种配置了 Logback在SpringBoot中配置方式二 在resource目录下新建logback-spring.xml, 内容如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;configuration&gt; &lt;!--打印到控制台的格式--&gt; &lt;appender name="consoleLog" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt; &lt;pattern&gt; %d - %msg%n &lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--除了error级别的日志文件保存格式以及滚动策略--&gt; &lt;appender name="fileInfoLog" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--过滤器，将error级别过滤掉--&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;DENY&lt;/onMatch&gt; &lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt; %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;!--滚动策略--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--路径--&gt; &lt;fileNamePattern&gt;/var/log/tomcat/sell/info.%d.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;!--error级别日志文件保存格式以及滚动策略--&gt; &lt;appender name="fileErrorLog" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--只让error级别的日志进来--&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt; %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;!--滚动策略--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--路径--&gt; &lt;fileNamePattern&gt;/var/log/tomcat/sell/error.%d.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;root level="info"&gt; &lt;appender-ref ref="consoleLog" /&gt; &lt;appender-ref ref="fileInfoLog" /&gt; &lt;appender-ref ref="fileErrorLog" /&gt; &lt;/root&gt;&lt;/configuration&gt; 每一个appender你可以理解为一个日志处理策略。 第一个appender的name=&quot;consoleLog&quot;, 名字是自己随意取的，取这个名字，表示这个策略用于控制台的日志。 我们重点看第二个和第三个appender,因为要把info和error日志输入到不同文件，所以我们分别建了两个appender。 rollingPolicy是滚动策略，这里我们设置按时间滚动 filter是日志的过滤方式，我们在fileInfoLog里做了如下过滤 123&lt;level&gt;ERROR&lt;/level&gt;&lt;onMatch&gt;DENY&lt;/onMatch&gt;&lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; 上述代码翻译之后：拦截ERROR级别的日志。如果匹配到了，则禁用处理。如果不匹配，则接受，开始处理日志。 那有的同学要问了，不能这样写吗 1&lt;level&gt;INFO&lt;/level&gt; 这样不是只拦截INFO日志了吗？ 不对！ 这就得说一下日志级别了 DEBUG -&gt;INFO -&gt; WARN -&gt;ERROR 如果你设置的日志级别是INFO，那么是会拦截ERROR日志的哦。也就是说，如果直接写info，那么大于等于info级别的日志都会写进去，违背了我们的需求。 整理自： http://www.imooc.com/article/19005]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql面试高频理论知识]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2Fmysql%E9%9D%A2%E8%AF%95%E9%AB%98%E9%A2%91%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[整理一些面试题，简单看看。 目录 数据库三范式 事务 mysql数据库默认最大连接数 分页 触发器 存储过程 用jdbc怎么调用存储过程？ 对jdbc的理解 写一个简单的jdbc的程序。写一个访问oracle数据的jdbc程序 JDBC中的PreparedStatement相比Statement的好处 数据库连接池作用 选择合适的存储引擎 数据库优化-索引 数据库优化-分表 数据库优化-读写分离 数据库优化-缓存 数据库优化-sql语句优化的技巧 jdbc批量插入几百万数据怎么实现 聚簇索引和非聚簇索引 sql注入问题 mysql悲观锁和乐观锁 1. 数据库三范式 1.1 范式是什么 范式就是规范，要满足第二范式必须先满足第一范式，要满足第三范式，必须要先满足第二范式。 1NF(第一范式)：列数据不可分割，即一列不能有多个值 2NF(第二范式)：主键(每一行都有唯一标识) 3NF(第三范式)：外键(表中不包含已在其他表中包含的非主关键信息) 1.2 反三范式 反三范式：有时为了效率，可以设置重复或者推导出的字段，例如：订单总价格订单项的单价，这个订单总价虽然可以由订单项计算出来，但是当订单数目庞大时，效率比较低，所以订单的总价这个字段是必要的。 2. 事务 2.1 含义 事务时并发控制的单位，是用户定义的一个操作序列，要么都做，要么都不做，是不可分割的工作单位。 2.2 事务的四个特征(ACID特性) 原子性：表示事务内操作不可分割 一致性：要么成功，要么失败，若后面失败，前面则回滚 隔离性：一个事务开始了，不被其他事务干扰 持久性：事务开始了，就不能突然终止 3. mysql数据库默认最大连接数 3.1 为什么需要最大连接数 特定服务器上的数据库只能支持一定数目同时连接，这时需要我们设置最大连接数（最多同时服务多少连接）。在数据库安装时会有一个默认的最大连接数。 my.ini中max_connections=100 4. 分页 4.1 为什么需要分页？ 在很多数据时，不可能完全显示数据。进行分段显示. 4.2 mysql如何分页 12String sql = "select * from students order by id limit " + pageSize*(pageNumber-1) + "," + pageSize; 4.3 oracle分页 是使用了三层嵌套查询。 1234String sql = &quot;select * from &quot; + (select *,rownum rid from (select * from students order by postime desc) where rid&lt;=&quot; + pagesize*pagenumber + &quot;) as t&quot; + &quot;where t&gt;&quot; + pageSize*(pageNumber-1); 5. 触发器 略。 6. 存储过程 6.1 数据库存储过程具有如下优点： 1、存储过程只在创建时进行编译，以后每次执行存储过程都不需再重新编译，而一般 SQL 语句每执行一次就编译一次，因此使用存储过程可以大大提高数据库执行速度。 2、通常，复杂的业务逻辑需要多条 SQL 语句。这些语句要分别地从客户机发送到服务器，当客户机和服务器之间的操作很多时，将产生大量的网络传输。如果将这些操作放在一个存储过程中，那么客户机和服务器之间的网络传输就会大大减少，降低了网络负载。 3、存储过程创建一次便可以重复使用，从而可以减少数据库开发人员的工作量。 4、安全性高，存储过程可以屏蔽对底层数据库对象的直接访问，使用 EXECUTE 权限调用存储过程，无需拥有访问底层数据库对象的显式权限。 6.2 定义存储过程: 12345678create procedure insert_Student (_name varchar(50),_age int ,out _id int)begin insert into student value(null,_name,_age); select max(stuId) into _id from student;end;call insert_Student('wfz',23,@id);select @id; 7. 用jdbc怎么调用存储过程？ 贾琏欲执事 加载驱动 获取连接 设置参数 执行 释放连接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.sql.CallableStatement;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.Types;public class JdbcTest &#123; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stub Connection cn = null; CallableStatement cstmt = null; try &#123; //这里最好不要这么干，因为驱动名写死在程序中了 Class.forName("com.mysql.jdbc.Driver"); //实际项目中，这里应用DataSource数据，如果用框架， //这个数据源不需要我们编码创建，我们只需Datasource ds = context.lookup() //cn = ds.getConnection(); cn = DriverManager.getConnection("jdbc:mysql:///test","root","root"); cstmt = cn.prepareCall("&#123;call insert_Student(?,?,?)&#125;"); cstmt.registerOutParameter(3,Types.INTEGER); cstmt.setString(1, "wangwu"); cstmt.setInt(2, 25); cstmt.execute(); //get第几个，不同的数据库不一样，建议不写 System.out.println(cstmt.getString(3)); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; /*try&#123;cstmt.close();&#125;catch(Exception e)&#123;&#125; try&#123;cn.close();&#125;catch(Exception e)&#123;&#125;*/ try &#123; if(cstmt != null) cstmt.close(); if(cn != null) cn.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 8. 对jdbc的理解 Java database connection java数据库连接.数据库管理系统(mysql oracle等)是很多，每个数据库管理系统支持的命令是不一样的。 Java只定义接口，让数据库厂商自己实现接口，对于我们者而言。只需要导入对应厂商开发的实现即可。然后以接口方式进行调用.(mysql + mysql驱动（实现）+jdbc) 9. 写一个简单的jdbc的程序。写一个访问oracle数据的jdbc程序 贾琏欲执事 加载驱动(com.mysql.jdbc.Driver,oracle.jdbc.driver.OracleDriver) 取连接(DriverManager.getConnection(url,usernam,passord)) 设置参数 Statement PreparedStatement cstmt.setXXX(index, value); 执行 executeQuery executeUpdate 释放连接(是否连接要从小到大，必须放到finnaly) 10. JDBC中的PreparedStatement相比Statement的好处 大多数我们都使用PreparedStatement代替Statement 1：PreparedStatement是预编译的，比Statement速度快 2：代码的可读性和可维护性 虽然用PreparedStatement来代替Statement会使代码多出几行,但这样的代码无论从可读性还是可维护性上来说.都比直接用Statement的代码高很多档次： 123456789stmt.executeUpdate("insert into tb_name (col1,col2,col2,col4) values('"+var1+"','"+var2+"',"+var3+",'"+var4+"')"); perstmt = con.prepareStatement("insert into tb_name (col1,col2,col2,col4) values (?,?,?,?)");perstmt.setString(1,var1);perstmt.setString(2,var2);perstmt.setString(3,var3);perstmt.setString(4,var4);perstmt.executeUpdate(); 3：安全性 PreparedStatement可以防止SQL注入攻击，而Statement却不能。 比如说： String sql = “select * from tb_name where name= '”+varname+&quot;’ and passwd=’&quot;+varpasswd+&quot;’&quot;; 如果我们把[' or '1' = '1]作为varpasswd传入进来.用户名随意,看看会成为什么? select * from tb_name = ‘随意’ and passwd = ‘’ or ‘1’ = ‘1’; 因为'1'='1'肯定成立，所以可以任何通过验证。 更有甚者：把[';drop table tb_name;]作为varpasswd传入进来,则： select * from tb_name = ‘随意’ and passwd = ‘’;drop table tb_name; 有些数据库是不会让你成功的，但也有很多数据库就可以使这些语句得到执行。 而如果你使用预编译语句你传入的任何内容就不会和原来的语句发生任何匹配的关系，只要全使用预编译语句你就用不着对传入的数据做任何过虑。而如果使用普通的statement,有可能要对drop等做费尽心机的判断和过虑。 11. 数据库连接池作用 1、限定数据库的个数，不会导致由于数据库连接过多导致系统运行缓慢或崩溃 2、数据库连接不需要每次都去创建或销毁，节约了资源 3、数据库连接不需要每次都去创建，响应时间更快。 12. 选择合适的存储引擎 在开发中，我们经常使用的存储引擎 myisam / innodb/ memory MyISAM存储引擎 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. 比如 bbs 中的 发帖表，回复表. INNODB存储引擎: 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表. Memory 存储 我们数据变化频繁，不需要入库，同时又频繁的查询和修改，我们考虑使用memory, 速度极快. MyISAM 和 INNODB的区别(主要) 事务安全 myisam不支持事务而innodb支持 查询和添加速度 myisam不用支持事务就不用考虑同步锁，查找和添加和添加的速度快 支持全文索引 myisam支持innodb不支持 锁机制 myisam支持表锁而innodb支持行锁(事务) 外键 MyISAM 不支持外键， INNODB支持外键. (通常不设置外键，通常是在程序中保证数据的一致) 下面是数据库的优化手段，但是只是表面，需要以后再好好探究 在项目自验项目转测试之前，在启动mysql数据库时开启慢查询，并且把执行慢的语句写到日志中，在运行一定时间后。通过查看日志找到慢查询语句。 1234567891011121314151617181920212223242526272829show variables like '%slow%'; #查看MySQL慢查询是否开启set global slow_query_log=ON; #开启MySQL慢查询功能show variables like "long_query_time"; #查看MySQL慢查询时间设置，默认10秒set global long_query_time=5; #修改为记录5秒内的查询select sleep(6); #测试MySQL慢查询show variables like "%slow%"; #查看MySQL慢查询日志路径show global status like '%slow%'; #查看MySQL慢查询状态或者vi /etc/my.cnf #编辑，在[mysqld]段添加以下代码slow-query-log = on #开启MySQL慢查询功能slow_query_log_file = /var/run/mysqld/mysqld-slow.log #设置MySQL慢查询日志路径long_query_time = 5 #修改为记录5秒内的查询，默认不设置此参数为记录10秒内的查询log-queries-not-using-indexes = on #记录未使用索引的查询:wq! #保存退出service mysqld restart #重启MySQL服务 13. 数据库优化-索引 13.1 索引的概念 索引（Index）是帮助DBMS高效获取数据的数据结构。 13.2 索引有哪些 分类：普通索引/唯一索引/主键索引/全文索引 普通索引:允许重复的值出现 唯一索引:除了不能有重复的记录外，其它和普通索引一样(用户名、用户身份证、email,tel) 主键索引：是随着设定主键而创建的，也就是把某个列设为主键的时候，数据库就会給改列创建索引。这就是主键索引.唯一且没有null值 全文索引:用来对表中的文本域(char，varchar，text)进行索引， 全文索引针对MyIsam explain select * from articles where match(title,body) against(‘database’);【会使用全文索引】 13.3 使用索引的注意事项 索引弊端 占用磁盘空间。 对dml(插入、修改、删除)操作有影响，变慢。 使用场景： 肯定在where条件经常使用,如果不做查询就没有意义 该字段的内容不是唯一的几个值(sex) 字段内容不是频繁变化. 注意事项 对于创建的多列索引（复合索引），不是使用的第一部分就不会使用索引。 123alter table dept add index my_ind (dname,loc); // dname 左边的列,loc就是右边的列explain select * from dept where dname='aaa'\G 会使用到索引explain select * from dept where loc='aaa'\G 就不会使用到索引 对于使用like的查询，查询如果是%aaa不会使用到索引而aaa%会使用到索引。 12explain select * from dept where dname like '%aaa'\G不能使用索引explain select * from dept where dname like 'aaa%'\G使用索引. 所以在like查询时，‘关键字’的最前面不能使用% 或者 _这样的字符，如果一定要前面有变化的值，则考虑使用 全文索引-&gt;sphinx. 索引列排序 MySQL查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来。否则不使用索引。 123expain select * from dept where dname=’111’;expain select * from dept where dname=111;（数值自动转字符串）expain select * from dept where dname=qqq;报错 也就是，如果列是字符串类型，无论是不是字符串数字就一定要用 ‘’ 把它包括起来. 如果mysql估计使用全表扫描要比使用索引快，则不使用索引。 表里面只有一条记录 索引不会包含有NULL值的列 只要列中包含有NULL值都将不会被包含在MySQL索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 使用短索引 对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 不要在列上进行运算，不使用NOT IN和&lt;&gt;操作，不支持正则表达式。 14. 数据库优化-分表 分表分为水平(按行)分表和垂直(按列)分表 水平分表情形： 根据经验，Mysql表数据一般达到百万级别，查询效率会很低，容易造成表锁，甚至堆积很多连接，直接挂掉；水平分表能够很大程度较少这些压力。 垂直分表情形： 如果一张表中某个字段值非常多(长文本、二进制等)，而且只有在很少的情况下会查询。这时候就可以把字段多个单独放到一个表，通过外键关联起来。考试详情，一般我们只关注分数，不关注详情。 水平分表策略： 1.按时间分表 这种分表方式有一定的局限性，当数据有较强的实效性，如微博发送记录、微信消息记录等，这种数据很少有用户会查询几个月前的数据，如需要就可以按月分表。 2.按区间范围分表 一般在有严格的自增id需求上，如按照user_id水平分表： 123table_1 user_id从1~100w table_2 user_id从101~200w table_3 user_id从201~300w 3.hash分表 通过一个原始目标的ID或者名称通过一定的hash算法计算出数据存储表的表名，然后访问相应的表。 15. 数据库优化-读写分离 一台数据库支持的最大并发连接数是有限的，如果用户并发访问太多。一台服务器满足不要要求是就可以集群处理。Mysql的集群处理技术最常用的就是读写分离。 主从同步 数据库最终会把数据持久化到磁盘，如果集群必须确保每个数据库服务器的数据是一直的。能改变数据库数据的操作都往主数据库去写，而其他的数据库从主数据库上同步数据。 读写分离 使用负载均衡来实现写的操作都往主数据去，而读的操作往从服务器去。 16. 数据库优化-缓存 什么是缓存 在持久层(dao)和数据库(db)之间添加一个缓存层，如果用户访问的数据已经缓存起来时，在用户访问时直接从缓存中获取，不用访问数据库。而缓存是在操作内存级，访问速度快。 作用 减少数据库服务器压力，减少访问时间。 Java中常用的缓存有 hibernate的二级缓存。该缓存不能完成分布式缓存。 可以使用redis(memcahe等)来作为中央缓存。对缓存的数据进行集中处理 17. 数据库优化-sql语句优化的技巧 DDL优化 通过禁用索引来提供导入数据性能，这个操作主要针对现有数据库的表追加数据 123456//去除键alter table test3 DISABLE keys;//批量插入数据insert into test3 ***//恢复键alter table test3 ENABLE keys; 关闭唯一校验 12set unique_checks=0 关闭set unique_checks=1 开启 修改事务提交方式(导入)（变多次提交为一次） 123set autocommit=0 关闭//批量插入set autocommit=1 开启 DML优化（变多次提交为一次） 12345insert into test values(1,2);insert into test values(1,3);insert into test values(1,4);//合并多条为一条insert into test values(1,2),(1,3),(1,4) DQL优化 Order by优化 多用索引排序 普通结果排序（非索引排序）Filesort group by优化 使用order by null,取消默认排序 等等等等… 18. jdbc批量插入几百万数据怎么实现 1、变多次提交为一次 2、使用批量操作 3、像这样的批量插入操作能不使用代码操作就不使用，可以使用存储过程来实现 mysql优化手段介绍到这里。 19. 聚簇索引和非聚簇索引 索引分为聚簇索引和非聚簇索引。 “聚簇索引” 以一本英文课本为例，要找第8课，直接翻书，若先翻到第5课，则往后翻，再翻到第10课，则又往前翻。这本书本身就是一个索引，即“聚簇索引”。 “非聚簇索引” 如果要找&quot;fire”这个单词，会翻到书后面的附录，这个附录是按字母排序的，找到F字母那一块，再找到&quot;fire”，对应的会是它在第几课。这个附录，为“非聚簇索引”。 由此可见，聚簇索引，索引的顺序就是数据存放的顺序，所以，很容易理解，一张数据表只能有一个聚簇索引。 聚簇索引要比非聚簇索引查询效率高很多，特别是范围查询的时候。所以，至于聚簇索引到底应该为主键，还是其他字段，这个可以再讨论。 1、MYSQL的索引 mysql中，不同的存储引擎对索引的实现方式不同，大致说下MyISAM和InnoDB两种存储引擎。 MyISAM存储引擎的索引实现 MyISAM的B+Tree的叶子节点上的data，并不是数据本身，而是数据存放的地址。主索引和辅助索引没啥区别，只是主索引中的key一定得是唯一的。这里的索引都是非聚簇索引。 MyISAM还采用压缩机制存储索引，比如，第一个索引为“her”，第二个索引为“here”，那么第二个索引会被存储为“3,e”，这样的缺点是同一个节点中的索引只能采用顺序查找。 InnoDB存储引擎的索引实现 InnoDB 的数据文件本身就是索引文件，B+Tree的叶子节点上的data就是数据本身，key为主键，这是聚簇索引。非聚簇索引，叶子节点上的data是主键 (所以聚簇索引的key，不能过长)。为什么存放的主键，而不是记录所在地址呢，理由相当简单，因为记录所在地址并不能保证一定不会变，但主键可以保证。 至于为什么主键通常建议使用自增id呢？ 2.聚簇索引 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。如果主键不是自增id，那么可以想 象，它会干些什么，不断地调整数据的物理地址、分页，当然也有其他一些措施来减少这些操作，但却无法彻底避免。但，如果是自增的，那就简单了，它只需要一 页一页地写，索引结构相对紧凑，磁盘碎片少，效率也高。 聚簇索引不但在检索上可以大大滴提高效率，在数据读取上也一样。比如：需要查询f~t的所有单词。 一个使用MyISAM的主索引，一个使用InnoDB的聚簇索引。两种索引的B+Tree检索时间一样，但读取时却有了差异。 因为MyISAM的主索引并非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。 不过，如果涉及到大数据量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的。 鉴于聚簇索引的范围查询效率，很多人认为使用主键作为聚簇索引太多浪费，毕竟几乎不会使用主键进行范围查询。但若再考虑到聚簇索引的存储，就不好定论了。 20. sql注入问题 20.1 什么是sql注入 sql注入大家都不陌生，是一种常见的攻击方式，攻击者在界面的表单信息或url上输入一些奇怪的sql片段，例如“or ‘1’=’1’”这样的语句，有可能入侵参数校验不足的应用程序。所以在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性很高的应用中，比如银行软件，经常使用将sql语句全部替换为存储过程这样的方式，来防止sql注入，这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 20.2 PrepareStatement解决SQL注入的问题 在使用JDBC的过程中，可以使用PrepareStatement进行预处理，预处理的优势就是预防绝大多数的SQL注入；而且针对多次操作数据库的情况，可以极大的提高访问数据库的效率。 那为什么它这样处理就能预防SQL注入提高安全性呢？其实是因为SQL语句在程序运行前已经进行了预编译。在程序运行时第一次操作数据库之前，SQL语句已经被数据库分析，编译和优化，对应的执行计划也会缓存下来并允许数据库以参数化的形式进行查询。当运行时动态地把参数传给PreprareStatement时，即使参数里有敏感字符如 or ‘1=1’，数据库也会作为一个参数一个字段的属性值来处理而不会作为一个SQL指令。如此，就起到了SQL注入的作用了！ 20.3 MyBatis如何防止sql注入 mybatis框架作为一款半自动化的持久层框架，其sql语句都要我们自己来手动编写，这个时候当然需要防止sql注入。其实Mybatis的sql是一个具有“输入+输出”功能，类似于函数的结构，如下： 12345&lt;select id=“getBlogById“ resultType=“Blog“ parameterType=”int”&gt; select id,title,author,content from blog where id=#&#123;id&#125; &lt;/select&gt; 这里，parameterType标示了输入的参数类型，resultType标示了输出的参数类型。回应上文，如果我们想防止sql注入，理所当然地要在输入参数上下功夫。上面代码中“#{id}”即输入参数在sql中拼接的部分，传入参数后，打印出执行的sql语句，会看到sql是这样的： select id,title,author,content from blog where id = ? 不管输入什么参数，打印出的sql都是这样的。这是因为mybatis启用了预编译功能，在sql执行前，会先将上面的sql发送给数据库进行编译，执行时，直接使用编译好的sql，替换占位符“？”就可以了。因为sql注入只能对编译过程起作用，所以这样的方式就很好地避免了sql注入的问题。 mybatis是如何做到sql预编译的呢？其实在框架底层，是jdbc中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的sql语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行一个sql时，能够提高效率，原因是sql已编译好，再次执行时无需再编译。 补充 12345&lt;select id=“orderBlog“ resultType=“Blog“ parameterType=”map”&gt; select id,title,author,content from blog order by $&#123;orderParam&#125; &lt;/select&gt; 仔细观察，内联参数的格式由“#{xxx}”变为了${xxx}。如果我们给参数“orderParam”赋值为”id”,将sql打印出来，是这样的： select id,title,author,content from blog order by id 显然，这样是无法阻止sql注入的。在mybatis中，”${xxx}”这样格式的参数会直接参与sql编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式，所以，这样的参数需要我们在代码中手工进行处理来防止注入。 21. mysql悲观锁和乐观锁 21.1 悲观锁 悲观锁（Pessimistic Lock），顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 Java synchronized 就属于悲观锁的一种实现，每次线程要修改数据时都先获得锁，保证同一时刻只有一个线程能操作数据，其他线程则会被block。 21.2 乐观锁 乐观锁（Optimistic Lock），顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。乐观锁适用于读多写少的应用场景，这样可以提高吞吐量。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 乐观锁一般来说有以下2种方式： 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 version 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 使用时间戳（timestamp）。乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。 Java JUC中的atomic包就是乐观锁的一种实现，AtomicInteger 通过CAS（Compare And Set）操作实现线程安全的自增。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂查询基础]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2F%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[这一节从group by和having两个关键语法入手，学习一下写sql的基本思路。 数据库准备： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677-- ------------------------------ 学生表-- ----------------------------DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `student_id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` varchar(8) DEFAULT NULL, PRIMARY KEY (`student_id`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4;-- ------------------------------ student表数据-- ----------------------------INSERT INTO `student` VALUES ('1', 'lilei', '19', 'female');INSERT INTO `student` VALUES ('2', 'huangmeimei', '18', 'male');INSERT INTO `student` VALUES ('3', 'pollu', '17', 'female');INSERT INTO `student` VALUES ('4', 'tom', '18', 'male');INSERT INTO `student` VALUES ('5', 'david', '17', 'male');INSERT INTO `student` VALUES ('6', 'lucy', '19', 'female');INSERT INTO `student` VALUES ('7', 'jacky', '20', 'male');-- ------------------------------ 课程表-- ----------------------------DROP TABLE IF EXISTS `course`;CREATE TABLE `course` ( `course_id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`course_id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4;-- ------------------------------ course表数据-- ----------------------------INSERT INTO `course` VALUES ('1', 'chinese');INSERT INTO `course` VALUES ('2', 'math');INSERT INTO `course` VALUES ('3', 'english');INSERT INTO `course` VALUES ('4', 'physics');-- ------------------------------ 分数表-- ----------------------------DROP TABLE IF EXISTS `score`;CREATE TABLE `score` ( `student_id` int(11) DEFAULT NULL, `course_id` int(11) DEFAULT NULL, `score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;-- ------------------------------ score表数据-- ----------------------------INSERT INTO `score` VALUES ('1', '2', '78');INSERT INTO `score` VALUES ('1', '3', '67');INSERT INTO `score` VALUES ('1', '4', '67');INSERT INTO `score` VALUES ('2', '1', '52');INSERT INTO `score` VALUES ('2', '2', '81');INSERT INTO `score` VALUES ('2', '3', '92');INSERT INTO `score` VALUES ('2', '4', '67');INSERT INTO `score` VALUES ('3', '1', '52');INSERT INTO `score` VALUES ('3', '2', '47');INSERT INTO `score` VALUES ('3', '3', '88');INSERT INTO `score` VALUES ('3', '4', '67');INSERT INTO `score` VALUES ('4', '2', '88');INSERT INTO `score` VALUES ('4', '3', '90');INSERT INTO `score` VALUES ('4', '4', '67');INSERT INTO `score` VALUES ('5', '1', '52');INSERT INTO `score` VALUES ('5', '3', '78');INSERT INTO `score` VALUES ('5', '4', '67');INSERT INTO `score` VALUES ('6', '1', '52');INSERT INTO `score` VALUES ('6', '2', '68');INSERT INTO `score` VALUES ('6', '4', '67');INSERT INTO `score` VALUES ('1', '1', '52');INSERT INTO `score` VALUES ('5', '2', '72');INSERT INTO `score` VALUES ('7', '2', '72'); 第一个问题 查询所有同学的学号、选课数、总成绩 针对sql问题，不需要一口气全部写出来，我们先分解逐个击破，最后再合体。 分析题目，他要查询的是每个学生的学号，选课数，总成绩 那么先把关键字列出来： 12345#首先是要查询，肯定有select关键字select#要查询的几个关键字段student_id,count(course_id),sum(score) 我们看到，有一些函数在里面，比如count和sum，那么我们会想到一般情况下是与group by结合使用的。 因为是查询每个学生，那么必然是根据每个学生的id进行分组了。 1group by student_id 此时，因为涉及的student_id,course_id以及score只需要一张score表就可以解决,那么拼接起来就是： 我们进行explain分析一下： 123explain SELECT student_id,count(course_id),sum(score)from scoregroup by student_id 显示： 基本的原理就是：首先根据group by进行分组，分组出来的数据缓存到一张临时表中，然后再做count之类的计算显示。 并且，本题是针对一张表，所以有一个规则是：如果用group by，那么你的select语句中选出的列要么是group by里用到的列，要么就是sum min等列函数的列。所以这里group by和后面是student_id，所以select后面可以查询student_id，但是不能查询course_id等字段。 第二个问题 查询所有同学的学号、姓名、选课数、总成绩 注意观察，其实就是比上一个问题多一个字段name，但是区别比较大，因为一张score表已经不够用了。这个时候还需要student表了，即两张表联合查询。那么只要搞一个连接条件即可： 第三个问题 查询平均成绩大于60分的同学的学号和平均成绩 我们再来分解看看： 12345678#查询肯定用到selectselect #要查询的两个字段student_id,avg(score) #由于存在avg，那么必然要分组group by student_id 最后，有一个条件是：平均成绩大于60分，此时就需要对查询出来的分组进行过滤筛选了，此时having闪亮登场。 测试了一下，下面两条sql都是一样的效果： 12select * from course where course_id = 1select * from course having course_id = 1 第四个问题 查询没有学全所有课的同学的学号、姓名 这个稍微复杂一点点，我们还是分解来看看： 1234567891011#查询肯定用到selectselect #要查询的两个字段student_id,name#由于存在avg，那么必然要分组group by student_id#两张表连接，要起个别名where sc.student_id = stu.student_id 因为需要查询课程没有学满的学生，所以需要先查询所有课程的数量： 1select count(1) from course 此时，我们需要利用这个查询语句作为结果再进行查询，即子查询。对于上面的分组要进行筛选 1having count(sc.course_id) &lt; (select count(1) from course) 所以最终的语句是：]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL必知必会知识点提炼]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2FSQL%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[这是对《mysql必知必会》的知识提炼。 一、基础 模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。 主键的值不允许修改，也不允许复用（不能使用已经删除的主键值赋给新数据行的主键）。 SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。 SQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。 SQL 支持以下三种注释： 12345# 注释SELECT *FROM mytable; -- 注释/* 注释1 注释2 */ 二、创建表 123456CREATE TABLE mytable ( id INT NOT NULL AUTO_INCREMENT, col1 INT NOT NULL DEFAULT 1, col2 VARCHAR(45) NULL, col3 DATE NULL, PRIMARY KEY (`id`)); 三、修改表 添加列 12ALTER TABLE mytableADD col CHAR(20); 删除列 12ALTER TABLE mytableDROP COLUMN col; 删除表 1DROP TABLE mytable; 四、插入 普通插入 12INSERT INTO mytable(col1, col2)VALUES(val1, val2); 插入检索出来的数据 123INSERT INTO mytable1(col1, col2)SELECT col1, col2FROM mytable2; 将一个表的内容插入到一个新表 12CREATE TABLE newtable ASSELECT * FROM mytable; 五、更新 123UPDATE mytableSET col = valWHERE id = 1; 六、删除 12DELETE FROM mytableWHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。 1TRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。 七、查询 DISTINCT 相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。 12SELECT DISTINCT col1, col2FROM mytable; LIMIT 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。 返回前 5 行： 123SELECT *FROM mytableLIMIT 5; 123SELECT *FROM mytableLIMIT 0, 5; 返回第 3 ~ 5 行： 123SELECT *FROM mytableLIMIT 2, 3; 八、排序 ASC ：升序（默认） DESC ：降序 可以按多个列进行排序，并且为每个列指定不同的排序方式： 123SELECT *FROM mytableORDER BY col1 DESC, col2 ASC; 九、过滤 不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。 123SELECT *FROM mytableWHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符 操作符 说明 = 等于 &lt; 小于 &gt; 大于 &lt;&gt; != 不等于 &lt;= !&gt; 小于等于 &gt;= !&lt; 大于等于 BETWEEN 在两个值之间 IS NULL 为 NULL 值 应该注意到，NULL 与 0、空字符串都不同。 AND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。 IN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。 NOT 操作符用于否定一个条件。 十、通配符 通配符也是用在过滤语句中，但它只能用于文本字段。 % 匹配 &gt;=0 个任意字符； \_ 匹配 ==1 个任意字符； [ ] 可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。 使用 Like 来进行通配符匹配。 123SELECT *FROM mytableWHERE col LIKE '[^AB]%'; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。 十一、计算字段 在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。 计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。 12SELECT col1 * col2 AS aliasFROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。 12SELECT CONCAT(TRIM(col1), '(', TRIM(col2), ')') AS concat_colFROM mytable; 十二、函数 汇总 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。 使用 DISTINCT 可以让汇总函数值汇总不同的值。 12SELECT AVG(DISTINCT col1) AS avg_colFROM mytable; 文本处理 函数 说明 LEFT() 左边的字符 RIGHT() 右边的字符 LOWER() 转换为小写字符 UPPER() 转换为大写字符 LTRIM() 去除左边的空格 RTRIM() 去除右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。 123SELECT *FROM mytableWHERE SOUNDEX(col1) = SOUNDEX('apple') 日期和时间处理 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 1mysql&gt; SELECT NOW(); 12018-4-14 20:25:11 数值处理 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 十三、分组 分组就是把具有相同的数据值的行放在同一组中。 可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。 指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。 123SELECT col, COUNT(*) AS numFROM mytableGROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。 1234SELECT col, COUNT(*) AS numFROM mytableGROUP BY colORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。 12345SELECT col, COUNT(*) AS numFROM mytableWHERE col &gt; 2GROUP BY colHAVING num &gt;= 2; 分组规定： GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。 十四、子查询 子查询中只能返回一个字段的数据。 可以将子查询的结果作为 WHRER 语句的过滤条件： 1234SELECT *FROM mytable1WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次： 123456SELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_numFROM CustomersORDER BY cust_name; 十五、连接 连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。 连接可以替换子查询，并且比子查询的效率一般会更快。 可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。 内连接 内连接又称等值连接，使用 INNER JOIN 关键字。 123SELECT A.value, B.valueFROM tablea AS A INNER JOIN tableb AS BON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。 123SELECT A.value, B.valueFROM tablea AS A, tableb AS BWHERE A.key = B.key; 在没有条件语句的情况下返回笛卡尔积。 自连接 自连接可以看成内连接的一种，只是连接的表是自身而已。 一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。 子查询版本 123456SELECT nameFROM employeeWHERE department = ( SELECT department FROM employee WHERE name = "Jim"); 自连接版本 1234SELECT e1.nameFROM employee AS e1 INNER JOIN employee AS e2ON e1.department = e2.department AND e2.name = "Jim"; 自然连接 自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。 内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。 12SELECT A.value, B.valueFROM tablea AS A NATURAL JOIN tableb AS B; 外连接 外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。 检索所有顾客的订单信息，包括还没有订单信息的顾客。 123SELECT Customers.cust_id, Orders.order_numFROM Customers LEFT OUTER JOIN OrdersON Customers.cust_id = Orders.cust_id; customers 表： cust_id cust_name 1 a 2 b 3 c orders 表： order_id cust_id 1 1 2 1 3 3 4 3 结果： cust_id cust_name order_id 1 a 1 1 a 2 3 c 3 3 c 4 2 b Null 十六、组合查询 使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。 每个查询必须包含相同的列、表达式和聚集函数。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 1234567SELECT colFROM mytableWHERE col = 1UNIONSELECT colFROM mytableWHERE col =2; 十七、视图 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。 对视图的操作和对普通表的操作一样。 视图具有如下好处： 简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 1234CREATE VIEW myview ASSELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_colFROM mytableWHERE col5 = val; 十八、存储过程 存储过程可以看成是对一系列 SQL 操作的批处理； 使用存储过程的好处： 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。 包含 in、out 和 inout 三种参数。 给变量赋值都需要用 select into 语句。 每次只能给一个变量赋值，不支持集合的操作。 123456789101112delimiter //create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end //delimiter ; 12call myprocedure(@ret);select @ret; 十九、游标 在存储过程中使用游标可以对一个结果集进行移动遍历。 游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。 使用游标的四个步骤： 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标； 1234567891011121314151617181920delimiter //create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; # 定义了一个 continue handler，当 sqlstate '02000' 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate '02000' set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器 触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。 触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。 INSERT 触发器包含一个名为 NEW 的虚拟表。 1234CREATE TRIGGER mytrigger AFTER INSERT ON mytableFOR EACH ROW SELECT NEW.col into @result;SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。 UPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改地，而 OLD 是只读的。 MySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。 二十一、事务处理 基本术语： 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。 MySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。 通过设置 autocommit 为 0 可以取消自动提交，直到 autocommit 被设置为 1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。 如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。 1234567START TRANSACTION// ...SAVEPOINT delete1// ...ROLLBACK TO delete1// ...COMMIT 二十二、字符集 基本术语： 字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。 除了给表指定字符集和校对外，也可以给列指定： 123CREATE TABLE mytable(col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci )DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对： 123SELECT *FROM mytableORDER BY col COLLATE latin1_general_ci; 二十三、权限管理 MySQL 的账户信息保存在 mysql 这个数据库中。 12USE mysql;SELECT user FROM user; 创建账户 1CREATE USER myuser IDENTIFIED BY 'mypassword'; 新创建的账户没有任何权限。 修改账户名 1RENAME myuser TO newuser; 删除账户 1DROP USER myuser; 查看权限 1SHOW GRANTS FOR myuser; 授予权限 1GRANT SELECT, INSERT ON mydatabase.* TO myuser; 账户用 username@host 的形式定义，username@% 使用的是默认主机名。 删除权限 1REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; GRANT 和 REVOKE 可在几个层次上控制访问权限： 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.\*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 更改密码 必须使用 Password() 函数 1SET PASSWROD FOR myuser = Password('new_password'); 转自： SQL]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务核心问题]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文从事务引出了Mysql InnoDB RR隔离级别下是如何防止幻读的。 1. 数据库事务的四大特性 最重要的是ACID特性。 原子性(Atomicity)：事务要么都成功要么都失败 一致性(Consistency)：关系型数据库有很多约束，事务前后都要满足这些约束(不仅仅是数据库物理约束，还包括内部逻辑上的一些假设) 隔离性(Isolation)：两个事务互相独立，不能互相干扰 持久性(Durability)：事务执行成功之后结果可以持久化，永久存储下来(redo日志) 对于一致性，可能解释比较抽象，他的实际含义是：数据库的数据应满足完整性约束。拿转账业务来说，假设用户A和用户B一共有2000块钱，那么他们之间无论如何转账，总共的钱应该都是2000. 2. 事务并发访问引起的问题 更新丢失-mysql所有事务隔离级别在数据库层面均可避免 取款事务 存款事务 开始事务 开始事务 查询余额为100元 无 无 查询余额为100元 无 存入20，余额变为120元 无 提交事务 取出10元，余额改为90元 无 回滚事务，余额恢复为100元 更新丢失 脏读问题-一个事务读到另一个事务未提交的数据 不可重复读-事务A多次读取数据，未提交数据，此时事务B提交新的数据，导致A多次读取数据期间数据不一致，不满足隔离性 幻读-事务A受到另一个事务插入新的一行或者删除一行的影响，导致幻觉 不可重复读的重点是修改: 同样的条件的select, 你读取过的数据, 再次读取出来发现值不一样了 幻读的重点在于新增或者删除: 同样的条件的select, 第1次和第2次读出来的记录数不一样 具体可以自己设置不同的隔离级别进行演示。 3. 事务的隔离级别 Read uncommitted：读到其他事务未commit的值 Read committed：解决了脏读问题，但是会读到其他事务commit的值，读两次可能会读到两个值，所以又叫不可重复读 Repeatable Read：解决了不可重复读问题，可重复读，别人commit对我没有影响，但是对于别的事务插入操作，可能会产生幻读 Serializable：串行化，当发生两个事务同时提交，结果只可能有一个，相当于串行执行后的某个结果 级别越来越高，安全性也越来越高，但是但是性能越来越低。说明一下，出现幻读只是针对这种Repeatable Read隔离级别，但是InnoDB已经不存在幻读问题了，如何解决的呢？主要是用next-key锁来解决，下文会讲到。 4. 当前读和快照读 4.1 当前读 读取的都是当前数据的最新版本，并且在读的时候对其加锁，不允许其他事务进行修改操作。 select ... lock in share mode（共享锁）以及 select ... for update、update、delete、insert（排他锁）这些操作都是当前读。 为什么将 插入/更新/删除 操作，都归为当前读？可以看看下面这个 更新 操作，在数据库中的执行流程： 从图中，可以看到，一个Update操作的具体流程。当Update SQL被发给MySQL后，MySQL Server会根据where条件，发出current read 读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回，并加锁 (current read)。 待MySQL Server收到这条加锁的记录之后，会再发起一个Update请求，更新这条记录。 一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此，Update操作内部，就包含了一个当前读。 4.2 快照读 不加锁的非阻塞读，简单的select（前提是事务级别不是serializable，因为在serializable级别下都是串行读，普通的select也会退化为当前读即select ... lock in share mode） 快照读的实现是基于多版本并发控制（MVCC）实现，旨在提高性能。有可能读到的不是数据的最新版本。（创建快照的时机决定了读到的数据的版本，如果事务A先快照读，事务B修改，那么事务A再快照读就还是更新前的版本，事务A的当前读会读到最新的数据；而当事务B先更新，事务A再快照读，就会读到数据最新版本了） 4.3 MVCC MVCC在MySQL的InnoDB中的实现 在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下： SELECT时，读取创建版本号&lt;=当前事务版本号，删除版本号为空或&gt;当前事务版本号。 INSERT时，保存当前事务版本号为行的创建版本号 DELETE时，保存当前事务版本号为行的删除版本号 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 通过MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。 说白了，就是乐观锁的一种实现。免去了加锁解锁的过程，对于读多写少的场景特别适用。 5. RC，RR级别下的InnoDB非阻塞读（快照读）如何实现 通过数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID这三个字段 DB_TRX_ID，最后一次修改本行事务的ID DB_ROLL_PTR，即回滚指针,与undo日志配合 DB_ROW_ID，随着新行插入而单调递增的行号（innoDB中如果既没有主键索引也没有唯一索引的时候，就会自动生成一个隐藏主键，就是这个玩意） 这三个字段结合undo日志，这个日志里面记录的都是老版本的数据，这样，快照读就可以读出适合的一个版本的数据出来。在数据库中，日志是非常重要的东西，可以说其重要性是大于数据本身的，因为数据丢失可以通过日志找回来，但是日志丢失了，那么以后数据库出现崩溃等就麻烦了。 6. 日志 数据库数据存放的文件称为data file；日志文件称为log file；数据库数据是有缓存的，如果没有缓存，每次都写或者读物理disk，那性能就太低下了。数据库数据的缓存称为data buffer，日志（redo）缓存称为log buffer；既然数据库数据有缓存，就很难保证缓存数据（脏数据）与磁盘数据的一致性。比如某次数据库操作： 1update driver_info set driver_status = 2 where driver_id = 10001; 更新driver_status字段的数据会存放在缓存中，等待存储引擎将driver_status刷新data_file，并返回给业务方更新成功。如果此时数据库宕机，缓存中的数据就丢失了，业务方却以为更新成功了，数据不一致，也没有持久化存储。 上面的问题就可以通过事务的ACID特性来保证。 12345BEGIN trans；update driver_info set driver_status = 2 where driver_id = 10001;COMMIT; 这样执行后，更新要么成功，要么失败。业务方的返回和数据库data file中的数据保持一致。要保证这样的特性这就不得不说存储引擎innodb的redo和undo日志。 6.1 undo是啥 undo日志用于存放数据修改被修改前的值，假设修改 tba 表中 id=2的行数据，把Name=‘B’ 修改为Name = ‘B2’ ，那么undo日志就会用来存放Name='B’的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 对数据的变更操作，主要来自 INSERT UPDATE DELETE，而UNDO LOG中分为两种类型，一种是 INSERT_UNDO（INSERT操作，事务提交后可以立即丢弃），记录插入的唯一键值；一种是 UPDATE_UNDO（包含UPDATE及DELETE操作），记录修改的唯一键值以及old column记录。 6.2 redo是啥 存储引擎也会为redo undo日志开辟内存缓存空间，log buffer。磁盘上的日志文件称为log file，是顺序追加的，性能非常高，注：磁盘的顺序写性能比内存的写性能差不了多少。 redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。例如某一事务的事务序号为T1，其对数据X进行修改，设X的原值是5，修改后的值为15，那么Undo日志为&lt;T1, X, 5&gt;，Redo日志为&lt;T1, X, 15&gt;。 梳理下事务执行的各个阶段： 写undo日志到log buffer； 执行事务，并写redo日志到log buffer； 如果innodb_flush_log_at_trx_commit=1，则将redo日志写到log file，并刷新落盘。 提交事务。 那redo日志是写进去了，但是数据呢？ 在数据库的世界里，数据从来都不重要，日志才是最重要的，有了日志就有了一切。 因为data buffer中的数据会在合适的时间 由存储引擎写入到data file，如果在写入之前，数据库宕机了，根据落盘的redo日志，完全可以将事务更改的数据恢复。好了，看出日志的重要性了吧。先持久化日志的策略叫做Write Ahead Log，即预写日志。 6.3 Undo + Redo事务的简化过程 假设有A、B两个数据，值分别为1,2，开始一个事务，事务的操作内容为：把1修改为3，2修改为4，那么实际的记录如下（简化）： 事务开始. 记录A=1到undo log buffer. 修改A=3. 记录A=3到redo log buffer. 记录B=2到undo log buffer. 修改B=4. 记录B=4到redo log buffer. 将redo log写入磁盘。 事务提交 我们可以看到，2，4，5，7，8都是新增操作，但是2，4，5，7都是缓冲到buffer区，只有8是磁盘IO操作。为了保证Redo Log有较好的IO性能，设计一般有以下特点： 尽量保持Redo Log存储在一段连续的空间上。因此在系统第一次启动时就会将日志文件的空间完全分配。 以顺序追加的方式记录Redo Log,通过顺序IO来改善性能。 批量写入日志。日志并不是直接写入文件，而是先写入redo log buffer.当需要将日志刷新到磁盘时 (如事务提交),将许多日志一起写入磁盘. 并发的事务共享Redo Log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起， 以减少日志占用的空间。例如,Redo Log中的记录内容可能是这样的： 记录1: &lt;trx1, insert …&gt; 记录2: &lt;trx2, update …&gt; 记录3: &lt;trx1, delete …&gt; 记录4: &lt;trx3, update …&gt; 记录5: &lt;trx2, insert …&gt; 因为上一条的原因,当一个事务将Redo Log写入磁盘时，也会将其他未提交的事务的日志写入磁盘 Redo Log上只进行顺序追加的操作，当一个事务需要回滚时，它的Redo Log记录也不会从Redo Log中删除掉。 6.4 回滚 前面说到未提交的事务和回滚了的事务也会记录Redo Log，因此在进行恢复时,这些事务要进行特殊的的处理。有2种不同的恢复策略： 进行恢复时，只重做已经提交了的事务。 进行恢复时，重做所有事务包括未提交的事务和回滚了的事务。然后通过Undo Log回滚那些未提交的事务。 MySQL数据库InnoDB存储引擎使用了第二个策略。 InnoDB可重复读隔离级别下如何避免幻读 表象原因:快照读（非阻塞读）–伪MVCC 内在原因：next-key锁（行锁+gap锁） 6.5 next-key锁 在 RR 级别下，如果查询条件能使用上唯一索引，或者是一个唯一的查询条件，那么仅加行锁，如果是一个范围查询，那么就会给这个范围加上 gap 锁或者 next-key锁 (行锁+gap锁)。 那么gap锁啥时候出现呢？ 使用主键索引或者唯一索引时： 如果where条件全部命中，则不会用Gap锁，只会加记录锁 如果where条件部分命中或者全不命中，则会加Gap锁 在走非唯一索引或者不走索引的当前读中，也会出现Gap锁。对于不走索引的情况，那么就会锁住整张表。 总结一下：只有对唯一索引+全部命中才不会加gap锁。 具体来个例子说明间隙锁如何工作。 7. 例子-走唯一索引 7.1 准备工作 有这样一个表test，其中name为主键，id为唯一键。 123456CREATE TABLE `test` ( `name` varchar(11) primary key, `id` int, unique KEY `id` (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into test(name,id) values ("f",1), ("h",2), ("b",3), ("a",5), ("c",6),("d",9); name id f 1 h 2 b 3 a 5 c 6 d 9 首先验证一下使用主键索引或者唯一索引时会怎么样。 7.2 第一种情况：唯一索引+命中所有数据 session1执行 1delete from test where id = 3; session2执行： 1insert into test(name,id) values("swg",4); 此时由于id是唯一索引，并且是命中的，所以只是对这一行加排他锁，而没有加gap锁，所以session2是可以正常执行的，不能被阻塞。 7.3 第一种情况：唯一索引+不命中数据 session1执行 1delete from test where id = 7; session2执行： 1insert into test(name,id) values("swg",8); 此时session2会阻塞住，证明id=7周围加了gap锁。gap锁的范围遵从左开右闭的原则，这里就是(6,7）以及(7,9)都会被锁住。加上record锁组成next-key锁，所以next-key锁的范围是(6,7]以及(7,9]这个范围。 7.4 第三种情况：唯一索引+不命中所有数据 session1执行 1select * from test where id in (5,7,9) lock in share mode; 这里是一个范围，5和9都是存在的，但是7不存在，即部分数据不存在。 session2执行： 1234567insert into test(name,id) values("swg",4);&lt;!--可以--&gt;insert into test(name,id) values("swg",7);&lt;!--不可以--&gt;insert into test(name,id) values("swg",8);&lt;!--不可以--&gt;insert into test(name,id) values("swg",10);&lt;!--可以--&gt; 那么对于(5,9]的范围内就阻塞住了，那么部分命中就是部分加gap锁。 7.5 第四种情况：唯一索引+命中所有数据 session1执行 1select * from test where id in (5,6,9) lock in share mode; 这里全部命中，那么 session2执行： 123insert into test(name,id) values("swg",7);&lt;!--可以--&gt;insert into test(name,id) values("swg",8);&lt;!--可以--&gt; 这个时候就不会加gap锁了。 8. 例子-不走唯一索引或者不走索引 下面来看看不走非唯一索引的当前读是什么情况。 此时表的数据为： name id h 2 c 6 b 9 d 9 f 11 a 15 把id上的唯一索引换成了普通索引。 8.1 第五种情况：非唯一索引 session1执行 1delete from test where id = 9; session2执行： 1insert into test(name,id) values("swg",9); 此时session2是会被block住的。gap的范围是(6,9]以及(9,11]. 12345insert into test(name,id) values("swg",5);&lt;!--可以--&gt;insert into test(name,id) values("swg",7);&lt;!--不可以--&gt;insert into test(name,id) values("swg",12);&lt;!--可以--&gt; 上面的原理都是一样的，即只要是6和11之间的数，不包含临界值的时候，无论插入什么数据，都是会阻塞的。 但是关于临界值6和11，这里就比较特殊了，因为需要加上主键的值才能进行精准的判断。 123insert into test(name,id) values(&quot;bb&quot;,6);&lt;!--可以--&gt;insert into test(name,id) values(&quot;dd&quot;,6);&lt;!--不可以--&gt; 这是什么原因呢？ 我们将数据画成图： 这里的gap区间可能是(负无穷，2],(2,6],(6,9],(9,11],(11,15],(15,正无穷) 我们可以看到，id为6的行，对应的name为c(不要忘记name是主键，主键按照顺序排序)，那么主键中就是按照字母表的顺序进行排列的（ASCII码），如果插入的name小于c，那么就不在gap的范围内(c,)，就可以插入，但是dd在gap的范围内,所以就会阻塞住。 8.2 第五种情况：不走索引 这个时候，所有的间隙都会加上间隙锁，那么就是锁表了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锁模块]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E9%94%81%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[本章对行锁表所、共享锁排他锁进行详细说明。这是数据库锁的核心知识。 MySQL中几种重要的锁概念 共享锁（S）和 排他锁（X） InnoDB 实现了标准的行级锁，包括两种：共享锁（简称 s 锁）、排它锁（简称 x 锁） 共享锁允许持锁事务读取一行 排它锁允许持锁事务更新或者删除一行 如果事务 T1 持有行 r 的 s 锁，那么另一个事务 T2 请求 r 的锁时，会做如下处理： T2 请求 s 锁立即被允许，结果 T1 T2 都持有 r 行的 s 锁 T2 请求 x 锁不能被立即允许 如果 T1 持有 r 的 x 锁，那么 T2 请求 r 的 x、s 锁都不能被立即允许，T2 必须等待T1释放 x 锁才行。 意向锁 innodb的意向锁主要用户多粒度的锁并存的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。 举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。 说白了意向锁的主要作用是处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁” 意向排它锁（简称 IX 锁）表明一个事务意图在某个表中设置某些行的 x 锁 意向共享锁（简称 IS 锁）表明一个事务意图在某个表中设置某些行的 s 锁 例如， SELECT ... LOCK IN SHARE MODE 设置一个 IS 锁, SELECT ... FOR UPDATE 设置一个 IX 锁。 意向锁的原则如下： 一个事务必须先持有该表上的 IS 或者更强的锁才能持有该表中某行的 S 锁 一个事务必须先持有该表上的 IX 锁才能持有该表中某行的 X 锁 next-key锁 InnoDB有三种行锁的算法： Record Lock：单个行记录上的锁。分为S Lock和X Lock Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。 Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 在默认情况下，mysql的事务隔离级别是可重复读，并且innodb_locks_unsafe_for_binlog参数为0，这时默认采用next-key locks。所谓Next-Key Locks，就是Record lock和gap lock的结合，即除了锁住记录本身，还要再锁住索引之间的间隙。 例子：假设一个索引包含值 10,11,13和20，索引上可能的NK 锁包括如下几个区间（注意开闭区间） 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) Innodb使用NK 锁来进行索引搜索和扫描，阻止了幻读。 间隙锁在Innodb中是被“十足的抑制”的，也就是说，他们只阻止其他事务插入到间隙中，他们不阻止其他事物在同一个间隙上获得间隙锁。 下篇文章会详细介绍一下。 MyISAM和InnoDB关于锁方面的区别 结论： MyISAM默认使用的是表级锁，不支持行级锁 InnoDB默认使用的是行级锁，也支持表级锁 所谓表级锁，就是锁住整张表。开销小，加锁快；不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 MyISAM在执行select的时候会产生一个表共享读锁，当进行更新等操作的时候会产生表独占写锁（排他锁）。所以： myISAM表的读操作，不会阻塞其他用户对同一个表的读请求，但会阻塞对同一个表的写请求。 myISAM表的写操作，会阻塞其他用户对同一个表的读和写操作。 myISAM表的读、写操作之间、以及写操作之间是串行的。 这里的读是共享锁，也可以将其变为排他锁，语法是select … for update 上面说完了MyISAM的表锁，下面要说说InnoDB啦。InnoDB支持行级锁。 所谓行级锁，就是锁住一行数据。开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发性也最高。 SELECT … LOCK IN SHARE MODE 和 SELECT … FOR UPDATE 如果你在查询数据，然后在同一个事务里插入或者修改相关的数据，常规的 select 语句不会提供足够的保护。其他的事务可以修改或者删除你正在查询的行。InnoDB 支持两种可以提供安全机制的读取锁： SELECT ... LOCK IN SHARE MODE SELECT ... FOR UPDATE SELECT … LOCK IN SHARE MODE 在读取的行上设置一个共享锁，其他的session可以读这些行，但在你的事务提交之前不可以修改它们。如果这些行里有被其他的还没有提交的事务修改，你的查询会等到那个事务结束之后使用最新的值。 索引搜索遇到的记录，SELECT … FOR UPDATE 会锁住行及任何关联的索引条目，和你对那些行执行 update 语句相同。其他的事务会被阻塞在比如执行 update 操作，获取共享锁，或从某些事务隔离级别读取数据等操作。 使用 SELECT FOR UPDATE 为 update 操作锁定行，只适用于 autocommit 被禁用（当使用 START TRANSACTION 开始事务或者设置 autocommit 为0时）。如果 autocommit 已启用，符合规范的行不会被锁定。 以上是对官方文档的翻译解读。 SELECT … LOCK IN SHARE MODE ：共享锁(S锁, share locks)。其他事务可以读取数据，但不能对该数据进行修改，直到所有的共享锁被释放。 如果事务对某行数据加上共享锁之后，可进行读写操作；其他事务可以对该数据加共享锁，但不能加排他锁，且只能读数据，不能修改数据。 SELECT … FOR UPDATE：排他锁(X锁, exclusive locks)。如果事务对数据加上排他锁之后，则其他事务不能对该数据加任何的锁。获取排他锁的事务既能读取数据，也能修改数据。 注：普通 select 语句默认不加锁，而CUD操作默认加排他锁。 当前事务获取共享锁后，可以读写，其他事务是否可以进行读写操作和获取共享锁：可以读，可以获取共享锁，不可以写 两个事务同时获取共享锁后，是否可以进行update操作：不可以 当前事务获取排他锁后，其他事务是否可以进行读写操作和获取共享锁：其他事务可以读，不可以获取共享锁，不可以写 是否可对一条数据加多个排他锁：不可以 行锁和索引的关系：查询字段未加索引（主键索引、普通索引等）时，使用表锁 注：InnoDB行级锁基于索引实现。 未加索引时，两种行锁情况为（使用表锁）： 事务1获取某行数据共享锁，其他事务可以获取不同行数据的共享锁，不可以获取不同行数据的排他锁 事务1获取某行数据排他锁，其他事务不可以获取不同行数据的共享锁、排他锁 加索引后，两种行锁为（使用行锁）： 事务1获取某行数据共享锁，其他事务可以获取不同行数据的排他锁 事务1获取某行数据排他锁，其他事务可以获取不同行数据的共享锁、排他锁 索引数据重复率太高会导致全表扫描：当表中索引字段数据重复率太高，则MySQL可能会忽略索引，进行全表扫描，此时使用表锁。可使用 force index 强制使用索引。 总结（很重要） MyISAM默认使用的是表级锁，不支持行级锁 执行select的时候会产生一个表共享读锁 当进行更新等操作的时候会产生表独占写锁（排他锁） 读不会阻塞其他session的读以及获取表共享读锁 写会阻塞其他session读和写操作 写与读之间是串行的 InnoDB默认使用的是行级锁，也支持表级锁 InnoDB 支持两种可以提供安全机制的读取锁：SELECT … LOCK IN SHARE MODE以及SELECT … FOR UPDATE SELECT … LOCK IN SHARE MODE 在读取的行上设置一个共享锁 SELECT … FOR UPDATE：排他锁 一个session对某一行上共享锁，其他的session可以读这行，也可以获取共享锁，但是不允许写，更不允许获取写锁。对于其他行，可以读写其他行数据也可以上读写锁。 一个session对某一行上排他锁，其他的session则不能加任何锁，包括共享锁。允许读这一行，但是不能写。允许对其他行数据进行读写以及上读写锁。 InnoDB中行级锁基于索引实现，所以在不加索引的时候，这两者上的其实都是表锁；加上索引之后，使用行锁。 以上的内容都是从博客：https://blog.csdn.net/u012099869/article/details/52778728 中整理而来，具体的实验也在他的博客中进行了详细的展示。 MyISAM适合场景 频繁执行全表count语句(MyISAM已经用一个表保存了行数) 对数据进行增删改的频率不高，查询非常频繁 没有事务 InnoDB适合场景 数据增删改查都相当频繁 可靠性要求比较高，要求支持事务]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于索引失效和联合索引]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E5%85%B3%E4%BA%8E%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E5%92%8C%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[理解最左前缀原则，可以帮助我们避免索引失效。 索引失效 查询条件包含or 当or左右查询字段只有一个是索引，该索引失效，explain执行计划key=null；只有当or左右查询字段均为索引时，才会生效； 组合索引，不是使用第一列索引，索引失效 如果select * from key1=1 and key2= 2;建立组合索引（key1，key2）; select * from key1 = 1;组合索引有效； select * from key1 = 1 and key2= 2;组合索引有效； select * from key2 = 2;组合索引失效；不符合最左前缀原则 like 以%开头 使用like模糊查询，当%在前缀时，索引失效； 如何列类型是字符串，where时一定用引号括起来，否则索引失效 当全表扫描速度比索引速度快时，mysql会使用全表扫描，此时索引失效 最左前缀原则 建立以下sql： 123456789CREATE TABLE IF NOT EXISTS `test_index`( `id` int(4) NOT NULL AUTO_INCREMENT, `a` int(4) NOT NULL DEFAULT '0', `b` int(4) NOT NULL DEFAULT '0', `c` int(4) NOT NULL DEFAULT '0', `data` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `union_index` (`a`,`b`,`c`))ENGINE=InnoDB ROW_FORMAT=DYNAMIC DEFAULT CHARSET=binary; 测试的mysql版本是 5.7. 首先以列a作为条件查询数据，我们看到 type: ref 表示引用查找, key_len: 4 表示索引长度为4，也就是利用上了索引来进行查找: 123456789101112131415explain select data from test_index where a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.01 sec) 然后以列b作为条件查询数据，可以看到type: ALL表示全表查找, key_len: NULL 表示没有索引，也就说明如果只使用b作为查询条件，不能利用索引来加快查找速度. 123456789101112131415explain select data from test_index where b = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 接着以列c作为条件查询数据，可以看到type: ALL表示全表查找, key_len: NULL 表示没有索引，情况与用b作为条件一样，只使用c作为查询条件也不能利用索引来加快查找速度 123456789101112131415explain select data from test_index where c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 现在来测一下使用a、b作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 8 表示索引长度为8，也就是说我们利用上了a、b联合索引来进行查找 123456789101112131415explain select data from test_index where a = 1 and b = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 紧接着来测一下使用a、c作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 4 表示索引长度为4，这就奇怪了，按照最左原则来说，a、c上是不会建立索引的，为什么会有索引长度呢？其实与a、b上的索引一比较我们就能发现，a、c上的索引长度只有4，而且单独的c上是没有索引的，所以4字节长度的索引只能是a上的，也就是说这种情况我们只使用了a列上的索引来进行查找 123456789101112131415explain select data from test_index where a = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 为了进一步验证上面的想法，这一次测一下使用b、c作为条件的情况，我们看到 type: ALL 表示全表查找, key_len: NULL 表示没有索引可以使用，按照最左原则来说，b列上没有索引，c列上也没有索引，同时b、c的上也不存在联合索引，所以使用b、c作为查询条件时无法利用联合索引 123456789101112131415explain select data from test_index where b = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 1.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 测试完两个条件的情况，接下来测试一下使用a、b、c作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 12 表示索引长度为12，这完全符合联合索引的最左原则，同时使用3个条件查询可以利用联合索引 123456789101112131415explain select data from test_index where a = 1 and b = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 12 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 下面这种情况也能利用a、b上的联合索引，索引长度为8 123456789101112131415explain select data from test_index where b = 1 and a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 再来试试这种情况，按照最左原则，c上没有建立索引，a上有索引，c、a没有建立联合索引，所以只能使用a上的索引进行查找，结果索引长度只有4，验证了我们的想法，联合查询条件使用索引时满足“交换律” 123456789101112131415explain select data from test_index where c = 1 and a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 联合索引总结 联合索引的最左原则就是建立索引KEY union_index (a,b,c)时，等于建立了(a)、(a,b)、(a,b,c)三个索引，从形式上看就是索引向左侧聚集，所以叫做最左原则，因此最常用的条件应该放到联合索引的组左侧。 **对于&quot;=&quot;和&quot;in&quot;可以乱序。**利用联合索引加速查询时，联合查询条件符合“交换律”，也就是where a = 1 and b = 1 等价于 where b = 1 and a = 1。这归功于mysql查询优化器，mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。 mysql会一直向右匹配直到遇到范围查询(&lt;,&gt;,between,like)就停止匹配。比如a=3 and b=4 and c&gt;5 and d=6，如果建立(a,b,c,d)顺序的索引，d是用不到索引的。如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 对于最左匹配原则的理解 mysql索引最左匹配原则的理解?–沈杰的回答 其实我觉得只要理解一点就是，只要有最左边的索引元素，那么这个索引结构一定是按照最左索引元素排序的，后序的索引元素也是依赖于最左元素之后才有可能变得有意义。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL调优]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2FMySQL%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[本文介绍最基本的sql调优手段。 根据慢日志定位慢查询sql 12&lt;!--这里是用模糊查询查出关于查询的一些配置项--&gt;show variables like '%query%' 我们关注slow_query_log：OFF，表示慢查询处于关闭状态。关注long_query_time：超出这个时间就是慢查询，记录到slow_query_log_file文件中。 1show status like '%slow_queries%' 这一句作用是统计慢查询的数量。 如何打开慢查询呢？ 1234&lt;!--打开慢查询--&gt;set global slow_query_log = on;&lt;!--慢查询的标准是1秒--&gt;set global long_query_time = 1; 注意要重启一下客户端。或者在配置文件中设置，重启服务端就永久保留了。 explain分析慢日志 上一步时打开慢查询日志。下面要进行分析。 1explain select ... 对这个命令进行分析。有两个关键字段： type：表示mysql找到数据行的方式，下面的顺序是由快到慢： system&gt;const&gt; eq_ref&gt;ref&gt;fulltext&gt;ref_or_null&gt;index_merge&gt;unique_subquery&gt;index_subquery&gt;range&gt;index&gt;all 其中index和all为全表扫描。说明需要优化。 extra： using_filesort：表示MySQL会对结果使用一个外部索引排序，而不是从表里按索引次序读到相关内容。可能在内存或者磁盘上进行排序。MqSQL中无法利用索引完成的排序操作称为“文件排序” using temporary：表示MySQL在对查询结果排序时使用临时表。常见于排序order by 和分组查询 group by。 当extra中出现以上两项意味着MYSQL根本不能使用索引，效率会受到重大影响，应尽可能对此进行优化。 修改sql或者尽量让sql走索引 上一步分析完之后，就要采取一定的措施来修正。 如果是没有加索引，可以对其加上索引。extra就会变成using index，表示走了索引。 索引是越多越好吗 数据量小的表不需要建立索引，建立会增加额外的索引开销 数据变更需要维护索引，因此更多的索引意味着更多的维护成本 更多的索引意味着也需要更多的空间 可以理解为，一个几页的宣传手册 对于几页的宣传手册我们还需要建立一个目录吗？ 变更这个小的宣传手册里面的章节还要修改目录不是更烦吗？ 一个宣传手册内容就两页，结果你的目录就占了一页，这合理吗？]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引全面解读]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2FMySQL%E7%B4%A2%E5%BC%95%E5%85%A8%E9%9D%A2%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[彻底理解MySQL的存储引擎数据结构以及聚集索引。 1.为什么需要索引 如果不用索引，那么最简单的方案是：将数据全部或者分批次地加载到内存，因为数据是以页的形式存储的，所以我们可以轮询这些页，找出有没有我们需要的数据。 在大数据量的情况下，显然是会非常慢的。因为它要进行全表的扫描。 而索引的灵感来源于字典，我们知道，新华字典前面有按照拼音或者按偏旁部首排序的一个列表页，我们可以快速地根据这个索引目录迅速定位到某几页，然后我们到这个某几页找一下就可以找到了。不需要全表扫描。 2.索引的数据结构 2.1 二叉查找树上阵 二叉查找树的特点： 二分搜索树本质上是一棵二叉树。不需要是一棵完全二叉树。 每个节点的键值大于左孩子 每个节点的键值小于右孩子 以左右孩子为根的子树仍为二分搜索树 二叉查找树有点很明显，我们查询一个数据只需要O(logn)的时间。但是它存在一个致命问题： 我们有时会删除增加数据，搞的不好，会把他恶化成一个链表。 但是有的同学说，我们可以利用红黑树之类的数据结构来维持住平衡二叉树的特性，这样不就好了吗？ 这里还存在另一个问题，就是IO。我们知道从磁盘查询数据，影响性能的关键点是iO的次数，然后这种一个节点只有两个孩子，在海量数据里，IO的次数还是太多，影响性能。 我们这个时候就知道了方向，我们想找一个数据结构，它既包含了二叉树的优点，还要是平衡的树，还能使树的高度变矮，并且每个节点存储更多的数据。 2.2 BTree上阵 B数又叫平衡多路查找树。M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉 有几个特点： 根节点至少包含两个孩子 排序方式：所有节点关键字是按递增次序排列，并遵循左小右大原则 子节点数：树中每个节点最多包含m个孩子(m&gt;=2)；除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子 关键字数：枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于m-1个（分别比孩子数少一个） 所有叶子节点都位于同一层，即所有叶子节点高度都一样 我们结合这个图来理解。 可以看到，我们这是一个3路B树，根节点有3个孩子，有2个关键字。根据规则，子节点数最多为m个即3个，最少为ceil(1.5)个即2个；关键字数最多为m-1个即2个，最少为于ceil(1.5)-1个即1个. 那么我们进行插入的时候，关键字这里最多为2个，所以大于2就要进行拆分。 如何拆分呢？拿个例子来： 定义一个5阶树（平衡5路查找树;），现在我们要把3、8、31、11、23、29、50、28 这些数字构建出一个5阶树出来; 那么关键字最多为4个，超过4个就拆分。 先插入 3、8、31、11 再插入23、29 再插入50、28 大概就是这样的流程。总之要维护一个从左到右逐渐增大的一个特性，并且必须是平衡的。(大概忽略里面可能存在的一些小错误，理解其中意思即可) 对于删除也是如此，要满足以上的特性才行，这里就不再赘述了。 对B树总结一下： B树相对于平衡二叉树的不同是，每个节点包含的关键字增多了，特别是在B树应用到数据库中的时候，数据库充分利用了磁盘块的原理（磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来）把节点大小限制和充分使用在磁盘快大小范围；把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度; 2.3 B+树 是B树的变体，其定义基本上与B树是差不多的。除了： 非叶子节点的子树指针与关键字个数相同 非叶子节点仅用来索引，数据都保存在叶子节点中 所有叶子节点均有一个链指针指向下一个叶子节点 这就是B+树相对于B树的改进的几个点。 由于数据存在叶子节点，优点是非叶子节点保存的关键字更多了，树的高度就会更矮。 2.4 总结 B+树更适合用来做存储索引： B+树的磁盘读写代价更低（因为内部不存放数据，一次性读取的关键字更多，IO次数降低） B+树的查询效率更加稳定（任何关键字的查找都要到叶子节点，导致每个查询都差不多） B+树有利于对数据库扫描（遍历叶子节点就可以直接扫描整个表，这个适合做范围查询） 2.5 Hash索引也可以考虑一下 Hash结构可以一次性地定位到响应位置。如果遇到碰撞的情况，只需要遍历链表即可。那么性能这么高，为什么我们不用Hash索引呢？ 它也有缺点： 只能做等值操作，不能使用范围查询 hash索引不是按照索引值顺序存储，无法使用于排序。 不能利用部分索引键查询（比如组合索引，hash索引是对这几个索引一起hash计算的，而我们用组合索引中的部分索引时就无法用了） 不能避免表扫描（会出现hashs冲突，必然要扫描里面具体的数据才行） 遇到大量hash值相等的时候性能不一定比B树高(同上) 3. 聚集索引 3.1 什么是聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 3.2 MyISAM和InnoDB索引实现 第一个不同是：InnoDB的数据文件本身就是索引文件。而MyISAM的索引和数据是分开的。 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址，是没有任何顺序而言的，所以MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。 InnoDB也使用B+Tree作为索引结构。InnoDB的数据文件本身就是索引文件，即 InnoDB 表是基于聚簇索引建立的。 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址（这一点可以通过在data目录下查看数据库文件验证。Innodb每一个数据库只有一个数据文件，而Myisam则有三个（数据文件、索引文件、表结构文件））。 而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。聚集索引的确定规则为： 若一个主键被定义，该主键则作为聚集索引 若没有主键被定义，该表的第一个唯一非空索引作为聚集索引 若上述都找不到，innodb内部会生成一个隐藏主键(聚集索引) 非主键索引存储相关键位和其对应的主键值，包含两次查找 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。 换句话说，InnoDB的所有辅助索引都引用主键作为data域。 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择 4. 总结 本文首先介绍的是为什么要索引。这个问题很简单。 然后介绍了几种数据结构：二叉搜索树、二叉平衡树、B树以及B+树。一步一步引出为什么最终是B+树。面试的时候就看解决：为什么不能用二叉搜索树、为什么不用红黑树、B树和B+树各自的数据结构特点、B+树的优点 最后介绍了聚集索引，因为这是MyISAM与InnoDB索引结构最大的不同。 之前还是不能太准确理解聚集索引，这两种存储引擎都是以B+树数据结构建立索引结构的，但是InnoDB本身这个B+树就作为了索引文件，即索引与数据是放在一起的，所以逻辑上这样排的数据，它物理上也是这么排。 而MyISAM的索引结构(B+树)与数据是分离的，虽然B+树可能是按照主键有序地组织，但是表的数据在另一个地方是随机放的，找数据是根据地址来找即可，所以这种结构就不是聚集的。 理解了这个，下面就非常好理解了，InnoDB这个B+树，我们知道，叶子节点的核心数据就是主键。所以是按照主键递增的方式进行排列。这样子，无论是按照主键排序还是范围搜索，都会非常地快。 那么如果是非主键索引的辅助索引呢？InnoDB只能通过两次查询来实现了，首先第一步是根据这个辅助索引找到存放在叶子节点中的主键值，然后根据主键再去主键索引中去查找对应的数据。 而MyISAM索引，主键索引和辅助索引就区别不大了。都是单独一个索引结构，然后根据最后叶子节点中的该条数据的地址去找。 上面说的按照主键排列，就是这里所谓的聚集索引啦。当然了，如果没有指定主键，会按照上面所说的规则去构建聚集索引。 那么，面试的时候，就可以应对InnoDB与MyISAM索引结构的各自的实现和不同点啦。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引入门]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[索引是数据库中提高性能的一大利器。本篇入门索引的基本知识。 1. 什么是索引 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2. 为什么要用索引 索引主要就是为了提高查询速度用的。 3. 索引的一些缺点 第一，创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 第二，索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 第三，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4. 哪些字段适合用索引 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。（这同一） 5. 不应该创建索引的的这些列具有下列特点 第一，对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 第二，对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 第三，对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 第四，当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 6. 索引的分类 B-Tree 索引， Hash 索引， Fulltext 索引和R-Tree 索引 最主要关心的是B-Tree 索引。下面再提一下聚集索引，因为这是innodb最主要的组织方式。 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 下一节会详细讲到InnoDB和MyISAM的索引实现方式，他们最大的区别就是InnoDB是聚集索引，而MyISAM不是。 7. 局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 8.B树/B+树 想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是B tree或者 B+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然， 有的数据库也使用哈希桶作用索引的数据结构 ， 然而， 主流的RDBMS都是把平衡树当做数据表默认的索引数据结构的。 我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个加了主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。 如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。 假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。 这里不讲解平衡树的运行细节， 但是从上图能看出，树一共有三层， 从根节点至叶节点只需要经过三次查找就能得到结果。如下图 这一节先对索引入个门，关于B+树以及聚集索引下篇文章来具体分析。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一个关系型数据库]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[这是一个比较空的面试题，这里说一下如何回答。 如何设计一个关系型数据库 我们考虑开发一个数据库最重要的模块是什么。首先数据存储是其核心功能。因此会有一个存储模块来存储数据。介质主要是硬盘。 可是，光有存储是不行的。我们需要有以下程序模块对数据进行组织。 存储管理 我们需要对数据的格式和文件的分隔进行统一的管理，通过逻辑的形式来组合和表示出来。 我们知道程序处理，需要将数据先加载到内存中去，不可能直接在硬盘上进行处理。 我们通过io读取磁盘数据，磁盘的io是非常耗时的，所以硬盘以页的形式存储数据，根据局部性原理，往往用户要查询的数据周围的数据也会被查询到，所以取数据都是以页为单位查取多个数据，提高效率。 缓存机制 也就是上面提到的，一次IO不会只取用户所需要的一点数据，所以会涉及到缓存，缓存可能会不够放，那就涉及一些缓存淘汰的算法，比如比较常用的是LRU算法。 SQL解析 将SQL进行编译执行。如何提高SQL解析效率呢？可能也用缓存，缓存好SQL解析后的结果，下次再执行一样的SQL就可以免去解析的过程。 日志管理 要记录SQL操作，方便主从同步、灾难恢复等。这里要了解一下binlog. 权限划分 就是权限。 容灾机制 要对异常情况做好准备，比如数据库挂了怎么办。 索引管理 优化数据库执行效率。 锁模块 使得数据库支持并发操作。 总结 了解了上面的内容，我们就可以对这个问题做一个简单的总结性回答了，如何设计关系型数据库呢？首先数据库有一个存储的功能，使得它能存储在比如机械硬盘或者固态硬盘上面。其次，我们需要一个存储管理模块来映射程序逻辑与物理地址，实现存储管理。还需要缓存机制，对一些数据进行缓存提高效率，并且缓存不能太大，必须配备缓存淘汰机制；然后需要一个SQL解析模块，来解析SQL；然后需要日志管理来提供主从赋值、主从同步等功能；还需要一个权限划分模块，来提供给多用户使用场景；还需要容灾机制面对异常情况；最后，为了提高数据查询效率需要有索引管理模块；为了支持并发操作需要有锁模块。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[delete和truncate以及drop区别]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fdelete%E5%92%8Ctruncate%E4%BB%A5%E5%8F%8Adrop%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[这个题目我自己也被问过，这里简单整理一下。 先来个总结： drop直接删掉表； truncate删除的是表中的数据，再插入数据时自增长的数据id又重新从1开始； delete删除表中数据，可以在后面添加where字句。 日志是否记录 DELETE语句执行删除操作的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。 TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 是否可以回滚 delete 这个操作会被放到 rollback segment 中,事务提交后才生效。truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment 中，不能回滚. 所以在没有备份情况下，谨慎使用 drop 与 truncate。 表和索引占的空间 当表被 TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小。 而 DELETE 操作不会减少表或索引所占用的空间。 drop语句将表所占用的空间全释放掉。 TRUNCATE 和 DELETE 只删除数据，而 DROP 则删除整个表（结构和数据） 所以从干净程度，drop &gt; truncate &gt; delete ok，差不多了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最基础知识小结]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fmysql%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文介绍关于数据库的最最最最基本的一些语法知识，如果这些都不熟悉，建议多多练习，因为后续的文章会比较深入原理。 一、DDL语句 1、创建数据库：create database dbname; 2、删除数据库：drop database dbname; 3、创建表：create table tname; 4、删除表：drop table tname; 5、修改表：略，懒得看 二、DML语句 插入： 1insert into table(字段1，字段2，...) values (value1,value2,...) , (value3,value4,..) 更新： 1update table set 字段=value where ... 删除： 1delete from table where ... 这里要注意下delete和truncate以及drop三者的区别，下篇文章详解。 单表查询： 1select 字段 from table 连表查询方式1： 1select 别名1.字段,别名2.字段 from table1 别名1,table2 别名2 where ... 连表查询方式2： 1select 别名1.字段,别名2.字段 from table1 别名1 join table2 别名2 on ... 这是全连接，这里就要了解一下笛卡儿积，简单来说，最后行数是左边表的函数乘以右边表的行数。详细的可以自行google. 查询不重复的记录： 1select distinct 字段 from table ... 排序：默认是升序 1select 字段 from table where ... order by ... asc/desc limit：主要用于分页 1select * from table where ... order by ... asc/desc limit 起始偏移位置，显示条数 聚合： 1select count(*)/avg(..)/sum(...)/max(...)/min(...) from table group by ... having .... 注意这里的having和where的区别：where是对表结果进行筛选，having 是对查询结果进行筛选，与group by 合用 左连接和右连接 左连接意思就是左表中的记录都在，右表没有匹配项就以null显示。记录数等于左表的行数。 右连接与之同理，尽量转为左连接做。 子查询： 1select * from table where ... in (select ....) 所谓子查询就是根据另一个select的结果再进行筛选，常用的是in,not in,=,!=,exits,not exits union 主要用于两张表中的数据的合并： 1select 字段 from table1 union all select 字段 from table2 要想不重复用union]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常见的面试题]]></title>
    <url>%2F2019%2F01%2F25%2Fnetwork%2F8.%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[对这一块整理一些常见的面试题。 1.TCP三次握手、四次挥手 这部分略。前面已经说的很详细，包括握手为什么不是两次、为什么不是四次，为什么挥手要等2MSL的时间。 2.常见的HTTP状态码及其含义 200 OK：正常返回信息 400 Bad Reqest：客户端请求有语法错误，不能被服务器所理解 401 Unauthorized：请求未经授权，这个状态码必须与WWW-Authenticate报头域一起使用 403 Forbidden：服务器收到请求，但是拒绝提供服务 404 Not Found：请求资源不存在 500 Internal Server Error：服务器发生不可预期的错误 503 Server Unavilable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常 3.Get请求和Post请求的区别 Http报文层面：GET将请求信息放在URL，POST则放在报文体中 数据库层面：GET符合幂等性和安全性(查询不会改变数据库)，POST不符合 其他层面：GET可以被缓存、被存储为书签，而POST不行 4.Cookie和Session的区别 对于session，字面上理解是会话，可以理解为用户与服务端一对一的交互。是一个比较抽象的概念。 但是我们常说的session其实是这里抽象概念的一种实现方式罢了，我觉得没有必要咬文嚼字，下面直接从面试角度来分析一下。 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。 这个Session是保存在服务端的，有一个唯一标识，这个唯一标识对应一个用户。在服务端保存Session的方法很多，内存、数据库、文件都有。 服务端解决了用户标识问题，但是服务端怎么知道此时操作浏览器的用户是谁呢？ 这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。 实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端(放在响应头中返回)，需要在 Cookie 里面记录一个Session ID，以后每次请求(请求头)把这个会话ID发送到服务器，我就知道你是谁了。 有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 总结： Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中； Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 下面说一下很常见的一种写法。比如在单体应用中，我此时登陆你的网站了，你可以将我的信息保存在session中： 12User currentUserInfo = userService.getUserByUsernameAndPasswd(username,password);session.setAttribute("currentUser",currentUserInfo); 下次，我就可以在我们之间的会话中随时获取我的个人信息： 1User currentUser = session.getAttribute("currentUser"); 其实这些就是利用存放在Cookie中的JSESSIONID来实现的。 5.HTTP和HRTTPS的关系 来说一下SSL(Security Sockets Layer，安全套接层) 为网络通信提供安全及数据完整性的一种安全协议 是操作系统对外的API，SSL3.0之后更名为TLS 采用身份验证和数据加密保证网络通信的安全和数据的完整性 HTTPS数据传输流程： 浏览器将支持的加密算法信息发送给服务器 服务器选择一套浏览器支持的加密算法，以证书的形式回发浏览器 浏览器验证证书合法性，并结合证书公钥加密信息发给服务器 服务器使用私钥解密信息，验证哈希，加密响应消息回发浏览器 浏览器解密响应消息，并对消息进行验证，之后进行加密交互数据 这个也就不赘述了，下面直接说说区别。 HTTPS需要到CA申请证书，HTTP不需要 HTTPS密文传输，HTTP明文传输 连接方式不同，HTTPS默认使用443端口，HTTP使用80端口 HTTPS=HTTP+加密+认证+完整性保护，更安全 但是仍然存在一定的风险： 浏览器默认填充http://，请求需要进行跳转，有被劫持的风险 可以使用HSTS(HTTP Strict Transport Security)优化（这个还不未主流，面试问的少） Socket简介 我们知道，进程与进程直接的通信最基本的要求是：可以唯一确定进程。 在本地进程通信中，可以用PID来唯一标识一个进程。 但是PID只在本地唯一，网络中PID冲突的几率还是存在的。 我们知道，到IP层就可以唯一定位到一台主机了，TCP层(tcp协议+端口号)可以唯一定位一台主机中的一个进程。 这样，我们可以通过ip地址+协议+端口号可以唯一标识一台主机的一个进程。这样就可以通过socket进行网络通信了。 socket是对TCP/IP协议的抽象，是操作系统对外开放的接口。 socket起源于unix，而unix是遵从一切皆文件的哲学。Socket是一种基于从打开、读/写、关闭的模式实现的。客户端和服务器各自维护一个文件，在连接建立后，可以供对方读取或者读取对方内容。 socket相关题目 编写一个网络程序，有客户端和服务端，客户端向服务端发送一个字符串，服务器收到字符串之后打印到命令行上，然后向客户端返回该字符串的长度，最后，客户端输出服务端返回的该字符串的长度，分别用TCP和UDP两种方式去实现。 代码地址：https://github.com/sunweiguo/TcpAndUdp/]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础核心-理解类、对象、面向对象编程、面向接口编程]]></title>
    <url>%2F2019%2F01%2F24%2Fjava-basic%2FJAVA%E5%9F%BA%E7%A1%80%E6%A0%B8%E5%BF%83-%E7%90%86%E8%A7%A3%E7%B1%BB%E3%80%81%E5%AF%B9%E8%B1%A1%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E3%80%81%E9%9D%A2%E5%90%91%E6%8E%A5%E5%8F%A3%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是类，什么是对象，什么是面向对象编程，什么是面向接口编程。学习面向对象思想的语言，比如java，第一关可能就是要理解这些概念。下面就来好好琢磨一下。 类和对象的概念 首先总结一下：类是一个模板，对象就是用这个模板创造出来的东西。 比如，男孩，他就是一个模板，男的就行，那么对象是什么呢？就是具体某个男孩，比如男孩BOB，男孩fourColor. 请看下面一张图： 男孩女孩是比较抽象的概念，是模板，左边一排就是其具体的一些对象。你看长的都不一样，有的黑，有的白，有的高，有的矮，国家地区也不一样。但是他们都属于男孩或者女孩。 那么同理，人就是一个类，男孩女孩就是人的子类，因为人可能不仅包括男孩女孩，还包括第三性别这个类。 这里还引出了JAVA特性中的继承。继承简单理解就是父类有的东西(访问级别不能是private)的，那都是你的。比如你老爸的房子，就是属于你的，你出入自由。 人还可以分为胖人和瘦人这个子类。所以只要是抽象的模板，就是一个类。 对象：对象是类的一个实例（对象不是找个女朋友），有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。 类：类是一个模板，它描述一类对象的行为和状态。 下面就拿狗这个类来说事。狗是动物这个类的子类。 Java中创建类 构造器方法说明 需要创造一个类对象出来的时候，要用到这个类的构造器方法，那么啥是构造器方法呢？构造器方法就是创造类时的初始化方法，和类同名的方法，你可以在里面写自己的代码 1234567891011121314151617181920//模版class 类名称 &#123; 访问权限 构造方法名称()&#123; &#125;&#125;//例子public class Dog&#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125;&#125; 一个相对比较完整的类 1234567891011121314151617181920212223242526272829//模版class 类名称 &#123; //构造器方法 //声明成员变量---这个变量属于这个类 //声明成员方法 //在方法里面定义的变量是局部变量，区别于成员变量&#125;//例子public class Dog &#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125; //狗的颜色--成员属性 public String color;//一般是private，赋值用set方法，取值用get方法，这里只是演示 //狗的行为，它会叫---成员方法 private void say()&#123; int i = 0;//局部变量 System.out.println("我会叫：汪汪汪~"); &#125;&#125; 创建对象 语法： 类名 对象名 = new 类名() ; 举例： 12Dog fourcolor ; // 先声明一个 Dog 类的对象 fourcolorfourcolor = new Dog(&quot;fourcolor&quot;) ; // 用 new 关键字实例化 Dog 的对象 fourcolor,此时调用构造方法二 通过Dog这个类可以创造出fourcolor对象.下面我才能操作这个对象： 1234//让它的颜色为黑色fourcolor.color = &quot;black&quot;;//让它叫fourcolor.say(); 面向对象 在理解了什么是类，什么是对象，就可以来说说面向对象到底是什么了。 先来说说面向过程，大家都学习过C语言。C语言就是典型的面向过程的语言。 举个例子：要把大象装进冰箱里，这件事，面向过程的程序员是这样思考的： 把冰箱门儿打开。 把大象装进去。 把冰箱门儿关上。 上面的每一件事都用一个函数来实现。抽象为下面三个函数： openTheDoor()； pushElephant()； closeTheDoor()； 这样不挺好的吗？为什么不用面向过程的这种思维来编程呢，还要搞出什么面向对象来。 需求又来啦： 「我要把大象装微波炉里」 「我要把狮子也装冰箱里」 「我要把大象装冰箱，但是门别关，敞着就行」 这个时候，面向过程的程序员就悲剧了，来一个需求我就写一个函数，我还能下班吗？ 面向对象从另一个角度来解决这个问题。它抛弃了函数，把「对象」作为程序的基本单元。 面向对象的世界里，到处都是对象。即：万物皆对象。 比如人这个类，每个具体的人(对象)都要有这样的属性：身高、体重、年龄。每个人都有这样的行为：吃饭、睡觉、上厕所。 那么，这些通用的属性+方法可以构建一个模板：人这个类。因为每个具体的人（对象）都需要这些基本的东西。当然了，每个人具体什么身高、什么体重、吃什么都是不一样的，所以每个对象一般都是不一样的。但是模板是一样的。 那么，回到刚才的需求，面向对象是如何思考这件事的呢？ 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 就是说，我不用亲自管开门的细节，我只要叫他开门即可。 我们创建的对象，应该是刚刚好能做完它能做的事情，不多做，不少做。多做了容易耦合，各种功能杂糅在一个对象里。比如我有一个对象叫「汽车」，可以「行驶」，可以「载人」，现在的需求是要实现「载人飞行」，就不能重用这个对象，必须新定义一个对象「飞机」来做。如果你给「汽车」插上了翅膀，赋予了它「飞行」的能力，那么新来的同学面对你的代码就会莫名其妙，无从下手。 但是不禁要问：怎么实现这种下达命令就可以自动去执行的效果呢？或者说，我怎么知道它有这个功能啊！ 面向接口编程 现在我们把「数据」和「行为」都封装到了对象里，相当于对象成了一个黑匣子，那我们怎么知道对象具有什么样的能力呢？这个问题的关键就是接口。 因为无论是把大象装进洗衣机还是冰箱，都要求洗衣机或者冰箱有开门和关门的功能。这个时候，我们就可以抽象出来一个接口：【自动门】。这个接口里面定义两个能力：【开门】和【关门】。 让洗衣机、冰箱、微波炉这些带门的东西全部实现【自动门】接口。 这个时候，每个具体的实现可能略有不同，比如冰箱开门是往外拽，但是洗衣机开门可能是往上翻盖子。 此时，我有一个需求，把大象放进冰箱。我一看，冰箱实现了【自动门】这个接口，里面有【开门】和【关门】两个方法，ok，我知道冰箱是可以开门和关门了，那就好办了。我直接下达命令即可。还是跟上面一样的步骤. 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，需要将狮子也装冰箱里。那还是一样： 向冰箱下达「开门」的命令。 向狮子下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，我要把大象装冰箱，但是门别关，敞着就行，那就： 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 是不是很方便？冰箱也可以换，我可以换成任何东西，只要实现了这个接口，这些东西就都有这些能力，那我才不管里面到底怎么实现的呢，直接下达【开门】【关门】命令即可。 这也引入了JAVA特性中另一个特性：封装。外界不知道里面实现细节，只需要知道它的功能和入参即可。 这就是面向过程和面向对象编程的区别，也顺带地理解了什么是面向接口编程。这是学习JAVA最基础也是最核心的点。 整理自： https://tryenough.com/java05 http://www.woshipm.com/pmd/294180.html]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串核心一网打尽]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%B8%E5%BF%83%E4%B8%80%E7%BD%91%E6%89%93%E5%B0%BD%2F</url>
    <content type="text"><![CDATA[对字符串中最核心的点：对象创建和动态加入常量池这些点进行深入分析。 比如有两个面试题： Q1：String s = new String(&quot;abc&quot;); 定义了几个对象。 Q2：如何理解String的intern方法？ A1：对于通过 new 产生的对象，会先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象，然后在堆中创建一个常量池中此 “abc” 对象的拷贝对象。所以答案是：一个或两个。如果常量池中原来没有 ”abc”, 就是两个。如果原来的常量池中存在“abc”时，就是一个。 A2：当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； 字面量和运行时常量池 JVM为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化。为了减少在JVM中创建的字符串的数量，字符串类维护了一个字符串常量池。 在JVM运行时区域的方法区中，有一块区域是运行时常量池，主要用来存储编译期生成的各种字面量和符号引用。 了解过JVM就会知道，在java代码被javac编译之后，文件结构中是包含一部分Constant pool的。比如以下代码： 123public static void main(String[] args) &#123; String s = "abc";&#125; 经过编译后，常量池内容如下： 1234567891011Constant pool: #1 = Methodref #4.#20 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #21 // abc #3 = Class #22 // StringDemo #4 = Class #23 // java/lang/Object ... #16 = Utf8 s .. #21 = Utf8 abc #22 = Utf8 StringDemo #23 = Utf8 java/lang/Object 上面的Class文件中的常量池中，比较重要的几个内容： 123#16 = Utf8 s#21 = Utf8 abc#22 = Utf8 StringDemo 上面几个常量中，s就是前面提到的符号引用，而abc就是前面提到的字面量。而Class文件中的常量池部分的内容，会在运行期被运行时常量池加载进去。 new String创建了几个对象 下面，我们可以来分析下String s = new String(&quot;abc&quot;);创建对象情况了。 这段代码中，我们可以知道的是，在编译期，符号引用s和字面量abc会被加入到Class文件的常量池中。由于是new的方式，在类加载期间，先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象。 在运行期间，在堆中创建一个常量池中此 “abc” 对象的拷贝对象。 运行时常量池的动态扩展 编译期生成的各种字面量和符号引用是运行时常量池中比较重要的一部分来源，但是并不是全部。那么还有一种情况，可以在运行期像运行时常量池中增加常量。那就是String的intern方法。 当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； intern()有两个作用，第一个是将字符串字面量放入常量池（如果池没有的话），第二个是返回这个常量的引用。 一个例子： 123456789String s1 = "hello world";String s2 = new String("hello world");System.out.println("s==s1:"+(s==s1));String s3 = new String("hello world").intern();System.out.println("s==s2:"+(s==s2)); 运行结果是： 12s1==s2:falses2==s3:true 你可以简单的理解为String s1 = &quot;hello world&quot;;和String s3 = new String(&quot;hello world&quot;).intern();做的事情是一样的（但实际有些区别，这里暂不展开）。都是定义一个字符串对象，然后将其字符串字面量保存在常量池中，并把这个字面量的引用返回给定义好的对象引用。 对于String s3 = new String(&quot;hello world&quot;).intern();，在不调intern情况，s3指向的是JVM在堆中创建的那个对象的引用的（如s2）。但是当执行了intern方法时，s3将指向字符串常量池中的那个字符串常量。 由于s1和s3都是字符串常量池中的字面量的引用，所以s1==s3。但是，s2的引用是堆中的对象，所以s2!=s1。 intern的正确用法 不知道，你有没有发现，在String s3 = new String(&quot;abc&quot;).intern();中，其实intern是多余的？ 因为就算不用intern，“abc&quot;作为一个字面量也会被加载到Class文件的常量池”&quot;，进而加入到运行时常量池中，为啥还要多此一举呢？到底什么场景下才会用到intern呢? 在解释这个之前，我们先来看下以下代码： 1234String s1 = "hello";String s2 = "world";String s3 = s1 + s2;String s4 = "hello" + "world"; 在经过反编译后，得到代码如下： 1234String s1 = "hello";String s2 = "world";String s3 = (new StringBuilder()).append(s1).append(s2).toString();String s4 = "helloworld"; 这就是阿里巴巴文档里为什么规定循环拼接字符串不准使用&quot;+&quot;而必须使用StringBuilder，因为反编译出的字节码文件显示每次循环都会 new 出一个 StringBuilder 对象，然后进行append 操作，最后通过 toString 方法返回 String 对象，造成内存资源浪费。 不恰当的方式形如： 1234String str = "start";for (int i = 0; i &lt; 100; i++) &#123; str = str + "hello";&#125; 好了，言归正传，可以发现，同样是字符串拼接，s3和s4在经过编译器编译后的实现方式并不一样。s3被转化成StringBuilder及append，而s4被直接拼接成新的字符串。 如果你感兴趣，你还能发现，String s4 = s1 + s2; 经过编译之后，常量池中是有两个字符串常量的分别是 hello、world（其实hello和world是String s1 = &quot;hello&quot;;和String s2 = &quot;world&quot;;定义出来的），拼接结果helloworld并不在常量池中。 如果代码只有String s4 = &quot;hello&quot; + &quot;world&quot;;，那么常量池中将只有helloworld而没有hello和 world。 究其原因，是因为常量池要保存的是已确定的字面量值。也就是说，对于字符串的拼接，纯字面量和字面量的拼接，会把拼接结果作为常量保存到字符串。 如果在字符串拼接中，有一个参数是非字面量，而是一个变量的话，整个拼接操作会被编译成StringBuilder.append，这种情况编译器是无法知道其确定值的。只有在运行期才能确定。 那么，有了这个特性了，intern就有用武之地了。那就是很多时候，我们在程序中用到的字符串是只有在运行期才能确定的，在编译期是无法确定的，那么也就没办法在编译期被加入到常量池中。 这时候，对于那种可能经常使用的字符串，使用intern进行定义，每次JVM运行到这段代码的时候，就会直接把常量池中该字面值的引用返回，这样就可以减少大量字符串对象的创建了。 总结 第一种情况： 12String str1 = "abc"; System.out.println(str1 == "abc"); 栈中开辟一块空间存放引用str1； String池中开辟一块空间，存放String常量&quot;abc&quot;； 引用str1指向池中String常量&quot;abc&quot;； str1所指代的地址即常量&quot;abc&quot;所在地址，输出为true 第二种情况： 12String str2 = new String("abc"); System.out.println(str2 == "abc"); 栈中开辟一块空间存放引用str2； 堆中开辟一块空间存放一个新建的String对象&quot;abc&quot;； 引用str2指向堆中的新建的String对象&quot;abc&quot;； str2所指代的对象地址为堆中地址，而常量&quot;abc&quot;地址在池中，输出为false； 第三、四种情况 1234567891011121314//（3）String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//str1 和 str2 是字符串常量，所以在编译期就确定了。//str3 中有个 str1 是引用，所以不会在编译期确定。//又因为String是 final 类型的，所以在 str1 + "b" 的时候实际上是创建了一个新的对象，在把新对象的引用传给str3。//（4）final String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//这里和(3)的不同就是给 str1 加上了一个final，这样str1就变成了一个常量。//这样 str3 就可以在编译期中就确定了 这里的细节在上面已经详细说明了。 第五种情况 1234String str1 = "ab"；String str2 = new String("ab");System.out.println(str1== str2);//falseSystem.out.println(str2.intern() == str1);//true 整理自： 我终于搞清楚了和String有关的那点事儿 https://www.jianshu.com/p/2624036c9daa]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[关于java字符串不可变特性的深入理解。 什么是不可变对象？ 众所周知， 在Java中， String类是不可变的。那么到底什么是不可变的对象呢？ 可以这样认为：如果一个对象，在它创建完成之后，不能再改变它的状态，那么这个对象就是不可变的。不能改变状态的意思是，不能改变对象内的成员变量，包括基本数据类型的值不能改变，引用类型的变量不能指向其他的对象，引用类型指向的对象的状态也不能改变。 区分对象和对象的引用 对于Java初学者， 对于String是不可变对象总是存有疑惑。看下面代码： 12345String s = "ABCabc"; System.out.println("s = " + s); s = "123456"; System.out.println("s = " + s); 打印结果: 12s = ABCabcs = 123456 首先创建一个 String 对象 s ，然后让 s 的值为 ABCabc ， 然后又让 s 的值为 123456 。 从打印结果可以看出，s 的值确实改变了。那么怎么还说 String 对象是不可变的呢？ 其实这里存在一个误区：s只是一个String对象的引用，并不是对象本身。 对象在内存中是一块内存区，成员变量越多，这块内存区占的空间越大。 引用只是一个4字节的数据，里面存放了它所指向的对象的地址，通过这个地址可以访问对象。 也就是说，s 只是一个引用，它指向了一个具体的对象，当 s=“123456”; 这句代码执行过之后，又创建了一个新的对象“123456”， 而引用s重新指向了这个新的对象，原来的对象“ABCabc”还在内存中存在，并没有改变。内存结构如下图所示： 为什么String对象是不可变的？ 要理解 String 的不可变性，首先看一下 String 类中都有哪些成员变量。 在JDK1.6中，String 的成员变量有以下几个： 1234567891011121314public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 在JDK1.7和1.8中，String 类做了一些改动，主要是改变了substring方法执行时的行为，这和本文的主题不相关。JDK1.7中 String 类的主要成员变量就剩下了两个： 1234567public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 由以上的代码可以看出， 在Java中 String 类其实就是对字符数组的封装。 JDK6中， value是String封装的数组，offset是String在这个value数组中的起始位置，count是String所占的字符的个数。 在JDK7中，只有一个value变量，也就是value中的所有字符都是属于String这个对象的。这个改变不影响本文的讨论。 除此之外还有一个hash成员变量，是该 String 对象的哈希值的缓存，这个成员变量也和本文的讨论无关。在Java中，数组也是对象。 所以value也只是一个引用，它指向一个真正的数组对象。其实执行了 1String s = “ABCabc"; 这句代码之后，真正的内存布局应该是这样的： value，offset和count这三个变量都是private的，并且没有提供setValue， setOffset和setCount等公共方法来修改这些值，所以在String类的外部无法修改String。也就是说一旦初始化就不能修改， 并且在String类的外部不能访问这三个成员。 此外，value，offset和count这三个变量都是final的， 也就是说在 String 类内部，一旦这三个值初始化了， 也不能被改变。所以可以认为 String 对象是不可变的了。 那么在 String 中，明明存在一些方法，调用他们可以得到改变后的值。这些方法包括substring， replace， replaceAll， toLowerCase等。例如如下代码： 1234String a = "ABCabc"; System.out.println("a = " + a); //ABCabca = a.replace('A', 'a'); System.out.println("a = " + a); //aBCabc 那么a的值看似改变了，其实也是同样的误区。再次说明， a只是一个引用， 不是真正的字符串对象，在调用a.replace('A', 'a')时， 方法内部创建了一个新的String对象，并把这个心的对象重新赋给了引用a。String中replace方法的源码可以说明问题： 1234567891011121314151617181920212223242526public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(buf, true);//new出了新的String对象 &#125; &#125; return this;&#125; String对象真的不可变吗？ 从上文可知String的成员变量是private final 的，也就是初始化之后不可改变。那么在这几个成员中， value比较特殊，因为他是一个引用变量，而不是真正的对象。 value是final修饰的，也就是说final不能再指向其他数组对象，那么我能改变value指向的数组吗？ 比如将数组中的某个位置上的字符变为下划线“_”。 至少在我们自己写的普通代码中不能够做到，因为我们根本不能够访问到这个value引用，更不能通过这个引用去修改数组。 那么用什么方式可以访问私有成员呢？ 没错，用反射， 可以反射出String对象中的value属性， 进而改变通过获得的value引用改变数组的结构。下面是实例代码： 123456789101112131415161718192021public static void testReflection() throws Exception &#123; //创建字符串"Hello World"， 并赋给引用s String s = "Hello World"; System.out.println("s = " + s); //Hello World //获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField("value"); //改变value属性的访问权限 valueFieldOfString.setAccessible(true); //获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); //改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println("s = " + s); //Hello_World &#125; 在这个过程中，s始终引用的同一个 String 对象，但是再反射前后，这个 String 对象发生了变化， 也就是说，通过反射是可以修改所谓的“不可变”对象的。但是一般我们不这么做。 这个反射的实例还可以说明一个问题：如果一个对象，他组合的其他对象的状态是可以改变的，那么这个对象很可能不是不可变对象。例如一个Car对象，它组合了一个Wheel对象，虽然这个Wheel对象声明成了private final 的，但是这个Wheel对象内部的状态可以改变， 那么就不能很好的保证Car对象不可变。 参考： https://blog.csdn.net/zhangjg_blog/article/details/18319521]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer拆箱和装箱]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2FInteger%E6%8B%86%E7%AE%B1%E5%92%8C%E8%A3%85%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[由于笔试经常遇到，所以这里整理一下。将Integer这一块一网打尽。 拆箱和装箱 这里以面试笔试经常出现的Integer类型为例，请看下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; /*第一组*/ Integer i = new Integer(128); Integer i2 = 128; System.out.println(i == i2); Integer i3 = new Integer(127); Integer i4 = 127; System.out.println(i3 == i4); /*第二组*/ Integer i5 = 128; Integer i6 = 128; System.out.println(i5 == i6); Integer i7 = 127; Integer i8 = 127; System.out.println(i7 == i8); /*第三组*/ Integer i9 = new Integer(128); int i10 = 128; System.out.println(i9 == i10); Integer i11 = new Integer(127); int i12 = 127; System.out.println(i11== i12); /*第四组*/ Integer i13 = new Integer(128); Integer i14 = Integer.valueOf(128); System.out.println(i13 == i14); Integer i15 = new Integer(127); Integer i16 = Integer.valueOf(127); System.out.println(i13 == i14); /*第五组*/ Integer i17 = Integer.valueOf(128); Integer i18 = 128; System.out.println(i17 == i18); Integer i19 = Integer.valueOf(127); Integer i20 = 127; System.out.println(i19 == i20);&#125; 执行结果为： 1234567891011121314falsefalsefalsetruetruetruefalsefalsefalsetrue 翻开源码(jdk8)，我们可以看到一个私有的静态类，叫做整形缓存。顾名思义，就是缓存某些整型值，我们可以看到，它默认将-127-128之间数字封装成对象，放进一个常量池中，以后定义类似于Integer a = 1里面的a就可以直接从这个常量池中取对象即可，不需要重新new 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 对于Integer.valueOf(): 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 我们可以看到，也是先看看是不是在-127到128之间的范围，是的话就从 cache 中取出相应的 Integer 对象即可。 第一组中 第一种情况，i 是创建的一个Integer的对象，取值是128。i2 是进行自动装箱的实例，因为这里超出了-128到127的范围，所以是创建了新的Integer对象。由于==比较的是地址，所以两者必然不一样。 第二种情况就不一样了，i4是不需要自己new，而是可以直接从缓存中取，但是i3是new出来的，地址还是不一样。 第二组中 第一种情况是都超出范围了，所以都要自己分别去new，所以不一样 第二种情况是在范围内，都去缓存中取，实际上都指向同一个对象，所以一样 第三组中 i10和i12都是int型，i9和i11与它们比较的时候都要自动拆箱，所以比较的是数值，所以都一样 第四组中 与第一组原理一样 四五组中 与第二组原理一样 所以啊，new Integer()是每次都直接new对象出来，而Integer.valueOf()可能会用到缓存，所以后者效率高一点。 总结 int 和 Integer 在进行比较的时候， Integer 会进行拆箱，转为 int 值与int` 进行比较。 Integer 与 Integer 比较的时候，由于直接赋值的时候会进行自动的装箱，那么这里就需要注意两个问题，一个是 -128&lt;= x&lt;=127 的整数，将会直接缓存在 IntegerCache 中，那么当赋值在这个区间的时候，不会创建新的 Integer 对象，而是从缓存中获取已经创建好的 Integer 对象。二：当大于这个范围的时候，直接 new Integer 来创建 Integer 对象。 new Integer(1) 和 Integer a = 1 不同，前者会创建对象，存储在堆中，而后者因为在-128到127的范围内，不会创建新的对象，而是从 IntegerCache 中获取的。那么 Integer a = 128, 大于该范围的话才会直接通过 new Integer(128)创建对象，进行装箱。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务解决方案思考]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F10%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[在分布式系统中，最头疼的就是分布式事务问题，处理起来一定要小心翼翼。由于没有此方面实战，本文就从理论上看看比较好的分布式事务处理方案。 什么是分布式事务 众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？ 比如用户下单过程。当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 方案1：基于可靠消息服务的分布式事务 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程中，如果任务A处理失败，那么需要进入回滚流程: 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 注意，这个方案需要消息队列具有事务消息的能力，阿里的RocketMQ可以实现这个目标。其他的MQ还不行。 方案2：最大努力通知（定期校对） 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。 如果这两步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第一种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。还有其他的一些解决思路，这里就暂时只描述这些。后续再学习。 参考自：https://juejin.im/post/5aa3c7736fb9a028bb189bca]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[库存扣减问题]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F09%E5%BA%93%E5%AD%98%E6%89%A3%E5%87%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[库存扣减问题一直是秒杀中最关键的一个点。如果把控不好，扣成负数，那可就麻烦了，如么如何保证不会出现超卖还能保证性能呢？ 一、扣减库存问题分析 在提交订单的时候，要扣减库存，对于sql，是这么写的： 1update t_stcok set stock = stock-2 where sku_id = 1 首先这条sql存在超卖问题，很有可能会减成负数。可能会改成如下： 1update t_stcok set stock = stock-2 where sku_id = 1 and stock &gt; 2 这样好像解决了超卖问题。但是引入了新的问题。由于库存牵涉进货、补货等系统，所以是个独立的服务。 并且，比如我是通过MQ去通知库存进行扣减库存，但是由于网络抖动，请求扣减库存没有结果，这个时候可能需要进行重试。重试之后，可能成功了，这个时候，有可能这两次都成功了。那么，一个用户买一样东西，但是库存扣了两遍。这就是幂等。如果不做幂等处理，重试会出现上述这种致命问题。 那么如何做到幂等呢？ 实际上就是追求数据一致性。那么就可以考虑锁来保证，比如我这里用乐观锁来实现： 123select stock,version from t_stock;if(stock &gt; 用户购买数量) update t_stcok set stock = stock-2 where sku_id = 1 and version = last_version 但是，一旦出现并发，那么可能这个用户是执行update失败的，所以还需要去重试(guava retry或者spring retry都可以优雅地实现重试)，直到成功或者库存已经不足。 那么，在少量并发的情况下，可以考虑乐观锁，要不然会大量失败，此时需要用悲观锁： 12select * from t_stock for update;下面执行update操作。。。 在一个事务内，第一句为select for update，那么这一行数据就会被本线程锁住，整个事务执行完才能允许其他线程进来。 存在的问题：一个线程锁住这行数据，那么其他线程都要等待，效率很低。 那么，如何保证数据一致性，还可以提高效率呢？ 对于扣减库存，往往是先在redis中进行扣减库存。redis是单线程，是高速串行执行，不存在并发问题。 如果是单机redis，可以在同一个事务中保证一次性执行: 12345watch stockmultiif stock &gt; count stock = stock - count;exec 但是不能在集群中用（分布在不同节点上时），所以用watch不通用。 redis都是原子操作，比如自增:incrby，用这个就可以判断库存是否够。就是所谓的redis预减库存。 但是在实际中，库存表里有两个字段：库存和锁定库存。 锁定库存是表示多少用户真正下单了，但是还没有支付。锁定库存+库存=总库存，等用户真正支付之后，就可以将锁定库存减掉。那么，此时，redis中需要存库存和锁定库存这两个值，上面单一的原子操作就不行了。 解决方案：redis+lua 为什么要用lua呢？可以用lua将一系列操作封装起来执行，输入自己的参数即可。lua脚本在redis中执行是串行的、原子性的。 OK，下面就实战一波：根据skuId查询缓存中的库存值。 二、查询库存（设置库存） 首先，我们要明确一点，redis中的库存初始值是由后台的系统人工提前配置好的，在进行商品销售时（用户下单时），直接从redis中先进行库存的扣减。 这里呢，我们没有进行初始化，而是在程序中进行判断：如果redis已经有了这个库存值，就将他查询出来返回；否则，就去数据库查询，然后对redis进行初始化。 这里的一个问题是：如果存在并发问题，但是我们初始化两个值（库存值和库存锁定值），这里采用lua脚本，在lua脚本中完成初始化，并且对于两个用户同时进行初始化库存的问题，可以在lua中进行判断,因为redis是单线程，lua也是单线程，不用担心会同时初始化两次。 下面首先写一个接口，根据skuid查询库存(库存和锁定库存)： 123456789101112@RequestMapping("/query/&#123;skuId&#125;")public ApiResult&lt;Stock&gt; queryStock(@PathVariable long skuId)&#123; ApiResult&lt;Stock&gt; result = new ApiResult(Constants.RESP_STATUS_OK,"库存查询成功"); Stock stock = new Stock(); stock.setSkuId(skuId); int stockCount = stockService.queryStock(skuId); stock.setStock(stockCount); result.setData(stock); return result;&#125; service层： 123456789101112131415161718192021222324252627282930@Overridepublic int queryStock(long skuId) &#123; //先查redis Stock stock ; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+skuId; String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+skuId; //只需要查询一个即可，比如我这里只查询库存就行 Object stockObj = redisTemplate.opsForValue().get(stockKey); Integer stockInRedis = null ; if(stockObj!=null)&#123; stockInRedis = Integer.valueOf(stockObj.toString()); &#125; //没有，那么我就需要将数据库中的数据初始化到redis中 if(stockInRedis==null)&#123; //去数据库查询 然后对redis进行初始化 stock = stockMapper.selectBySkuId(skuId); //两个key和两个库存值通过lua脚本塞到redis中 //这里如果发生两个用户并发初始化redis，脚本中会进行判断，如果已经初始化了，脚本就会停止执行 // 设置库存不应该在这配置，应该是后台管理系统进行设置，所以正常情况下，这里redis中应该是必然存在的 //如果是在后台配置，就没有必要这么复杂了 redisUtils.skuStockInit(stockKey,stockLockKey,stock.getStock().toString(),stock.getLockStock().toString()); &#125;else&#123; return stockInRedis;//缓存中有就直接返回 &#125; //缓存结果可能会返回设置不成功，所以还是返回数据库查询结果 return stock.getStock();&#125; 那么这个工具类为： 123456789101112131415161718192021222324252627282930313233/** * 查看redis是否已经初始化好库存初始值，没有就初始化 */public static final String STOCK_CACHE_LUA = "local stock = KEYS[1] " + "local stock_lock = KEYS[2] " + "local stock_val = tonumber(ARGV[1]) " + "local stock_lock_val = tonumber(ARGV[2]) " + "local is_exists = redis.call(\"EXISTS\", stock) " + "if is_exists == 1 then " + " return 0 " + "else " + " redis.call(\"SET\", stock, stock_val) " + " redis.call(\"SET\", stock_lock, stock_lock_val) " + " return 1 " + "end"; /** * @Description 缓存sku库存 以及锁定库存 */public boolean skuStockInit(String stockKey,String stockLockKey,String stock,String stockLock)&#123; //用jedis去执行lua脚本 输入的参数要注意顺序 都是写死的 第一组是key，第二组是stock Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_CACHE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stock, stockLock))); &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 对于lua脚本进行稍微的解释一下： 12345678910111213141516171819202122//第一组数据是key数组；第二组数据是args数组，是与key数组对应的值，就是库存//我们这里第一组为[stockKey,stockLockKey],就是存在redis中的名字，这里是在service层中定义好了//第二组为[50,0]，这个值就是可以从数据库表t_stock中查询出来的//因为执行这段lua脚本的话，说明redis中没有缓存的数据，所以需要先查询数据库，然后将缓存设置好//lua中定义变量用locallocal stock = KEYS[1]local stock_lock = KEYS[2]local stock_val = tonumber(ARGV[1])local stock_lock_val = tonumber(ARGV[2])//再查询一遍缓存是否存在，防止两个线程同时进来设置缓存//存在就不用设置缓存了，否则就设置缓存local is_exists = redis.call("EXISTS", stock)if is_exists == 1 then return 0else redis.call("SET", stock, stock_val) redis.call("SET", stock_lock, stock_lock_val) return 1end 那么，启动工程mama-buy-stock：假如我去查询skuId=1的商品： 第一次库存不存在，那么就会去查询数据库： 我们再来看看redis中的数据： 三、扣减库存 下面来看看扣减库存是如何实现的。因为提交订单后，往往是不止一件商品的，往往购物车内有很多件商品，同时过来，假设有五件商品，但是其中只有一件暂时没有库存了，那么我还是希望其他的四件商品能够卖出去，只是没有库存的商品就不算钱了。所以扣减库存用一个map来装，即Map&lt;skuId,count&gt; controller层： 1234567@RequestMapping("/reduce")public ApiResult&lt;Map&lt;Long,Integer&gt;&gt; reduceStock(@RequestBody List&lt;StockReduce&gt; stockReduceList)&#123; ApiResult result = new ApiResult(Constants.RESP_STATUS_OK,"库存扣减成功"); Map&lt;Long,Integer&gt; resultMap = stockService.reduceStock(stockReduceList); result.setData(resultMap); return result;&#125; service层： 1234567891011121314151617181920212223242526272829303132333435@Override@Transactionalpublic Map&lt;Long, Integer&gt; reduceStock(List&lt;StockReduce&gt; stockReduceList) &#123; Map&lt;Long, Integer&gt; resultMap = new HashMap&lt;&gt;(); //遍历去减redis中库存，增加锁定库存 stockReduceList.stream().forEach(param -&gt; &#123; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+param.getSkuId(); String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+param.getSkuId(); Object result = redisUtils.reduceStock(stockKey, stockLockKey, param.getReduceCount().toString(),//incrby一个负数，就是减 String.valueOf(Math.abs(param.getReduceCount())));//incrby一个正数，就是加 if(result instanceof Long)&#123; //库存不存在或者不足 扣减失败 sku下单失败 记录下来 resultMap.put(param.getSkuId(),-1); &#125;else if (result instanceof List)&#123; //扣减成功 记录扣减流水 List resultList = ((List) result); int stockAftChange = ((Long)resultList.get(0)).intValue(); int stockLockAftChange = ((Long) resultList.get(1)).intValue(); StockFlow stockFlow = new StockFlow(); stockFlow.setOrderNo(param.getOrderNo()); stockFlow.setSkuId(param.getSkuId()); stockFlow.setLockStockAfter(stockLockAftChange); stockFlow.setLockStockBefore(stockLockAftChange+param.getReduceCount()); stockFlow.setLockStockChange(Math.abs(param.getReduceCount())); stockFlow.setStockAfter(stockAftChange); stockFlow.setStockBefore(stockAftChange+Math.abs(param.getReduceCount())); stockFlow.setStockChange(param.getReduceCount()); stockFlowMapper.insertSelective(stockFlow); resultMap.put(param.getSkuId(),1); &#125; &#125;); return resultMap;&#125; 对于redis的操作，基本与上一致： 12345678910111213141516171819202122232425262728293031323334/** * @Description 扣减库存lua脚本 * @Return 0 key不存在 错误 -1 库存不足 返回list 扣减成功 */public static final String STOCK_REDUCE_LUA= "local stock = KEYS[1]\n" + "local stock_lock = KEYS[2]\n" + "local stock_change = tonumber(ARGV[1])\n" + "local stock_lock_change = tonumber(ARGV[2])\n" + "local is_exists = redis.call(\"EXISTS\", stock)\n" + "if is_exists == 1 then\n" + " local stockAftChange = redis.call(\"INCRBY\", stock,stock_change)\n" + " if(stockAftChange&lt;0) then\n" + " redis.call(\"DECRBY\", stock,stock_change)\n" + " return -1\n" + " else \n" + " local stockLockAftChange = redis.call(\"INCRBY\", stock_lock,stock_lock_change)\n" + " return &#123;stockAftChange,stockLockAftChange&#125;\n" + " end " + "else \n" + " return 0\n" + "end"; public Object reduceStock(String stockKey,String stockLockKey,String stockChange,String stockLockChange)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_REDUCE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stockChange, stockLockChange))); &#125;); return result;&#125; 此时，一旦数据库发生异常，那么就会回滚，但是redis中是无法回滚的。这个问题不用担心，因为数据库发生异常是及其严重的问题，是很少会发生的，一旦发生，只需要去这个流水的表中去查看情况，然后去执行脚本去初始化这个redis即可。所以是可以补救的。 但是接口的幂等性还没有做。重复尝试调用这个接口（通常是发生在MQ的失败重传机制，客户端的连续点击一般是可以避免的），可能会重复减redis库存并且重复地去插入流水记录。这个问题该如何解决呢？ 四、redis分布式锁来实现幂等性 主流的方案，比如有用一张表来控制，比如以这个orderID为唯一主键，一旦插入成功，就可以根据这个唯一主键的存在与否判断是否为重复请求（也就是说，这里的扣减库存和插入去重表放在一个事务里，去重表中有一个字段为orderId，全局唯一不重复，用唯一索引进行约束，那么插入的时候判断这个去重表是否可以插入成功，如果不成功，那么数据库操作全部回滚）。 可以用redis分布式锁给这个订单上锁。以订单id为锁，不会影响其他线程来扣减库存，所以不影响性能。 针对这个订单，第一次肯定是可以去扣减库存的，但是第二次再接收到这个请求，那么就要返回已经成功了，不要再重复扣减。 对于reduceStock()这个方法最前面增加锁： 123456789//防止扣减库存时MQ正常重试时的不幂等//以订单ID 加个缓存锁 防止程序短时间重试 重复扣减库存 不用解锁 自己超时Long orderNo = stockReduceList.get(0).getOrderNo();boolean lockResult = redisUtils.distributeLock(Constants.ORDER_RETRY_LOCK+orderNo.toString(),orderNo.toString(),300000);if(!lockResult)&#123; //锁定失败 重复提交 返回一个空map return Collections.EMPTY_MAP;&#125;... 123456789101112131415161718192021222324252627282930313233343536373839404142private static final String LOCK_SUCCESS = "OK";private static final String SET_IF_NOT_EXIST = "NX";private static final String SET_WITH_EXPIRE_TIME = "PX";private static final Long EXCUTE_SUCCESS = 1L;/**lua脚本 在redis中 lua脚本执行是串行的 原子的 */private static final String UNLOCK_LUA= "if redis.call('get', KEYS[1]) == ARGV[1] then " + " return redis.call('del', KEYS[1]) " + "else " + " return 0 " + "end";/** * @Description 获取分布式锁 * @Return boolean */public boolean distributeLock(String lockKey, String requestId, int expireTime)&#123; String result = redisTemplate.execute((RedisCallback&lt;String&gt;) redisConnection -&gt; &#123; JedisCommands commands = (JedisCommands)redisConnection.getNativeConnection(); return commands.set(lockKey,requestId,SET_IF_NOT_EXIST,SET_WITH_EXPIRE_TIME,expireTime);//一条命令实现setnx和setexpire这些操作，原子性 &#125;); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125;/** * @Description 释放分布式锁 * @Return boolean */public boolean releaseDistributelock(String lockKey, String requestId)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(UNLOCK_LUA, Collections.singletonList(lockKey), Collections.singletonList(requestId));//lua脚本中原子性实现：get查询和delete删除这两个操作 &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 注意，这里不需要我们主动去释放分布式锁，只要设置一个大于重试时间的过期时间即可。让它自己删除。 注意redis在集群下做分布式锁，最好要用Redission。这里如果用于集群，如何lua脚本在一个事务里同时操作多个key的时候，如果要保证这个事务生效，就需要保证这几个key都要在同一个节点上。但是，比如我们这里的两个key： 12public static final String CACHE_PRODUCT_STOCK = "product:stock";public static final String CACHE_PRODUCT_STOCK_LOCK = "product:stock:lock"; 因为我们这里要同时对库存和锁定库存这两个key进行操作，需要放在一个事务内执行，不处理的话，一旦他们不在一个节点，那么事务就不会生效，解决方案： 12public static final String CACHE_PRODUCT_STOCK = "&#123;product:stock&#125;";public static final String CACHE_PRODUCT_STOCK_LOCK = "&#123;product:stock&#125;:lock"; 如果加上花括号，那么在进行计算hash值的时候，他们两就会是一样的，会被投放到同一个slot中，自然就保证了在同一个节点上。 五、测试一下]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK平台搭建]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F08ELK%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[因为要完成产品的全文搜索这个功能，所以需要准备一下ES的环境。本节安装ELK。 ELK由Elasticsearch、Logstash和Kibana三部分组件组成。 前言 Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 简单来说，他是个全文搜索引擎，可以快速地储存、搜索和分析海量数据。 Logstash是一个完全开源的工具，它可以把分散的、多样化的日志日志，或者是其他数据源的数据信息进行收集、分析、处理，并将其存储供以后使用。 Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。 你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。 你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。 Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。 一、安装ES 1.1 首先是安装JDK： 12345cd /opt/wget --no-cookies --no-check-certificate --header &quot;Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie&quot; &quot;http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz&quot;tar xzf jdk-8u141-linux-x64.tar.gz 1.2 添加环境变量： 123456789101112vim /etc/profileJAVA_HOME=/opt/jdk1.8.0_141JAVA_JRE=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATHsource /etc/profilejava -version 1.3 下载6.2.4版本： 123456wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gztar -xzvf elasticsearch-6.2.4.tar.gztar -zxvf elasticsearch-6.2.4.tar.gzmv elasticsearch-6.2.4 elasticsearch 1.4 配置sysctl.conf 1234567891011#修改sysctl配置vim /etc/sysctl.conf #添加如下配置vm.max_map_count=262144 #让配置生效sysctl -p #查看配置的数目sysctl -a|grep vm.max_map_count 1.5 elasticsearch从5.0版本之后不允许root账户启动 1234567891011121314151617181920#添加用户adduser dev #设定密码passwd dev #添加权限chown -R dev /opt/elasticsearch #切换用户su dev #查看当前用户who am i #启动./elasticsearch/bin/elasticsearch #后台启动./elasticsearch/bin/elasticsearch -d 1.6 配置limits.conf 123456789101112131415vim /etc/security/limits.conf 把* soft nofile 65535* hard nofile 65535 改为* soft nofile 65536* hard nofile 65536 #切换用户su dev #查看配置是否生效ulimit -Hn 1.7 配置所有用户访问 1vim /opt/elasticsearch/config/elasticsearch.yml 1.8 添加一下内容 1network.host: 0.0.0.0 1.9 重启 12ps -ef | grep elastickill -9 xxxx 1.10 测试： 1curl http://localhost:9200/ 显示： 123456789101112131415&#123; &quot;name&quot; : &quot;MmiaBfA&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;zjX-q5PDRLyrWMy5TiBDkw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.2.4&quot;, &quot;build_hash&quot; : &quot;ccec39f&quot;, &quot;build_date&quot; : &quot;2018-04-12T20:37:28.497551Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.2.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 就说明成功了。 二、安装Kibana 6.2.4 1234567wget https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gztar -zxvf kibana-6.2.4-linux-x86_64.tar.gzmv kibana-6.2.4-linux-x86_64 kibanavim /opt/kibana/config/kibana.yml 2.1 添加以下内容： 123server.port: 5601server.host: &quot;0.0.0.0&quot;elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 2.2 切换到bin目录下，启动即可。 12345#不能关闭终端./kibana #可关闭终端nohup ./kibana &amp; 2.3 开放防火墙和安全组对应的这个端口 浏览器访问：http://106.14.163.235:5601 看到一个控制台页面就成功啦。 2.4 关闭这个进程 1234567891011121314ps -ef|grep kibana ps -ef|grep 5601 都找不到 尝试 使用 fuser -n tcp 5601 kill -9 端口 启动即可 ./kibana或者去这个目录下的.out日志中可以看到看到它占用的pid 三、logstash 1234567891011# 下载wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.4.tar.gz# 解压tar -zxvf logstash-6.2.4.tar.gz# 重命名mv logstash-6.2.4.tar.gz logstash# 进入cd logstash 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 新建一个配置文件 我这里是mysqltones.confinput &#123; stdin &#123; &#125; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/mama-buy-trade&quot; jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;root&quot; jdbc_driver_library =&gt; &quot;/opt/logstash/mysql-connector-java-5.1.46-bin.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; # mysql文件, 也可以直接写SQL语句在此处，如下： statement =&gt; &quot;SELECT * from t_product&quot; # statement_filepath =&gt; &quot;/opt/logstash/conf/jdbc.sql&quot; # 这里类似crontab,可以定制定时操作，比如每10分钟执行一次同步(分 时 天 月 年) schedule =&gt; &quot;*/10 * * * *&quot; type =&gt; &quot;jdbc&quot; # 是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中 record_last_run =&gt; &quot;true&quot; # 是否需要记录某个column 的值,如果record_last_run为真,可以自定义我们需要 track 的 column 名称，此时该参数就要为 true. 否则默认 track 的是 timestamp 的值. use_column_value =&gt; &quot;true&quot; # 如果 use_column_value 为真,需配置此参数. track 的数据库 column 名,该 column 必须是递增的. 一般是mysql主键 tracking_column =&gt; &quot;id&quot; last_run_metadata_path =&gt; &quot;/opt/logstash/conf/last_id&quot; # 是否清除 last_run_metadata_path 的记录,如果为真那么每次都相当于从头开始查询所有的数据库记录 clean_run =&gt; &quot;false&quot; # 是否将 字段(column) 名称转小写 lowercase_column_names =&gt; &quot;false&quot; &#125;&#125;# 此处我不做过滤处理filter &#123;&#125;output &#123; # 输出到elasticsearch的配置 elasticsearch &#123; hosts =&gt; [&quot;127.0.0.1:9200&quot;] index =&gt; &quot;jdbc&quot; # 将&quot;_id&quot;的值设为mysql的autoid字段 document_id =&gt; &quot;%&#123;id&#125;&quot; template_overwrite =&gt; true &#125; # 这里输出调试，正式运行时可以注释掉 stdout &#123; codec =&gt; json_lines &#125;&#125; 12# 启动./bin/logstash -f ./mysqltones.conf 看到这个就说明成功了： 安装mysql数据库 这一步要在执行logstash之前搞定，我的是阿里云centos7.3版本，mysql版本是5.7，安装过程如下： 123456789101112131415161718192021222324252627282930313233343536373839# 下载MySQL源安装包: wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm# 安装MySQL源：yum localinstall mysql57-community-release-el7-8.noarch.rpm # 检查MySQL源安装情况： yum repolist enabled | grep &quot;mysql.*-community.*&quot;# 安装MySQL: yum install mysql-community-server# 启动MySQL: systemctl start mysqld# 查看MySQL状态: systemctl status mysqld# 设置开机启动MySQL：systemctl enable mysqld systemctl daemon-reload# 查找并修改MySQL默认密码（注意密码要符合规范，否则会失败）：grep &apos;temporary password&apos; /var/log/mysqld.log mysql -uroot -p alter user root@localhost identified by &apos;你的新密码&apos;;# 远程连接测试添加远程账户：GRANT ALL PRIVILEGES ON *.* TO &apos;用户&apos;@&apos;%&apos; IDENTIFIED BY &apos;密码&apos; WITH GRANT OPTION;# 立即生效：flush privileges;# 退出MySQL：exit# 最后远程将数据给导入数据库 安装分词器 ik_max_word是分词比较细腻的一款，我们就用它来做分词，首先需要安装一下： 12345678# 直接安装./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.4/elasticsearch-analysis-ik-6.2.4.zip # 重新启动ESps -ef | grep elastickill -9 xxxxsu dev./bin/elasticsearch -d 对这个分词器在kibana中进行测试： 下面结合数据库模拟一下：]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Curator]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F07Curator%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[从技术角度出发，注册一个网站，再高并发的时候，有可能出现用户名重复这样的问题（虽然一般情况下不会出现这种问题），如何解决呢？ 从数据库角度，对于单表，我可以用select .. for update悲观锁实现，或者用version这种乐观锁的思想。 更好的方法是将这个字段添加唯一索引，用数据库来保证不会重复。一旦插入重复，那么就会抛出异常，程序就可以捕获到。 但是，假如我们这里分表了，以上都是针对单表，第一种方案是锁表，不行，设置唯一索引是没有用。怎么办呢？ 解决方案：用ZK做一个分布式锁。 首先准备一个ZK客户端，用的是Curator来连接我们的ZK： 1234567891011121314151617181920@Componentpublic class ZkClient &#123; @Autowired private Parameters parameters; @Bean public CuratorFramework getZkClient()&#123; CuratorFrameworkFactory.Builder builder= CuratorFrameworkFactory.builder() .connectString(parameters.getZkHost()) .connectionTimeoutMs(3000) .retryPolicy(new RetryNTimes(5, 10)); CuratorFramework framework = builder.build(); framework.start(); return framework; &#125;&#125; 注册用一个分布式锁来控制： 1234567891011121314151617181920212223242526272829303132333435@Overridepublic void registerUser(User user) throws Exception &#123; InterProcessLock lock = null; try&#123; lock = new InterProcessMutex(zkClient, Constants.USER_REGISTER_DISTRIBUTE_LOCK_PATH); boolean retry = true; do&#123; if (lock.acquire(3000, TimeUnit.MILLISECONDS))&#123; //查询重复用户 User repeatedUser = userMapper.selectByEmail(user.getEmail()); if(repeatedUser!=null)&#123; throw new MamaBuyException("用户邮箱重复"); &#125; user.setPassword(passwordEncoder.encode(user.getPassword())); user.setNickname("码码购用户"+user.getEmail()); userMapper.insertSelective(user); //跳出循环 retry = false; &#125; //可以适当休息一会...也可以设置重复次数，不要无限循环 &#125;while (retry); &#125;catch (Exception e)&#123; log.error("用户注册异常",e); throw e; &#125;finally &#123; if(lock != null)&#123; try &#123; lock.release(); log.info(user.getEmail()+Thread.currentThread().getName()+"释放锁"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 思路非常简单，就是先尝试上锁，即acquire，但是有可能失败，所以这里用一个超时时间，即3000ms之内上不了锁就失败，进入下一次循环。最后释放锁即可。 ok，这里要来说说ZK实现分布式锁了。这里用了开源客户端Curator，他对于实现分布式锁进行了封装，但是，我还是想了解一下它的实现原理： 每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。 也就是说，最小的那个节点就是Leader，进来判断是不是为那个节点，是的话就可以获取到锁，反之不行。 为什么不能通过大家一起创建节点，如果谁成功了就算获取到了锁。 多个client创建一个同名的节点，如果节点谁创建成功那么表示获取到了锁，创建失败表示没有获取到锁。 答：使用临时顺序节点可以保证获得锁的公平性，及谁先来谁就先得到锁，这种方式是随机获取锁，会造成无序和饥饿。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Session]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F06Spring%20Session%2F</url>
    <content type="text"><![CDATA[在单体应用中，我们经常用http session去管理用户信息，但是到了分布式环境下，显然是不行的，因为session对于不同的机器是隔离的，而http本身是无状态的，那么就无法判断出用户在哪一个服务器上登陆的。这个时候就需要有一个独立的地方存储用户session。spring session可以做到无代码侵入的方式实现分布式session存储。 在spring boot开发中，我们先注册相应bean并且打开相应的注解： 1234567891011121314151617181920212223242526272829@Configuration@EnableRedisHttpSession //(maxInactiveIntervalInSeconds = 604800)//session超时public class HttpSessionConfig &#123; @Autowired private Parameters parameters; @Bean public HttpSessionStrategy httpSessionStrategy()&#123; return new HeaderHttpSessionStrategy(); &#125; @Bean public JedisConnectionFactory connectionFactory()&#123; JedisConnectionFactory connectionFactory = new JedisConnectionFactory(); String redisHost = parameters.getRedisNode().split(":")[0]; int redisPort = Integer.valueOf(parameters.getRedisNode().split(":")[1]); connectionFactory.setTimeout(2000); connectionFactory.setHostName(redisHost); connectionFactory.setPort(redisPort);// connectionFactory.setPassword(parameters.getRedisAuth()); return connectionFactory; &#125;&#125; ok，这样子其实就配置好了，一开始我也云里雾里的，这是啥玩意？ 其实官网的文档中讲的是最准确的。所以还是官网看看吧！ ok，来spring session的官网(https://spring.io/projects/spring-session)来看看把，我们来看看1.3.4GA版本的文档(https://docs.spring.io/spring-session/docs/1.3.4.RELEASE/reference/html5/#httpsession-rest). spring session可以存在很多介质中，比如我们的数据源，比如redis，甚至是mongodb等。但是我们常用的是存在redis中，结合redis的过期机制来做。 所以其实我们只要关心如何跟redis整合，以及restful接口。 我们可以看到一开始文档就告诉我们要配置一下HttpSessionStrategy和存储介质。从HttpSessionStrategy语义就能大致看出配置的是它的策略，是基于header的策略。这个是什么意思，下面会提到。 那么我们就来看看文档吧！ 好了，我们知道了它的基本原理，下面来看看是如何在restful接口中实现用户session的管理的： 也就是说要想在restful接口应用中用这种方式，直接告诉spring session:return new HeaderHttpSessionStrategy();即可。进入源码我们就会知道，它默认给这个header里面放置的一条类似于token的名字是private String headerName = &quot;x-auth-token&quot;;。 那么在用户登陆成功之后，到底存到是什么呢，先来看看响应数据的header里面是什么： 这一串数字正好可以跟redis中对应上，我们可以先来redis中看看到底在里面存储了啥玩意： 我们已经看到了想要看到的一串字符串，这里解释一下redis中存储的东西： spring:session是默认的Redis HttpSession前缀（redis中，我们常用’:’作为分割符） 每一个session都会有三个相关的key，第一个key(spring:session:sessions:37...)最为重要，它是一个HASH数据结构，将内存中的session信息序列化到了redis中。如本项目中用户信息,还有一些meta信息，如创建时间，最后访问时间等。 另外两个key，一个是spring:session:expiration，还有一个是spring:session:sessions:expires，前者是一个SET类型，后者是一个STRING类型，可能会有读者发出这样的疑问，redis自身就有过期时间的设置方式TTL，为什么要额外添加两个key来维持session过期的特性呢？redis清除过期key的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致session不被清除。于是引入了专门的expiresKey，来专门负责session的清除，包括我们自己在使用redis时也需要关注这一点。 这样子，就可以用独立的redis来存储用户的信息，通过前端传来的header里面的token，就可以到redis拿出当前登陆用户的信息了。 OK，在解决了spring session的问题之后，下面就可以来实现登陆啦： controller: 1234567891011121314@RequestMapping("login")public ApiResult login(@RequestBody @Valid User user, HttpSession session)&#123; ApiResult&lt;UserElement&gt; result = new ApiResult&lt;&gt;(Constants.RESP_STATUS_OK,"登录成功"); UserElement ue= userService.login(user); if(ue != null)&#123; if(session.getAttribute(Constants.REQUEST_USER_SESSION) == null)&#123; session.setAttribute(Constants.REQUEST_USER_SESSION,ue); &#125; result.setData(ue); &#125; return result;&#125; 就跟以前一样，将session直接存进去就可以了。 12345678910111213141516171819202122@Overridepublic UserElement login(User user) &#123; UserElement ue = null; User userExist = userMapper.selectByEmail(user.getEmail()); if(userExist != null)&#123; //对密码与数据库密码进行校验 boolean result = passwordEncoder.matches(user.getPassword(),userExist.getPassword()); if(!result)&#123; throw new MamaBuyException("密码错误"); &#125;else&#123; //校验全部通过，登陆通过 ue = new UserElement(); ue.setUserId(userExist.getId()); ue.setEmail(userExist.getEmail()); ue.setNickname(userExist.getNickname()); ue.setUuid(userExist.getUuid()); &#125; &#125;else &#123; throw new MamaBuyException("用户不存在"); &#125; return ue;&#125;]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式ID生成策略]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F05%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[分布式环境下如何保证ID的不重复呢？一般我们可能会想到用UUID来实现嘛。但是UUID一般可以获取当前时间的毫秒数再加点随机数，但是在高并发下仍然可能重复。最重要的是，如果我要用这种UUID来生成分表的唯一ID的话，重复不谈，这种随机的字符串对于我们的innodb存储引擎的插入效率是很低的。所以我们生成的ID如果作为主键，最好有两种特性：分布式唯一和有序。 唯一性就不用说了，有序保证了对索引字段的插入的高效性。我们来具体看看ShardingJDBC的分布式ID生成策略是如何保证。 snowflake算法 sharding-jdbc的分布式ID采用twitter开源的snowflake算法，不需要依赖任何第三方组件，这样其扩展性和维护性得到最大的简化；但是snowflake算法的缺陷（强依赖时间，如果时钟回拨，就会生成重复的ID）。 雪花算法是由Twitter公布的分布式主键生成算法，它能够保证不同进程主键的不重复性，以及相同进程主键的有序性。 在同一个进程中，它首先是通过时间位保证不重复，如果时间相同则是通过序列位保证。 同时由于时间位是单调递增的，且各个服务器如果大体做了时间同步，那么生成的主键在分布式环境可以认为是总体有序的，这就保证了对索引字段的插入的高效性。例如MySQL的Innodb存储引擎的主键。 使用雪花算法生成的主键，二进制表示形式包含4部分，从高位到低位分表为：1bit符号位、41bit时间戳位、10bit工作进程位以及12bit序列号位。 雪花算法主键的详细结构见下图。 符号位(1bit) 预留的符号位，恒为零。 时间戳位(41bit) 41位的时间戳可以容纳的毫秒数是2的41次幂，一年所使用的毫秒数是：365 * 24 * 60 * 60 * 1000。通过计算可知： 1Math.pow(2, 41) / (365 * 24 * 60 * 60 * 1000L); 结果约等于69.73年。ShardingSphere的雪花算法的时间纪元从2016年11月1日零点开始，可以使用到2086年，相信能满足绝大部分系统的要求。 工作进程位(10bit) 该标志在Java进程内是唯一的，如果是分布式应用部署应保证每个工作进程的id是不同的。该值默认为0，可通过调用静态方法DefaultKeyGenerator.setWorkerId()设置。 序列号位(12bit) 该序列是用来在同一个毫秒内生成不同的ID。如果在这个毫秒内生成的数量超过4096(2的12次幂)，那么生成器会等待到下个毫秒继续生成。 时钟回拨 服务器时钟回拨会导致产生重复序列，因此默认分布式主键生成器提供了一个最大容忍的时钟回拨毫秒数。 如果时钟回拨的时间超过最大容忍的毫秒数阈值，则程序报错；如果在可容忍的范围内，默认分布式主键生成器会等待时钟同步到最后一次主键生成的时间后再继续工作。 最大容忍的时钟回拨毫秒数的默认值为0，可通过调用静态方法DefaultKeyGenerator.setMaxTolerateTimeDifferenceMilliseconds()设置。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springMVC全局异常+spring包扫描包隔离+spring事务传播]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F04springMVC%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%2Bspring%E5%8C%85%E6%89%AB%E6%8F%8F%E5%8C%85%E9%9A%94%E7%A6%BB%2Bspring%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[在开发中，springMVC全局异常+spring包扫描包隔离+spring事务传播这三个不可能不会遇到。下面来好好说说他们吧。 1、全局异常引入原因 假设在我们的login.do的controller方法中第一行增加一句： 1int i = 1/0; 重新启动服务器进行用户登录操作，那么就会抛出异常： 123java.lang.ArithmeticException: / by zero com.swg.controller.portal.UserController.login(UserController.java:37) ...其他的堆栈信息 这些信息会直接显示在网页上，如果是关于数据库的错误，同样，会详细地将数据库中的字段都显示在页面上，这对于我们的项目来说是存在很大的安全隐患的。这个时候，需要用全局异常来处理，如果发生异常，我们就对其进行拦截，并且在页面上显示我们给出的提示信息。 对于SpringBoot，一般全局异常是: 1234567891011121314151617@ControllerAdvice@ResponseBody@Slf4jpublic class ExceptionHandlerAdvice &#123; @ExceptionHandler(Exception.class) public ServerResponse handleException(Exception e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(Constants.RESP_STATUS_INTERNAL_ERROR,"系统异常，请稍后再试"); &#125; @ExceptionHandler(SnailmallException.class) public ServerResponse handleException(SnailmallException e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(e.getExceptionStatus(),e.getMessage()); &#125;&#125; 2、引入全局异常 12345678910111213@Slf4j@Componentpublic class ExceptionResolver implements HandlerExceptionResolver&#123; @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) &#123; log.error("exception:&#123;&#125;",httpServletRequest.getRequestURI(),e); ModelAndView mv = new ModelAndView(new MappingJacksonJsonView()); mv.addObject("status",ResponseEnum.ERROR.getCode()); mv.addObject("msg","接口异常，详情请查看日志中的异常信息"); mv.addObject("data",e.toString()); return mv; &#125;&#125; 那么，再执行登陆操作之后，就不会在页面上直接显示异常信息了。有效地屏蔽了关键信息。 3、spring和springmvc配置文件的优化 3.1 包隔离优化 在编写全局异常之前，先进行了包隔离和优化，一期中的扫描包的写法是： 12345&lt;!--spring:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt;&lt;!--springmvc:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt; 即spring和springmvc扫描包下面的所有的bean和controller.优化后的代码配置： 1234567891011#spring&lt;context:component-scan base-package="com.swg" annotation-config="true"&gt;&lt;!--将controller的扫描排除掉--&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt;#springmvc&lt;context:component-scan base-package="com.swg.controller" annotation-config="true" use-default-filters="false"&gt;&lt;!--添加白名单，只扫描controller，总之要将service给排除掉即可--&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt; 这样做的原因是：Spring和SpringMVC是有父子容器关系的，而且正是因为这个才往往会出现包扫描的问题。 针对包扫描只要记住以下几点即可： spring是父容器，springmvc是子容器，子容器可以访问父容器的bean,父容器不能访问子容器的bean。 只有顶级容器（spring）才有加强的事务能力，而springmvc容器的service是没有的。 如果springmvc不配置包扫描的话，页面404. 3.2 事务的传播机制 针对事务，不得不展开说明spring事务的几种传播机制了。在 spring 的 TransactionDefinition 接口中一共定义了七种事务传播属性： PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择（默认）。 PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 4、补充 Spring默认情况下，会对运行期例外(RunTimeException)，即uncheck异常，进行事务回滚。如果遇到checked异常就不回滚。如何改变默认规则： 让checked例外也回滚：在整个方法前加上 @Transactional(rollbackFor=Exception.class) 让unchecked例外不回滚： @Transactional(notRollbackFor=RunTimeException.class) 不需要事务管理的(只查询的)方法：@Transactional(propagation=Propagation.NOT_SUPPORTED) 5、那么什么是嵌套事务呢？ 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫做save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点在于那个save point，看以下几个问题： 问题1：如果子事务回滚，会发生什么？ 父事务会回到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。 问题2：如果父事务回滚，会发生什么？ 父事务回滚，子事务也会跟着回滚，为什么呢？因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理/ 问题3：父事务先提交，然后子事务再提交；还是子事务先提交，然后父事务再提交呢？ 答案是第二种情况，子事务是父事务的一部分，由父事务同意提交。 6、spring配置文件的一些理解： 容器 在Spring整体框架的核心概念中，容器是核心思想，就是用来管理Bean的整个生命周期的，而在一个项目中，容器不一定只有一个，Spring中可以包括多个容器，而且容器有上下层关系，目前最常见的一种场景就是在一个项目中引入Spring和SpringMVC这两个框架，那么它其实就是两个容器，Spring是父容器，SpringMVC是其子容器，并且在Spring父容器中注册的Bean对于SpringMVC容器中是可见的，而在SpringMVC容器中注册的Bean对于Spring父容器中是不可见的，也就是子容器可以看见父容器中的注册的Bean，反之就不行。 1&lt;context:component-scan base-package="com.springmvc.test" /&gt; 我们可以使用统一的如下注解配置来对Bean进行批量注册，而不需要再给每个Bean单独使用xml的方式进行配置。 从Spring提供的参考手册中我们得知该配置的功能是扫描配置的base-package包下的所有使用了@Component注解的类，并且将它们自动注册到容器中，同时也扫描@Controller，@Service，@Respository这三个注解，因为他们是继承自@Component。 1&lt;context:annotation-config/&gt; 其实有了上面的配置，这个是可以省略掉的，因为上面的配置会默认打开以下配置。以下配置会默认声明了@Required、@Autowired、 @PostConstruct、@PersistenceContext、@Resource、@PreDestroy等注解。 1&lt;mvc:annotation-driven /&gt; 这个是SpringMVC必须要配置的，因为它声明了@RequestMapping、@RequestBody、@ResponseBody等。并且，该配置默认加载很多的参数绑定方法，比如json转换解析器等。 7、总结 在实际工程中会包括很多配置，我们按照官方推荐根据不同的业务模块来划分不同容器中注册不同类型的Bean：Spring父容器负责所有其他非@Controller注解的Bean的注册，而SpringMVC只负责@Controller注解的Bean的注册，使得他们各负其责、明确边界。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis实现分布式锁]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F03redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[为了讲解redis分布式锁，我将引入一个场景：定时关单。因为往往订单服务是一个集群，那么定时器会同时触发这些集群去取消订单，显然是浪费机器资源的，所以目的是：只让其中一台机器去执行取消订单即可。这里可以用分布式锁来实现。 项目是从练手项目中截取出来的，框架是基于SSM的XML形式构成，所以下面还涉及一点XMl对于定时器spring schedule的配置内容。 1、引入目标 定时自动对超过两个小时还未支付的订单对其进行取消，并且重置库存。 2、配置 首先是spring配置文件引入spring-schedule 123456xmlns:task="http://www.springframework.org/schema/task"...http://www.springframework.org/schema/taskhttp://www.springframework.org/schema/task/spring-task.xsd...&lt;task:annotation-driven/&gt; 补充：针对applicationContext-datasource.xml中的dataSource读取配置文件的信息无法展现的问题，在spring的配置文件中增加一条配置： 1&lt;context:property-placeholder location="classpath:datasource.properties"/&gt; 3、定时调度代码 此代码的主要功能是：定时调用取消订单服务。 1234567891011121314@Component@Slf4jpublic class CloseOrderTask &#123; @Autowired private OrderService orderService; @Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍的时候执行 public void closeOrderTaskV1()&#123; log.info("关闭订单定时任务启动"); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); log.info("关闭订单定时任务结束"); &#125;&#125; @Component一定要加，否则spring扫描不到。 close.order.task.time.hour 也是配置在snailmall.properties中的，这里配置的是默认的2，即两个小时，下订单超过两个小时仍然不支付，就取消该订单。 对于orderService里面的具体方法： 这里是关单的具体逻辑，细节是行锁。这段代码只要知道他是具体关单的逻辑即可，不需要仔细了解代码。 1234567891011121314151617181920212223242526@Overridepublic void closeOrder(int hour) &#123; Date closeDateTime = DateUtils.addHours(new Date(),-hour); //找到状态为未支付并且下单时间是早于当前检测时间的两个小时的时间,就将其置为取消 //SELECT &lt;include refid="Base_Column_List"/&gt; from mmall_order WHERE status = #&#123;status&#125; &lt;![CDATA[ and create_time &lt;= #&#123;date&#125; ]]&gt; order by create_time desc List&lt;Order&gt; orderList = orderMapper.selectOrderStatusByCreateTime(Const.OrderStatusEnum.NO_PAY.getCode(),DateTimeUtil.dateToStr(closeDateTime)); for(Order order:orderList)&#123; List&lt;OrderItem&gt; orderItemList = orderItemMapper.getByOrderNo(order.getOrderNo()); for(OrderItem orderItem:orderItemList)&#123; //一定要用主键where条件，防止锁表。同时必须是支持MySQL的InnoDB. Integer stock = productMapper.selectStockByProductId(orderItem.getProductId()); if(stock == null)&#123; continue; &#125; //更新产品库存 Product product = new Product(); product.setId(orderItem.getProductId()); product.setStock(stock+orderItem.getQuantity()); productMapper.updateByPrimaryKeySelective(product); &#125; //关闭order //UPDATE mmall_order set status = 0 where id = #&#123;id&#125; orderMapper.closeOrderByOrderId(order.getId()); log.info("关闭订单OrderNo:&#123;&#125;",order.getOrderNo()); &#125;&#125; 这样，debug启动项目，一分钟后就会自动执行closeOrderTaskV1方法了。找一个未支付的订单，进行相应测试。 4、存在的问题 经过实验发现，同时部署两台tomcat服务器，执行定时任务的时候是两台都同时执行的，显然不符合我们集群的目标，我们只需要在同一时间只有一台服务器执行这个定时任务即可。那么解决方案就是引入redis分布式锁。 redis实现分布式锁，核心命令式setnx命令。所以阅读下面，您需要对redis分布式锁的基本实现原理必须要先有一定的认识才行。 5、第一种方案 第一步：setnx进去，如果成功，说明塞入redis成功，抢占到锁 第二步：抢到锁之后，先设置一下过期时间，即后面如果执行不到delete，也会将这个锁自动释放掉，防止死锁 第三步：关闭订单，删除redis锁 存在的问题：如果因为tomcat关闭或tomcat进程在执行closeOrder()方法的时候，即还没来得及设置锁的过期时间的时候，这个时候会造成死锁。需要改进。 123456789101112131415161718192021222324252627//第一个版本，在突然关闭tomcat的时候有可能出现死锁@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV2()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; log.info("关闭订单定时任务结束");&#125;private void closeOrder(String lockName) &#123; //给锁一个过期时间，如果因为某个原因导致下面的锁没有被删除，造成死锁 RedisShardPoolUtil.expire(lockName,50); log.info("获取&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); //关闭订单之后就立即删除这个锁 RedisShardPoolUtil.del(lockName); log.info("释放&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); System.out.println("=============================================");&#125; 6、改进 图看不清，可以重新打开一个窗口看。具体的逻辑代码： 12345678910111213141516171819202122232425262728@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV3()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; //在没有拿到锁的情况下，也要进行相应的判断，确保不死锁 String lockValueStr = RedisShardPoolUtil.get(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); //如果判断锁是存在的并且现在已经超时了，那么我们这个进程就有机会去占有这把锁 if(lockValueStr != null &amp;&amp; System.currentTimeMillis() &gt; Long.parseLong(lockValueStr))&#123; //当前进程进行get set操作，拿到老的key，再塞进新的超时时间 String getSetResult = RedisShardPoolUtil.getset(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); //如果拿到的是空的，说明老的锁已经释放，那么当前进程有权占有这把锁进行操作； //如果拿到的不是空的，说明老的锁仍然占有，并且这次getset拿到的key与上面查询get得到的key一样的话，说明没有被其他进程刷新，那么本进程还是有权占有这把锁进行操作 if(getSetResult == null || (getSetResult != null &amp;&amp; StringUtils.equals(lockValueStr,getSetResult)))&#123; closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125; log.info("关闭订单定时任务结束");&#125; 这样两次的防死锁措施，不仅可以防止死锁，还可以提高效率。 7、扩展 mysql四种事务隔离机制 read uncommitted:读取未提交内容 两个线程，其中一个线程执行了更新操作，但是没有提交，另一个线程在事务内就会读到该线程未提交的数据。 read committed:读取提交内容（不可重复读） 针对第一种情况，一个线程在一个事务内不会读取另一个线程未提交的数据了。但是，读到了另一个线程更新后提交的数据，也就是说重复读表的时候，数据会不一致。显然这种情况也是不合理的，所以叫不可重复读。 repeatable read:可重复读（默认） 可重复读，显然解决2中的问题，即一个线程在一个事务内不会再读取到另一个线程提交的数据，保证了该线程在这个事务内的数据的一致性。 对于某些情况，这种方案会出现幻影读，他对于更新操作是没有任何问题的了，但是对于插入操作，有可能在一个事务内读到新插入的数据（但是MySQL中用多版本并发控制机制解决了这个问题），所以默认使用的就是这个机制，没有任何问题。 serializable:序列化 略。 存储引擎 MySQL默认使用的是InnoDB，支持事务。还有例如MyISAM,这种存储引擎不支持事务，只支持只读操作，在用到数据的修改的地方，一般都是用默认的InnoDB存储引擎。 索引的一个注意点 一般类型为normal和unique，用btree实现，对于联合索引(字段1和字段2)，在执行查询的时候，例如 1select * from xxx where 字段1="xxx" ... 是可以利用到索引的高性能查询的，但是如果是 1select * from xxx where 字段2="xxx" ... 效率跟普通的查询时一样的，因为用索引进行查询，最左边的那个字段必须要有，否则无效。 扩展的内容知识顺便提一下，在数据库这一块，会详细介绍一下。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson实现Redis分布式锁原理]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F02Redisson%E5%AE%9E%E7%8E%B0Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[我们可以自己来实现一个redis分布式锁，但是如何用Redisson优雅地实现呢？本文探讨一下它的原理。 用Redisson来实现分布式锁异常地简单，形如： 还支持redis单实例、redis哨兵、redis cluster、redis master-slave等各种部署架构，都可以给你完美实现。 加锁 原理图： 现在某个客户端要加锁。如果该客户端面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。紧接着，就会发送一段lua脚本到redis上，那段lua脚本如下所示： 为啥要用lua脚本呢？因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的原子性。 解释一下这段脚本的意思。 这里的KEYS[1]代表的是你加锁的那个key的名字。这个key就是我们常看到的： 1RLock lock = redisson.getLock("myLock"); 中的myLock，我就是对这个key进行加锁。 这里的ARGV[1]代表的就是锁key的默认生存时间，默认30秒。ARGV[2]代表的是加锁的客户端的ID:比如8743c9c0-0795-4907-87fd-6c719a6b4586:1 第一段if判断语句，就是相当于用exists myLock命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。如何加锁呢？很简单，用下面的命令：hset myLock。 执行完hest之后，设置了一个hash数据结构：8743c9c0-0795-4907-87fd-6c719a6b4586:1 1，这行命令执行后，会出现一个类似下面的数据结构： 紧接着会执行pexpire myLock 30000命令，设置myLock这个锁key的生存时间是30秒。好了，到此为止，ok，加锁完成了。 锁互斥 那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？很简单，第一个if判断会执行exists myLock，发现myLock这个锁key已经存在了。接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。 所以这个客户端2两个if都不能进入，只能执行最后的pttl myLock，返回值代表了myLock这个锁key的剩余生存时间。比如还剩15000毫秒的生存时间。此时客户端2会进入一个while循环，不停的尝试加锁。 watch dog自动延期机制 客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？ 简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制 看一下代码，相同的客户进来，会进入第二个if，会执行hincrby，即增1，那么这个hash结构就会变成： 释放锁 如果执行lock.unlock()，就可以释放分布式锁，此时的业务逻辑也是非常简单的。其实说白了，就是每次都对myLock数据结构中的那个加锁次数减1。如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用：del myLock命令，从redis里删除这个key。然后呢，另外的客户端2就可以尝试完成加锁了。 这就是所谓的分布式锁的开源Redisson框架的实现机制。 存在的问题 其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。 但是复制的这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。 假设客户端1在redis master上获得锁，然后主机宕机，redis slave成为新的redis master，但是还未同步到redis slave上，但是客户端1已经觉得自己获取到了锁。 此时，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，此时就会发生多个客户端完成对一个key的加锁。这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring事务的传播行为]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F01%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BC%A0%E6%92%AD%E8%A1%8C%E4%B8%BA%2F</url>
    <content type="text"><![CDATA[经常听到别人说事务传播行为，那到底什么是事务的传播行为呢？ 1.什么是事务？ 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。 这里注意，其实事务就是数据库才能保证的，所以抛开数据库层面来谈事务本身就是不存在的，所以事务的概念就是数据库一系列操作的一个完整单元。 2.什么是ACID？ ACID是指数据库管理系统在写入或更新资料的过程中，为保证事务是正确可靠的，所必须具备的四个特性：原子性、一致性、隔离性、持久性。 Atomicity：一个事务中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 Isolation：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔 离分为不同级别，包括读未提交(Read uncommitted)、读提交(read committed)、可重复读(repeatable read)和串行化(Serializable)。 Durability：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 3.spring事务传播行为 在我们用SSM开发项目的时候，我们一般都是将事务设置在Service层 那么当我们调用Service层的一个方法的时候它能够保证我们的这个方法中执行的所有的对数据库的更新操作保持在一个事务中，在事务层里面调用的这些方法要么全部成功，要么全部失败。那么事务的传播特性也是从这里说起的。 如果你在你的`Service`层的这个方法中，除了调用了`Dao`层的方法之外，还调用了本类的其他的`Service`方法，那么在调用其他的`Service`方法的时候，这个事务是怎么规定的呢，我必须保证我在我方法里调用的这个方法与我本身的方法处在同一个事务中，否则如果保证事物的一致性。事务的传播特性就是解决这个问题的. 在Spring中有针对传播特性的多种配置我们大多数情况下只用其中的一种:PROPGATION_REQUIRED：这个配置项的意思是说当我调用service层的方法的时候开启一个事务(具体调用那一层的方法开始创建事务，要看你的aop的配置),那么在调用这个service层里面的其他的方法的时候,如果当前方法产生了事务就用当前方法产生的事务，否则就创建一个新的事务。这个工作使由Spring来帮助我们完成的。 默认情况下当发生`RuntimeException`的情况下，事务才会回滚，所以要注意一下：如果你在程序发生错误的情况下，有自己的异常处理机制定义自己的`Exception`，必须从`RuntimeException`类继承，这样事务才会回滚！ 4.事务隔离级别 1、Serializable：最严格的级别，事务串行执行，资源消耗最大； 2、REPEATABLE READ：保证了一个事务不会修改已经由另一个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 3、READ COMMITTED:大多数主流数据库的默认事务等级，保证了一个事务不会读到另一个并行事务已修改但未提交的数据，避免了“脏读取”。该级别适用于大多数系统。 4、Read Uncommitted：保证了读取过程中不会读取到非法数据。 5.总结 本文的重点是在于理解事务的传播行为这个概念，从事务的概念，到事务的ACID介绍，引出事务传播传播行为和隔离级别这两个概念加以理解。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap]]></title>
    <url>%2F2019%2F01%2F22%2Fjava-collection%2F12.ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[对于并发场景下，推荐使用线程安全的 concurrentHashMap ，而不是 HashMap 或者是 HashTable .concurrentHashMap在JDK7和JDK8中的实现原理是不一样的。本文分别对其核心思想和方法进行阐述。 一、JDK7实现 ConcurrentHashMap 的内部细分了若干个小的 HashMap ，称之为段（ SEGMENT ）。 ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment ，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel ( Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 1.1 get方法 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值. 内部 HashEntry 类 ： 12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 1.2 put方法 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。 首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。 而 ConcurrentHashMap 不一样，它是在将数据插入之前检查是否需要扩容，之后再做插入操作。 1.3 size方法 每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。 但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。 在 JDK7 中，第一种方案他会使用不加锁的模式去尝试多次计算 ConcurrentHashMap 的 size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的。 第二种方案是如果第一种方案不符合，他就会给每个 Segment 加上锁，然后计算 ConcurrentHashMap 的 size 返回。其源码实现: 12345678910111213141516171819202122232425262728293031323334353637public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 其中 12// 锁之前重试次数static final int RETRIES_BEFORE_LOCK = 2; 二、JDK8实现 jdk8 中的 ConcurrentHashMap 数据结构和实现与 jdk7 还是有着明显的差异。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 也将 jdk7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。 其中的 val next 都用了 volatile 修饰，保证了可见性。 2.1 put方法 重点来看看 put 函数： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足(不需要初始化、Node不为空、不需要扩容)，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 2.2 get方法 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 都不满足那就按照链表的方式遍历获取值。 2.3 size方法 JDK8 实现相比 JDK7 简单很多，只有一种方案，我们直接看 size() 代码： 123456789101112131415161718192021public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; //获取baseCount值 long sum = baseCount; //遍历CounterCell数组全部加到baseCount上，它们的和就是size if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; 可能你会有所疑问，ConcurrentHashMap 中的 baseCount 属性不就是记录的所有键值对的总数吗？直接返回它不就行了吗？ 之所以没有这么做，是因为我们的 addCount 方法用于 CAS 更新 baseCount，但很有可能在高并发的情况下，更新失败，那么这些节点虽然已经被添加到哈希表中了，但是数量却没有被统计。 还好，addCount 方法在更新 baseCount 失败的时候，会调用 fullAddCount 将这些失败的结点包装成一个 CounterCell 对象，保存在 CounterCell 数组中。那么整张表实际的 size 其实是 baseCount 加上 CounterCell数组中元素的个数。 三、总结 并发情况下请使用concurrentHashMap 在jdk7中，用的是分段锁，默认是12段，那么并发量最多也就12. get不加锁，第一次hash定位到segment，第二次hash定位到元素，元素值是用volatile保证内存可见性 put需要加锁，hash定位到segment后，先检查是否需要扩容再插入。 size先使用不加锁的模式去尝试多次计算size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入；如果不一致，给每个 Segment 加上锁再依次去计算个数 在jdk8中，采用了 CAS + synchronized 来保证并发安全性 put的过程比较复杂，简单来说是：先计算hash定位到node—》判断是否初始化—》如果node为空则表示可以插入，用cas插入—》判断是否需要扩容—》如果不需要初始化、Node不为空、不需要扩容，则利用 synchronized 锁写入数据—》判断是否需要转换为红黑树 get就比较简单，直接根据hash定位到node，然后以链表或者红黑树的方式拿到 size方法就一种方案：baseCount+CounterCell[]中所有元素 整理自： https://crossoverjie.top/JCSprout/#/thread/ConcurrentHashMap?id=size-方法 https://www.jianshu.com/p/e99e3fcface4]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap死循环问题]]></title>
    <url>%2F2019%2F01%2F21%2Fjava-collection%2F11.HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JDK1.7或者更老的版本中在多线程情况下是会存在死循环问题，究其原因是put过程中的resize方法在调用transfer方法的时候导致的死锁。这次我们来看看到底是哪里出了问题！ 核心源码 在JDK8中，内部已经调整，解决了死循环问题，是如何解决的呢？将JDK7中头插入法改为末端插入。就是这么简单。关于这个，可以查看jdk8源码中的resize方法。 上面提到是由于put时出现问题，那么先来到put()中看看： 我们看到，put一个不存在的新元素，必然增加一个节点，我们进入这个增加节点的方法： 检查是否需要扩容，需要的话就resize: 下面就是对链表数据进行迁移： 核心的代码就是这么多，首先要强调一下：两个线程进来，是分别建立了两个独立的扩容后的数组，比如这里是两个长度为4的数组。老的数组为2个数就是唯一的。所以在第一步，线程2运行结束时，老的数组元素已经空了。 下面先演示一下正常的rehash过程。 正常情况 假设了我们的hash算法就是简单的用 key mod 一下数组(hash表)的大小 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。 接下来的三个步骤是Hash表 resize 成4，然后所有的 &lt;key,value&gt; 重新 rehash 的过程 注意到，在JDK7中，是按照头插入法依次插入的。所以7插到了3前面。 并发情况 1.初始情况 假设我们有两个线程。我用红色和浅蓝色标注了一下。 对于第一个线程先执行完这一行，然后挂起，此时 e 和 next 都附好值了： 而让线程二执行完成。于是我们有下面的这个样子： 因为Thread1的 e 指向了 key(3) ，而 next 指向了 key(7) ，其在 Thread2 rehash后，指向了 Thread2 重组后的链表。 2.Thread1被调度回来执行 先是执行 newTalbe[i] = e ：此时线程1的第三个位置就是指向元素3; 然后是 e = next，导致了 e 指向了 key(7) ; 而下一次循环的 next = e.next 导致了 next 指向了 key(3) ; 3.一切安好 线程一接着工作。把 key(7) 摘下来，放到 newTable[i] 的第一个，然后把 e 和 next 往下移。 4.环形链接出现 e.next = newTable[i] 导致 key(3).next 指向了 key(7) 注意：此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 自己的简单整理 这里还是比较绕的，理解的最好方式左边放源码，右边放图，中间用草稿纸画一画。 那么，这里我在对其过程尽可能地讲明白一点。我们先确定7和3会全部落到扩容后的下标为3的位置(3%4=3,7%4=3)。 规定线程1开辟的数组为 arr1 ，线程2开辟的数组为 arr2; 1. 初始状态 线程一： e -&gt; key3 , next -&gt; key7 线程二： 数组3号位置 arr2[3] -&gt; key7 -&gt; key3 注意此时 key7 指向 key3 . 我们要明确一下，发生死循环，是指在put操作完毕之后，最终生成的数组中有死循环引用才行，千万不要一开始看线程一种key3指向key7，然后线程二种key7指向key3就是死循环了。。。 2. 线程一继续执行 i = 3 e.next = key7,此时 e=key3 ,所以是 key3.next = key7（这是线程1的初始状态决定的） arr1[3] 指向 key3 e 为 key7 3.由于e不为空，所以还会循环： 上一步 e 为 key7，所以 next = key7.next ，到线程2中一看是 key3 ，所以 next = key3（线程2中key7.next就是key3） i = 3 e.next = key3------注意，这里就是Key7指向了key3,key7的next引用下面没有变过，所以这里做一下记录，即key7指向key3 newTable[3] = key7 e = key3 4.由于e不为空，所以还会循环： 上一步 e=key3 , next=null i=3 key3.next = key7，注意,由于key7已经指向了key3，此时key3又指向key7,发生死循环 newTable[3] = key3 e = null 5.e为null，跳出循环。 此时发现key3又指向了key7。发生死循环。 整理自:https://coolshell.cn/articles/9606.html/comment-page-2#comments]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux重要的一些命令]]></title>
    <url>%2F2019%2F01%2F21%2Flinux%E9%87%8D%E8%A6%81%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[很多命令之所以重要，是因为用它们会大幅提高工作效率。所以，熟悉它们是我们必须要做的一件事情。下面着重提取了find、grep、管道、wc、awk以及sed等几个命令来看看如何使用。 一、Linux体系结构 对这幅图进行详细说明一下。 体系结构主要分为用户态(用户上层活动)和内核态 内核：本质是一段管理计算机硬件设备的程序，这个程序直接管理硬件：包括CPU、内存空间、硬盘接口、网络接口等。所有的计算机操作都要通过内核来操作。 系统调用：内核的访问接口，是一种不能再简化的操作(可以认为系统调用已经是最小的原子操作，上层完成一个功能要依托于若干系统调用才能完成) 由于系统调用比较基础，要完成一个功能需要很多系统调用组合才能实现，对于程序员来说比较复杂。这个时候怎么办呢？我们可以调用公共函数库：系统调用的组合拳。简化程序员操作。 Shell也是一种特殊的应用程序，是一个命令解释器，可以编程。 Shell下通系统调用，上通各种应用，是上层和下层之间粘合的胶水，让不同程序可以偕同工作。 在没有图形界面之前，用户通过shell命令行或者可编程的shell脚本可以完成很多事情。 二、如何根据文件名检索文件 find 在指定目录下查找文件 语法 find path [options] params 2.1 精确查找文件 比如我在当前目录(可能在子目录下)下找一个文件叫做test.java： 1find -name "test.java" 这个指令就可以在本目录以及子目录下递归查找这个文件了。 实例：精确查询名字叫snailmall-api-gateway-8080.jar这个文件： 2.2 全局搜索 如果是全局查找，也很简单，无非是从根目录开始递归查找。 1find / -name "test.java" 实例：我对这台服务器全局查找文件名以snailmall开头的所有文件： 2.3 模糊查询 如果找以test打头的文件： 1find -name "test*" 即用 * 通配符就可以模糊查询到以 test 打头的文件。 实例，我的这台服务器上部署了几个关于商城的服务，这个目录下我放了jar包和相应的启动信息文件。我对其进行模糊查询： 2.4 忽略大小写 1find -iname "test*" 三、如何根据文件内的内容进行检索 3.1 grep命令 grep 查找文件里符合条件的字符串 语法：grep [options] pattern file 比如 test.java 中有一句话是： 1System.out.println("i love java"); 那么如何查找 test.java 中的 java 呢？ 1grep "java" test* 这句话意思就是查找以 test 打头的文件中的包含 java 字符串所在的行。 直接输入： 1grep "hello" 会等待用户输入文本。然后再对输入的内容进行检索。 3.2 管道操作符 | 可将指令连接起来，前一个指令的输出作为后一个指令的输入。 1find / | grep "test" 作用同： 1find / -name "test" 注意： 只有前一个指令正确才会再处理。 管道右边命令必须能接收标准输入流，否则传递过程中数据会被抛弃 3.3 grep结合管道 1grep 'xxx' hello.info 可以将 xxx 所在的行全部筛选出来，但是还是特别多，我比如关心这每一行中某个字段的信息，比如是 param[xx12]这种信息。如何实现筛选呢？ 1grep 'xxx' hello.info | grep -o 'param\[[0-9a-z]*\]' 这样就只把类似于 param[xx12] 这样的信息清晰地展现出来。 如何过滤掉不要的信息呢？可以用： 1grep -v 比如我们查询 tomcat 进程信息： 1ps -ef | grep tomcat 我们会发现，不仅 tomcat 的信息展现出来了，执行 grep 命令本身的进程信息也展示出来了。我们要将这个 grep 命令过滤掉，只展现 tomcat 进程信息，可以： 1ps -ef | grep tomcat | grep -v "grep" 这样就把 grep 进程信息过滤掉了。 四、如何对文件内容做统计 awk 一次读取一行文本，按输入分隔符进行切片，切成多个组成部分 将切片直接保存再内建的变量中，$1$2…($0表示行的全部) 支持对单个切片的判断，支持循环判断，默认分隔符为空格 语法：awk [options] ‘cmd’ file 有这样一个文件text1.txt： 123456789proto Recv-Q Send-Q Local Address Foreign Address statetcp 0 48 111.34.134.2:ssh 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.13.2:s0 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 1 48 localhots:webcac 124.213.2.12:12565 ESTABLISHEDtcp 1 48 111.34.134.2:s1 124.213.2.12:12565 ESTABLISHEDudp 1 48 111.34.134.2:s2 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.134.2:s3 124.213.2.12:12565 ESTABLISHED 列出切分出来的第一列和第二列： 1awk '&#123;print $1,$2&#125;' test1.txt 结果： 筛选出第一列为tcp和第二列为1的所在行，将这些行数据全部打印出来： 1awk '$1="tcp" &amp;&amp; $2==1&#123;print $0&#125;' test1.txt 结果： 打印带有表头的数据： 1awk '($1="tcp" &amp;&amp; $2==1) || NR==1 &#123;print $0&#125;' test1.txt 默认是以空格分隔，那么以逗号或者其他符号可以吗？答案当然是可以。对于这样的文件text2.txt： 12345adas,123wqe,54412321,dddfsdaasd,1235465547,fjigj 1awk -F "," '&#123;print $2&#125;' text2.txt 五、WC统计 有一个文件test2.txt，里面的内容是： 123swg123eh shwfshsfswg7 121 32n dswg17328 123swg1 2h1jhwjqbsjwqbsh ddddh wg ehdedhd dhsjh 六、sed命令 sed是一个很好的文件处理工具，本身是一个管道命令，主要是以行为单位进行处理，可以将数据行进行替换、删除、新增、选取等特定工作 sed [-n/e/f/r/i] ‘command’ 输入文本 常用选项： -n∶使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN的资料一般都会被列出到萤幕上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e∶直接在指令列模式上进行 sed 的动作编辑； -f∶直接将 sed 的动作写在一个档案内， -f filename 则可以执行 filename 内的sed 动作； -r∶sed 的动作支援的是延伸型正规表示法的语法。(预设是基础正规表示法语法) -i∶直接修改读取的档案内容，而不是由萤幕输出。 常用命令： a ∶新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ∶取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ∶删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ∶插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ∶列印，亦即将某个选择的资料印出。通常 p 会与参数 sed -n 一起运作～ s ∶取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 这里我就主要看一下批量替换这个功能。 如果只是给一个文件中的若干字符串批量替换，只需要： 1sed -i "s/oldstring/newstring/g" filename 如果是对某一路径下很多的文件批量替换： 1sed -i &quot;s/oldstring/newstring/g&quot; `grep oldstring -rl path` 其中，oldstring是待被替换的字符串，newstring是待替换oldstring的新字符串，grep操作主要是按照所给的路径查找oldstring，path是所替换文件的路径； -i选项是直接在文件中替换，不在终端输出； -r选项是所给的path中的目录递归查找； -l选项是输出所有匹配到oldstring的文件； 这里只是模拟一下，将目录下的所有文件进行批量修改：]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[站点文章汇总]]></title>
    <url>%2F2019%2F01%2F21%2F%E7%AB%99%E7%82%B9%E6%96%87%E7%AB%A0%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[☝️☝️☝️🔝🔝🔝本文为置顶文章，为了方便管理和查阅，在这里详细展示目录索引，看完索引，你就知道本站的大体内容啦！我相信一定会给小伙伴们一些收获！ 计算机网络🐭🐭🐭 这一部分主要是关于HTTP和TCP的必备知识。 《计算机网络相关系列》： 《从下到上看五层模型》 《从上到下看五层模型》 《HTTP的前世今生》 《TCP协议入门》 《TCP三次握手和四次挥手》 《HTTP基础知识提炼》 《一步一步理解HTTPS》 《一些常见的面试题》 JAVA容器🐱🐱🐱 这一部分是JAVA容器一系列文章，主要讲了常用JAVA容器的源码和一些特性，面试必问点。 《JAVA容器》： 《ArrayList/Vector》 《LinkedList》 《CopyOnWriteArrayList》 《HashCode/Equals》 《HashMap》 《HashSet》 《LinkedHashMap》 《HashMap和LinkedHashMap遍历机制》 《HashTable》 《LinkedHashSet》 《JDK7中HashMap死循环原因剖析》 《ConcurrentHashMap》 Linux&amp;操作系统🐶🐶🐶 一些必备的Liunx相关的知识点整理。 《Linux》： 《Linux面试重要命令》 《操作系统相关》： 《面试-进程与线程》 JAVA虚拟机相关🐹🐹🐹 主要是介绍JVM相关知识。轻松以应付面试。 《JVM》： 《Java如何执行一个最简单的程序》 《浅谈ClassLoader》 《双亲委派模型》 《细谈loadClass》 《JAVA内存模型-线程私有》 《JAVA内存模型-线程共享》 《JAVA内存模型常问面试题》 《GC相关》 《垃圾收集器介绍》 《内存分配和回收策略》 《类的初始化过程》 《静态分派和动态分派》 《实例说明类加载过程》 以下是扩展阅读部分，主要是对Class文件的结构进行详细的解读 《补充阅读1-Class类文件结构》 《补充阅读2-Class文件中的常量池》 《补充阅读3-Class文件中的访问标志、类索引、父类索引、接口索引集合》 《补充阅读4-Class文件中的字段表集合–field字段在class文件中是怎样组织的》 《补充阅读5-Class文件中的方法表集合–method方法在class文件中是怎样组织的》 JAVA核心基础知识🐺🐺🐺 主要是介绍比较核心的JAVA基础知识，属于JAVA基础进阶。 《Integer拆箱和装箱》 《String为什么不可变》 《java字符串核心一网打尽》 《JAVA基础核心-理解类、对象、面向对象编程、面向接口编程》 《补码的前世今生》 《数值计算精度丢失问题》 《彻底理解java反射机制》 JAVA多线程🐸🐸🐸 多线程这一块比较棘手，且学且保重。 《JAVA多线程和并发》： 《线程基本知识梳理》 Redis🐯🐯🐯 系统学习redis的笔记整理。 《Redis》： 《初步认识Redis》 《Redis基本数据结构和操作》 《Redis其他的功能介绍》 《Redis为什么快》 《Redis持久化》 《Redis主从复制》 《Redis-Sentinel实现高可用读写分离》 《Redis-Cluster理论详解》 《Redis缓存设计与优化》 《Redis缓存更新问题》 《Redis事务》 《几种主流缓存框架介绍》 《关于Redis一些重要的面试点》 MySQL数据库🐨🐨🐨 作为必备技能，用法和原理都要会。 《MySQL》： 《mysql最基础知识小结》 《SQL必知必会知识点提炼》 《复杂查询基础》 《内连接和外连接》 《delete和truncate以及drop区别》 《如何设计一个关系型数据库》 《数据库索引入门》 《MySQL索引全面解读》 《MySQL调优》 《关于索引失效和联合索引》 《锁模块》 《数据库事务核心问题》 《mysql面试高频理论知识》 算法🐻🐻🐻 算法这一块也是面试痛点和难点，头发越来越少了呢！ Spring🐷🐷🐷 大厂必问啊啊啊啊，源码终究还是要读的~ Spring Cloud相关🐮🐮🐮 这一块就比较偏实践了。分布式。。。路漫漫。。。 Zookeeper🐗🐗🐗 作为当今分布式协调中心，核心的Paxos算法你不想了解一下吗？ 杂记🐵🐵🐵 在这个板块，不划分类别，文章尽可能地简短，也可谓之记忆碎片。 《技术短文杂记》： 《spring事务的传播行为》 《Redisson实现Redis分布式锁原理》 《redis实现分布式锁》 《springMVC全局异常+spring包扫描包隔离+spring事务传播》 《分布式ID生成策略》 《Spring Session》 《Curator》 《ELK平台搭建》 《库存扣减问题》 《分布式事务解决方案思考》 《SpringBoot使用logback实现日志按天滚动》 《地理位置附近查询的GEOHASH解决方案》 《深入探究Nginx原理》 《简明理解一致性hash算法》 随笔🐖🐖🐖 《随笔》： 《2019年展望》 实战作品🐰🐰🐰 记录一些实战作品，代码主要存放在github上。 《我的实战》： 《快乐蜗牛商城代码》 《码码购分布式电商实战代码》]]></content>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F10.LinkedHashSet%2F</url>
    <content type="text"><![CDATA[HashSet 和 LinkedHashSet 的关系类似于 HashMap 和 LinkedHashMap 的关系，即后者维护双向链表，实现迭代顺序可为插入顺序或是访问顺序。所以也就轻松加愉快快速了解一下即可。 从源码中可以看到其空的构造函数为： 123public LinkedHashSet() &#123; super(16, .75f, true);&#125; 这个super即父类是HashSet，从它的继承关系就可以显然看到： 123public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable 那么HashSet内部的数据结构就是一个 HashMap，其方法的内部几乎就是在调用 HashMap 的方法。 LinkedHashSet 首先我们需要知道的是它是一个 Set 的实现，所以它其中存的肯定不是键值对，而是值。此实现与 HashSet 的不同之处在于，LinkedHashSet 维护着一个运行于所有条目的双向循环链表。 这一切都与LinkedHashMap类似。 LinkedHashSet 内部有个属性 accessOrder 控制着遍历次序。默认情况下该值为 false ,即按插入排序访问。如果将该值设置为 true 的话，则按访问次序排序(即最近最少使用算法，最近最少使用的放在链表头部，最近访问的则在链表尾部)。 一、 示例 HashSet的遍历： 123456789101112public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new HashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125;&#125; 输出结果是： aaa ccc bbb eee LinkedHashSet的遍历： 1234567891011121314public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new LinkedHashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); linkedHashSet.add(null); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; 输出结果是： aaa eee ccc bbb null 可以看到与输入顺序是一致的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashtable]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F9.HashTable%2F</url>
    <content type="text"><![CDATA[Hashtable 是个过时的集合类，不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换。但这并不是我们不去了解它的理由。最起码 Hashtable 和 HashMap 的面试题在面试中经常被问到。 一、前言 Hashtable和HashMap，从存储结构和实现来讲基本上都是相同的。 它和HashMap的最大的不同是它是线程安全的，另外它不允许key和value为null。 为了能在哈希表中成功地保存和取出对象，用作key的对象必须实现hashCode方法和equals方法。 二、fail-fast机制 iterator方法返回的迭代器是fail-fast的。如果在迭代器被创建后hashtable被结构型地修改了，除了迭代器自己的remove方法，迭代器会抛出一个ConcurrentModificationException异常。 因此，面对在并发的修改，迭代器干脆利落的失败，而不是冒险的继续。 关于这个的理解，其实在上一章讲LinkedHashMap中的第八点提到： 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 简单说，就是两个线程同时分别进行修改和遍历时，会抛出这个异常。 面试题：集合在遍历过程中是否可以删除元素，为什么迭代器就可以安全删除元素？ 集合在使用 for 循环迭代的过程中不允许使用，集合本身的 remove 方法删除元素，如果进行错误操作将会导致 ConcurrentModificationException 异常的发生 Iterator 可以删除访问的当前元素(current)，一旦删除的元素是Iterator 对象中 next 所正在引用的，在 Iterator 删除元素通过 修改 modCount 与 expectedModCount 的值，可以使下次在调用 remove 的方法时候两者仍然相同因此不会有异常产生。 迭代器的fail-fast机制并不能得到保证，它不能够保证一定出现该错误。一般来说，fail-fast会尽最大努力抛出ConcurrentModificationException异常。因此，为提高此类操作的正确性而编写一个依赖于此异常的程序是错误的做法，正确做法是：ConcurrentModificationException 应该仅用于检测 bug。 Hashtable是线程安全的。如果不需要线程安全的实现是不需要的，推荐使用HashMap代替Hashtable。如果需要线程安全的实现，推荐使用java.util.concurrent.ConcurrentHashMap代替Hashtable。 二、继承关系 123public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable&#123;&#125; extends Dictionary&lt;K,V&gt;：Dictionary类是一个抽象类，用来存储键/值对，作用和Map类相似。 implements Map&lt;K,V&gt;：实现了Map，实现了Map中声明的操作和default方法。 hashMap以及TreeMap的源码，都没有继承于这个类。不过当我看到注释中的解释也就明白了，其 Dictionary 源码注释是这样的：NOTE: This class is obsolete. New implementations should implement the Map interface, rather than extending this class. 该话指出 Dictionary 这个类过时了，新的实现类应该实现Map接口。 三、属性 1234567891011121314//哈希表private transient Entry&lt;?,?&gt;[] table;//记录哈希表中键值对的个数private transient int count;//扩容的阈值private int threshold;//负载因子private float loadFactor;//hashtable被结构型修改的次数。private transient int modCount = 0; HashTable并没有像HashMap那样定义了很多的常量，而是直接写死在了方法里。 Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。 四、构造函数 12345678/** * 使用默认初始化容量（11）和默认负载因子（0.75）来构造一个空的hashtable. * * 这里可以看到，Hashtable默认初始化容量为16，而HashMap的默认初始化容量为11。 */public Hashtable() &#123; this(11, 0.75f);&#125; 我们可以获取到这些信息：HashTable默认的初始化容量为11（与HashMap不同），负载因子默认为0.75（与HashMap相同）。而正因为默认初始化容量的不同，同时也没有对容量做调整的策略，所以可以先推断出，HashTable使用的哈希函数跟HashMap是不一样的（事实也确实如此）。 五、重要方法 5.1 get方法 12345678910111213public synchronized V get(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //通过哈希函数，计算出key对应的桶的位置 int index = (hash &amp; 0x7FFFFFFF) % tab.length; //遍历该桶的所有元素，寻找该key for (Entry&lt;?,?&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return (V)e.value; &#125; &#125; return null;&#125; 这里可以看到，Hashtable和HashMap确认key在数组中的索引的方法不同。 Hashtable通过index = (hash &amp; 0x7FFFFFFF) % tab.length;来确认 HashMap通过i = (n - 1) &amp; hash;来确认 跟HashMap相比，HashTable的get方法非常简单。我们首先可以看见get方法使用了synchronized来修饰，所以它能保证线程安全。并且它是通过链表的方式来处理冲突的。另外，我们还可以看见HashTable并没有像HashMap那样封装一个哈希函数，而是直接把哈希函数写在了方法中。而哈希函数也是比较简单的，它仅对哈希表的长度进行了取模。 5.2 put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public synchronized V put(K key, V value) &#123; // 确认value不为null if (value == null) &#123; throw new NullPointerException(); &#125; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //找到key在table中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") //获取key所在索引的entry Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，判断key是否已经存在 for(; entry != null ; entry = entry.next) &#123; //如果key已经存在 if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; //保存旧的value V old = entry.value; //替换value entry.value = value; //返回旧的value return old; &#125; &#125; //如果key在hashtable不是已经存在，就直接将键值对添加到table中，返回null addEntry(hash, key, value, index); return null;&#125;private void addEntry(int hash, K key, V value, int index) &#123; modCount++; Entry&lt;?,?&gt; tab[] = table; //哈希表的键值对个数达到了阈值，则进行扩容 if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; //把新节点插入桶中（头插法） tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++;&#125; 从代码中可以总结出Hashtable的put方法的总体思路： 确认value不为null。如果为null，则抛出异常 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key已经存在，替换value，返回旧的value 如果key在hashtable不是已经存在，就直接添加，否则直接将键值对添加到table中，返回null 在方法中可以看到，在遍历桶中元素时，是按照链表的方式遍历的。可以印证，HashMap的桶中可能为链表或者树。但Hashtable的桶中只可能是链表。 5.3 remove方法 12345678910111213141516171819202122232425public synchronized V remove(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //计算key在hashtable中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，如果entry中存在key为参数key的键值对，就删除键值对，并返回键值对的value for(Entry&lt;K,V&gt; prev = null ; e != null ; prev = e, e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; modCount++; if (prev != null) &#123; prev.next = e.next; &#125; else &#123; tab[index] = e.next; &#125; count--; V oldValue = e.value; e.value = null; return oldValue; &#125; &#125; //如果不存在key为参数key的键值对，返回value return null;&#125; 从代码中可以总结出Hashtable的remove方法的总体思路： 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key存在，删除key映射的键值对，返回旧的value 如果key在hashtable不存在，返回null 5.4 rehash方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 增加hashtable的容量，为了更有效地存放和找到它的entry。 * 当键值对的数量超过了临界值（capacity*load factor）这个方法自动调用 * 长度变为原来的2倍+1 * */@SuppressWarnings("unchecked")protected void rehash() &#123; //记录旧容量 int oldCapacity = table.length; //记录旧桶的数组 Entry&lt;?,?&gt;[] oldMap = table; // overflow-conscious code //新的容量为旧的容量的2倍+1 int newCapacity = (oldCapacity &lt;&lt; 1) + 1; //如果新的容量大于容量的最大值MAX_ARRAY_SIZE if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; //如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; //如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE newCapacity = MAX_ARRAY_SIZE; &#125; //创建新的数组，容量为新容量 Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; //结构性修改次数+1 modCount++; //计算扩容的临界值 threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; //将旧的数组中的键值对转移到新数组中 for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125;&#125; 看完代码，我们可以总结出rehash的总体思路为： 新建变量新的容量，值为旧的容量的2倍+1 如果新的容量大于容量的最大值MAX_ARRAY_SIZE 如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE 创建新的数组，容量为新容量 将旧的数组中的键值对转移到新数组中 这里可以看到，一般情况下，HashMap扩容后容量变为原来的两倍，而Hashtable扩容后容量变为原来的两倍加一。 HashTable的rehash方法相当于HashMap的resize方法。跟HashMap那种巧妙的rehash方式相比，HashTable的rehash过程需要对每个键值对都重新计算哈希值，而比起异或和与操作，取模是一个非常耗时的操作，所以这也是导致效率较低的原因之一。 六、遍历 可以使用与HashMap一样的遍历方式，但是由于历史原因，多了Enumeration的方式。 针对Enumeration，这里与iterator进行对比一下。 相同点 Iterator和Enumeration都可以对某些容器进行遍历。 Iterator和Enumeration都是接口。 不同点 Iterator有对容器进行修改的方法。而Enumeration只能遍历。 Iterator支持fail-fast，而Enumeration不支持。 Iterator比Enumeration覆盖范围广，基本所有容器中都有Iterator迭代器，而只有Vector、Hashtable有Enumeration。 Enumeration在JDK 1.0就已经存在了，而Iterator是JDK2.0新加的接口。 七、Hashtable与HashMap对比 HashTable的应用非常广泛，HashMap是新框架中用来代替HashTable的类，也就是说建议使用HashMap。 下面着重比较一下二者的区别： 1.继承不同 Hashtable是基于陈旧的Dictionary类的，HashMap是java1.2引进的Map接口的一个实现。 2.同步 Hashtable 中的方法是同步的，保证了Hashtable中的对象是线程安全的。 HashMap中的方法在缺省情况下是非同步的,HashMap中的对象并不是线程安全的。在多线程并发的环境下，可以直接使用Hashtable，但是要使用HashMap的话就要自己增加同步处理了。 3.效率 单线程中, HashMap的效率大于Hashtable。因为同步的要求会影响执行的效率，所以如果你不需要线程安全的集合，HashMap是Hashtable的轻量级实现，这样可以避免由于同步带来的不必要的性能开销，从而提高效率。 4.null值 Hashtable中，key和value都不允许出现null值，否则出现NullPointerException。 在HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键，而应该用containsKey()方法来判断。 5.遍历方式 Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable可以使用Enumeration的方式。 6.容量 Hashtable和HashMap它们两个内部实现方式的数组的初始大小和扩容的方式。 HashTable中hash数组默认大小是11，增加的方式是 old*2+1。 HashMap中hash数组的默认大小是16，而且一定是2的指数。 八、总结 无论什么时候有多个线程访问相同实例的可能时，就应该使用Hashtable，反之使用HashMap。非线程安全的数据结构能带来更好的性能。 如果在将来有一种可能—你需要按顺序获得键值对的方案时，HashMap是一个很好的选择，因为有HashMap的一个子类 LinkedHashMap。 所以如果你想可预测的按顺序迭代（默认按插入的顺序），你可以很方便用LinkedHashMap替换HashMap。反观要是使用的Hashtable就没那么简单了。 如果有多个线程访问HashMap，Collections.synchronizedMap（）可以代替，总的来说HashMap更灵活，或者直接用并发容器ConcurrentHashMap。 整理自： http://blog.csdn.net/panweiwei1994/article/details/77428710 http://blog.csdn.net/u013124587/article/details/52655042]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap和LinkedHashMap遍历机制]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F8.HashMap%E5%92%8CLinkedHashMap%E9%81%8D%E5%8E%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本篇单独讲一下HashMap和LinkedHashMap遍历方式。 一、对HashMap和LinkedHashMap遍历的几种方法 这里以HashMap为例，LinkedHashMap一样的方式。 12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println("key=" + next.getKey() + " value=" + next.getValue());&#125; 123456Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println("key=" + key + " value=" + map.get(key));&#125; 123map.forEach((key,value)-&gt;&#123; System.out.println("key=" + key + " value=" + value);&#125;); 强烈建议使用第一种 EntrySet 进行遍历。 第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。 我们知道，HashMap的输出顺序与元素的输入顺序无关，LinkedHashMap可以按照输入顺序输出，也可以根据读取元素的顺序输出。这一现象，已经在上一篇中展示出来了。 二、HashMap的遍历机制 HashMap 提供了两个遍历访问其内部元素Entry&lt;k,v&gt;的接口： Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()-------&gt;返回此映射所包含的映射关系的 Set 视图。 Set&lt;K&gt; keySet()--------&gt;返回此映射中所包含的键的 Set 视图。 实际上，第二个接口表示的Key的顺序，和第一个接口返回的Entry顺序是对应的，也就是说：这两种接口对HashMap的元素遍历的顺序相相同的。 那么，HashMap遍历内部Entry&lt;K,V&gt; 的顺序是什么呢？ 搞清楚这个问题，先要知道其内部结构是怎样的。 HashMap在存储Entry对象的时候，是根据Key的hash值判定存储到Entry[] table数组的哪一个索引值表示的链表上。 对HashMap遍历Entry对象的顺序和Entry对象的存储顺序之间没有任何关系。 HashMap散列图、Hashtable散列表是按“有利于随机查找的散列(hash)的顺序”。并非按输入顺序。遍历时只能全部输出，而没有顺序。甚至可以rehash()重新散列，来获得更利于随机存取的内部顺序。 所以对HashMap的遍历，由内部的机制决定的，这个机制是只考虑利于快速存取，不考虑输入等顺序。 三、LinkedHashMap 的遍历机制 LinkedHashMap 是HashMap的子类，它可以实现对容器内Entry的存储顺序和对Entry的遍历顺序保持一致。 为了实现这个功能，LinkedHashMap内部使用了一个Entry类型的双向链表，用这个双向链表记录Entry的存储顺序。当需要对该Map进行遍历的时候，实际上是遍历的是这个双向链表。 LinkedHashMap内部使用的LinkedHashMap.Entry类继承自Map.Entry类，在其基础上增加了LinkedHashMap.Entry类型的两个字段，用来引用该Entry在双向链表中的前面的Entry对象和后面的Entry对象。 它的内部会在Map.Entry类的基础上，增加两个Entry类型的引用：before，after。LinkedHashMap使用一个双向连表，将其内部所有的Entry串起来。 1234LinkedHashMap linkedHashMap = new LinkedHashMap(); linkedHashMap.put("name","louis"); linkedHashMap.put("age","24"); linkedHashMap.put("sex","male"); 对LinkedHashMap进行遍历的策略： 从 header.after 指向的Entry对象开始，然后一直沿着此链表遍历下去，直到某个entry.after == header 为止，完成遍历。 根据Entry&lt;K,V&gt;插入LinkedHashMap的顺序进行遍历的方式叫做：按插入顺序遍历。 另外，LinkedHashMap还支持一种遍历顺序，叫做：Get读取顺序。 如果LinkedHashMap的这个Get读取遍历顺序开启，那么，当我们在LinkedHashMap上调用get(key) 方法时，会导致内部key对应的Entry在双向链表中的位置移动到双向链表的最后。 四、遍历机制的总结 HashMap对元素的遍历顺序跟Entry插入的顺序无关，而LinkedHashMap对元素的遍历顺序可以跟Entry&lt;K,V&gt;插入的顺序保持一致：从双向。 当LinkedHashMap处于Get获取顺序遍历模式下，当执行get() 操作时，会将对应的Entry&lt;k,v&gt;移到遍历的最后位置。 LinkedHashMap处于按插入顺序遍历的模式下，如果新插入的&lt;key,value&gt; 对应的key已经存在，对应的Entry在遍历顺序中的位置并不会改变。 除了遍历顺序外，其他特性HashMap和LinkedHashMap基本相同。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F7.LinkedHashMap%2F</url>
    <content type="text"><![CDATA[大多数情况下，只要不涉及线程安全问题， Map 基本都可以使用 HashMap ，不过 HashMap 有一个问题，就是迭代 HashMap 的顺序并不是 HashMap 放置的顺序，也就是无序。 HashMap 的这一缺点往往会带来困扰，因为有些场景，我们期待一个有序的 Map。 篇幅有点长，但是在理解了HashMap之后就比较简单了。 这个时候，LinkedHashMap就闪亮登场了，它虽然增加了时间和空间上的开销，但是可以解决有排序需求的场景。 它的底层是继承于 HashMap 实现的，由一个双向循环链表所构成。 LinkedHashMap 的排序方式有两种： 根据写入顺序排序。 根据访问顺序排序。 其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。 一、LinkedHashMap数据结构 LinkedHashMap是通过哈希表和双向循环链表实现的，它通过维护一个双向循环链表来保证对哈希表迭代时的有序性，而这个有序是指键值对插入的顺序。 我们可以看出，遍历所有元素只需要从header开始遍历即可，一直遍历到下一个元素是header结束。 另外，当向哈希表中重复插入某个键的时候，不会影响到原来的有序性。也就是说，假设你插入的键的顺序为1、2、3、4，后来再次插入2，迭代时的顺序还是1、2、3、4，而不会因为后来插入的2变成1、3、4、2。（但其实我们可以改变它的规则，使它变成1、3、4、2） LinkedHashMap的实现主要分两部分，一部分是哈希表，另外一部分是链表。哈希表部分继承了HashMap，拥有了HashMap那一套高效的操作，所以我们要看的就是LinkedHashMap中链表的部分，了解它是如何来维护有序性的。 二、demo示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public static void main(String[] args) &#123; /** * HashMap插入数据，遍历输出无序 */ System.out.println("----------HashMap插入数据--------"); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("apple", "a"); map.put("watermelon", "b"); map.put("banana", "c"); map.put("peach", "d"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap插入数据，遍历，默认以插入顺序为序 */ System.out.println("----------LinkedHashMap插入数据,按照插入顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap = new LinkedHashMap&lt;&gt;(); linkedHashMap.put("apple", "a"); linkedHashMap.put("watermelon", "b"); linkedHashMap.put("banana", "c"); linkedHashMap.put("peach", "d"); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = linkedHashMap.entrySet().iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; /** * LinkedHashMap插入数据，设置accessOrder=true实现使得其遍历顺序按照访问的顺序输出，这里先用get方法来演示 */ System.out.println("----------LinkedHashMap插入数据,accessOrder=true:按照访问顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap2 = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); linkedHashMap2.put("apple", "aa"); linkedHashMap2.put("watermelon", "bb"); linkedHashMap2.put("banana", "cc"); linkedHashMap2.put("peach", "dd"); linkedHashMap2.get("banana");//banana移动到了内部的链表末尾 linkedHashMap2.get("apple");//apple移动到了内部的链表末尾 Iterator iter2 = linkedHashMap2.entrySet().iterator(); while (iter2.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter2.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap的put方法在accessOrder=true的情况下 */ System.out.println("-----------"); linkedHashMap2.put("watermelon", "bb");//watermelon移动到了内部的链表末尾 linkedHashMap2.put("stawbarrey", "ee");//末尾插入新元素stawbarrey linkedHashMap2.put(null, null);//插入新的节点 null Iterator iter3 = linkedHashMap2.entrySet().iterator(); while (iter3.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter3.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; &#125; 输出结果是: 12345678910111213141516171819202122----------HashMap插入数据--------banana=capple=apeach=dwatermelon=b----------LinkedHashMap插入数据,按照插入顺序进行排序--------apple=awatermelon=bbanana=cpeach=d----------LinkedHashMap插入数据,按照访问顺序进行排序--------watermelon=bbpeach=ddbanana=cc//banana到了末尾apple=aa//apple到了末尾-----------peach=ddbanana=ccapple=aawatermelon=bb//watermelon到了链表末尾stawbarrey=ee//新插入的放在末尾null=null//新插入的放在末尾 三、属性 LinkedHashMap可以认为是HashMap+LinkedList，即它既使用HashMap操作数据结构，又使用LinkedList维护插入元素的先后顺序 3.1 继承关系 123public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; LinkedHashMap是HashMap的子类，自然LinkedHashMap也就继承了HashMap中所有非private的方法。所以它已经从 HashMap 那里继承了与哈希表相关的操作了，那么在LinkedHashMap中，它可以专注于链表实现的那部分，所以与链表实现相关的属性如下。 3.2 属性介绍 1234567891011121314151617//LinkedHashMap的链表节点继承了HashMap的节点，而且每个节点都包含了前指针和后指针，所以这里可以看出它是一个双向链表static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125;//头指针transient LinkedHashMap.Entry&lt;K,V&gt; head;//尾指针transient LinkedHashMap.Entry&lt;K,V&gt; tail;//默认为false。当为true时，表示链表中键值对的顺序与每个键的插入顺序一致，也就是说重复插入键，也会更新顺序//简单来说，为false时，就是上面所指的1、2、3、4的情况；为true时，就是1、3、4、2的情况final boolean accessOrder; 五、构造方法 1234public LinkedHashMap() &#123; super(); accessOrder = false;&#125; 其实就是调用的 HashMap 的构造方法: HashMap 实现： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap init();&#125; 可以看到里面有一个空的 init()，具体是由 LinkedHashMap 来实现的： 12345@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 其实也就是对 header 进行了初始化。 六、添加元素 LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法. newNode() 会在HashMap的putVal() 方法里被调用，putVal() 方法会在批量插入数据putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) 或者插入单个数据public V put(K key, V value)时被调用。 LinkedHashMap重写了newNode(),在每次构建新节点时，通过linkNodeLast(p);将新节点链接在内部双向链表的尾部。 12345678910111213141516171819//在构建新节点时，构建的是`LinkedHashMap.Entry` 不再是`Node`.Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125;//将新增的节点，连接在链表的尾部private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; //集合之前是空的 if (last == null) head = p; else &#123;//将新节点连接在链表的尾部 p.before = last; last.after = p; &#125;&#125; 以及HashMap专门预留给LinkedHashMap的afterNodeAccess() 、afterNodeInsertion() 、afterNodeRemoval() 方法。 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 如果你没有注意到注释的解释的话，你可能会很奇怪为什么会有三个空方法，而且有不少地方还调用过它们。其实这三个方法表示的是在访问、插入、删除某个节点之后，进行一些处理，它们在LinkedHashMap有各自的实现。LinkedHashMap正是通过重写这三个方法来保证链表的插入、删除的有序性。 123456789101112131415161718//回调函数，新节点插入之后回调,判断是否需要删除最老插入的节点。//如果实现LruCache会用到这个方法。void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; //LinkedHashMap 默认返回false 则不删除节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;//LinkedHashMap 默认返回false 则不删除节点。 //返回true 代表要删除最早的节点。//通常构建一个LruCache会在达到Cache的上限是返回trueprotected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; void afterNodeInsertion(boolean evict)以及boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) 是构建LruCache需要的回调，在这可以忽略它们。 七、删除元素 LinkedHashMap也没有重写remove() 方法，因为它的删除逻辑和HashMap并无区别。 但它重写了afterNodeRemoval() 这个回调方法。该方法会在Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) 方法中回调，removeNode() 会在所有涉及到删除节点的方法中被调用，上文分析过，是删除节点操作的真正执行者。 1234567891011121314151617//在删除节点e时，同步将e从双向链表上删除void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //待删除节点 p 的前置后置节点都置空 p.before = p.after = null; //如果前置节点是null，则现在的头结点应该是后置节点a if (b == null) head = a; else//否则将前置节点b的后置节点指向a b.after = a; //同理如果后置节点时null ，则尾节点应是b if (a == null) tail = b; else//否则更新后置节点a的前置节点为b a.before = b;&#125; 八、查询元素 LinkedHashMap重写了get()和getOrDefault() 方法： 12345678910111213141516public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125;public V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return defaultValue; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 对比HashMap中的实现,LinkedHashMap只是增加了在成员变量(构造函数时赋值)accessOrder为true的情况下，要去回调void afterNodeAccess(Node&lt;K,V&gt; e) 函数。 1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 在afterNodeAccess() 函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。 12345678910111213141516171819202122232425262728293031void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last;//原尾节点 //如果accessOrder 是true ，且原尾节点不等于e if (accessOrder &amp;&amp; (last = tail) != e) &#123; //节点e强转成双向链表节点p LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //p现在是尾节点， 后置节点一定是null p.after = null; //如果p的前置节点是null，则p以前是头结点，所以更新现在的头结点是p的后置节点a if (b == null) head = a; else//否则更新p的前直接点b的后置节点为 a b.after = a; //如果p的后置节点不是null，则更新后置节点a的前置节点为b if (a != null) a.before = b; else//如果原本p的后置节点是null，则p就是尾节点。 此时 更新last的引用为 p的前置节点b last = b; if (last == null) //原本尾节点是null 则，链表中就一个节点 head = p; else &#123;//否则 更新 当前节点p的前置节点为 原尾节点last， last的后置节点是p p.before = last; last.after = p; &#125; //尾节点的引用赋值成p tail = p; //修改modCount。 ++modCount; &#125;&#125; 图示(注意这个图，1和6也应该是连在一起的，因为是双向循环链表，所以视为一个小错误)： 说明：从图中可以看到，结点3链接到了尾结点后面。 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 九、判断元素是否存在 它重写了该方法，相比HashMap的实现，更为高效。 123456789public boolean containsValue(Object value) &#123; //遍历一遍链表，去比较有没有value相等的节点，并返回 for (LinkedHashMap.Entry&lt;K,V&gt; e = head; e != null; e = e.after) &#123; V v = e.value; if (v == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; return false;&#125; 对比HashMap，是用两个for循环遍历，相对低效。 12345678910111213public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false;&#125; 十、替换某个元素 1234567891011121314// 用dst替换srcprivate void transferLinks(LinkedHashMap.Entry&lt;K,V&gt; src, LinkedHashMap.Entry&lt;K,V&gt; dst) &#123; LinkedHashMap.Entry&lt;K,V&gt; b = dst.before = src.before; LinkedHashMap.Entry&lt;K,V&gt; a = dst.after = src.after; if (b == null) head = dst; else b.after = dst; if (a == null) tail = dst; else a.before = dst;&#125; 十二、总结 LinkedHashMap相对于HashMap的源码比，是很简单的。因为大树底下好乘凉。它继承了HashMap，仅重写了几个方法，以改变它迭代遍历时的顺序。这也是其与HashMap相比最大的不同。 在每次插入数据，或者访问、修改数据时，会增加节点、或调整链表的节点顺序。以决定迭代时输出的顺序。 accessOrder默认是false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。为true时，可以在这基础之上构建一个LruCache. LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法.在每次构建新节点时，将新节点链接在内部双向链表的尾部 accessOrder=true的模式下,在afterNodeAccess()函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。值得注意的是，afterNodeAccess()函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 nextNode() 就是迭代器里的next()方法 。该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。 而双链表节点的顺序在LinkedHashMap的增、删、改、查时都会更新。以满足按照插入顺序输出，还是访问顺序输出。 它与HashMap比，还有一个小小的优化，重写了containsValue()方法，直接遍历内部链表去比对value值是否相等。 整理自： http://blog.csdn.net/zxt0601/article/details/77429150 http://wiki.jikexueyuan.com/project/java-collection/linkedhashmap.html http://blog.csdn.net/u013124587/article/details/52659741 http://www.cnblogs.com/leesf456/p/5248868.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F6.HashSet%2F</url>
    <content type="text"><![CDATA[HashSet 是一个不允许存储重复元素的集合，它是基于 HashMap 实现的， HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成。所以只要理解了 HashMap，HashSet 就水到渠成了。 成员变量 首先了解下HashSet的成员变量: 1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 发现主要就两个变量: map ：用于存放最终数据的。 PRESENT ：是所有写入map的value值。 构造方法 1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 构造函数很简单，利用了HashMap初始化了map。 add 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 比较关键的就是这个 add() 方法。 可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会受到影响，这样就保证了 HashSet 中只能存放不重复的元素。 该方法如果添加的是在 HashSet 中不存在的，则返回 true；如果添加的元素已经存在，返回 false。其原因在于我们之前提到的关于 HashMap 的 put 方法。该方法在添加 key 不重复的键值对的时候，会返回 null。 总结 HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。 所以 HashMap 会出现的问题 HashSet 依然不能避免。 对于 HashSet 中保存的对象，请注意正确重写其 equals 和 hashCode 方法，以保证放入的对象的唯一性。这两个方法是比较重要的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F5.HashMap%2F</url>
    <content type="text"><![CDATA[HashMap基本是面试必问的点，因为这个数据结构用的太频繁了，jdk1.8中的优化也是比较巧妙。有必要去深入探讨一下。但是涉及的内容比较多，这里只先探讨jdk8中HashMap的实现，至于jdk7中HashMap的死循环问题、红黑树的原理等都不会在本篇文章扩展到。其他的文章将会再去探讨整理。 本篇文章较长，高能预警。 一、前言 之前的List，讲了ArrayList、LinkedList，最后讲到了CopyOnWriteArrayList，就前两者而言，反映的是两种思想： （1）ArrayList以数组形式实现，顺序插入、查找快，插入、删除较慢 （2）LinkedList以链表形式实现，顺序插入、查找较慢，插入、删除方便 那么是否有一种数据结构能够结合上面两种的优点呢？有，答案就是HashMap。 HashMap是一种非常常见、方便和有用的集合，是一种键值对（K-V）形式的存储结构，在有了HashCode的基础后，下面将还是用图示的方式解读HashMap的实现原理。 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 二、HashMap的结构 其中哈希表是一个数组，我们经常把数组中的每一个节点称为一个桶，哈希表中的每个节点都用来存储一个键值对。 在插入元素时，如果发生冲突（即多个键值对映射到同一个桶上）的话，就会通过链表的形式来解决冲突。 因为一个桶上可能存在多个键值对，所以在查找的时候，会先通过key的哈希值先定位到桶，再遍历桶上的所有键值对，找出key相等的键值对，从而来获取value。 如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数： 容量 负载因子 容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。 三、继承关系 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 说明： HashMap继承自AbstractMap，AbstractMap是Map接口的骨干实现，AbstractMap中实现了Map中最重要最常用和方法，这样HashMap继承AbstractMap就不需要实现Map的所有方法，让HashMap减少了大量的工作。 四、属性 123456789101112131415161718192021222324252627//默认的初始容量为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//最大的容量上限为2^30static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的负载因子为0.75static final float DEFAULT_LOAD_FACTOR = 0.75f;//变成树型结构的临界值为8static final int TREEIFY_THRESHOLD = 8;//恢复链式结构的临界值为6static final int UNTREEIFY_THRESHOLD = 6;/** * 哈希表的最小树形化容量 * 当哈希表中的容量大于这个值时，表中的桶才能进行树形化 * 否则桶内元素太多时会扩容，而不是树形化 * 为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD */static final int MIN_TREEIFY_CAPACITY = 64;//哈希表transient Node&lt;K,V&gt;[] table;//哈希表中键值对的个数transient int size;//哈希表被修改的次数transient int modCount;//它是通过capacity*load factor计算出来的，当size到达这个值时，就会进行扩容操作int threshold;//负载因子final float loadFactor; 4.1 几个属性的详细说明 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍（为什么是两倍下文会说明）。 默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子`Load factor`的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子`loadFactor`的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意size和table的长度length、容纳最大键值对数量threshold的区别。 而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，因为常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考 http://blog.csdn.net/liuqiyao_01/article/details/14475159 ，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。下文会说明。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，并且链表的长度超过64时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 这里着重提一下MIN_TREEIFY_CAPACITY字段，容易与TREEIFY_THRESHOLD打架，TREEIFY_THRESHOLD是指桶中元素达到8个，就将其本来的链表结构改为红黑树，提高查询的效率。MIN_TREEIFY_CAPACITY是指最小树化的哈希表元素个数，也就是说，小于这个值，就算你(数组)桶里的元素数量大于8了，还是要用链表存储，只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。注意扩容是数组的复制。 4.2 Node结构 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 例如程序执行下面代码： 1map.put("美团","小美"); 系统将调用&quot;美团&quot;这个key的hashCode()方法得到其hashCode值（该方法适用于每个Java对象）。 然后再通过Hash算法来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。 当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。 那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法(5.4节)和扩容机制(5.5节)。下文会讲到。 五、方法 5.1 get方法 123456789101112131415161718192021222324252627282930//get方法主要调用的是getNode方法，所以重点要看getNode方法的实现public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //如果哈希表不为空 &amp;&amp; key对应的桶上不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //是否直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //判断是否有后续节点 if ((e = first.next) != null) &#123; //如果当前的桶是采用红黑树处理冲突，则调用红黑树的get方法去获取节点 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //不是红黑树的话，那就是传统的链式结构了，通过循环的方法判断链中是否存在该key do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 实现步骤大致如下： 通过hash值获取该key映射到的桶。 桶上的key就是要查找的key，则直接命中。 桶上的key不是要查找的key，则查看后续节点： 如果后续节点是树节点，通过调用树的方法查找该key。 如果后续节点是链式节点，则通过循环遍历链查找该key。 5.2 put方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法的具体实现也是在putVal方法中，所以我们重点看下面的putVal方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果哈希表为空，则先创建一个哈希表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果当前桶没有碰撞冲突，则直接把键值对插入，完事 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果桶上节点的key与当前key重复，那你就是我要找的节点了 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果是采用红黑树的方式处理冲突，则通过红黑树的putTreeVal方法去插入这个键值对 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //否则就是传统的链式结构 else &#123; //采用循环遍历的方式，判断链中是否有重复的key for (int binCount = 0; ; ++binCount) &#123; //到了链尾还没找到重复的key，则说明HashMap没有包含该键 if ((e = p.next) == null) &#123; //创建一个新节点插入到尾部 p.next = newNode(hash, key, value, null); //如果链的长度大于TREEIFY_THRESHOLD这个临界值，则把链变为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //找到了重复的key if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //这里表示在上面的操作中找到了重复的键，所以这里把该键的值替换为新值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //判断是否需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 1234567891011121314151617181920final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; put方法比较复杂，实现步骤大致如下： 先通过hash值计算出key映射到哪个桶。 如果桶上没有碰撞冲突，则直接插入。 如果出现碰撞冲突了，则需要处理冲突： 如果该桶使用红黑树处理冲突，则调用红黑树的方法插入。 否则采用传统的链式方法插入。如果链的长度到达临界值，则把链转变为红黑树。 如果桶中存在重复的键，则为该键替换新值。 如果size大于阈值，则进行扩容。 5.3 remove方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//remove方法的具体实现在removeNode方法中，所以我们重点看下面的removeNode方法public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果当前key映射到的桶不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //如果桶上的节点就是要找的key，则直接命中 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //如果是以红黑树处理冲突，则构建一个树节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); //如果是以链式的方式处理冲突，则通过遍历链表来寻找节点 else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //比对找到的key的value跟要删除的是否匹配 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //通过调用红黑树的方法来删除节点 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //使用链表的操作来删除节点 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 5.4 hash方法(确定哈希桶数组索引位置) 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。 注意get方法和put方法源码中都需要先计算key映射到哪个桶上，然后才进行之后的操作，计算的主要代码如下： 1(n - 1) &amp; hash 上面代码中的n指的是哈希表的大小，hash指的是key的哈希值，hash是通过下面这个方法计算出来的，采用了二次哈希的方式，其中key的hashCode方法是一个native方法： 123456static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 总结就是：由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2^n 。这样用 2^n - 1 做位运算与取模效果一致，并且效率还要高出许多。这样回答了上文中：好的Hash算法到底是什么。 5.5 resize方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //计算扩容后的大小 if (oldCap &gt; 0) &#123; //如果当前容量超过最大容量，则无法进行扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //没超过最大值则扩为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //新的resize阈值 threshold = newThr; //创建新的哈希表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; //遍历旧哈希表的每个桶，重新计算桶里元素的新位置 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //如果桶上只有一个键值对，则直接插入 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果是通过红黑树来处理冲突的，则调用相关方法把树分离开 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果采用链式处理冲突 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //通过上面讲的方法来计算节点的新位置 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。 例如，原来的容量为32，那么应该拿hash跟31（0x11111）做与操作；在扩容扩到了64的容量之后，应该拿hash跟63（0x111111）做与操作。新容量跟原来相比只是多了一个bit位，假设原来的位置在23，那么当新增的那个bit位的计算结果为0时，那么该节点还是在23；相反，计算结果为1时，则该节点会被分配到23+31的桶上。 这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。回答了上文中好的扩容机制。 六、总结 HashMap的结构底层是一个数组，每个数组元素是一个桶，后面可能会连着一串因为碰撞而聚在一起的(key,value)节点，以链表的形式或者树的形式挂着 按照原来的拉链法来解决冲突，如果一个桶上的冲突很严重的话，是会导致哈希表的效率降低至O（n），而通过红黑树的方式，可以把效率改进至O（logn）。相比链式结构的节点，树型结构的节点会占用比较多的空间，所以这是一种以空间换时间的改进方式。 threshold是数组长度扩容的临界值 modCount字段主要用来记录HashMap内部结构发生变化的次数，这里结构变化必须是新的值塞进来或者某个值删除这种类型，而不是仅仅是覆盖 只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。 好的hash算法：由于在计算中位运算比取模运算效率高的多，所以HashMap规定数组的长度为 2^n 。这样用 2^n - 1 与 hash 做位运算与取模效果一致，并且效率还要高出许多。 好的扩容机制：因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。 还有就是要记住put的过程。 整理自： https://zhuanlan.zhihu.com/p/21673805 http://blog.csdn.net/u013124587/article/details/52649867]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashcode/Equals]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F4.hashcode%E5%92%8Cequals%2F</url>
    <content type="text"><![CDATA[hashcode涉及到集合HashMap等集合，此篇侧重于了解hashcode和equals方法的作用的原理。有助于下一篇HashMap的理解。 一、Hash是什么 Hash是散列的意思，就是把任意长度的输入，通过散列算法变换成固定长度的输出，该输出就是散列值。这个玩意还可以做加密。 不同关键字经过散列算法变换后可能得到同一个散列地址，这种现象称为碰撞。 如果两个Hash值不同（前提是同一Hash算法），那么这两个Hash值对应的原始输入必定不同 二、什么是hashcode HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的。 如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置。 如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写。为什么呢？下文会说。 三、HashCode有什么用 比方说Set里面已经有1000个元素了，那么第1001个元素进来的时候，最多可能调用1000次equals方法，如果equals方法写得复杂，对比的东西特别多，那么效率会大大降低。 使用HashCode就不一样了，比方说HashSet，底层是基于HashMap实现的，先通过HashCode取一个模，这样一下子就固定到某个位置了，如果这个位置上没有元素，那么就可以肯定HashSet中必定没有和新添加的元素equals的元素，就可以直接存放了，都不需要比较； 如果这个位置上有元素了，逐一比较，比较的时候先比较HashCode，HashCode都不同接下去都不用比了，肯定不一样，HashCode相等，再equals比较，没有相同的元素就存，有相同的元素就不存。 如果原来的Set里面有相同的元素，只要HashCode的生成方式定义得好（不重复），不管Set里面原来有多少元素，只需要执行一次的equals就可以了。这样一来，实际调用equals方法的次数大大降低，提高了效率。 当俩个对象的`hashCode`值相同的时候，`Hashset`会将对象保存在同一个位置，但是他们`equals`返回`false`，所以实际上这个位置采用链式结构来保存多个对象。 四、为什么重写Object的equals()方法尽量要重写Object的hashCode()方法 面临问题：若两个对象equals相等，但由于不在一个区间，因为hashCode的值在重写之前是对内存地址计算得出，所以根本没有机会进行比较，会被认为是不同的对象(这就是为什么还要重写hashcode方法了)。所以Java对于eqauls方法和hashCode方法是这样规定的： 1 如果两个对象相同(equals为true)，那么它们的hashCode值一定要相同。也告诉我们重写equals方法，一定要重写hashCode方法，也就是说hashCode值要和类中的成员变量挂上钩，对象相同–&gt;成员变量相同—-&gt;hashCode值一定相同。 2 如果两个对象的hashCode相同(只是映射到同一个位置而已)，它们并不一定相同，这里的对象相同指的是用eqauls方法比较。 简单来说，如果只重写equals方法而不重写hashcode方法，会导致重复元素的产生。具体通过下面的例子进行说明。 五、举例 6.1 Student类 很简单，定义了id和name两个字段，无参和有参构造函数，toString方法。 1234567891011121314151617181920public class Student &#123; private int id; private String name; get(),set()略... public Student()&#123;&#125; public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; @Override public String toString() &#123; return "Student [id=" + id + ", name=" + name + "]"; &#125;&#125; 6.2 main方法 1234567891011121314151617181920public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 执行结果： 1234set集合容量为: 3Student [id=1, name=hh]---1735600054Student [id=1, name=hh]---356573597Student [id=2, name=gg]---21685669 我们可以看到，只要是new的对象，他们的hashcode是不一样的。所以，就会认为他们是不一样的对象。所以，集合里面数量为3. 6.3 只重写equals()方法，而不重写HashCode()方法 输出： 1234set集合容量为: 3Student [id=2, name=gg]---2018699554Student [id=1, name=hh]---366712642Student [id=1, name=hh]---1829164700 结论：覆盖equals（Object obj）但不覆盖hashCode(),导致数据不唯一性。 在这里，其实我们可以看到，student1和student2其实是一个对象，但是由于都是new并且没有重写hashcode导致他们变成了两个不一样的对象。 分析： （1）当执行set.add(student1)时，集合为空，直接存入集合； （2）当执行set.add(student2)时，首先判断该对象（student2）的hashCode值所在的存储区域是否有相同的hashCode，因为没有覆盖hashCode方法，所以jdk使用默认Object的hashCode方法，返回内存地址转换后的整数，因为不同对象的地址值不同，所以这里不存在与student2相同hashCode值的对象，因此jdk默认不同hashCode值，equals一定返回false，所以直接存入集合。 （3）当执行set.add(student3)时,与2同理。 （4）当最后执行set.add(student1)时，因为student1已经存入集合，同一对象返回的hashCode值是一样的，继续判断equals是否返回true，因为是同一对象所以返回true。此时jdk认为该对象已经存在于集合中，所以舍弃。 6.4 只重写HashCode()方法，equals()方法直接返回false 1234set集合容量为: 3Student [id=1, name=hh]---4320Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 按照上面的分析，可能会觉得里面应该装4个，因为两次add的student1，虽然他们的hashcode一样，但是equals直接返回false，那么应该判定为两个不同的对象。但是结果确跟我们预想的不一样。 分析： 首先student1和student2的对象比较hashCode，因为重写了HashCode方法，所以hashcode相等,然后比较他们两的equals方法，因为equals方法始终返回false,所以student1和student2也是不相等的，所以student2也被放进了set 首先student1(student2)和student3的对象比较hashCode，不相等，所以student3放进set中 最后再看最后重复添加的student1,与第一个student1的hashCode是相等的，在比较equals方法，因为equals返回false,所以student1和student4不相等;同样，student2和student4也是不相等的;student3和student4的hashcode都不相等，所以肯定不相等的，所以最后一个重复的student1应该可以放到set集合中，那么结果应该是size:4,那为什么会是3呢？ 这时候我们就需要查看HashSet的源码了，下面是HashSet中的add方法的源码： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 这里我们可以看到其实HashSet是基于HashMap实现的，我们在点击HashMap的put方法，源码如下： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 首先是判断hashCode是否相等，不相等的话，直接跳过，相等的话，然后再来比较这两个对象是否相等或者这两个对象的equals方法，因为是进行的或操作，所以只要有一个成立即可，那这里我们就可以解释了，其实上面的那个集合的大小是3,因为最后的一个r1没有放进去，以为r1==r1返回true的，所以没有放进去了。所以集合的大小是3，如果我们将hashCode方法设置成始终返回false的话，这个集合就是4了。 6.5 同时重写 我的写法是： 12345678910111213141516171819@Overridepublic int hashCode() &#123; int result = 17; result = result * 31 + name.hashCode(); result = result * 31 + id; return result;&#125;@Overridepublic boolean equals(Object obj) &#123; if(obj == this) return true; if(!(obj instanceof Student)) return false; Student o = (Student)obj; return o.name.equals(name) &amp;&amp; o.id == id;&#125; 结果： 123set集合容量为: 2Student [id=2, name=gg]---118515Student [id=1, name=hh]---119506 达到我们预期的效果。 六、内存泄露 我们上面实验了重写equals和hashcode方法，执行main，执行结果是： 123set集合容量为: 2Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 将main方法改为： 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); //------新增的开始------- student3.setId(11); set.remove(student3); System.out.println("set集合容量为: "+set.size()); //------新增的结束------- Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 运行结果是： 1234set集合容量为: 2set集合容量为: 2Student [id=1, name=hh]---4320Student [id=11, name=gg]---4598 我们调用了remove删除student3对象，以为删除了student3,但事实上并没有删除，这就叫做内存泄露，就是不用的对象但是他还在内存中。所以我们多次这样操作之后，内存就爆了。 原因： 在调用remove方法的时候，会先使用对象的hashCode值去找到这个对象，然后进行删除，这种问题就是因为我们在修改了对象student3的id属性的值，又因为RectObject对象的hashCode方法中有id值参与运算,所以student3对象的hashCode就发生改变了，所以remove方法中并没有找到student3了，所以删除失败。即student3的hashCode变了，但是他存储的位置没有更新，仍然在原来的位置上，所以当我们用他的新的hashCode去找肯定是找不到了。 总结： 上面的这个内存泄露告诉我一个信息：如果我们将对象的属性值参与了hashCode的运算中，在进行删除的时候，就不能对其属性值进行修改，否则会出现严重的问题。 七、总结 hashCode是为了提高在散列结构存储中查找的效率，在线性表中没有作用。 equals和hashCode需要同时覆盖。 若两个对象equals返回true，则hashCode有必要也返回相同的int数。 若两个对象equals返回false，则hashCode不一定返回不同的int数,但为不相等的对象生成不同hashCode值可以提高哈希表的性能。 若两个对象hashCode返回相同int数，则equals不一定返回true。 同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 整理自： http://blog.csdn.net/haobaworenle/article/details/53819838 http://www.cnblogs.com/xrq730/p/4842028.html http://blog.csdn.net/qq_21688757/article/details/53067814 http://blog.csdn.net/fyxxq/article/details/42066843]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CopyOnWriteArrayList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F3.CopyOnWriteArrayList%2F</url>
    <content type="text"><![CDATA[CopyOnWriteArrayList是ArrayList的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 CopyOnWriteArrayList是一个写时复制的容器，采用了读写分离的思想。通俗点来讲，在对容器进行写操作时，不直接修改当前容器，而是先对当前容器进行拷贝得到一个副本，然后对副本进行写操作，最后再将原容器的引用指向拷贝出来的副本。这样做的好处就是可以对容器进行并发读而不用进行加锁。 一、类的继承关系 12public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 含义不需要再赘述了。 二、类的属性 12345678910111213141516171819202122/** 用于在对数组产生写操作的方法加锁. */final transient ReentrantLock lock = new ReentrantLock();/** 底层的存储结构. */private transient volatile Object[] array;/** 反射机制. */private static final sun.misc.Unsafe UNSAFE;/** lock域的内存偏移量.是通过反射拿到的 */private static final long lockOffset;static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = CopyOnWriteArrayList.class; lockOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("lock")); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 三、数组末尾添加一个元素 12345678910111213141516171819202122public boolean add(E e) &#123; // 可重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 元素数组 Object[] elements = getArray(); // 数组长度 int len = elements.length; // 复制数组 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将要添加的元素放到副本数组的末尾去 newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 基本原理很简单，就是对当前数组加锁，内部复制一个新数组，处理完毕，修改引用即可，达到最终一致的效果。 四、如果没有这个元素则添加 12345public boolean addIfAbsent(E e) &#123; Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false : addIfAbsent(e, snapshot);&#125; 该函数用于添加元素（如果数组中不存在，则添加；否则，不添加，直接返回）。如何可以保证多线程环境下不会重复添加元素？ 答案：通过快照数组和当前数组进行对比来确定是否一致，确保添加元素的线程安全 12345678910111213141516171819202122232425262728293031323334private boolean addIfAbsent(E e, Object[] snapshot) &#123; // 重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 获取数组 Object[] current = getArray(); // 数组长度 int len = current.length; if (snapshot != current) &#123; // 快照不等于当前数组，对数组进行了修改 // 取较小者 int common = Math.min(snapshot.length, len); for (int i = 0; i &lt; common; i++) // 遍历 if (current[i] != snapshot[i] &amp;&amp; eq(e, current[i])) // 当前数组的元素与快照的元素不相等并且e与当前元素相等 // 表示在snapshot与current之间修改了数组，并且设置了数组某一元素为e，已经存在 // 返回false return false; if (indexOf(e, current, common, len) &gt;= 0) // 在当前数组中找到e元素 // 返回false return false; &#125; // 复制数组 Object[] newElements = Arrays.copyOf(current, len + 1); // 对数组len索引的元素赋值为e newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 该函数的流程如下： 获取锁，获取当前数组为current，current长度为len，判断数组之前的快照snapshot是否等于当前数组current，若不相等，则进入步骤2；否则，进入步骤3 不相等，表示在snapshot与current之间，对数组进行了修改，直接返回false结束; 说明当前数组等于快照数组，说明数组没有被改变。在当前数组中索引指定元素，若能够找到，说明已经存在此元素，直接返回false结束；否则进入4 说明没有当前要插入的元素，通过数组复制的方式添加到末尾 无论如何，都要释放锁 五、获取指定索引的元素 1234567public E get(int index) &#123; return get(getArray(), index);&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 通过写时复制的方式，CopyOnWriteArrayList 的 get 方法不用加锁也可以保证线程安全，所以 CopyOnWriteArrayList 并发读的效率是非常高的，它是直接通过数组下标获取元素的。 六、总结 简单而言要记住它的三个特点： CopyOnWriteArrayList 是一个并发的数组容器，它的底层实现是数组。 CopyOnWriteArrayList 采用写时复制的方式来保证线程安全。 通过写时复制的方式，可以高效的进行并发读，但是对于写操作，每次都要进行加锁以及拷贝副本，效率非常低，所以 CopyOnWriteArrayList 仅适合读多写少的场景。 Vector虽然是线程安全的，但是只是一种相对的线程安全而不是绝对的线程安全，它只能够保证增、删、改、查的单个操作一定是原子的，不会被打断，但是如果组合起来用，并不能保证线程安全性。 CopyOnWriteArrayList在并发下不会产生任何的线程安全问题，也就是绝对的线程安全 另外，有两点必须讲一下。 我认为CopyOnWriteArrayList这个并发组件，其实反映的是两个十分重要的分布式理念： （1）读写分离 我们读取CopyOnWriteArrayList的时候读取的是CopyOnWriteArrayList中的Object[] array，但是修改的时候，操作的是一个新的Object[] array，读和写操作的不是同一个对象，这就是读写分离。这种技术数据库用的非常多，在高并发下为了缓解数据库的压力，即使做了缓存也要对数据库做读写分离，读的时候使用读库，写的时候使用写库，然后读库、写库之间进行一定的同步，这样就避免同一个库上读、写的IO操作太多 （2）最终一致 对CopyOnWriteArrayList来说，线程1读取集合里面的数据，未必是最新的数据。因为线程2、线程3、线程4四个线程都修改了CopyOnWriteArrayList里面的数据，但是线程1拿到的还是最老的那个Object[] array，新添加进去的数据并没有，所以线程1读取的内容未必准确。不过这些数据虽然对于线程1是不一致的，但是对于之后的线程一定是一致的，它们拿到的Object[] array一定是三个线程都操作完毕之后的Object array[]，这就是最终一致。最终一致对于分布式系统也非常重要，它通过容忍一定时间的数据不一致，提升整个分布式系统的可用性与分区容错性。当然，最终一致并不是任何场景都适用的，像火车站售票这种系统用户对于数据的实时性要求非常非常高，就必须做成强一致性的。 最后总结一点，随着CopyOnWriteArrayList中元素的增加，CopyOnWriteArrayList的修改代价将越来越昂贵，因此，CopyOnWriteArrayList适用于读操作远多于修改操作的并发场景中。 感谢 http://www.cnblogs.com/xrq730/p/5020760.html http://blog.csdn.net/u013124587/article/details/52863533 https://www.cnblogs.com/leesf456/p/5547853.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F2.LinkedList%2F</url>
    <content type="text"><![CDATA[提到ArrayList，就会比较与LinkedList的区别。本文来看看LinkedList的核心原理。 如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。 一、LinkedList属性 123456//链表的节点个数.transient int size = 0;//Pointer to first node.transient Node&lt;E&gt; first;//Pointer to last node.transient Node&lt;E&gt; last; 二、Node的结构 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next;//后置指针 Node&lt;E&gt; prev;//前置指针 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 三、添加元素 3.1 LinkedList表头添加一个元素 当向表头插入一个节点时，很显然当前节点的前驱一定为 null，而后继结点是 first 指针指向的节点，当然还要修改 first 指针指向新的头节点。除此之外，原来的头节点变成了第二个节点，所以还要修改原来头节点的前驱指针，使它指向表头节点，源码的实现如下： 1234567891011121314151617public void addFirst(E e) &#123; linkFirst(e);&#125;private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); //新节点作为新的first节点 first = newNode; if (f == null) last = newNode;//初始就是个空LinkedList的话，last指向当前新节点 else f.prev = newNode;//初始值不为空，将其前置指针指向新节点 size++; modCount++;&#125; 3.2 LinkedList表尾添加一个元素 当向表尾插入一个节点时，很显然当前节点的后继一定为 null，而前驱结点是 last 指针指向的节点，然后还要修改 last 指针指向新的尾节点。此外，还要修改原来尾节点的后继指针，使它指向新的尾节点，源码的实现如下： 123456789101112131415161718public void addLast(E e) &#123; linkLast(e);&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); //新节点作为新的last节点 last = newNode; //如果原来有尾节点，则更新原来节点的后继指针，否则更新头指针 if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 3.3 LinkedList在指定节点前添加一个元素 12345678910111213141516171819202122232425262728293031323334public void add(int index, E element) &#123; //判断数组是否越界 checkPositionIndex(index); if (index == size) linkLast(element);//直接插在最后一个 else linkBefore(element, node(index));//在index节点之前插入&#125;private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; //找到索引位置的前面一个元素pred final Node&lt;E&gt; pred = succ.prev; //新节点，前置指针指向pred,后置指针指向索引处元素 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); //修改索引出元素的前置指针为新节点 succ.prev = newNode; if (pred == null) first = newNode;//说明是插在表头 else pred.next = newNode;//说明是插在非表头位置，修改pred后置指针为新指针 size++; modCount++;&#125; 可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。 四、删除元素 删除操作与添加操作大同小异，例如删除指定节点的过程如下图所示，需要把当前节点的前驱节点的后继修改为当前节点的后继，以及当前节点的后继结点的前驱修改为当前节点的前驱。 就不赘述了。 五、获取元素 12345678910111213141516171819202122//获取指定索引对应的元素public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;//寻找元素的方向是根据index在表中的位置决定的Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123;//索引小于表长的一半，从表头开始往后找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123;//索引大于表长的一半，从表尾往前开始找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 上述代码，利用了双向链表的特性，如果index离链表头比较近，就从节点头部遍历。否则就从节点尾部开始遍历。使用空间（双向链表）来换取时间。 node()会以O(n/2)的性能去获取一个结点 如果索引值大于链表大小的一半，那么将从尾结点开始遍历 这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 总结 1、理论上无容量限制，只受虚拟机自身限制影响，所以没有扩容方法。 2、和ArrayList一样，LinkedList也是是未同步的，多线程并发读写时需要外部同步，如果不外部同步，那么可以使用Collections.synchronizedList方法对LinkedList的实例进行一次封装。 3、和ArrayList一样，LinkedList也对存储的元素无限制，允许null元素。 4、顺序插入速度ArrayList会比较快，因为ArrayList是基于数组实现的，数组是事先new好的，只要往指定位置塞一个数据就好了；LinkedList则不同，每次顺序插入的时候LinkedList将new一个对象出来，如果对象比较大，那么new的时间势必会长一点，再加上一些引用赋值的操作，所以顺序插入LinkedList必然慢于ArrayList 5、基于上一点，因为LinkedList里面不仅维护了待插入的元素，还维护了Entry的前置Entry和后继Entry，如果一个LinkedList中的Entry非常多，那么LinkedList将比ArrayList更耗费一些内存 6、数据遍历的速度，看最后一部分，这里就不细讲了，结论是：使用各自遍历效率最高的方式，ArrayList的遍历效率会比LinkedList的遍历效率高一些 7、有些说法认为LinkedList做插入和删除更快，这种说法其实是不准确的： LinkedList做插入、删除的时候，慢在寻址，快在只需要改变前后Entry的引用地址 ArrayList做插入、删除的时候，慢在数组元素的批量copy，快在寻址 所以，如果待插入、删除的元素是在数据结构的前半段尤其是非常靠前的位置的时候，LinkedList的效率将大大快过ArrayList，因为ArrayList将批量copy大量的元素；越往后，对于LinkedList来说，因为它是双向链表，所以在第2个元素后面插入一个数据和在倒数第2个元素后面插入一个元素在效率上基本没有差别，但是ArrayList由于要批量copy的元素越来越少，操作速度必然追上乃至超过LinkedList。 从这个分析看出，如果你十分确定你插入、删除的元素是在前半段，那么就使用LinkedList；如果你十分确定你删除、删除的元素在比较靠后的位置，那么可以考虑使用ArrayList。如果你不能确定你要做的插入、删除是在哪儿呢？那还是建议你使用LinkedList吧，因为一来LinkedList整体插入、删除的执行效率比较稳定，没有ArrayList这种越往后越快的情况；二来插入元素的时候，弄得不好ArrayList就要进行一次扩容，记住，ArrayList底层数组扩容是一个既消耗时间又消耗空间的操作. 8、ArrayList使用最普通的for循环遍历，LinkedList使用foreach循环比较快.注意到ArrayList是实现了RandomAccess接口而LinkedList则没有实现这个接口.关于RandomAccess这个接口的作用，看一下JDK API上的说法： 9、如果使用普通for循环遍历LinkedList，在大数据量的情况下，其遍历速度将慢得令人发指 整理自： 1、http://www.cnblogs.com/xrq730/p/5005347.html 2、http://blog.csdn.net/u013124587/article/details/52837848 3、http://blog.csdn.net/u011392897/article/details/57115818 4、http://blog.csdn.net/fighterandknight/article/details/61476335]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList/Vector]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F1.ArrayList%E5%92%8CVector%2F</url>
    <content type="text"><![CDATA[面试中，关于java的一些容器，ArrayList是最简单也是最常问的，尤其是里面的扩容机制。 ArrayList 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 构造函数为： 123456789101112131415161718//用初始容量作为参数的构造方法public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //初始容量大于0，实例化数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //初始容量等于0，赋予空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125;//无参的构造方法public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 从构造方法中我们可以看见，默认情况下，elementData是一个大小为0的空数组，当我们指定了初始大小的时候，elementData的初始大小就变成了我们所指定的初始大小了。 ArrayList相当于动态数据，其中最重要的两个属性分别是: elementData 数组，以及 size 大小。 在调用 add() 方法的时候： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话： 12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码: 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 也是一个数组复制的过程，ArrayList每次扩容都是扩1.5倍，然后调用Arrays类的copyOf方法，把元素重新拷贝到一个新的数组中去。 由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。 序列化 由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 从实现中可以看出 ArrayList 只序列化了被使用的数据。 Vector Vector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 Vector比ArrayList多了一个属性： 1protected int capacityIncrement; 这个属性是在扩容的时候用到的，它表示每次扩容只扩capacityIncrement个空间就足够了。该属性可以通过构造方法给它赋值。先来看一下构造方法： 123456789101112131415public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 从构造方法中，我们可以看出Vector的默认大小也是10，而且它在初始化的时候就已经创建了数组了，这点跟ArrayList不一样。再来看一下grow方法： 12345678910private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从grow方法中我们可以发现，newCapacity默认情况下是两倍的oldCapacity，而当指定了capacityIncrement的值之后，newCapacity变成了oldCapacity+capacityIncrement。 以下是 add() 方法： 123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据: 12345678910111213public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年的最后一天，对商城项目的架构做个改造]]></title>
    <url>%2F2019%2F01%2F20%2F2018%E5%B9%B4%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%A4%A9%EF%BC%8C%E5%AF%B9%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%9E%B6%E6%9E%84%E5%81%9A%E4%B8%AA%E6%94%B9%E9%80%A0%2F</url>
    <content type="text"><![CDATA[一直以来都是学习慕课的实战视频，虽然也跟着做出了一些东西，但是思路都是别人提供好的，脱离了老师，我一直在问自己一个问题：能不能独立地按照自己的思路做出一些东西来？ 前方图片高能…更有几十兆gif演示动画，图片全部存放于七牛云上。 在去年，即2017年年底，我在慕课上学习了这两门课程： 第一期项目实现了比较简单的电商业务，整合SSM，并且部署到云端。 第二期实现了tomcat集群，配合redis实现分布式session，还有一些定时任务、redis分布式锁、maven环境隔离的一些东西，还涉及很多spring和springmvc的有用的机巧。 整体感觉是：一期实现业务，二期对于一期的提高不是太大，跟分布式无太大关系，仅仅实现了单点登陆和分布式session存储而已。 个人感觉下一期的课程应该是springCloud的分布式改造，进行服务拆分和治理。所以，在整合这两个课程的基础上用springCloud进行微服务治理。 项目详细描述 项目源码地址：https://github.com/sunweiguo/MMall 整体效果演示： 下面贴个小一点的gif: 部分页面截图： 经过一遍遍测试，商城还是存在一些无伤大雅的bug，但是主要还是锻炼自己的能力嘛！ 下订单的时候，报错：商品不存在或库存不足，是因为我模拟的秒杀，所以商品的库存要提前预置于redis中，后端管理系统的商品管理页面有预置库存的按钮。 新增商品的时候，对于上传图片，需要耐心等待一会，需要等待FTP服务器上传完毕，给一个返回信息(Map，是图片的文件名)才能真正显示（对于富文本中的图片上传，在上传之后需要等待一会，时间与小图上传差不多，否则直接保存不报错，但是前端看不到图，因为还没上传完毕,url还没回传回来） 普通注册的账号，没有管理员权限，所以不能登陆后台管理系统。 后来做了eureka集群，但是配置文件还是只指向其中一个eureka 在学习的视频中，一期只是实现业务功能，单体架构，一个tomcat。二期对其做了集群，并且解决了集群模式下session存储问题，实现了比较简单的单点登陆功能。架构如下： 对于上面的架构来说，只是做了一些集群进行优化，随着业务的发展，用户越来越多，用户服务等其他服务必然要拆分出来独立成为一个服务，这样做的好处是，一方面一个团队负责一个服务可以提高开发效率，另外，对于扩展性也是非常有利的，但是也是有缺点的，会带来很多的复杂性，尤其是引入了分布式事务，所以不能为了分布式而分布式，而是针对不同的业务场景而采用合适的架构。 微服务的实现，主要有两种，国内是阿里系的以dubbo+zookeeper为核心的一套服务治理和发现生态。另一个则是大名鼎鼎的spring cloud栈。 spring cloud并不是像spring是一个框架，他是解决微服务的一种方案，是由各种优秀开源组件共同配合而实现的微服务治理架构。下面的图是我构思的项目结构图： 最前面是Nginx，这里就作为一个静态资源映射和负载均衡，nginx中有几个配置文件，分别为www.oursnail.cn.conf，这个主要是对zuul网关地址做一个负载均衡，指向网关所在的服务器，并且找到前台页面所在位置对页面进行渲染。admin.oursnail.cn.conf，这个主要是配置后端以及后端的页面文件；img.oursnail.cn.conf是对图片服务器地址进行映射。 然后是zuul网关，这里主要是用来限流、鉴权以及路由转发。 再后面就是我们的应用服务器啦。对服务器进行了服务追踪(sleuth)，实现了动态刷新配置(spring cloud config+bus)等功能。以http restful的方式进行通信(openFeign),构建起以eureka为注册中心的分布式架构。 每个服务都是基于springboot打造，结合mybatis持久层操作的框架，完成基本的业务需求。springboot基于spring，特点是快速启动、内置tomcat以及无xml配置。将很多东西封装起来，引入pom就可以直接使用，比如springMVC就基本上引入starter-web即可。 由于资源的原因，只有三台最低配的服务器，所以本来想做的基于ES的全文检索服务没有做，也没有分库分表。 至于定时任务以及Hystrix服务熔断和降级，比较简单，就不做了。 项目的接口文档详见wiki：https://github.com/sunweiguo/spring-cloud-for-snailmall/wiki 项目的数据库表设计：snailmall.sql 下面详细介绍每个模块实现的大体思路（仅供参考，毕竟应届生，真实项目没做过）： 用户模块 关于用户模块，核心的功能是登陆。再核心是如何验证以及如何存储用户信息。这里采取的方案为： 对于用户注册，我这里就是用户名（昵称），那么如何保证不重复呢（高并发）？这里还是用了分布式锁来保证的。 对于未登陆章台下用户修改密码，逻辑为： 购物车模块 订单模块 针对这些问题，我想说一下我的思路。 对于幂等性，这里产生幂等性的主要原因在于MQ的重传机制，可能第一个消息久久没有发出去，然后重新发送一条，结果第一条消息突然又好了，那么就会重复发两跳，对于用户来说，只下一次单，但是服务器下了多次订单。网上解决这个问题的思路是创建一张表，如果是重复的订单号，就不可能创建多次了。还有一种可能方案是用分布式锁对该订单号锁住一段时间，由于只是锁住订单号，所以不影响性能，在这一段时间内是不可以再放同一个订单号的请求进来。 对于MQ消息不丢失，只能是订阅模式了。消息发出去之后，消费端给MQ回复一个接收到的信息，MQ本次消费成功，给订阅者一个回复。 对于全局唯一ID生成，这里用的是雪花算法，具体介绍可以看我的笔记 对于分布式事务，比较复杂，这里其实并没有真正处理，对于数据库扣减库存和数据库插入订单，他们在不同的数据库，廖师兄比较倾向的方式是： 这一切的基础还是需要有一个可靠的消息服务，确保消息要能送达。 针对redis预减库存存在的并发问题，这里的思路是用lua+redis，在预减之前判断库存是否够，这两个操作要在一个原子操作里面才行，lua恰好可以实现原子性、顺序性地操作。 支付模块 这里对接的是支付宝-扫码支付，用到是支付宝沙箱环境。支付的扫码支付详细流程在这里聊一聊哈。 商户前台将商品参数发送至商户后台，商户后台生成内部订单号并用于请求支付平台创建预下单，支付平台创建完预订单后将订单二维码信息返还给商户，此时用户即可扫取二维码进行付款操作。 内部订单号：这是相对于支付宝平台而言的，这个订单号是我们商城自己生成的，对于我们商城来说是内部订单号，但是对于支付宝来说是外部订单号。 将一系列的数据按照支付宝的要求发送给支付宝平台，包括商品信息，生成的验签sign，公钥；支付宝去将sign解密，进行商品的各种信息校验。校验通过，同步返回二维码串。 支付业务流程图： 在获取支付的二维码串之后，用工具包将其转换未二维码展示给用户扫码。 用户扫码后，会收到第一次支付宝的回调，展示要支付的金额，商品信息等。 用户输入密码成功后，正常情况会收到支付宝的第二次回调，即支付成功信息。 但是也可能会由于网络等原因，迟迟收不到支付宝的回调，这个时候就需要主动发起轮询去查看支付状态。 在支付成功之后，接收回调的接口要记得返回success告诉支付宝我已经收到你的回调了，不要再重复发给我了。接收回调的接口也要做好去除重复消息的逻辑。 这个流程是多么地简单而理所当然！ 对应于代码层面，其实就是两个接口，一个是用户点击去支付按钮，此时发起预下单，展示付款二维码，另一个是接收支付宝回调： 预下单： 接收支付宝支付状态回调： 项目进展 [x] 2018/12/31 完成了聚合工程的创建、Eureka服务注册中心、spring cloud config+gitHub+spring cloud bus（rabbitMQ）实现配置自动刷新–v1.0 [x] 2018/12/31 将Eureka注册中心(单机)和配置中心部署到服务器上，这比较固定，所以先部署上去，以后本地就直接用这两个即可，对配置进行了一点点修改 [ ] 2018/12/31 关于配置的自动刷新，用postman发送post请求是可以的，但是用github webhook不行，不知道是不是这个版本的问题 [x] 2018/12/31 用户模块的逻辑实现,首先增加了一些pom文件的支持，整合mybatis，测试数据库都通过，下面就可以真正去实现业务代码了 [x] 2019/1/1 完成用户注册、登陆、校验用户名邮箱有效性、查看登陆用户信息、根据用户名去拿到对应的问题、校验答案是否正确、重置密码这个几个接口，在注册这个接口，增加一个ZK分布式锁来防止在高并发场景下出现用户名或邮箱重复问题 [x] 2019/1/2 上午完成门户用户模块所有接口–v2.0 [x] 2019/1/2 下午完成品类管理模块，关于繁琐的获取用户并且鉴权工作，这里先放每个接口里面处理，后面放到网关中去实现–v3.0 [x] 2019/1/3 上午引入网关服务，将后台重复的权限校验统一放到网关中去做，并且加了限流，解决了一下跨域问题。–v4.0 [x] 2019/1/3 下午和晚上完成门户和后台的商品管理模块所有的接口功能，除了上传文件的两个接口没有测试以外，其他接口都进行了简单的测试，其中还用Feign去调用了品类服务接口–v5.0 [x] 2019/1/3 初步把购物车模块和模块引入，通过基础测试，后面在此基础上直接开发代码即可，明天下午看《大黄蜂》，晚上师门聚餐吃火锅，明天早上赶一赶吧，今天任务结束！ [x] 2019/1/4 整理了接口文档，并且画了一下购物车模块的流程图以及订单服务的流程图，针对订单服务中，记录了需要一些注意的问题，尽可能地完善，提高可用性和性能。并且完成购物车模块的controller层。 [x] 2019/1/5 完成购物车模块，并且进行了简单的测试，这里进行了两处改造，一个是判断了一下是否需要判断库存；另一个是商品信息从redis中取，取不出来则调用商品服务初始化值 [x] 2019/1/5 收货地址管理模块，这个模块就是个增删改查，没啥东西写，这里就不加缓存了。 [x] 2019/1/6 完成了后台订单管理模块并且进行了测试，调用收货地址服务时，发现收货地址服务无法读取到cookie，通过这个方法(https://blog.csdn.net/WYA1993/article/details/84304243) 暂时解决了问题 [x] 2019/1/6 预置所有商品库存到redis中；预置所有商品到redis中；大概确定好订单服务思路：预减库存（redis判断库存）—对userID增加分布式锁防止用户重复提交订单–MQ异步下订单 [x] 2019/1/6 新增全局唯一ID生成服务，雪花算法实现 [x] 2019/1/7 完善订单服务-这一块涉及跨库操作，并且不停地调用其他服务，脑子都快晕了，这里采取的策略是：用到购物车的时候，去调用购物车服务获取；产品详情从redis中获取。首先将商品以及商品库存全部缓存到redis中，然后用户下单，先从redis中判断库存，够则减，判断 和扣减放在lua脚本中原子执行，然后MQ异步出去生成订单（生成订单主表和订单详情表放在一个本地事务中），这两步操作成功之后，再用MQ去异步删除购物车。MQ消费不成功则重试。 对于扣减库存这一步，想法是用定时任务，定时与redis中进行同步。这里是模拟了秒杀场景，预减库存+MQ异步，提交订单–&gt;redis判断并且减库存–&gt;调用cart-service获取购物车–&gt;MQ异步(userId,shippingId)生成订单主表和详情表–&gt;上面都成功，则MQ异步(userId) 去清除购物车，库存用定时任务去同步(未做)，理想的做法是：MQ异步扣减库存，订单服务订阅扣减库存消息，一旦库存扣减成功，则进行订单生成。 [x] 2019/1/8 继续完善订单接口，完成支付服务，就直接放在订单服务里面了，因为与订单逻辑紧密，就放在一起了。 [x] 2019/1/8 使用了一下swagger，发现代码侵入比较强，每一个接口上面都要手动打上响应的注解 [x] 2019/1/8 关于hystrix熔断与降级，可以引入hystrix的依赖，用@HystrixCommand注解来控制超时时间、服务降级以及服务熔断。也可以直接再@FeignClient接口中指定服务降级的类，这里不演示了，因为设置比如超时时间，我还要重新测试，写起来很简单，测起来有点儿麻烦 [x] 2019/1/9 服务跟踪，服务端是直接用的线程的，只需要下载：wget -O zipkin.jar ‘https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec’，然后nohup java -jar zipkin.jar &gt; zipkin.server.out &amp; ，开放9411端口，打开浏览器http://ip:9411看到页面即可。 客户端只需要添加相应依赖和配置文件即可。用客户端测试，发现死活不出现我的请求，经过搜索，发现需要增加spring.zipkin.sender.type= web这个配置项才行. [x] 2019/1/10 初步把项目部署到服务器上，进行测试，bug多多，修改中… [x] 2019/1/10 改了一天的bug，其中网关的超时时间以及feign的超时时间都要改大一点，否则会超时报错。最终成功，花了三台服务器，部署了11个服务。后面把部署过程写一下。 [x] 2019/1/11 将注册中心做成集群，因为早上一起来，注册中心挂了？？？ [ ] 2019/1/11 docker部署(商城第四期的改造目标:容器化+容器编排)，本期改造结束。 [x] 2019/1/11 完善readme文档 [x] 2019/1/12 两次发现redis数据被莫名其妙清空，我确定不是缓存到期，为了安全起见，设置了redis的密码，明天看缓存数据还在不在。 [x] 2019/1/14 redis数据没有再丢失，修复用户更新信息的bug 项目启动 安装redis、zookeeper、mysql、jdk、nginx以及rabbitMQ。 对代码进行maven-package操作。打包成jar包。将其放到服务器上： 执行nohup java -jar snailmall-user-service.8081 &gt; user-service.out &amp;后台启动即可。 补充：针对配置刷新，修改了github信息，用postman请求http://xxxxx:8079/actuator/bus-refresh 触发更新。 本改造是基于快乐慕商城一期和快乐慕商城二期的基础上进行改造。所以需要在其业务基础上改造会比较顺手。关于微服务，尤其是电商中的一些处理手段，很多思路都是学习于码吗在线中分布式电商项目。再加上慕课网廖师兄的spring cloud微服务实践。 前台项目 只要阅读readme文档即可。代码仓库为：https://github.com/sunweiguo/snailmall-front 学习不仅要有输入，更要有自己的输出，实践是提升的捷径！]]></content>
      <tags>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-进程与线程]]></title>
    <url>%2F2019%2F01%2F19%2F%E9%9D%A2%E8%AF%95-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[都是操作系统管理的对象，比较容易混淆，但是又是两样完全不同的东西，因此区别很多。从他们区别也可以发散出很多关于操作系统比较重要的知识。所以面试比较常问。 1. 进程到底是什么？ 翻了一下书：《操作系统概念》第三章中提及了进程的概念。他是这样说的： 进程是执行中的程序，这是一种非正式说法。进程不只是程序代码（代码块），进程还包含了当前活动，通过程序计数器的值和处理器寄存器的内容来表示。此外，进程还包含进程堆栈段（包括临时数据，如函数参数、返回地址和局部变量）和数据段（包括全局变量）。进程还可能包含堆，是在进程运行期间动态分配的内存。 程序本身不是进程。程序只是被动实体，如存储在磁盘上包含一系列指令的文件内容。而进程是活动实体，它有一个程序计数器来表示下一个要执行的命令和相关资源集合。当一个可执行文件被装入内存时，一个程序才能成为进程。 总结一下进程是什么，是我个人理解：它是一个活动实体，运行在内存上。然后它占用很多独立的资源，比如：内存资源、程序运行肯定涉及CPU计算、占用的端口资源（公共的）、文件资源（公共的）、网络资源（公共的）等等等。要想执行这个进程，首先要有一个可执行文件，有了这个可执行文件，还要有相应的执行需要的资源。所以将可执行文件、当前进程的上下文、内存等资源结合起来，才是一个真正的进程。 那么，我们就可以理解一句话：进程是资源分配的基本单位。 进程中的内存空间（虽然空间大小都一样，下文会说明）是独立的，否则就会出现一种情况：修改自己程序中的某个指针就可以指向其他程序中的地址，然后拿到里面的数据，岂不是很恐怖的场景？ 如上图，进程中包含了线程。操作系统可能会运行几百个进程，进程中也可能有几个到几百个线程在运行。 文件和网络句柄是所有进程共享的，多个进程可以去打开同一个文件，去抢占同一个网络端口。 图中还有个内存。这个内存不是我们经常说的内存条，即物理内存，而是虚拟内存，是进程独立的，大小与实际物理内存无关。 2. 寻址空间 比如8086只有20根地址线，那么它的寻址空间就是1MB，我们就说8086能支持1MB的物理内存，及时我们安装了128M的内存条在板子上，我们也只能说8086拥有1MB的物理内存空间。 以前叫卖的32位的机子，32位是指寻址空间为2的32次方。32位的386以上CPU就可以支持最大4GB的物理内存空间了。 3. 为什么会有虚拟内存和物理内存的区别 正在运行的一个进程，他所需的内存是有可能大于内存条容量之和的，比如你的内存条是256M，你的程序却要创建一个2G的数据区，那么不是所有数据都能一起加载到内存（物理内存）中，势必有一部分数据要放到其他介质中（比如硬盘），待进程需要访问那部分数据时，在通过调度进入物理内存。 所以，虚拟内存是进程运行时所有内存空间的总和，并且可能有一部分不在物理内存中，而物理内存就是我们平时所了解的内存条。 关键的是不要把虚拟内存跟真实的插在主板上的内存条相挂钩，虚拟内存它是“虚拟的”不存在，假的啦，它只是内存管理的一种抽象！ 4. 虚拟内存地址和物理内存地址是如何映射呢 假设你的计算机是32位，那么它的地址总线是32位的，也就是它可以寻址0 ~ 0xFFFFFFFF（4G）的地址空间，但如果你的计算机只有256M的物理内存0x~0x0FFFFFFF（256M），同时你的进程产生了一个不在这256M地址空间中的地址，那么计算机该如何处理呢？回答这个问题前，先说明计算机的内存分页机制。 计算机会对虚拟内存地址空间（32位为4G）分页产生页（page），对物理内存地址空间（假设256M）分页产生页帧（page frame），这个页和页帧的大小是一样大的，所以呢，在这里，虚拟内存页的个数势必要大于物理内存页帧的个数。 在计算机上有一个页表（page table），就是映射虚拟内存页到物理内存页的，更确切的说是页号到页帧号的映射，而且是一对一的映射。但是问题来了，虚拟内存页的个数 &gt; 物理内存页帧的个数，岂不是有些虚拟内存页的地址永远没有对应的物理内存地址空间？ 不是的，操作系统是这样处理的。操作系统有个页面失效（page fault）功能。操作系统找到一个最少使用的页帧，让他失效，并把它写入磁盘，随后从磁盘中把把需要访问的数据所在的页放到最少使用的页帧中，并修改页表中的映射（即修改页号指向当前页帧），这样就保证所有的页都有被调度的可能了。这就是处理虚拟内存地址到物理内存的步骤。 至于里面如何实现的细节，我没有过多去探究。 5. 什么是虚拟内存地址和物理内存地址 虚拟内存地址由页号和偏移量组成。页号就是上面所说的。偏移量就是我上面说的页（或者页帧）的大小，即这个页（或者页帧）到底能存多少数据。 举个例子，有一个虚拟地址它的页号是4，偏移量是20，那么他的寻址过程是这样的：首先到页表中找到页号4对应的页帧号（比如为8），如果找不到对应的页桢，则用失效机制调入页。如果存在，把页帧号和偏移量传给MMU（CPU的内存管理单元）组成一个物理上真正存在的地址，接着就是访问物理内存中的数据了。 6. 线程里面有什么 写到这里，好像还与本标题无关，即进程和线程到底是什么关系和区别等。但是我们要知道，面试或者学习一个知识点，不是为了学习这个区别而学习， 我们应该学习为什么有进程和线程，有了进程还需要线程吗？有了线程还要进程吗？你说进程是资源分配的单位，分配的是什么资源呢？进程中的内存是咋管理的呢？虚拟内存和物理内存是什么？什么是虚拟内存地址和物理内存地址？等等等，所以面试是千变万化的，重要的是我们尽可能地多问自己几个为什么，然后从为什么开始去逐个击破，形成一个体系。 说说这个栈，我们知道，执行程序从主程序入口进入开始，可能会调用很多的函数，那么这些函数的参数和返回地址都会被压入栈中，包括这些函数中定义的临时局部变量都会压入栈中，随着函数的执行完毕，再逐层地弹出栈，回到主函数运行的地方，再继续执行。 PC(program counter)，就是程序计数器，指向的下一条指令执行的地址。 由此可见，操作系统运行的其实是一个一个的线程，而进程只是一个隔离资源的容器。 上面说到，PC是指向下一条指令执行的地址。而这些指令是放在内存中的。 我们的计算机大多数是存储程序型的。就是说数据和程序是同时存储在同一片内存里的。 所以我们经常会听到一个漏洞叫做“缓冲区溢出”：比如有一个地方让用户输入用户名，但是黑客输入很长很长的字符串进去，那么很有可能就会超出存放这个用户名的一片缓冲区，而直接侵入到存放程序的地方，那么黑客就可以植入程序去执行。解决方案就是限制输入的用户名长度，不要超过缓冲区大小。 还有一块是TLS(thread local storage)，我们知道进程有自己独立的内存，那么我们的线程能不能也有一小块属于自己的内存区域呢？ 这个东西，其实很简单，就是说，比如new一个对象，往往是在堆中开辟空间的，但是现在的情况是：在一个函数内，new出来一个对象，这个对象不引用外部对象，也不会被外部引用，是纯粹属于这个函数段，可以理解为这个对象是属于这个函数的局部临时变量。 此时，new这个对象就不需要再去堆中开辟空间了，因为一方面不需要共享，另一方面是在堆中开辟是比较慢的，并且可能有很多函数，这种局部对象零零总总加起来还是很多的，在堆中开辟会浪费空间。 所以，能不能在栈中就可以new出这个对象，反正用完就扔。TLS可以是现在这个。栈中直接new多方便多快，因为不需要走垃圾回收机制，还避免了线程安全问题。可以去搜索：栈上分配和逃逸分析 7. 线程VS进程 到这里，就清晰了很多。我们也可以多多少少理解他们的区别。 可以做个简单的比喻，便于记忆：进程=火车，线程=车厢 线程在进程下行进（单纯的车厢无法运行） 一个进程可以包含多个线程（一辆火车可以有多个车厢） 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘） 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易） 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源） 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢） 进程可以拓展到多机，进程最多适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上） 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－“互斥锁” 进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量” 再补充几句。 线程是调度的基本单位，进程是资源分配的基本单位 进程间没有共享内存，所以交互要通过TCP/IP端口的等方式来实现。线程间由于有共享内存，所以交互比较方便。 线程占用很多资源，而线程只需要分配栈和PC即可。 8. 针对虚拟内存和物理内存的总结 每个进程都有自己独立的4G(32位系统下)内存空间，各个进程的内存空间具有类似的结构 一个新进程建立的时候，将会建立起自己的内存空间，此进程的数据，代码等从磁盘拷贝到自己的进程空间（建立一个进程，就要把磁盘上的程序文件拷贝到进程对应的内存中去，对于一个程序对应的多个进程这种情况，浪费内存！），哪些数据在哪里，都由进程控制表中的task_struct记录 每个进程的4G内存空间只是虚拟内存空间，每次访问内存空间的某个地址，都需要把地址翻译为实际物理内存地址 所有进程共享同一物理内存，每个进程只把自己目前需要的虚拟内存空间映射并存储到物理内存上。 进程要知道哪些内存地址上的数据在物理内存上，哪些不在，还有在物理内存上的哪里，需要用页表来记录 页表的每一个表项分两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址（如果在的话） 当进程访问某个虚拟地址，去看页表，如果发现对应的数据不在物理内存中，则缺页异常 缺页异常的处理过程，就是把进程需要的数据从磁盘上拷贝到物理内存中，如果内存已经满了，没有空地方了，那就找一个页覆盖，当然如果被覆盖的页曾经被修改过，需要将此页写回磁盘 9. 关于进程和线程更深的认识 关于为什么要分进程和线程，先抛出结论： 进程process：进程就是时间总和=执行环境切换时间+程序执行时间------&gt;CPU加载执行环境-&gt;CPU执行程序-&gt;CPU保存执行环境 线程thread：线程也是时间总和=执行环境切换时间（共享进程的）+程序模块执行时间------&gt;CPU加载执行环境（共享进程的）-&gt;CPU执行程序摸块-&gt;CPU保存执行环境（共享进程的） 进程和线程都是描述CPU工作的时间段，线程是更细小的时间段。 那么，如果CPU时间片临幸本进程，那么这个进程在恢复执行环境之后，执行里面的若干线程就不需要再不停地切换执行环境了，所以说，线程相比于进程是比较轻量的。 在CPU看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。。 进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文 线程是什么呢？进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成：程序A得到CPU =》CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。 这里a，b，c的执行是共享了A的上下文，CPU在执行的时候没有进行上下文切换的。这里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境、更为细小的CPU时间段。 进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同。 整理自： https://www.zhihu.com/question/25532384 https://blog.csdn.net/moshenglv/article/details/52242153 https://blog.csdn.net/u012861978/article/details/53048077]]></content>
      <tags>
        <tag>操作系统相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步一步理解HTTPS]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F7.%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3HTTPS%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第七篇文章。HTTPS（SSL/TLS）的加密机制是前端后端ios安卓等都应了解的基本问题。也是面试经常问的点。 一、为什么需要加密？ 小时候看谍战片，情报发过来了之后，用一个小本本进行翻译，然后解密出情报。加密就是防止明文被别人看到甚至篡改嘛！ 回到互联网，因为http的内容是明文传输的，明文数据会经过中间代理服务器、路由器、wifi热点、通信服务运营商等多个物理节点，如果信息在传输过程中被劫持，传输的内容就完全暴露了，他还可以篡改传输的信息且不被双方察觉，这就是中间人攻击。所以我们才需要对信息进行加密。最简单容易理解的就是对称加密 。 二、什么是对称加密？ 小明写个求爱信给小红，小明担心小红的妈妈看到这封信的内容，他灵机一动，对信加个密，并确定好我用这个密钥加密的，小红收到之后也用这个密钥解密才行。 但是呢，这里有个麻烦的地方就是，小明和小红不在一个学校，这个钥匙呢，不方便直接送到手里。所以呢，小明得想办法把这个钥匙寄一个送给小红，好吧，就用最贵的顺丰吧！ 就是有一个密钥，它可以对一段内容加密，加密后只能用它才能解密看到原本的内容，和我们日常生活中用的钥匙作用差不多。 三、用对称加密可行吗？ 顺丰快递到了，结果小红不在家，小红的妈妈收到了，一看是个男同学寄的，怎么能忍住，赶紧打开，以看是一把钥匙，作为程序猿，妈妈得意一笑：哼哼，还能逃过我的眼睛？我赶紧复制一把藏着，我倒要看看他后面要寄啥来，还要加密？！ 果然小红的妈妈等到了来自小明寄过来的情书，解密一看，实锤早恋。 同样地，小明这边也非常危险，快递员刚出发，就被小明的妈妈拦截了，拿到了这个钥匙，那小明还没寄出的信已经被妈妈看光了。 所以问题的根本就是，这把钥匙要传输，传输就可能被截取。 回到互联网，如果通信双方都各自持有同一个密钥，且没有别人知道，这两方的通信安全当然是可以被保证的（除非密钥被破解）。 然而最大的问题就是这个密钥怎么让传输的双方知晓，同时不被别人知道。 如果由服务器生成一个密钥并传输给浏览器，那这个传输过程中密钥被别人劫持弄到手了怎么办？ 换种思路？试想一下，如果浏览器内部就预存了网站A的密钥，且可以确保除了浏览器和网站A，不会有任何外人知道该密钥，那理论上用对称加密是可以的，这样浏览器只要预存好世界上所有HTTPS网站的密钥就行啦！这么做显然不现实。 怎么办？所以我们就需要神奇的非对称加密。 四、什么是非对称加密？ 有两把密钥，通常一把叫做公钥、一把叫做私钥。 用公钥加密的内容必须用私钥才能解开，同样，私钥加密的内容只有公钥能解开。 五、用非对称加密可行吗？ 公钥呢，还是要通过快递员送给小红的。OK，假设小红要回信，写好了用公钥加密，小红的妈妈因为拿不到私钥，看不到信的内容。 OK，但是反过来呢？小明用私钥加密传给小红，那么小红的妈妈可就能解密了（因为公钥可能会被小红的妈妈拿到）。 回到互联网，服务器先把公钥直接明文传输给浏览器，之后浏览器向服务器传数据前都先用这个公钥加密好再传，这条数据的安全似乎可以保障了！因为只有服务器有相应的私钥能解开这条数据。 然而由服务器到浏览器的这条路怎么保障安全？ 如果服务器用它的的私钥加密数据传给浏览器，那么浏览器用公钥可以解密它，而这个公钥是一开始通过明文传输给浏览器的，这个公钥被谁劫持到的话，他也能用该公钥解密服务器传来的信息了。 所以目前似乎只能保证由浏览器向服务器传输数据时的安全性（其实仍有漏洞，下文会说）。 六、改良的非对称加密方案，似乎可以？ 小明和小红年纪不大，但是很聪明，针对这个情况，还是迅速升级加密方法。他们想到既然一组公钥私钥不够，那两组呢？ OK，小明和小红各造了一对。下面就是互相交换公钥。那么就变成： 下面就好办啦，小明写信用公钥B加密，那么信的内容只有小红能破解，因为小红是随身携带私钥B。相反，小红用公钥A对信加密，这样只有小明能破解，因为小明也是随身携带私钥A。好像很安全啦！除了下面提到的漏洞，唯一的缺点可能是：小红得花半天时间才能解密完这封信，有点受不了。 回到互联网。请看下面的过程： 某网站拥有用于非对称加密的公钥A、私钥A；浏览器拥有用于非对称加密的公钥B、私钥B。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器把公钥B明文传输给服务器。 之后浏览器向服务器传输的所有东西都用公钥A加密，服务器收到后用私钥A解密。由于只有服务器拥有这个私钥A可以解密，所以能保证这条数据的安全。 服务器向浏览器传输的所有东西都用公钥B加密，浏览器收到后用私钥B解密。同上也可以保证这条数据的安全。 的确可以！抛开这里面仍有的漏洞不谈（下文会讲），HTTPS的加密却没使用这种方案，为什么？最主要的原因是非对称加密算法非常耗时，特别是加密解密一些较大数据的时候有些力不从心。 七、非对称加密+对称加密？ 小明也知道，这个信很长，用非对称加密，太慢！办法也有，没有必要对那么长的信加密，我只要保证这个真正解密的钥匙不被别人拿到就行，那么他灵机一动想到这个方法： 小明和小红利用非对称加密对钥匙加密，姑且认为是这个钥匙被放在了一个盒子里，这个盒子也被锁起来了，只有小红或者小明才能打开盒子，再用钥匙去解密。 这个真正用于对称加密解密的钥匙别人就拿不到啦！ 自从用了这个方案，感觉又安全，解密又快，感情又深温了呢！ 回到互联网，步骤如下： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都用密钥X加密解密。 HTTPS的基本思想就是基于这个。但是这个方案也存在上面一直在说的漏洞。 八、中间人攻击 像妈妈这样级别的程序猿可能是那他们两没办法啦，但是呢，校区有个看门的大爷，以前是个黑客，也不知道咋回事，明明才50岁，但是看起来像80岁，头上光溜溜的，冬天冷呢。整天在那胡言乱语：docker牛逼啊，spring cloud牛逼啊，这个开源软件XXX写的真好，跟周围的老大爷老大妈根本谈不到一起去。 他也是闲的蛋疼，非要掺和，因为据说他以前单身30年，苦逼敲代码，不知道谈恋爱是啥滋味，姑且认为他好奇心重吧。 在小明第一次寄公钥A的时候，大爷出手了，截取下来。换成自己做的公钥B。然后送给小红。 小红哪里会知道这公钥被掉包了呢，所以直接就用了，按照正常步骤，小红想了一个随机字符串，这次就叫xiaomingwoxuanni吧，OK，用这个公钥B对这个字符串加个密，这个字符串就被锁进了用大爷公钥B锁的盒子里。 老大爷在门口守着呢，一看到小红寄东西了，又偷偷地截取下来，用自己的私钥B来解密这个盒子。轻易地拿到了里面的字符串，OK，怕小明察觉，再用小明寄来的公钥A加密传给小明，这样双方都不知道他们的钥匙已经被大爷给获取了。 小明和小红之间的信就用xiaomingwoxuanni这个钥匙进行对称加密和对称解密，完全不知道有个大爷就天天拿着这个字符串去解密信件，看的不亦乐乎，甚至还偷偷改几个字呢。 回到互联网。中间人的确无法得到浏览器生成的密钥B，这个密钥本身被公钥A加密了，只有服务器才有私钥A解开拿到它呀！然而中间人却完全不需要拿到密钥A就能干坏事了。请看： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器向网站服务器请求，服务器把公钥A明文给传输浏览器。 中间人劫持到公钥A，保存下来，把数据包中的公钥A替换成自己伪造的公钥B（它当然也拥有公钥B对应的私钥B）。 浏览器随机生成一个用于对称加密的密钥X，用公钥B（浏览器不知道公钥被替换了）加密后传给服务器。 中间人劫持后用私钥B解密得到密钥X，再用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样在双方都不会发现异常的情况下，中间人得到了密钥B。根本原因是浏览器无法确认自己收到的公钥是不是网站自己的。只要解决了这个公钥一定是这个网站发来的，那么基本就OK了 九、如何证明浏览器收到的公钥一定是该网站的公钥？ 现实生活中，如果想证明某身份证号一定是小明的，怎么办？看身份证。这里政府机构起到了“公信”的作用，身份证是由它颁发的，它本身的权威可以对一个人的身份信息作出证明。互联网中能不能搞这么个公信机构呢？给网站颁发一个“身份证”？ 十、数字证书 网站在使用HTTPS前，需要向“CA机构”申请颁发一份数字证书，即SSL证书，数字证书里有证书持有者、证书持有者的公钥等信息，服务器把证书传输给浏览器，浏览器从证书里取公钥就行了，证书就如身份证一样，可以证明“该公钥对应该网站”。然而这里又有一个显而易见的问题了，证书本身的传输过程中，如何防止被篡改？即如何证明证书本身的真实性？身份证有一些防伪技术，数字证书怎么防伪呢？解决这个问题我们就基本接近胜利了！ SSL证书内容： 证书的发布机构CA 证书的有效期 公钥 证书所有者 签名 十一、如何放防止数字证书被篡改？ 我们把证书内容生成一份“签名”，比对证书内容和签名是否一致就能察觉是否被篡改。这种技术就叫数字签名。 提到数字签名，其实原理很简单啦，就是比如我要传输一句话叫：“你给我转100块钱，我的账号是123456，转完了告诉我一声。”，如果不做任何处理，被刚才的老大爷截取了，他偷偷地改一下内容“你给我转200块钱，我的账号是654321，不要告诉任何人，尤其是你嫂子。” 是不是太坏了，弄不好被抓，大爷可不敢做大的，只敢骗个喝酒钱。 那么怎么防止大爷这种猥琐技术又高的人篡改呢？数字签名排上用场啦！ 以后再传消息就是“你给我转100块钱，我的账号是123456，转完了告诉我一声。”+“！……@&amp;@%#……！￥@￥！@%……#￥！%……”,后面那一串东西就是数字签名，简单来说，就是想办法对前面的内容进行非对称加密（这样别人根本不知道你加密的私钥是什么，也就伪装不了签名了）。传过去之后，我要对其进行解密，与传过来的明文一一对比参数，看有没有被改动过。一旦发现哪里不对应，说明已经被篡改了。 “CA机构”制作签名的过程： CA拥有非对称加密的私钥和公钥。 CA对证书明文信息进行hash。 对hash后的值用私钥加密，得到数字签名。 明文和数字签名共同组成了数字证书，这样一份数字证书就可以颁发给网站了。网站把这个数字证书传给浏览器。 那浏览器拿到服务器传来的数字证书后，如何验证它是不是真的？（有没有被篡改、掉包） 浏览器验证过程： 拿到证书，得到明文T，数字签名S。 用CA机构的公钥对S解密（由于是浏览器信任的机构，所以浏览器保有它的公钥。详情见下文），得到S’。 用证书里说明的hash算法对明文T进行hash得到T’。 比较S’是否等于T’，等于则表明证书可信。 为什么这样可以证明证书可信呢？我们来仔细想一下。 十二、中间人有可能篡改该证书吗？ 老大爷就算有天大的能耐，也拿不到加密的私钥，那么只是单纯地篡改明文，只会造成校验不通过。 回到互联网，假设中间人篡改了证书的原文，由于他没有CA机构的私钥，所以无法得到此时加密后签名，无法相应地篡改签名。 浏览器收到该证书后会发现原文和签名解密后的值不一致，则说明证书已被篡改，证书不可信，从而终止向服务器传输信息，防止信息泄露给中间人。 十三、中间人有可能把证书掉包吗？ 假设有另一个网站B也拿到了CA机构认证的证书，它想搞垮网站A，想劫持网站A的信息。于是它成为中间人拦截到了A传给浏览器的证书，然后替换成自己的证书，传给浏览器，之后浏览器就会错误地拿到B的证书里的公钥了，会导致上文提到的漏洞。 其实这并不会发生，因为证书里包含了网站A的信息，包括域名，浏览器把证书里的域名与自己请求的域名比对一下就知道有没有被掉包了。 总结：因为一个网站域名对应一个证书，你的证书根其他人的证书肯定是不一样的，那么你就算拿到了其他人的证书再掉包成自己的，也没用，毕竟浏览器那边只要看一下是不是我要查看的域名。 十四、为什么制作数字签名时需要hash一次？ 最显然的是性能问题，前面我们已经说了非对称加密效率较差，证书信息一般较长，比较耗时。而hash后得到的是固定长度的信息（比如用md5算法hash后可以得到固定的128位的值），这样加密解密就会快很多。 十五、怎么证明CA机构的公钥是可信的？ 让我们回想一下数字证书到底是干啥的？没错，为了证明某公钥是可信的，即“该公钥是否对应该网站/机构等”，那这个CA机构的公钥是不是也可以用数字证书来证明？没错，操作系统、浏览器本身会预装一些它们信任的根证书，如果其中有该CA机构的根证书，那就可以拿到它对应的可信公钥了。 实际上证书之间的认证也可以不止一层，可以A信任B，B信任C，以此类推，我们把它叫做信任链或数字证书链，也就是一连串的数字证书，由根证书为起点，透过层层信任，使终端实体证书的持有者可以获得转授的信任，以证明身份。 另外，不知你们是否遇到过网站访问不了、提示要安装证书的情况？这里安装的就是根证书。说明浏览器不认给这个网站颁发证书的机构，那么没有该机构的根证书，你就得手动下载安装（风险自己承担XD）。安装该机构的根证书后，你就有了它的公钥，就可以用它验证服务器发来的证书是否可信了。 也就是说，公钥是从证书中获取的。证书是网站从机构那边申请来的，证书+签名传给浏览器。只要校验通过，那么公钥必然没有被篡改过，并且一定是这个网站传来的，那么解决了我们最核心的问题：确定公钥是我们指定的网站传来的。 既然公钥是正确的，那么小红就会用正确的公钥对随机字符串加密，中间不会出现篡改。 十六、HTTPS必须在每次请求中都要先在SSL/TLS层进行握手传输密钥吗？ 这也是我当时的困惑之一，显然每次请求都经历一次密钥传输过程非常耗时，那怎么达到只传输一次呢？用session就行。 服务器会为每个浏览器（或客户端软件）维护一个session ID，在TSL握手阶段传给浏览器，浏览器生成好密钥传给服务器后，服务器会把该密钥存到相应的session ID下，之后浏览器每次请求都会携带session ID，服务器会根据session ID找到相应的密钥并进行解密加密操作，这样就不必要每次重新制作、传输密钥了！ 十七、HTTPS原理 下面再来看看HTTPS原理就特别简单啦！ HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：可以理解为HTTP+SSL/TLS， 即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL，用于安全的 HTTP 数据传输。 1234HTTPSSL/TLSTCPIP 我们只要知道，在SSL层里面可以完成校验和密钥的传输。 理解了上面，这个图也就没啥好解释的了。 整理自：https://zhuanlan.zhihu.com/p/43789231]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP基础知识提炼]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F6.HTTP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%8F%90%E7%82%BC%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第六篇文章。这里简单记录一些关于HTTP的基本概念，比较基础。下面的内容是对《图解HTTP》提炼的再提炼，主要原因是很多重要的东西前面已经详细说过了，还有一些东西知道即可，用到再去查，作为一个后端攻城狮，也没有必要了解那么琐碎。 HTTP是不保存状态的协议 HTTP是无状态协议。自身不对请求和响应之间通信状态进行保存（即不做持久化处理）。 HTTP之所以设计得如此简单，是为了更快地处理大量事物，确保协议的可伸缩性。 HTTP/1.1 随时无状态协议，但可通过 Cookie 技术保存状态。 告知服务器意图的HTTP方法 GET：获取资源 POST：传输实体主体 PUT：传输文件 HEAD：获得报文首部，与GET方法一样，只是不返回报文主体内容。用于确认URI的有效性及资源更新的日期时间等。 DELETE：删除文件，与PUT相反（响应返回204 No Content）。 OPTIONS：询问支持的方法，查询针对请求URI指定的资源支持的方法（Allow:GET、POST、HEAD、OPTIONS）。 TRACE：追踪路径 CONNECT：要求用隧道协议连接代理（主要使用SSL（Secure Sockets Layer，安全套接层）和TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输）。 URI、URL 官方解释都是什么乱起八糟的东西。各种博客也是跟着抄，这两者到底是什么关系和意义？ 统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来。 对应于实际例子就是：每个人都有身份证，这个身份证号码就对应这个人。比如张三的身份证号码为123456，那么我只要知道123456就可以找到这个人。 那什么是URL呢？从名字看是：统一资源定位器。 如果做类比，URL就是：动物住址协议://地球/中国/浙江省/杭州市/西湖区/某大学/14号宿舍楼/525号寝/张三.人 我们通过这个详细的地址也可以找到张三这个人。 那么他们俩到底是什么关系呢？ URI是以某种规则唯一地标识资源，手段不限，比如身份证号。当然了，地址可以唯一标识，那么也属于URI的一种手段。所以说URL是URI的子集。 回到Web上，假设所有的Html文档都有唯一的编号，记作html:xxxxx，xxxxx是一串数字，即Html文档的身份证号码，这个能唯一标识一个Html文档，那么这个号码就是一个URI。 而URL则通过描述是哪个主机上哪个路径上的文件来唯一确定一个资源，也就是定位的方式来实现的URI。 对于现在网址我更倾向于叫它URL，毕竟它提供了资源的位置信息，如果有一天网址通过号码来标识变成了http://741236985.html，那感觉叫成URI更为合适。 HTTP请求报文 返回结果的HTTP状态码 状态码的职责是当客户端向服务器端发送请求时，描述返回的请求结果。 状态码如200 OK，以3为数字和原因短语组成。 数字中的第一位定义了响应类别，后两位无分类。响应类别有以下五种： 类别 原因短语 1XX Informational(信息性状态码) 2XX Success（成功状态码） 3XX Redirection（重定向状态码） 4XX Client Error（客户端错误状态码） 5XX Server Error（服务器错误状态码） ⭐2XX 成功 200 OK：请求被正常处理 204 No Content：一般在只需从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。 206 Partial Content：客户端进行范围请求 ⭐3XX 重定向 301 Moved Permanently：永久重定向。表示请求的资源已被分配了新的URI，以后应使用资源现在所指的URI。 也就是说，如果已经把资源对应的URI保存为书签了，这时应该按Location首部字段提示的URI重新保存。 302 Found：临时性重定向。表示请求的资源已被分配了新的URI，希望用户（本次）能使用新的URI访问。 和301 Moved Permanently状态码相似，但302状态码代表的资源不是被永久移动，只是临时性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。比如，用户把URI保存成书签，但不会像301状态码出现时那样去更新书签，而是仍旧保留返回302状态码的页面对应的URI（在Chrome中，还是会保存为重定向后的URI，不解）。 303 See Other：表示由于请求对应的资源存在着另一个URI，应使用GET方法定向获取请求的资源。这与302类似，但303明确表示客户端应当采用GET方法获取资源。 304 Not Modified：该状态码表示客户端发送附带条件的请求（指采用GET方法的请求报文中包含If-Match,If-Modified-Since，If-None-March，If-Range，If-Unmodified-Since中任一首部。）时，服务器端允许请求访问资源，但因发生请求为满足条件的情况后，直接返回304（服务器端资源未改变，可直接使用客户端未过期的缓存）。304状态码返回时，不包含任何响应的主体部分。 304虽被划分在3XX类别，但是和重定向没有关系。 307 Temporary Redirect：临时重定向。与302有相同含义。307遵守浏览器标准，不会从POST变成GET。 ⭐4XX 客户端错误 4XX的响应结果表明客户端是发生错误的原因所在。 400 Bad Request：表示请求报文中存在语法错误。 401 Unauthorized：表示发送的请求需要有通过HTTP认证（BASIC认证、DIGEST认证）的认证信息。 403 Forbidden：表明对请求资源的访问被服务器拒绝了。服务器端可在实体的主体部分对原因进行描述（可选） 404 Not Found：表明服务器上无法找到请求的资源。除此之外，也可以在服务器端拒绝请求且不想说明理由时时用。 ⭐5XX 服务器错误 5XX的响应结果表明服务器本身发生错误。 500 Interval Server Error：表明服务器端在执行请求时发生了错误。也有可能是Web应用存在的bug或某些临时的故障。 503 Service Unavailable：表明服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。如果事先得知解除以上状况需要的时间，最好写入Retry-After首部字段再返回给客户端。 HTTP的瓶颈 一条连接上只可发送一个请求（前面讲到，持久化可保持TCP连接状态，但仍完成一次请求/响应后才能进行下一次请求/响应，而管线化方式可让一个TCP连接并行发送多个请求。） 请求只能从客户端开始。客户端不可以接收除响应以外的指令 请求/响应首部未经压缩就发送。首部信息越多延迟越大 发送冗长(重复)的首部。每次互相发送相同的首部造成的浪费较多 SPDY以会话层的形式加入，控制对数据的流动，但还是采用HTTP建立通信连接。因此，可照常使用HTTP的GET和POST等方法、Cookie以及HTTP报文等。 使用 SPDY后，HTTP协议额外获得以下功能。 多路复用流：通过单一的TCP连接，可以无限制处理多个HTTP请求。所有请求的处理都在一条TCP连接上完成，因此TCP的处理效率得到提高。 赋予请求优先级：SPDY不仅可以无限制地并发处理请求，还可以给请求逐个分配优先级顺序。这样主要是为了在发送多个请求时，解决因带宽低而导致响应变慢的问题。 压缩HTTP首部：压缩HTTP请求和响应的首部。 推送功能：支持服务器主动向客户端推送数据的功能。 服务器提示功能：服务器可以主动提示客户端请求所需的资源。由于在客户端发现资源之前就可以获知资源的存在，因此在资源已缓存等情况下，可以避免发送不必要的请求。 WebSocket 利用Ajax和Comet技术进行通信可以提升Web的浏览速度。但问题在于通信若使用HTTP协议，就无法彻底解决瓶颈问题。 WebSocket技术主要是为了解决Ajax和Comet里XMLHttpRequst附带的缺陷所引起的问题。 一旦Web服务器与客户端之间建立起WebSocket协议的通信连接，之后所有的通信都依靠这个专用协议进行。通信过程中可互相发送JSON、XML、HTML或图片等任意格式的数据。 WebSocket的主要特点： 推送功能：支持由服务器向客户端推送数据。 减少通信量：和HTTP相比，不但每次连接时的总开销减少，而且由于WebSocket的首部信息很小，通信量也相应较少。 为了实现WebSocket通信，在HTTP连接建立之后，需要完成一次“握手”的步骤。 握手·请求：为了实现WebSocket通信，需要用到HTTP的Upgrade首部字段，告知服务器通信协议发生改变，以达到握手的目的。 握手·响应：对于之前的请求，返回状态码101 Switching Protocols 的响应。 成功握手确立WebSocket连接后，通信时不再使用HTTP的数据帧，而采用WebSocket独立的数据帧。 由于是建立在HTTP基础上的协议，因此连接的发起方仍是客户端，而一旦确立WebSocket通信连接，不论服务器端还是客户端，任意一方都可直接向对方发送报文。 整理自：https://github.com/JChehe/blog/blob/master/posts/《图解HTTP》读书笔记.md]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手和四次挥手]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F5.TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第五篇文章。面试讲到TCP，那么基本都会问三次握手和四次挥手的过程，以及比如对于握手，为什么是三次，而不是两次或者四次，本章详细探讨其中的门道。 1.复习 首先针对http协议，我们有必要复习一下最重要的东西。 HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 在HTTP 1.0以0.9版本中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。 通常的做法是即使不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道 客户端“在线”。 若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 2.SOCKET原理 2.1 套接字（socket）概念 初次接触这个名词：“套接字”，说实话，心里是蒙蔽的，这是啥玩意，但是可以去搜索一下什么是套接管： 我们可以看出来，两个管子可能直接连的话连不起来，那么可以通过中间一个东西连接起来。 那么，现在就好理解了，两个程序要通信，需要知道对方的一些信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 它是什么呢？它是网络通信过程中端点的抽象表示，这个抽象里面就包含了网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 那为什么一定要用它呢？ 在同一台计算机上，TCP协议与UDP协议可以同时使用相同的port而互不干扰。 操作系统根据套接字地址，可以决定应该将数据送达特定的进程或线程。这就像是电话系统中，以电话号码加上分机号码，来决定通话对象一般。 因为我们电脑上可能会跑很多的应用程序，TCP协议端口需要为这些同时运行的程序提供并发服务，或者说，传输层需要为应用层的多个进程提供通信服务，每个进程起一个TCP连接，那么这多个TCP连接可能是通过同一个 TCP协议端口传输数据。 如何区别哪个进程对应哪个TCP连接呢？ 许多计算机操作系统为应用程序与TCP／IP协议交互提供了套接字(Socket)接口。应 用层可以和传输层通过Socket接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。 2.2 建立socket连接 建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。 套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。 客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发 给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 2.3 SOCKET连接与TCP连接 创建Socket连接时，可以指定使用的传输层协议，Socket可以支持不同的传输层协议（TCP或UDP），当使用TCP协议进行连接时，该Socket连接就是一个TCP连接。 3.TCP基本字段 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 针对协议中的字段，我们只需要了解一下：ACK、SYN、序号这三个部分。 ACK : 确认 TCP协议规定，只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 SYN ： 在连接建立时用来同步序号。 当SYN=1而ACK=0时，表明这是一个连接请求报文。 对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此, SYN置1就表示这是一个连接请求或连接接受报文。 FIN 即终结的意思， 用来释放一个连接。 当 FIN = 1 时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接。 4.三次握手(重要，细读) 首先，TCP作为一种可靠传输控制协议，其核心思想：既要保证数据可靠传输，又要提高传输的效率，而用三次恰恰可以满足以上两方面的需求！ 然后，要明确TCP连接握手，握的是啥？ 答案：通信双方数据原点的序列号！ 我们在上面一篇文章知道，消息的完整是靠给每个消息包搞一个编号，依次地ACK确认。确认机制是累计的，意味着 X 序列号之前(不包括 X) 包都是被确认接收到的。 TCP可靠传输的精髓：TCP连接的一方A，由操作系统动态随机选取一个32位长的序列号（Initial Sequence Number）。 假设A的初始序列号为1000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，1001，1002，1003…，并把自己的初始序列号ISN告诉B。 让B有一个思想准备，什么样编号的数据是合法的，什么编号是非法的，比如编号900就是非法的，同时B还可以对A每一个编号的字节数据进行确认。 如果A收到B确认编号为2001，则意味着字节编号为1001-2000，共1000个字节已经安全到达。 同理B也是类似的操作，假设B的初始序列号ISN为2000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，2001，2002，2003…，并把自己的初始序列号ISN告诉A，以便A可以确认B发送的每一个字节。如果B收到A确认编号为4001，则意味着字节编号为2001-4000，共2000个字节已经安全到达。 好了，在理解了握手的本质之后，下面就可以总结上面图的握手过程了。 对于A与B的握手过程，可以总结为： A 发送同步信号SYN + A's Initial sequence number（丢失会A会重传） B 确认收到A的同步信号，并记录 A's ISN 到本地，命名 B's ACK sequence number B发送同步信号SYN + B's Initial sequence number （丢失B会周期性超时重传，直到收到A的确认） A确认收到B的同步信号，并记录 B's ISN 到本地，命名 A's ACK sequence number 很显然2和3 这两个步骤可以合并，只需要三次握手，可以提高连接的速度与效率。 这里就会引出一个问题，两次不行吗？ A 发送同步信号SYN + A’s Initial sequence number B发送同步信号SYN + B’s Initial sequence number + B’s ACK sequence number 这里有一个问题，A与B就A的初始序列号达成了一致，但是B无法知道A是否已经接收到自己的同步信号，如果这个同步信号丢失了，A和B就B的初始序列号将无法达成一致。 所以A必须再给B一个确认，以确认A已经接收到B的同步信号。 如果A发给B的确认丢了，该如何？ A会超时重传这个ACK吗？不会！TCP不会为没有数据的ACK超时重传。 那该如何是好？B如果没有收到A的ACK，会超时重传自己的SYN同步信号，一直到收到A的ACK为止。 写到这里，其实我们已经明白了，握手其实就是各自确认对方的序列号。因为后面的数据编号就会以此为基础，从而保证后续数据的可靠性。 谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 但是有的人指出，其实这只是表象，或者说并不是三次握手的设计初衷，我表示认同，这个防止已失效的连接请求报文段应该只是附加的一些好处，而不应该是解释为什么是三次握手的原因。 TCP初始阶段为什么是三次握手，原因总结如下： 为了实现可靠数据传输， TCP 协议的通信双方， 都必须维护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。 三次握手的过程即是通信双方相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤，如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认。 5.四次挥手 当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了； 但是，这个时候主机1还是可以接受来自主机2的数据； 当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的； 当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了； 主机1告诉主机2知道了，主机2收到这个确认之后就立马关闭自己。 主机1等待2MSL之后也关闭了自己。 针对最后一条消息，即主机1发送ack后，主机2接收到此消息，即认为双方达成了同步：双方都知道连接可以释放了，此时B可以安全地释放此TCP连接所占用的内存资源、端口号。 所以被动关闭的B无需任何wait time，直接释放资源。 但是主机1并不知道主机2是否接到自己的ACK，主机1是这么想的： 如果主机2没有收到自己的ACK，主机2会超时重传FiN，那么主机1再次接到重传的FIN，会再次发送ACK 如果主机2收到自己的ACK，也不会再发任何消息，包括ACK 无论是情况1还是2，A都需要等待，要取这两种情况等待时间的最大值，以应对最坏的情况发生，这个最坏情况是： 主机2没有收到主机1的ACK，那么超时之后主机2会重传FIN，也就是说，要浪费一个主机1发出ACK的最大存活时间(MSL)+FIN消息的最大存活时间(MSL) 不可能时间再多了，这个已经针对最糟糕的状况。 等待2MSL时间，A就可以放心地释放TCP占用的资源、端口号，此时可以使用该端口号连接任何服务器。 在等待的时间内，主机2可以重试多次，因为2MSL时间为240秒，超时重传只有0.5秒，1秒，2秒，，16秒。 当主机2重试次数达到上限，主机2会reset连接。 那么为什么是2MSL我们已经了解了，但是为什么要等这个时间呢？ 如果不等，释放的端口可能会重连刚断开的服务器端口，这样依然存活在网络里的老的TCP报文可能与新TCP连接报文冲突，造成数据冲突，为避免此种情况，需要耐心等待网络老的TCP连接的活跃报文全部死翘翘，2MSL时间可以满足这个需求。 整理自： https://www.zhihu.com/question/24853633 https://www.zhihu.com/question/67013338 https://github.com/jawil/blog/issues/14 https://www.jianshu.com/p/9968b16b607e]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议入门]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F4.TCP%E5%8D%8F%E8%AE%AE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第四篇文章。首先要明确：信道本身不可靠（丢包、重复包、出错、乱序），不安全。所以引出了七层或五层模型来保证。因此，任何一个东西的提出都是为了解决某个问题的。学习计算机，从他的历史出发，理解为什么会有不断低迭代，因为是为了解决某个痛点问题。比如HTTP的发展，为什么在HTTP1.0基础上还要提出HTTP1.1，为什么还要提出HTTP2.0，我们学习了他的发展历史之后就会明白了。同样，下面再说一说为什么要有TCP协议，TCP到底解决了什么问题。 一、回顾 首先简单回顾一下。 1.1 物理层 物理层是相当于物理连接，将0101以电信号的形式传输出去。 1.2 数据链路层 数据链路层，有一个叫做以太网协议，规定了电子信号是如何组成数据包的，这个协议的头里面，包含了自身的网卡信息，还有目的地的网卡信息（一般我们可以知道对方的IP，IP可以通过DNS解析到，然后根据ARP协议将IP转换为MAC地址）。那么，如果在同一个局域网内，我们就可以通过广播的方式找到对应MAC地址的主机。—即以太网协议解决了子网内部的点对点通信。 1.3 网络层 但是呢，以太网协议不能解决多个局域网通信，每个局域网之间不是互通的，那么以太网这种广播的方式不可用，就算可用，网络那么大，通过广播进行找，是一个可怕的场景。那么，IP协议可以连接多个局域网，简单来说，IP 协议定义了一套自己的地址规则，称为 IP 地址。物理器件，比如说路由器，就是基于IP协议，里面保存一套地址指路牌，想去哪个局域网，可以通过这个牌子来找，然后逐步路由到目标局域网，最后就可以找到那台主机了。IP层就是对应了网络层。 1.4 传输层 那么，此时解决了多个局域网路由问题，也解决了局域网内寻址问题，即我这台主机已经可以找到那台主机了，下面还有什么事情需要做呢？显然，找到主机还不行啊，比如我用微信发一条消息，我发到你主机了，但是你主机上的微信不知道这条消息发给他了，这里说的就是端口，信息要发到这个端口上，监听这个端口的程序才会收到消息。 1.5 应用层 OK，最上层的应用层，就是最贴近用户的，他的一系列协议只是为了让两台主机会互相都理解而已。 二、问题和解决 2.1 存在的问题 在明白了计算机网络为什么要这几层模型之后，我们再回到一开始，如何保证安全、可靠、完整地传输信息呢？ 很显然，上面提到的，只是保证信息能找到对方主机和端口，但是这个信息中途被拦截了、甚至被篡改了、信息延迟了（几分钟或者几个小时，或者几个世纪）、网络不通或者挂了，信息自己可不会告诉你他挂了或者要迟到一会，如果没有一个协议来保障可靠性，那么我这条消息发出去，能不能到、能不能及时到、能不能完整到、能不能不被篡改到等这些问题将会造成灾难，网络传输也就没有了意义。 计算机的前辈们，为我们提供了一系列的措施来尽可能保证信息能正确送达。 2.2 数据校验 首先在数据链路层，可以通过各种校验，比如奇偶校验等手段来判断数据包传的是否正确。 2.3 数据可靠性 好了，解决了数据是否正确之后，但是还不能保证线路是可靠的，加入某个包没发出去或者发错了，应该有一个出错重传机制，保证信息传输的可靠性。这就引出了TCP协议。 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 下面来看看TCP是如何解决丢包、重复包、出错、乱序等不可靠的一些问题的。 三、滑动窗口协议的提出 这就引出了TCP中最终的一个东西：滑动窗口协议。 3.1 朴素的方法来确保可靠性 先从简单的角度出发，自己想一下，如何保证不丢包、不乱序。 按照顺序，发送一个确认一个，显然，吞吐量非常低。那么，一次性发几个包，然后一起确认呢？ 3.2 改进方案 那么就引出第二个问题，我一次性发几个包合适呢？就这引出了滑动窗口。 四、数据包编号和重传机制 4.1 数据包编号 在说明滑动窗口原理之前，必须要说一下TCP数据包的编号SEQ。 我们知道，由于以太网数据包大小限制，所以每个包承载的数据是有限的，如果发一个很大的包，必然是要拆分的。 发送的时候，TCP 协议为每个包编号（sequence number，简称 SEQ），以便接收的一方按照顺序还原。万一发生丢包，也可以知道丢失的是哪一个包。 第一个包的编号是一个随机数。为了便于理解，这里就把它称为1号包。假定这个包的负载长度是100字节，那么可以推算出下一个包的编号应该是101。这就是说，每个数据包都可以得到两个编号：自身的编号，以及下一个包的编号。接收方由此知道，应该按照什么顺序将它们还原成原始文件。 4.2 数据重传机制 TCP协议就是根据这些编号来重新还原文件的。并且接收端保证顺序性返回ACK确认，比如有两个包发过去，为1号和2号，2号接收成功，但是发现1号包还没接收到，所以2号的ACK是不会发回去的，这个时候，如果在重传时间内收到1号了，那么就把这两个包的ACK都返回回去，如果超时了，就重传1号包。知道1号包接收成功，后续的才会返回ACK。 具体是如何做到的呢？ 前面说过，每一个数据包都带有下一个数据包的编号。如果下一个数据包没有收到，那么 ACK 的编号就不会发生变化。 举例来说，现在收到了4号包，但是没有收到5号包。ACK 就会记录，期待收到5号包。过了一段时间，5号包收到了，那么下一轮 ACK 会更新编号。如果5号包还是没收到，但是收到了6号包或7号包，那么 ACK 里面的编号不会变化，总是显示5号包。这会导致大量重复内容的 ACK。 如果发送方发现收到三个连续的重复 ACK，或者超时了还没有收到任何 ACK，就会确认丢包，即5号包遗失了，从而再次发送这个包。通过这种机制，TCP 保证了不会有数据包丢失。 （图片说明：Host B 没有收到100号数据包，会连续发出相同的 ACK，触发 Host A 重发100号数据包。） 五、慢启动 下面再来说说慢启动。 服务器发送数据包，当然越快越好，最好一次性全发出去。但是，发得太快，就有可能丢包。带宽小、路由器过热、缓存溢出等许多因素都会导致丢包。线路不好的话，发得越快，丢得越多。 最理想的状态是，在线路允许的情况下，达到最高速率。但是我们怎么知道，对方线路的理想速率是多少呢？答案就是慢慢试。 TCP 协议为了做到效率与可靠性的统一，设计了一个慢启动（slow start）机制。开始的时候，发送得较慢，然后根据丢包的情况，调整速率：如果不丢包，就加快发送速度；如果丢包，就降低发送速度。 Linux 内核里面设定了（常量TCP_INIT_CWND），刚开始通信的时候，发送方一次性发送10个数据包，即&quot;发送窗口&quot;的大小为10。然后停下来，等待接收方的确认，再继续发送。 默认情况下，接收方每收到两个 TCP 数据包，就要发送一个确认消息。&quot;确认&quot;的英语是 acknowledgement，所以这个确认消息就简称 ACK。 ACK 携带两个信息： 期待要收到下一个数据包的编号 接收方的接收窗口的剩余容量 发送方有了这两个信息，再加上自己已经发出的数据包的最新编号，就会推测出接收方大概的接收速度，从而降低或增加发送速率。这被称为&quot;发送窗口&quot;，这个窗口的大小是可变的。 我们可以知道，发送发和接收方都维护了一个缓冲区，可以理解为窗口。根据接收速度可以调整发送速度，逐渐达到这条线路最高的传输速率。 ok，下面就可以研究一下滑动窗口了。 六、滑动窗口原理 正常情况下： （如图，123表示已经正常发送并且收到了ACK确认。4567属于已发送但是还没有收到ACK。8910表示待发送。这个窗口当前长度为7.正常情况下，4号包收到ACK，那么窗口就会右移一格。） 但是往往会出现一些问题，比如5号包迟迟收不到ACK，在接收端可能是没有收到5号包，但是可能会收到6号包甚至是7、8号包，那么此时只能等待5号包，如果5号包顺利到达了，那么就把5678号包的ACK都发给发送端，那么发送端滑动窗口向右右移四格。如果迟迟收不到5号包，只能重传。 以上就是关于TCP中比较重要的出错重传、编号、慢启动以及滑动窗口。这些保证了数据传输的可靠性。 整理自：http://www.ruanyifeng.com/blog/2017/06/tcp-protocol.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http的前世今生]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F3.http%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第三篇文章。作为一个后端攻城狮，每天打交道最多的就是HTTP协议，在如今火热的微服务实现方案中，除了阿里的dubbo，就是spring cloud，而spring cloud目前适用的服务间通信方式也就是基于HTTP 的 restful接口来实现。并且作为浏览器上用的最多的协议，无论是前端、后端还是测试都应该去熟悉它，软件的发展是循序渐进的，每次的迭代升级都是为了解决上一版本的痛点，HTTP协议的发展也是如此，本章着重讲解HTTP的前世今生，让我们更加了解HTTP。 HTTP 是基于 TCP/IP 协议的应用层协议。它不涉及数据包（packet）传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。 一、HTTP/0.9 1.1 简介 这是第一个定稿的HTTP协议。 内容非常简单，只有一个命令GET 没有HEADER等描述数据的信息 服务器发送完毕，就关闭TCP连接（一个HTTP请求在一个TCP连接中完成） 1.2 请求格式 比如发起一个GET请求： GET /index.html 上面命令表示，TCP 连接（connection）建立后，客户端向服务器请求（request）网页index.html。 1.3 响应格式 协议规定，服务器只能回应HTML格式的字符串，不能回应别的格式。 123&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 服务器发送完毕，就关闭TCP连接。 二、HTTP/1.0 2.1 简介 跟现在比较普遍适用的1.1版本已经相差不多。 增加很多命令，比如POST、HEAD等命令 增加status code 和 header 多字符集支持、多部分发送、权限、缓存等 首先，任何格式的内容都可以发送。这使得互联网不仅可以传输文字，还能传输图像、视频、二进制文件。这为互联网的大发展奠定了基础。 其次，除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。 再次，HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。 其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 2.2 请求格式 下面是一个1.0版的HTTP请求的例子。 123GET / HTTP/1.0User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: */* 可以看到，这个格式与0.9版有很大变化。 第一行是请求命令，必须在尾部添加协议版本（HTTP/1.0）。后面就是多行头信息，描述客户端的情况。 客户端请求的时候，可以使用Accept字段声明自己可以接受哪些数据格式。上面代码中，客户端声明自己可以接受任何格式的数据。 2.3 响应格式 服务器的回应如下： 12345678910HTTP/1.0 200 OK Content-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 回应的格式是&quot;头信息 + 一个空行（\r\n） + 数据&quot;。其中，第一行是&quot;协议版本 + 状态码（status code） + 状态描述&quot;。 2.4 Content-Type 字段 关于字符的编码，1.0版规定，头信息必须是 ASCII 码，后面的数据可以是任何格式。因此，服务器回应的时候，必须告诉客户端，数据是什么格式，这就是Content-Type字段的作用。 2.5 缺点 每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。 为了解决这个问题，有些浏览器在请求时，用了一个非标准的Connection字段。 Connection: keep-alive 这个字段要求服务器不要关闭TCP连接，以便其他请求复用。服务器同样回应这个字段。 Connection: keep-alive 一个可以复用的TCP连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段，不同实现的行为可能不一致，因此不是根本的解决办法。 三、HTTP/1.1 3.1 持久连接和管道机制 持久连接（以前的版本中，一个HTTP请求就创建一个TCP连接，请求返回之后就关闭TCP连接，然而建立一次TCP连接的过程是比较耗时的，效率会比较低，现在建立一个TCP连接后，后面的HTTP请求都可以复用这个TCP连接，即允许了在同一个连接里面发送多个请求，会提高效率） pipeline（解决了同一个TCP连接中客户端可以发送多个HTTP请求，但是对于服务端来说，对于进来的请求要按照顺序进行内容的返回，如果前一个请求处理时间长，而后一个请求处理时间端，即便后面一个请求已经处理完毕了，也要等待前一个请求处理完毕返回他才可以返回结果，这种串行的方式比较慢） 在1.1版本以前，每次HTTP请求，都会重新建立一次TCP连接，服务器响应后，就立刻关闭。众所周知，建立TCP连接的新建成本很高，因为需要三次握手，并且有着慢启动的特性导致发送速度较慢。而1.1版本添加的持久连接功能可以让一次TCP连接中发送多条HTTP请求，值得一提的是默认是，控制持久连接的Connection字段默认值是keep-alive，也就是说是默认打开持久连接，如果想要关闭，只需将该字段的值改为close。 Connection: close 而管道化则赋予了客户端在一个TCP连接中连续发送多个请求的能力，而不需要等到前一个请求响应，这大大提高了效率。值得一提的是，虽然客户端可以连续发送多个请求，但是服务器返回依然是按照发送的顺序返回。（强调的是request不需要等待上一个request的response，其实发送的request还是有顺序的，服务端按照这个顺序接收，依次返回响应） HTTP/1.1允许多个http请求通过一个套接字同时被输出 ，而不用等待相应的响应。然后请求者就会等待各自的响应，这些响应是按照之前请求的顺序依次到达。（me：所有请求保持一个FIFO的队列，一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出；同时，服务器端返回这些请求的响应时也是按照FIFO的顺序）。管道化的表现可以大大提高页面加载的速度，尤其是在高延迟连接中。 3.2 Content-Length 字段 一个TCP连接现在可以传送多个回应，势必就要有一种机制，区分数据包是属于哪一个回应的。这就是Content-length字段的作用，声明本次回应的数据长度。 Content-Length: 3495 上面代码告诉浏览器，本次回应的长度是3495个字节，后面的字节就属于下一个回应了。 在1.0版中，Content-Length字段不是必需的，因为浏览器发现服务器关闭了TCP连接，就表明收到的数据包已经全了。 3.3 分块传输编码 对于一些很耗时的动态操作来说，这意味着，服务器要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用&quot;流模式&quot;（stream）取代&quot;缓存模式&quot;（buffer）。 因此，1.1版规定可以不使用Content-Length字段，而使用&quot;分块传输编码&quot;（chunked transfer encoding）。只要请求或回应的头信息有Transfer-Encoding字段，就表明回应将由数量未定的数据块组成。 Transfer-Encoding: chunked 每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后是一个大小为0的块，就表示本次回应的数据发送完了。下面是一个例子。 1234567891011121314151617HTTP/1.1 200 OKContent-Type: text/plainTransfer-Encoding: chunked25This is the data in the first chunk1Cand this is the second one3con8sequence0 3.3 其他功能 1.1版还新增了许多动词方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。 另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名。 Host: www.example.com 有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础。 3.4 缺点 虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为&quot;队头堵塞&quot;（Head-of-line blocking）。 四、SPDY 协议 2009年，谷歌公开了自行研发的 SPDY 协议，主要解决 HTTP/1.1 效率不高的问题。 这个协议在Chrome浏览器上证明可行以后，就被当作 HTTP/2 的基础，主要特性都在 HTTP/2 之中得到继承。 五、HTTP/2 5.1 二进制协议 HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。 HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为&quot;帧&quot;（frame）：头信息帧和数据帧。 二进制协议的一个好处是，可以定义额外的帧。HTTP/2 定义了近十种帧，为将来的高级应用打好了基础。如果使用文本实现这种功能，解析数据将会变得非常麻烦，二进制解析则方便得多。 5.2 多工 HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了&quot;队头堵塞&quot;。 举例来说，在一个TCP连接里面，服务器同时收到了A请求和B请求，于是先回应A请求，结果发现处理过程非常耗时，于是就发送A请求已经处理好的部分， 接着回应B请求，完成后，再发送A请求剩下的部分。 这样双向的、实时的通信，就叫做多工（Multiplexing）。 5.3 数据流 因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。 5.4 头信息压缩 HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如Cookie和User Agent，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。 HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息使用gzip或compress压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。 5.5 服务器推送 HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。 常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析HTML源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。 整理自：http://www.ruanyifeng.com/blog/2016/08/http.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从上到下看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F2.%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第二篇文章。经过上一篇文章的详细介绍，我们了解了一个数据是如何从物理层一步一步到达应用层的，那么本章从上而下的角度来看看一条请求时如何从浏览器传递到服务器并且返回的。 这个过程看的就是用户从浏览器输入一条url之后，是如何发送过去的。 1.上一篇文章的小结 我们已经知道，网络通信就是交换数据包。电脑A向电脑B发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样： 发送这个包，需要知道两个地址： 对方的MAC地址 对方的IP地址 有了这两个地址，数据包才能准确送到接收者手中。但是，前面说过，MAC地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的MAC地址，必须通过网关（gateway）转发。 上图中，1号电脑要向4号电脑发送一个数据包。它先判断4号电脑是否在同一个子网络，结果发现不是（后文介绍判断方法），于是就把这个数据包发到网关A。网关A通过路由协议，发现4号电脑位于子网络B，又把数据包发给网关B，网关B再转发到4号电脑。 1号电脑把数据包发到网关A，必须知道网关A的MAC地址。 发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的MAC地址。 有了一台新电脑之后，要想上网，一种方式是自己配置静态IP： 很多人都没有进行过这个配置，因为一般情况下我们根本不需要这样。但是有的时候也会用到，比如我在电信实习的时候，他们每一个网口旁边都贴着这四个参数，你联网必须要适用他提供的一系列地址才行。其实经过上面的学习，我们已经知道，通信的时候，需要知道对方的IP（ARP知道对方的MAC地址）、子网掩码（确定所在的子网）、默认网关（不在一个子网，要通过网关取转发、路由）、DNS服务器（解析域名为IP地址）。 但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用&quot;动态IP地址上网&quot;。 2.DHCP协议 所谓&quot;动态IP地址&quot;，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。 这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做&quot;DHCP服务器&quot;。新的计算机加入网络，必须向&quot;DHCP服务器&quot;发送一个&quot;DHCP请求&quot;数据包，申请IP地址和相关的网络参数。 前面说过，如果两台计算机在同一个子网络，必须知道对方的MAC地址和IP地址，才能发送数据包。但是，新加入的计算机不知道DHCP服务器的两个地址，怎么发送数据包呢？ DHCP协议做了一些巧妙的规定。 首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的： （1）最前面的&quot;以太网标头&quot;，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 （2）后面的&quot;IP标头&quot;，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。 （3）最后的&quot;UDP标头&quot;，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。 因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。 当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道&quot;这个包是发给我的&quot;，而其他计算机就可以丢弃这个包。 接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个&quot;DHCP响应&quot;数据包。 这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。 动态拿到最核心的四个参数：自己的IP地址、子网掩码、网关地址、DNS服务器，就可以联网了。 3.访问google的过程 我们假定，经过上一节的步骤，用户设置好了自己的网络参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。 这意味着，浏览器要向Google发送一个网页请求的数据包。 3.1 DNS协议 我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。 DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。 然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 3.2 子网掩码 接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。 已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。 3.3 应用层协议 浏览网页用的是HTTP协议，它的整个数据包构造是这样的： HTTP部分的内容，类似于下面这样： 123456789 GET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) ...... Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: ... ... 我们假定这个部分的长度为4960字节，它会被嵌在TCP数据包之中。 3.4 TCP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 3.5 IP协议 然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。 IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 3.6 以太网协议 最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 3.7 服务器响应 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。 根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的&quot;HTTP请求&quot;，接着做出&quot;HTTP响应&quot;，再用TCP协议发回来。 本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 整理自：http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从下到上看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F1.%E4%BB%8E%E4%B8%8B%E5%88%B0%E4%B8%8A%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第一篇文章。要想了解HTTP协议，必然要从最基本的计算机网络知识开始入手。本篇文章从下到上具体介绍五层经典模型，极速入门计算机网络。 经典五层模型 下面我们先来了解一下各层做的事情！ 1.物理层 电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。 这就叫做&quot;物理层&quot;，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 2.数据链路层 单纯的0和1没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？ 这就是&quot;链接层&quot;的功能，它在&quot;实体层&quot;的上方，确定了0和1的分组方式。 2.1 以太网协议 早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做&quot;以太网&quot;（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做&quot;帧&quot;（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。 &quot;标头&quot;包含数据包的一些说明项，比如发送者、接受者、数据类型等等；&quot;数据&quot;则是数据包的具体内容。 &quot;标头&quot;的长度，固定为18字节。&quot;数据&quot;的长度，最短为46字节，最长为1500字节。因此，整个&quot;帧&quot;最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。 2.2 MAC地址 上面提到，以太网数据包的&quot;标头&quot;，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？ 以太网规定，连入网络的所有设备，都必须具有&quot;网卡&quot;接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。 每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。 前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 2.3 广播 定义地址只是第一步，后面还有更多的步骤。 首先，一块网卡怎么会知道另一块网卡的MAC地址？ 回答是有一种ARP协议，可以解决这个问题。下面介绍ARP。 其次，就算有了MAC地址，系统怎样才能把数据包准确送到接收方？ 回答是以太网采用了一种很&quot;原始&quot;的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 上图中，1号计算机向2号计算机发送一个数据包，同一个子网络的3号、4号、5号计算机都会收到这个包。它们读取这个包的&quot;标头&quot;，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做&quot;广播&quot;（broadcasting）。 有了数据包的定义、网卡的MAC地址、广播的发送方式，&quot;链接层&quot;就可以在多台计算机之间传送数据了。 3.网络层 以太网协议，依靠MAC地址发送数据。理论上，单单依靠MAC地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。 但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一&quot;包&quot;，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。 互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。 因此，必须找到一种方法，能够区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用&quot;路由&quot;方式发送。（&quot;路由&quot;的意思，就是指如何向不同的子网络分发数据包，这是一个很大的主题，本文不涉及。）遗憾的是，MAC地址本身无法做到这一点。它只与厂商有关，与所处网络无关。 这就导致了&quot;网络层&quot;的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做&quot;网络地址&quot;，简称&quot;网址&quot;。 于是，&quot;网络层&quot;出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。 3.1 IP协议 规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。 目前，广泛采用的是IP协议第四版，简称IPv4。这个版本规定，网络地址由32个二进制位组成。 习惯上，我们用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。 互联网上的每一台计算机，都会分配到一个IP地址。这个地址分成两个部分，前一部分代表网络，后一部分代表主机。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。处于同一个子网络的电脑，它们IP地址的网络部分必定是相同的，也就是说172.16.254.2应该与172.16.254.1处在同一个子网络。 但是，问题在于单单从IP地址，我们无法判断网络部分。还是以172.16.254.1为例，它的网络部分，到底是前24位，还是前16位，甚至前28位，从IP地址上是看不出来的。 那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数&quot;子网掩码&quot;（subnet mask）。 所谓&quot;子网掩码&quot;，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.254.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。 知道&quot;子网掩码&quot;，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 比如，已知IP地址172.16.254.1和172.16.254.233的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，结果都是172.16.254.0，因此它们在同一个子网络。 总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。 3.2 IP数据包 根据IP协议发送的数据，就叫做IP数据包。不难想象，其中必定包括IP地址信息。 但是前面说过，以太网数据包只包含MAC地址，并没有IP地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？ 回答是不需要，我们可以把IP数据包直接放进以太网数据包的&quot;数据&quot;部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。 具体来说，IP数据包也分为&quot;标头&quot;和&quot;数据&quot;两个部分。&quot;标头&quot;部分主要包括版本、长度、IP地址等信息，&quot;数据&quot;部分则是IP数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。 IP数据包的&quot;标头&quot;部分的长度为20个字节，整个数据包的总长度最大为65,535字节。因此，理论上，一个IP数据包的&quot;数据&quot;部分，最长为65,515字节。前面说过，以太网数据包的&quot;数据&quot;部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。 3.3 ARP协议 关于&quot;网络层&quot;，还有最后一点需要说明。 因为IP数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的MAC地址，另一个是对方的IP地址。通常情况下，对方的IP地址是已知的，但是我们不知道它的MAC地址。 所以，我们需要一种机制，能够从IP地址得到MAC地址。 这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的&quot;网关&quot;（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个&quot;广播&quot;地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 4. 传输层 有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。 接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？ 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做&quot;端口&quot;（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 &quot;端口&quot;是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。 &quot;传输层&quot;的功能，就是建立&quot;端口到端口&quot;的通信。相比之下，“网络层&quot;的功能是建立&quot;主机到主机&quot;的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做&quot;套接字”（socket）。有了它，就可以进行网络应用程序开发了。 4.1 UDP协议 现在，我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。 UDP数据包，也是由&quot;标头&quot;和&quot;数据&quot;两部分组成。 &quot;标头&quot;部分主要定义了发出端口和接收端口，&quot;数据&quot;部分就是具体的内容。然后，把整个UDP数据包放入IP数据包的&quot;数据&quot;部分，而前面说过，IP数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样： UDP数据包非常简单，&quot;标头&quot;部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 4.2 TCP协议 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 为了解决这个问题，提高网络可靠性，TCP协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的UDP协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 因此，TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。 TCP数据包和UDP数据包一样，都是内嵌在IP数据包的&quot;数据&quot;部分。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 关于TCP细节以后再探讨。 5. 应用层 应用程序收到&quot;传输层&quot;的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。 &quot;应用层&quot;的作用，就是规定应用程序的数据格式。 举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了&quot;应用层&quot;。 这是最高的一层，直接面对用户。它的数据就放在TCP数据包的&quot;数据&quot;部分。因此，现在的以太网的数据包就变成下面这样。 *注：UDP头为8个字节，TCP头为20个字节 整理于：http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
</search>
