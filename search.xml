<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CSS之定位]]></title>
    <url>%2F2019%2F03%2F11%2Ffront%2FCSS%E4%B9%8B%E5%AE%9A%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[本文来研究一下第三个重点：定位。 绝对定位和相对定位 绝对定位的主要特性是不占空间。 但是相对定位是占空间的。 我们看到，占用的是原来的空间。说明相对定位的话，原有的空间会给它保留住，实际占用的空间仍然是原来的，而不是现在移动后的。 嵌套子元素 我们可以发现，如果子元素用的是绝对定位，那么定位是跟父元素没有关系的，而是与浏览器边框有关。此时如何做到跟着父元素一起动呢?最简单的方法是将子元素改为相对定位。 改为相对定位确实可以解决这个问题，但是有的时候我们需要子元素不占用空间，那么怎么办呢？一般情况下，我们给父元素一个定位信息，一般是相对定位，然后子元素是绝对定位，这样子元素就不会占用空间，并且子元素的位置是相对父元素动的。 为什么父元素要是相对定位呢？因为父元素一般都是需要占用空间的。一举两得，所以口则是子绝父相。 此时就做到子元素相对于父元素定位了。 固定定位就是fixed，那么网页内容再多一直往下滑，它依然不动。 简单总结一下，就是相对定位是占空间的，绝对定位是不占空间的。默认情况下，绝对定位是相对于浏览器边框的，但是如果是父子关系，并且父元素设置relative子元素设置absolute的话，子元素就是相对于父元素进行排版。固定定位就是钉死不动。默认情况下是static。]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS之浮动]]></title>
    <url>%2F2019%2F03%2F11%2Ffront%2FCSS%E4%B9%8B%E6%B5%AE%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[本文来研究一下第二个重点：浮动。 浮动 那么我们如何让他变成一行呢？之前已经说过了，可以用inline-block来实现： 但是我们发现，有该死的间隙存在。此时就需要用到我的浮动了。什么是浮动，顾名思义，就是浮在上面，下面来直观感受一下： 我们会发现，一个加了浮动，一个没有加的话，那么这两个在排版的时候是互相不冲突的，也就是说，浮动的元素根本就不占用标准元素的空间，所以这两者重叠在了一起。浮动就像是飞机，而不是浮动的就是汽车，汽车与汽车之间是互斥的，但是汽车与飞机之间不互斥，下面我们再来解决上面存在间隙的问题，我们只需要将这两个元素全部置为向左浮动即可。 那么既然已经是浮动的元素了，这个元素是属于谁的呢？也就是说，是属于整个body还是属于父元素的呢？如果是属于父元素的话，那么就非常好了，我可以调整好父元素的位置之后，里面的子元素采用浮动来置为一行。我们来测试一下。 这说明确实是属于父元素的。 一些问题 此时下面的div直接就将第一个outer给顶掉了，这不是我们想要的结果。结果方案有三种。 可能有的时候需要用第三种，因为第一种方法有的时候父元素会隐藏掉多出来的子元素。有的时候我们不想隐藏，就需要用到这个方法。]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS之盒子模型]]></title>
    <url>%2F2019%2F03%2F11%2Ffront%2FCSS%E4%B9%8B%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[盒子模型是比较基础但是比较重要的一点，本文来简单了解一下。 盒子模型 CSS盒模型本质上是一个盒子，封装周围的HTML元素，它包括：边距，边框，填充，和实际内容。 盒模型允许我们在其它元素和周围元素边框之间的空间放置元素。 下面的图片说明了盒子模型(Box Model)： 由上面我知道了：最终元素的总宽度计算公式是这样的： 总元素的宽度=宽度+左填充+右填充+左边框+右边框+左边距+右边距 元素的总高度最终计算公式是这样的： 总元素的高度=高度+顶部填充+底部填充+上边框+下边框+上边距+下边距 简单了解了盒子模型之后，我们看到这个margin是在盒子的最外面，那么两个这样的盒子相邻的话可能会出一些问题。 比如两个盒子一上一下放，两个都设置margin，那么这两者之间上下间隔会是多少呢？是两者的margin之和吗？ ⭐经过验证，间隔是以大的那个为准，而不是两者之和。 那对于内部嵌套的情况呢？ 看起来好像没什么问题，都是符合我们一开始的期望的。但是去掉父盒子中的padding或者同时删除掉padding和border会是什么样子呢？ 好像子中margin的设置没什么作用了，此时我增大一下margin： （删除掉border效果也一样），效果就是两者同时下移了。这就很奇怪了，用比较专业的词汇来说，就是这两者的margin咋合并了呢？这里确实是合并了，div距离最上面的距离由其中较大的margin的值决定。 解决方法是什么呢？正如上面第一种显示的，要么加一个border要么加一个padding. 还有一种方式是在父元素上增加overflow，顾名思义就是溢出的意思。 这也可以解决边框合并问题。我们稍微来看一下这个溢出是啥意思。起始就是说，如果子元素不在这个父元素范围内了，就直接不显示了。我们也可以显示，调整为auto模式即可，就可以下拉看到子元素了。 我们来稍微总结一下，盒子模型的计算方法跟margin决定的外边距，border决定的边框宽度，padding决定的内边距以及实际内容相关。 然后就是存在外边距合并的问题，一个是上下关系，一个是嵌套关系。重点是嵌套关系，要解除外边距合并问题，第一个比较可行的方案是给父元素添加一个白色的边框，一个是给父元素增加overflow。那个padding是不大好用的，毕竟增加了padding之后整个父元素的宽度或者高度都变化了。]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS之图片]]></title>
    <url>%2F2019%2F03%2F11%2Ffront%2FCSS%E4%B9%8B%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[本文来说一说图片相关的一些处理以及精灵图。 图片 这就是简单放一张图片，但是我们发现，当图片比我们设定的div要小的时候，它会自动复制取铺满整个div。如果我们仅仅显示原来的图片，不要它铺开呢？ 这样就不会重复显示了，那我们能不能移动移动这个图片呢？ 注意，正数是让它往左往右移动。这个有什么用呢？这就要说一说精灵图了。 还可以用background-repeat: repeat-x;表示横向自动填充。 精灵图 什么是精灵图呢？ 我们去腾讯游戏的官网看一下： 我们注意到有很多的小图标，这些小图片特点是数量多，并且很小。为了减轻与服务器交互带来的不必要的开销，可以将这些小图标全部放在一张大图上，要用哪个图标通过上面说过的background-position来指定即可。 比如我想只显示那个奖杯。 慢慢移动到这个位置就可以了。]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题40-数组中只出现一次的数字】]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9840-%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第四十题。 题目描述 一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。 解题思路 一种简单的思路，可以想到用HashSet这种数据结构来存，重复的就立即剔除，剩下的就是不重复的两个数字，将其取出即可。 12345678910111213141516171819202122232425//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果import java.util.*;public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; HashSet&lt;Integer&gt; set = new HashSet&lt;&gt;(); for(int i=0;i&lt;array.length;i++)&#123; if(!set.isEmpty() &amp;&amp; set.contains(array[i]))&#123; set.remove(array[i]); &#125;else&#123; set.add(array[i]); &#125; &#125; //这边处理的不够好 ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); if(set.size() == 2)&#123; for(Integer i:set)&#123; list.add(i); &#125; &#125; num1[0] = list.get(0); num2[0] = list.get(1); &#125;&#125; 但是对于题目中说除了两个单个数字外，其他的都出现偶数次。我们需要从这句话入手，寻求更优的解决思路。 我们知道，位运算中异或的性质是：两个相同数字异或=0，不相同的话肯定不为0，一个数和0异或还是它本身。 这个题目的突破口在哪里？题目为什么要强调有一个数字出现一次，其他的出现两次？我们想到了异或运算的性质：任何一个数字异或它自己都等于0 。也就是说，如果我们从头到尾依次异或数组中的每一个数字，那么最终的结果刚好是那个只出现一次的数字，因为那些出现两次的数字全部在异或中抵消掉了。 有了上面简单问题的解决方案之后，我们回到原始的问题。如果能够把原数组分为两个子数组。在每个子数组中，包含一个只出现一次的数字，而其它数字都出现两次。如果能够这样拆分原数组，按照前面的办法就是分别求出这两个只出现一次的数字了。 我们还是从头到尾依次异或数组中的每一个数字，那么最终得到的结果就是两个只出现一次的数字的异或结果。因为其它数字都出现了两次，在异或中全部抵消掉了。由于这两个数字肯定不一样，那么这个异或结果肯定不为0 ，也就是说在这个结果数字的二进制表示中至少就有一位为1 。 我们在结果数字中找到第一个为1 的位的位置，记为第N 位。现在我们以第N 位是不是1 为标准把原数组中的数字分成两个子数组，第一个子数组中每个数字的第N 位都为1 ，而第二个子数组的每个数字的第N 位都为0 。 现在我们已经把原数组分成了两个子数组，每个子数组都包含一个只出现一次的数字，而其它数字都出现了两次。因此到此为止，所有的问题我们都已经解决。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//num1,num2分别为长度为1的数组。传出参数//将num1[0],num2[0]设置为返回结果public class Solution &#123; public void FindNumsAppearOnce(int [] array,int num1[] , int num2[]) &#123; //【解决思路】：从简单的场景想起，假设一个数组中只有一个独特元素，其他出现次数都为2 //如何快速找出这个独特元素呢？那就是从头到尾两两异或，由于相同的数异或为0，则认为是抵消 //一直到最后，结果必然就是这个独特元素 //那么找出两个来也是这个思路，核心就是要将这两个独特的数分离开，下面详细介绍 if(array == null || array.length &lt;= 1)&#123; num1[0] = num2[0] = 0; return; &#125; //整个数组从头两两异或，最终的结果必然是两个不同数字的异或结果 //因为相同的数字两两异或之后为0 //0和任意一个数异或还是这个数本身 int len = array.length, index = 0, sum = 0; for(int i = 0; i &lt; len; i++)&#123; sum ^= array[i]; &#125; //java中int类型占4个字节，即32个bit //从左开始找到这个异或结果第一个为1的索引 while((sum&amp;1) == 0 &amp;&amp; index &lt; 32)&#123; sum = sum &gt;&gt; 1; index++; &#125; //以这个索引处是否为1作为判定标准，就将两个不同的数分离开了 //下面就是分两批不停地疑惑，就会得到这两个不同的数 for(int i = 0; i &lt; len; i++)&#123; //这样就可以分别找到index处为1的独特解以及为0的独特解 if(isBit(array[i],index))&#123; num1[0] ^= array[i]; &#125;else&#123; num2[0] ^= array[i]; &#125; &#125; &#125; //判断num的index（从左往右看）是否为1 private boolean isBit(int num,int index)&#123; num = num &gt;&gt; index; if((num &amp; 1) == 1)&#123; return true; &#125;else&#123; return false; &#125; &#125; &#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题39-平衡二叉树】]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9839-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十九题。 题目描述 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 解题思路 首先明确平衡二叉树的概念，就是数的最大高度和最小高度差不超过1.根据上一题我们得到灵感，我们可以分别求出左右子树的最大深度，然后对他们两进行比较即可。 我的答案 12345678910111213141516171819202122232425262728public class Solution &#123; public boolean IsBalanced_Solution(TreeNode root) &#123; if(root == null)&#123; return true; &#125; if(root.left == null &amp;&amp; root.right == null)&#123; return true; &#125; //获取左右子树的最大高度 int left = judge(root.left); int right = judge(root.right); //两边高度不超过1即可 return Math.abs(left-right) &lt;= 1; &#125; private int judge(TreeNode root)&#123; if(root == null)&#123; return 0; &#125; int left = judge(root.left); int right = judge(root.right); return Math.max(left,right) + 1; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题38-二叉树的深度】]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9838-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十八题。 题目描述 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 解题思路 看到二叉树，递归基本没跑了，我们只要想好递归关系即可。我们假设已经正确拿到了root结点左右子树的最大深度，那么最后加一即可。 我的答案 1234567891011121314public class Solution &#123; public int TreeDepth(TreeNode root) &#123; //递归的出口，root为0则返回0，这里可以理解为root为0那肯定没有层数了 if(root == null)&#123; return 0; &#125; //拿到左子树的最大深度 int leftDep = TreeDepth(root.left); //拿到右子树的最大深度 int rightDep = TreeDepth(root.right); //找出最大值，并且加上root这一层即可 return Math.max(leftDep,rightDep) + 1; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题37-数字在排序数组中出现的次数】]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9837-%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十七题。 题目描述 统计一个数字在排序数组中出现的次数。 解题思路 看到排序数组，第一个想到的是二分查找，我们来看看这里是如何应用二分查找法的。 我的答案 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public int GetNumberOfK(int [] array , int k) &#123; int low = 0; int high = array.length-1; //统计次数 int count = 0; //进入二分查找，注意边界 while(low &lt;= high)&#123; //先找到中间点 int mid = low + (high - low) / 2; if(array[mid] &gt; k)&#123; high = mid - 1; &#125;else if(array[mid] &lt; k)&#123; low = mid + 1; &#125;else&#123; //走到这边，说明找到了第一个相等的数，先将count加一 count++; //开始从index-1往前找有没有相等的 int index = mid-1; while(index &gt;= 0 &amp;&amp; array[index--] == k)&#123; count++; &#125; //开始从index+1往后找有没有相等的 index = mid + 1; while(index &lt; array.length &amp;&amp; array[index++] == k)&#123; count++; &#125; //提前跳出while循环，结束 break; &#125; &#125; return count; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题36-两个链表的第一个公共结点】]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9836-%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十六题。 题目描述 输入两个链表，找出它们的第一个公共结点。 解题思路 这个题目的两个链表应该是有共同的尾部，而不是简单的交叉。对于这种性质，可能有更好的思路，但是我这里还是用了比较简单的想法，遍历第一个链表放进set中，再遍历另一个链表，找到第一个一样的结点，就是公共结点。时间复杂度为O(m+n) 我的答案 123456789101112131415161718192021222324import java.util.HashSet;public class Solution &#123; public ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) &#123; if(pHead1 == null || pHead2 == null)&#123; return null; &#125; HashSet&lt;ListNode&gt; set = new HashSet&lt;&gt;(); while(pHead1 != null)&#123; set.add(pHead1); pHead1 = pHead1.next; &#125; while(pHead2 != null)&#123; if(set.contains(pHead2))&#123; return pHead2; &#125; pHead2 = pHead2.next; &#125; return null; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小练习]]></title>
    <url>%2F2019%2F03%2F10%2Ffront%2F%E5%B0%8F%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前端的小练习。 实现形如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;link rel="stylesheet" type="text/css" href=""&gt; &lt;style type="text/css"&gt; /*去除整个页面中一些边距*/ *&#123; margin: 0; padding: 0; &#125; /*设置大的div块，此时左右居中*/ .nav&#123; width: 1050px; height: 65px; margin: auto; &#125; li&#123; list-style: none; display: inline-block; /*重点1，向左浮动解决&lt;li&gt;之间间隙问题*/ float: left; &#125; a&#123; display: inline-block; height: 65px; width: 350px; /*重点2，设置a标签中内容所占的高度与外面一样，这样内容就上下居中了*/ line-height: 65px; /*左右居中*/ text-align: center; /*去除下划线*/ text-decoration: none; font-size: 18px; color: white; &#125; .nav .inner-1&#123; background-color: #696391; &#125; .nav .inner-2&#123; background-color: #9385f5; &#125; .nav .inner-3&#123; background-color: #3c64c2; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="nav"&gt; &lt;ul&gt; &lt;li&gt;&lt;a class="inner-1" href=""&gt;JAVA WEB&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a class="inner-2" href=""&gt;项目实战&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a class="inner-3" href=""&gt;微服务/分布式/中间件&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS显示模式]]></title>
    <url>%2F2019%2F03%2F10%2Ffront%2FCSS%E6%98%BE%E7%A4%BA%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[CSS显示模式一般分为两种，一种是独占一行的block一种是行内元素inline类型。他们之间可以互相转换。是比较常见的，比较重要。 显示模式 我们知道div是独占一行或者说一块block，默认情况下同行是不能再放其他元素了。span是行内元素，即inline，一行可以有很多列这样的元素。 他们还有一个区别是，块级元素可以设置行高，但是行内元素是不能设置行高的，比如span给他一个高度也是没有用的，它是随着里面内容的变化而变化的，比如文字，里面的文字变大，那么这个span区域也就会随着变大。 block和inline就是两种显示模式。 block转inline 我们可以验证上面的说法，就是div是个块级，一行默认独占一格，所以两个inner就分为了两行，那么有没有办法调整为inline元素呢？ 这又验证了一下上面的说法，就是inline元素是否显示以及显示的大小与里面的内容有关。如果我这里不写一点字占坑的话，就直接没了。 此时我们发现虽然可以将这两个div放到同一行去，但是称为inline元素之后我们就不能随意设置它的宽高了，我又想把他们搞到一行，又想设置宽高，怎么实现呢？答案就是用inline-block inline转block 比如比较常见的a标签，往往不是点文字才有用，而是点一大块区域都可以，但是我们知道a标签是一格inline标签，不能直接给他设置宽高，这个时候需要将它转为block就好了，怎么搞呢？ 通过display: block;之后就变成了inline-block，就可以设置宽高了，那一片区域都可以作为超链接点击。 下面再来一个例子，比如ul和li标签，默认情况下是每一个独占一行的，如果我想让它跟分页一样排成一行怎么弄呢？起始很简单就是将其置为inline-block即可。 下一节用之前的知识做一个小例子。]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS选择器相关]]></title>
    <url>%2F2019%2F03%2F10%2Ffront%2FCSS%E9%80%89%E6%8B%A9%E5%99%A8%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[CSS要想根据我们的需要对指定的东西进行美化，需要用到选择器。下面我们来看看基本的选择器是如何使用的。 一、内联样式 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div style="color: skyblue;border: 1px dashed red;"&gt;我是南邮吴镇宇！&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 一般情况下不会这么写，所以会涉及选择器，就是css到底对谁起作用。 1&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;&quot;&gt; 在外部放一个css文件。 二、选择器 2.1 ID选择器 就是给某个标签，比如div标签增加一个id=&quot;div1&quot;，那么我就可以通过 123#div1&#123; border: 1px dashed red;&#125; 完整如下： 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;style type="text/css"&gt; #div1&#123; border: 1px dashed red; color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="div1"&gt;我是南邮吴镇宇！&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 但是精准控制每个id是不现实的，要累死人的，下面介绍标签选择器。 2.2 标签选择器 123456&lt;style type="text/css"&gt; div&#123; border: 1px dashed red; color: skyblue; &#125;&lt;/style&gt; 标签为div的都起作用了。这种方式也不好，因为范围太大了。 2.3 类选择器 形如： 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;style type="text/css"&gt; .div1&#123; border: 1px dashed red; color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="div1"&gt;我是南邮吴镇宇！&lt;/div&gt; &lt;div&gt;我是南邮吴彦祖！&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 这里的class可以写多个。这边可能会出现覆盖。但是注意类选择器的权重是小于ID选择器的，所以类选择器无法覆盖ID选择器的效果。 2.4 后代选择器 比如我这里： 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;style type="text/css"&gt; .div1&#123; border: 1px dashed red; color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="div1"&gt; 我是南邮吴镇宇！ &lt;/div&gt; &lt;div class="div1"&gt; 我是南邮吴彦祖！ &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 那么这两个div都会起作用，但是如果我只想让吴镇宇变化咋办呢？我们可以给他加个span标签。span是.div的儿子。也可以给这个span里面加一个class，写法一样的。这就是后代选择器。 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;style type="text/css"&gt; .div1 span&#123; border: 1px dashed red; color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="div1"&gt; &lt;span&gt; 我是南邮吴镇宇！ &lt;/span&gt; &lt;/div&gt; &lt;div class="div1"&gt; 我是南邮吴彦祖！ &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 比较简单，这些都是比较常用的选择器，当然还有一些比较特殊的选择器，到时候再说。 三、字体相关 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;style type="text/css"&gt; .div1&#123; font-size: 16px;/*12px在谷歌中是最小的字体*/ font-family: "宋体";/*字体样式，一般用默认即可*/ font-style: italic;/*默认是normal,italic是斜体*/ font-weight: 900;/*100-900的范围，默认是normal*/ text-align: left;/*center置为中间*/ text-indent: 2em;/*首行缩进*/ line-height: 50px;/*调整一行的行高*/ &#125; a&#123; text-decoration: none;/*去掉a标签的下划线*/ color: green;/*设定默认链接是绿色*/ &#125; /*鼠标悬浮在a标签上之后就会变成红色并且出现下划线*/ a:hover&#123; color: red; text-decoration: underline; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="div1"&gt; 我是南邮吴镇宇！ &lt;/div&gt; &lt;a href="#"&gt; 我是南邮吴彦祖！ &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 四、交集和并集 123456789&lt;body&gt; &lt;p&gt;我是1号&lt;/p&gt; &lt;p id="id"&gt;我是2号&lt;/p&gt; &lt;p class="para"&gt;我是3号&lt;/p&gt; &lt;p class="para"&gt;我是4号&lt;/p&gt; &lt;p&gt;我是5号&lt;/p&gt;&lt;/body&gt; 如果想选标签是p并且class=&quot;para&quot;的行，这就是交集： 12345&lt;style type="text/css"&gt; p.para&#123; color: red; &#125;&lt;/style&gt; 如果我选择class=&quot;para&quot;或者id=&quot;id&quot;的行，这就是并集： 12345&lt;style type="text/css"&gt; #id,.para&#123; color: red; &#125;&lt;/style&gt;]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题35-数组中的逆序对】]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9835-%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十五题。 题目描述 解题思路 这个之前的笔记中已经说过这道题目了，是归并排序的典型应用。归并排序的基本思想是分治，在治的过程中有前后数字的大小对比，此时就是统计逆序对的最佳时机。 我的答案 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Solution &#123; //统计逆序对的个数 int cnt; public int InversePairs(int [] array) &#123; if(array.length != 0)&#123; divide(array,0,array.length-1); &#125; return cnt; &#125; //归并排序的分治---分 private void divide(int[] arr,int start,int end)&#123; //递归的终止条件 if(start &gt;= end) return; //计算中间值，注意溢出 int mid = start + (end - start)/2; //递归分 divide(arr,start,mid); divide(arr,mid+1,end); //治 merge(arr,start,mid,end); &#125; private void merge(int[] arr,int start,int mid,int end)&#123; int[] temp = new int[end-start+1]; //存一下变量 int i=start,j=mid+1,k=0; //下面就开始两两进行比较，若前面的数大于后面的数，就构成逆序对 while(i&lt;=mid &amp;&amp; j&lt;=end)&#123; //若前面小于后面，直接存进去，并且移动前面数所在的数组的指针即可 if(arr[i] &lt;= arr[j])&#123; temp[k++] = arr[i++]; &#125;else&#123; temp[k++] = arr[j++]; //a[i]&gt;a[j]了，那么这一次，从a[i]开始到a[mid]必定都是大于这个a[j]的，因为此时分治的两边已经是各自有序了 cnt = (cnt+mid-i+1)%1000000007; &#125; &#125; //各自还有剩余的没比完，直接赋值即可 while(i&lt;=mid) temp[k++] = arr[i++]; while(j&lt;=end) temp[k++] = arr[j++]; //覆盖原数组 for (k = 0; k &lt; temp.length; k++) arr[start + k] = temp[k]; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题34-第一个只出现一次的字符位置】]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9834-%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6%E4%BD%8D%E7%BD%AE%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十四题。 题目描述 在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 解题思路 这种题目，比较容易想到的方法自然是用map来装一下，最后再遍历一遍哪个是个数为1. 12345678910111213141516171819202122232425262728import java.util.Map;import java.util.LinkedHashMap;public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; //存储(值，次数)，LinkedHashMap有顺序，默认按照输入顺序排列，符合题目要求 Map&lt;Character,Integer&gt; map = new LinkedHashMap&lt;&gt;(); //存储(值，位置) Map&lt;Character,Integer&gt; indexMap = new LinkedHashMap&lt;&gt;(); //遍历一遍进行统计 for(int i=0;i&lt;str.length();i++)&#123; if(!map.containsKey(str.charAt(i)))&#123; map.put(str.charAt(i),1); indexMap.put(str.charAt(i),i); &#125;else&#123; map.put(str.charAt(i),map.get(str.charAt(i))+1); &#125; &#125; //再遍历一遍找出第一个次数为1对应的位置 for(char c:map.keySet())&#123; int count = map.get(c); if(count == 1)&#123; int index = indexMap.get(c); return index; &#125; &#125; return -1; &#125;&#125; 但是呢，既然题目中确定都是字母，注意区分大小写，那么就是给我们确定了范围，那么我们就可以用计数的思想来实现了。两个方法时间复杂度都一样，随便哪个都行。 我的答案 A-Z对应的ASCII码为65-90，a-z对应的ASCII码值为97-122。为了方便起见，我们设定一个65-122这个范围，统一减去65就是0-57，那么我只要准备一个长度为58的数组即可。 12345678910111213141516171819public class Solution &#123; public int FirstNotRepeatingChar(String str) &#123; //长度为58的空数组，存储对应字符的出现次数 int[] arr = new int[58]; //将字符串转为字符数组 char[] strArr = str.toCharArray(); //遍历字符数组，给对应索引处加一 for(int i=0;i&lt;strArr.length;i++)&#123; arr[strArr[i]-'A']++; &#125; //找出第一个出现次数为1的字符的位置返回 for(int i=0;i&lt;strArr.length;i++)&#123; if(arr[strArr[i]-'A'] == 1)&#123; return i; &#125; &#125; return -1; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题33-丑数】]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9833-%E4%B8%91%E6%95%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十三题。 题目描述 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 解题思路 刷着刷着越发觉得智商不够用了。。。这个解题思路相当于准备三个队列。第一个队列里面放的都是2的倍数，第二个队列放的都是3的倍数，第三个队列放的都是5的倍数。依次拿最前面的数，找出最小的，对应的索引加一。这样，就可以将只包含因子2，3，5的数按照从小到大的顺序拿出来了。智商确实是硬伤，这谁顶得住？ 我的答案 123456789101112131415161718192021222324252627import java.util.ArrayList;public class Solution &#123; public int GetUglyNumber_Solution(int index) &#123; if(index &lt;= 0)&#123; return 0; &#125; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); int i2 = 0,i3 = 0,i5 = 0; while(list.size() &lt; index)&#123; int m2 = list.get(i2) * 2; int m3 = list.get(i3) * 3; int m5 = list.get(i5) * 5; int min = Math.min(m2,Math.min(m3,m5)); list.add(min); if(m2 == min) i2++; if(m3 == min) i3++; if(m5 == min) i5++; &#125; return list.get(index-1); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题32-把数组排成最小的数】]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9832-%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十二题。 题目描述 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 解题思路 比如题中给的例子：{3，32，321} 我们来看看，假如只有{3,32}那么3 &lt; 32，但是332 &gt; 323,所以需要将3和32调个位置才行。 也就是说，两两拼接比较一下，如果前者大于后者则交换，所以需要将数组按照这种规则进行重新的排序。 我的答案 123456789101112131415161718192021222324252627import java.util.*;public class Solution &#123; public String PrintMinNumber(int [] numbers) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for(int temp:numbers)&#123; list.add(temp); &#125; String s = ""; //这里拼接其中两个进行比较 //比如输入&#123;3，32，321&#125;，采用compare比较之后，变成了&#123;321,32,3&#125; //比较的根据是,比如3和32,332&gt;323的，那么返回1，则交换这两个位置的数字 Collections.sort(list, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; String s1 = o1+""+o2; String s2 = o2+""+o1; return s1.compareTo(s2); &#125; &#125;); //拼接结果返回 StringBuilder sb = new StringBuilder(); for(int i:list)&#123; sb.append(i); &#125; return sb.toString(); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题31-整数中1出现的次数】]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9831-%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十一题。 题目描述 求出1 ~ 13的整数中1出现的次数,并算出100 ~ 1300的整数中1出现的次数？为此他特别数了一下1 ~ 13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 解题思路 一种比较简单的实现方式是： 12345678910111213141516public class Solution &#123; public int NumberOf1Between1AndN_Solution(int n) &#123; int count = 0; while(n &gt; 0)&#123; int tmp = n; while(tmp &gt; 0)&#123; if(tmp % 10 == 1)&#123; count++; &#125; tmp /= 10; &#125; n--; &#125; return count; &#125;&#125; 从1到n遍历，每次通过对10求余数判断整数的个位数字是不是1，大于10的除以10之后再判断。我们对每个数字都要做除法和求余运算以求出该数字中1出现的次数。如果输入数字n，n有O(logn)位，我们需要判断每一位是不是1，那么时间复杂度为O(n*logn)。 还有一种方法可以实现O(logn)的时间复杂度。有点难。。。先记录在这里： 12345678public class Solution &#123; public int NumberOf1Between1AndN_Solution(int n) &#123; int ones = 0; for (long m = 1; m &lt;= n; m *= 10) ones += (n/m + 8) / 10 * m + (n/m % 10 == 1 ? n%m + 1 : 0); return ones; &#125;&#125; 主题思想大概如下： 设N = abcde ,其中abcde分别为十进制中各位上的数字。 如果要计算百位上1出现的次数，它要受到3方面的影响：百位上的数字，百位以下（低位）的数字，百位以上（高位）的数字。 ① 如果百位上数字为0，百位上可能出现1的次数由更高位决定。比如：12013，则可以知道百位出现1的情况可能是：100 ~ 199，1100 ~ 1199,2100 ~ 2199，，…，11100 ~ 11199，一共1200个。可以看出是由更高位数字（12）决定，并且等于更高位数字（12）乘以 当前位数（100）。 ② 如果百位上数字为1，百位上可能出现1的次数不仅受更高位影响还受低位影响。比如：12113，则可以知道百位受高位影响出现的情况是：100~199，1100 ~ 1199,2100 ~ 2199，，…，11100 ~ 11199，一共1200个。和上面情况一样，并且等于更高位数字（12）乘以 当前位数（100）。但同时它还受低位影响，百位出现1的情况是：12100 ~ 12113,一共114个，等于低位数字（113）+1。 ③ 如果百位上数字大于1（2~9），则百位上出现1的情况仅由更高位决定，比如12213，则百位出现1的情况是：100 ~ 199,1100 ~ 1199，2100 ~ 2199，…，11100 ~ 11199,12100 ~ 12199,一共有1300个，并且等于更高位数字+1（12+1）乘以当前位数（100）。 这篇文章值得一看。]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题30-连续子数组的最大和】]]></title>
    <url>%2F2019%2F03%2F09%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9830-%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三十题。 题目描述 HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 解题思路 这一题，我们可以用动态规划的思想来解决。我们不断往前试，试到最后。这里从前往后或者从后往前开始算都是一样的。我这里就从后往前开始算吧。 主要的一个公式是 max = Max(array[i] , array[i]+max) 基本思想就是：max代表后面已经扫描过的数组中的最优解，即最大连续字串的和。 此时max加上自己的值与自己比较，如果还不如自己，那么显然扫描过的数组可以抛弃。直接从自己开始再算即可。如果比自己大，那么就可以把自己也包含进这个数组中。 所以，核心思想是：看已经算过的数组的价值有没有给当前元素拖后腿，如果拖后腿就砍掉。 如图所示： 我的答案 容我插一嘴，自底向上考虑问题比较符合动态规划的思想，但是这里好像没什么区别。 12345678910111213141516171819public class Solution &#123; public int FindGreatestSumOfSubArray(int[] array) &#123; if(array.length == 0 || array == null)&#123; return 0; &#125; int index = array.length-1; int max = array[index]; int res = array[index]; for(int i=index-1;i&gt;=0;i--)&#123; max = Math.max(array[i],array[i]+max); res = Math.max(res,max); &#125; return res; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题29-最小的k个数】]]></title>
    <url>%2F2019%2F03%2F09%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9829-%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二九题。 题目描述 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。 解题思路 对于这种不变的数组，第一种思路是快速排序，然后找出前几个数即可，这种方法的时间复杂度为nlogn。 第二种更优的思路是堆排，因为找到前k个数字的时间复杂度为nlogk 我的答案 快速排序： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int [] input, int k) &#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(k &gt; input.length || k == 0)&#123; return res; &#125; //快排 quick_sort(input,0,input.length-1); for(int i=0;i&lt;k;i++)&#123; res.add(input[i]); &#125; return res; &#125; //只要low&lt;high就满足递归条件 private void quick_sort(int[] arr,int low,int high)&#123; if(low &lt; high)&#123; //三色国旗，每次partion之后实现将小于基准数和大于基准数的值想办法搞到两边去 //返回的数组是一个长度为2的数组，分别放等于基准数的起始坐标和终止坐标 int[] p = partion(arr,low,high); //对小于基准数的地方再次递归来搞成三色国旗 quick_sort(arr,low,p[0]-1); //对大于基准数的地方也再次递归搞成三色国旗 quick_sort(arr,p[1]+1,high); &#125; &#125; //三色国旗，尤其注意的是下标 private int[] partion(int[] arr,int low,int high)&#123; int less = low - 1; int more = high + 1; int curr = low; int num = arr[curr]; while(curr &lt; more)&#123; //小于基准值则跟++less交换，大于基准值则跟--more交换，相等则不管，继续前进 if(arr[curr] &lt; num)&#123; swap(arr,++less,curr++); &#125;else if(arr[curr] &gt; num)&#123; swap(arr,curr,--more); &#125;else&#123; curr++; &#125; &#125; return new int[]&#123;less,more&#125;; &#125; private void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125; 原生堆排来实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.util.ArrayList;public class Solution &#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int [] input, int k) &#123; //由于是找前k个数字，是比较小的，所以适合用小跟堆来解决 //因为大根堆先得到的是最大值，时间复杂度无法达到理想的nO(k) //整个过程是对数组进行操作的，但是与操作一颗二叉树是一样的，因为二叉堆是可以用数组来表示的 //数组的第一个元素就是二叉堆的root //我们要保证是最小堆，那么每次从root拿到的数必然是最小的数 //root提取出来之后，将root和最后一个数交换后需要重新调整堆维持堆的性质 if(k == 0 || k &gt; input.length)&#123; return res; &#125; heapSort(input,k); return res; &#125; private void heapSort(int[] arr,int k)&#123; if(arr == null || arr.length &lt; 2)&#123; return; &#125; //初步构建起一个最小堆，此时root是最小的一个数 for(int i=0;i&lt;arr.length;i++)&#123; heapInsert(arr,i); &#125; int heapSize = arr.length; swap(arr,0,--heapSize); //将最小的数此时也放进list中，如果k恰好为1那么直接返回 res.add(arr[heapSize]); if(res.size() == k)&#123; return; &#125; while(heapSize &gt; 0)&#123; //在对[0,heapSize]间构建最小堆，每一轮都找到最小值，然后交换到最后 heapify(arr,0,heapSize); swap(arr,0,--heapSize); //每次都将堆中最小的数拿到heapSize索引处，所以直接添加进结果集中，结果集大小为k了则立即结束 res.add(arr[heapSize]); if(res.size() == k)&#123; return; &#125; &#125; &#125; //初步构建最小堆，即构建完毕之后root为堆中最小值 private void heapInsert(int[] arr,int i)&#123; while(arr[i] &lt; arr[(i-1)/2])&#123; //如果比它的父亲小则与父亲交换 swap(arr,i,(i-1)/2); i = (i-1)/2; &#125; &#125; //上浮过程，每次将root和最后一个数字进行交换，然后重新构建最小堆 private void heapify(int[] arr,int index,int heapSize)&#123; int left = index * 2 + 1; while(left &lt; heapSize)&#123; //如果右子节点也没有越界的话，则从左右中挑出一个最小值 int largest = left+1 &lt; heapSize &amp;&amp; arr[left+1]&lt;arr[left] ? left+1 : left; //再与当前结点做比较 int maxIndex = arr[index] &lt; largest ? index : largest; //最小的就是index的话，则不用再比较了，已经是最小值了 if(maxIndex == index)&#123; break; &#125; //不是的话，则要进行交换 swap(arr,index,largest); index = maxIndex; left = index * 2 + 1; &#125; &#125; private void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题28-数组中出现次数朝超过一半的数字】]]></title>
    <url>%2F2019%2F03%2F09%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9828-%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E6%9C%9D%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十八题。 题目描述 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 解题思路 因为全是数字，那么我开一个长度为10的数组，遍历原数组，根据计数的排序的思想，将新数组对应的索引值加一。遍历过程中发现某一个大于一半了就停止。 我的答案 1234567891011121314151617public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; //由于全是数字，大小为10就足够了 int[] bucket = new int[10]; //遍历原数组，时间复杂度为O(n) for(int i=0;i&lt;array.length;i++)&#123; //将对应索引加1 bucket[array[i]]++; //找到了就停止 if(bucket[array[i]] &gt; array.length/2)&#123; return array[i]; &#125; &#125; //找不到 return 0; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题27-字符串的排列】]]></title>
    <url>%2F2019%2F03%2F09%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9827-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十七题。 题目描述 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 解题思路 这是一个全排列问题，可以用回溯法一一试探。 我的答案 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.util.ArrayList;import java.util.Collections;public class Solution &#123; public ArrayList&lt;String&gt; Permutation(String str) &#123; //存放结果 ArrayList&lt;String&gt; res = new ArrayList&lt;&gt;(); //判断空的情况 if(str == null)&#123; return res; &#125; //下面利用回溯法处理这个字符串的全排列所有情况 fun(str.toCharArray(),res,0); //按照字典排序 Collections.sort(res); //返回结果 return res; &#125; //回溯获取所有的组合情况 private void fun(char[] strChar,ArrayList&lt;String&gt; res,int i)&#123; //主要的思路是：首先固定住i索引处的数字，然后递归对后面的字符递归处理 //如果i已经到了字符数组的最后一位，那么说明这一次已经结束了，添加进结果集即可 if(i == strChar.length-1)&#123; //去重 if(!res.contains(new String(strChar)))&#123; res.add(new String(strChar)); &#125; return; &#125;else&#123; //可以举个例子比如就是"abc" //①第一波是：swap(arr,0,0)，此时第一个元素还是"a"，即"abc"，下面就是对a后面的元素进行全排列（递归） //②第1.1波是：swap(arr,1,1)，此时第一个元素"a"是固定的，第二个元素此时就是"b"，即"abc" //③第1.2波是：swap(arr,1,2)，此时第二个元素此时就是"c",即"acb" //④第二波是：swap(arr,0,1)，此时第一个元素是"b"，即"bac"，下面就是对b后面的元素进行全排列（递归） //⑤第2.1波是：swap(arr,1,1)，此时为"bac" //⑥第2.2波是：swap(arr,1,2)，此时为"bca" //⑦第三波是：swap(arr,0,2)，此时第一个元素是"c"，即"cba"，下面就是对c后面的元素进行全排列（递归） //⑧第3.1波是：swap(arr,2,1)，此时为"cba" //⑨第3.2波是：swap(arr,2,2)，此时为"cab" //注意，上面的顺序可能与实际不一样 //停止条件就是i达到了数组的最后一个数字。此时已经对所有的字符组合完毕了，符合条件的就全部装进结果集中 for(int j=i;j&lt;strChar.length;j++)&#123; swap(strChar,i,j); fun(strChar,res,i+1); swap(strChar,i,j); &#125; &#125; &#125; //交换数组的两个下标的元素 private void swap(char[] str, int i, int j) &#123; if (i != j) &#123; char t = str[i]; str[i] = str[j]; str[j] = t; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题26-二叉搜索树与双向链表】]]></title>
    <url>%2F2019%2F03%2F09%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9826-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十六题。 题目描述 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 解题思路 我们直到二叉搜索树的一个重要性质就是中序遍历是有序序列，那么本体要求转换为一个有序的双向链表，那么我们就可以通过中序遍历，将遍历到的结点以双向链表的形式串联起来。 我的答案 123456789101112131415161718192021222324252627public class Solution &#123; //定义两个指针，分别表示双向链表的头和尾 //只是指针，不是新创建结点，符合题意 TreeNode head = null; TreeNode tail = null; public TreeNode Convert(TreeNode pRootOfTree) &#123; //就是一个中序遍历，然后将中序遍历出来的结果以双向链表的形式串起来即可 if(pRootOfTree == null)&#123; return null; &#125; Convert(pRootOfTree.left); //串成双向链表 if(head == null)&#123; head = tail = pRootOfTree; &#125;else&#123; tail.right = pRootOfTree; pRootOfTree.left = tail; tail = pRootOfTree; &#125; Convert(pRootOfTree.right); return head; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题25-复杂链表的复制】]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9825-%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十五题。 题目描述 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 解题思路 图4.8 是一个含有5 个结点的复杂链表。图中实线箭头表示next 指针，虚线箭头表示随机 引用。为简单起见，指向null 的指针没有画出。 理解了上面，下面我们就根据这个过程来实现一下。 我的答案 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/*public class RandomListNode &#123; int label; RandomListNode next = null; RandomListNode random = null; RandomListNode(int label) &#123; this.label = label; &#125;&#125;*/public class Solution &#123; public RandomListNode Clone(RandomListNode pHead) &#123; //0.判断空的情况 if(pHead == null)&#123; return null; &#125; //1.复制结点 RandomListNode node = pHead; while(node != null)&#123; //保存下一个结点next--&gt;新建一个克隆结点--&gt;指定node.next到克隆结点 //--&gt;克隆结点的next指向next结点--&gt;更新node为next结点 RandomListNode next = node.next; RandomListNode cloneNode = new RandomListNode(node.label); node.next = cloneNode; cloneNode.next = next; node = next; &#125; //2.复制随机引用 node = pHead; while(node != null)&#123; if(node.random != null)&#123; node.next.random = node.random.next; &#125; node = node.next.next; &#125; //3.分离两个链表 node = pHead; //记录复制的链表的头结点 RandomListNode newHead = pHead.next; while(node != null)&#123; RandomListNode currNode = node.next; //更新原结点的next node.next = currNode.next; //更新克隆结点的next if(currNode.next != null)&#123; currNode.next = currNode.next.next; &#125; //更新原结点指针 node = node.next; &#125; return newHead; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题24-二叉树中和为某一值的路径】]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9824-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%92%8C%E4%B8%BA%E6%9F%90%E4%B8%80%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十四题。 题目描述 输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 解题思路 递归到叶子节点，每次递归都减掉当前节点的值，到最后剩下的值与叶子结点是否相等。 我的答案 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.ArrayList;/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) &#123; //保存多条路径，每条路径是值的集合 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; paths =new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if(root == null) return paths; find(paths,new ArrayList&lt;&gt;(),root,target); return paths; &#125; private void find(ArrayList&lt;ArrayList&lt;Integer&gt;&gt; paths, ArrayList&lt;Integer&gt; path, TreeNode root, int target)&#123; //将当前的root结点添加进去 path.add(root.val); //到叶子结点，符合条件的就添加进去 if(root.left==null &amp;&amp; root.right==null)&#123; if(target == root.val)&#123; paths.add(path); &#125; return; &#125; //这是相当于path的一个副本，是为了左右两个分支互不影响而新建的，但是值与path是一样的 ArrayList&lt;Integer&gt; path2 = new ArrayList&lt;&gt;(); path2.addAll(path); //左右分别递归 if(root.left != null) find(paths,path,root.left,target-root.val); if(root.right != null) find(paths,path2,root.right,target-root.val); &#125; &#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题23-二叉搜索树后序遍历序列】]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9823-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十三题。 题目描述 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 解题思路 首先要了解二叉搜索树的重要性质：根节点root的所有左子树的值都小于他，右子树的所有值都大于他。并且一个节点的值大于他的左孩子的值，小于右孩子的值。 比如这个树就满足二叉搜索树的性质： 1234567二叉搜索树示例： 20 / \ 15 25 / \ 10 18 那么它的后续遍历是 10 18 15 25 20 显然，数组的最后一个值是树的root，25是其右子树，10-15都是其左子树，那么前三个数都要比20小，这样才能满足条件。然后再以15为root，递归判断。 我的答案 1234567891011121314151617181920212223242526272829303132public class Solution &#123; public boolean VerifySquenceOfBST(int [] sequence) &#123; if(sequence.length == 0)&#123; return false; &#125; if(sequence.length == 1)&#123; return true; &#125; return judge(sequence,0,sequence.length-1); &#125; private boolean judge(int[] sequence,int start,int end)&#123; //递归的停止条件 if(start &gt;= end)&#123; return true; &#125; //从后往前找到第一个比end小的数 int i = end-1; while(i&gt;start &amp;&amp; sequence[i] &gt; sequence[end])&#123; i--; &#125; //0-end都应该是左子树，所以值必须都比root小，有一个大则false for(int j=0;j&lt;i;j++)&#123; if(sequence[j] &gt; sequence[end])&#123; return false; &#125; &#125; //对左右子树再做递归判断 return judge(sequence,0,i) &amp;&amp; judge(sequence,i,end-1); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题22-从上往下打印二叉树】]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9822-%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十二题。 题目描述 从上往下打印出二叉树的每个节点，同层节点从左至右打印。 解题思路 树的层序遍历，没啥好说的了。常规题目，面试常见。 我的答案 123456789101112131415161718192021222324public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if(root == null)&#123; return result; &#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty())&#123; TreeNode tmpNode = queue.remove(0); if(tmpNode.left != null)&#123; queue.add(tmpNode.left); &#125; if(tmpNode.right != null)&#123; queue.add(tmpNode.right); &#125; result.add(tmpNode.val); &#125; return result; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题21-栈的压入、弹出序列】]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9821-%E6%A0%88%E7%9A%84%E5%8E%8B%E5%85%A5%E3%80%81%E5%BC%B9%E5%87%BA%E5%BA%8F%E5%88%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十一题。 题目描述 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 解题思路 一开始都看不懂题目… 后来才好像明白是什么个意思… 假设有一串数字要将他们压栈: 1 2 3 4 5 如果这个栈是很大很大，那么一次性全部压进去，再出栈：5 4 3 2 1 但是，如果这个栈高度为4，会发生什么？ 1 2 3 4都顺利入栈，但是满了，那么要先出栈一个，才能入栈，那么就是先出4，然后压入5，随后再全部出栈：4 5 3 2 1 那么我总结了所有可能的出栈情况: 5 4 3 2 1//栈高度为5 4 5 3 2 1//栈高度为4 3 4 5 2 1//栈高度为3 2 3 4 5 1//栈高度为2 1 2 3 4 5//栈高度为1 借助一个辅助的栈，遍历压栈的顺序，依次放进辅助栈中。 对于每一个放进栈中的元素，栈顶元素都与出栈的popIndex对应位置的元素进行比较，是否相等，相等则popIndex++，再判断，直到为空或者不相等为止。 我的答案 1234567891011121314151617181920212223242526272829import java.util.ArrayList;import java.util.Stack;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; //数组为空的情况 if(pushA.length == 0 || popA.length == 0)&#123; return false; &#125; //弹出序列的下表索引 int popIndex = 0; //辅助栈 Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); for(int i=0;i&lt;pushA.length;i++)&#123; //不停地将pushA中的元素压入栈中，一旦栈顶元素与popA相等了，则开始出栈 //不相等则继续入栈 stack.push(pushA[i]); while(!stack.isEmpty() &amp;&amp; stack.peek()==popA[popIndex])&#123; stack.pop(); popIndex++; &#125; &#125; //栈中没有元素了说明元素全部一致，并且符合弹出顺序，那么返回true return stack.isEmpty(); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题20-包含min函数的栈】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9820-%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二十题。 题目描述 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 解题思路 思路：利用一个辅助栈来存放最小值 栈 3，4，2，5，1 辅助栈 3，3，2，2，1 每入栈一次，就与辅助栈顶比较大小，如果小就入栈，如果大就入栈当前的辅助栈顶； 当出栈时，辅助栈也要出栈 这种做法可以保证辅助栈顶一定都当前栈的最小值 我的答案 123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.util.Stack;public class Solution &#123; //存放元素 Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); //存放当前stack1中的最小元素 Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); //stack1直接塞，stack2要塞比栈顶小的元素，要不然就重新塞一下栈顶元素 public void push(int node) &#123; stack1.push(node); if(stack2.isEmpty() || stack2.peek() &gt; node)&#123; stack2.push(node); &#125;else&#123; stack2.push(stack2.peek()); &#125; &#125; //都要pop一下 public void pop() throws Exception&#123; if(stack1.isEmpty())&#123; throw new Exception("no element valid"); &#125; stack1.pop(); stack2.pop(); &#125; public int top()&#123; if(stack1.isEmpty())&#123; return 0; &#125; return stack1.peek(); &#125; public int min()&#123; if(stack2.isEmpty())&#123; return 0; &#125; return stack2.peek(); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题19-顺时针打印矩阵】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9819-%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十九题。 题目描述 题目描述 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 解题思路 旋转打印有点难，不过也是有办法的。我们可以把矩阵想象为一个魔方，我读完第一行之后，我就把魔方左转90度，再读取第一行，这样循环，直到最后一行结束。具体看代码中注释。 我的答案 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/**************************用旋转魔方的方式，一直取出第一行； 例如 1 2 3 4 5 6 7 8 9输出并删除第一行后，变为 4 5 6 7 8 9再进行一次逆时针旋转，就变成： 6 9 5 8 4 7继续重复上述操作即可。***************************/import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printMatrix(int [][] matrix) &#123; //作为存放结果的容器 ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); //拿到出事数组的行数 int row = matrix.length; while(row != 0)&#123; //将数组的第一行先添加进容器中 for(int i=0;i&lt;matrix[0].length;i++) list.add(matrix[0][i]); //当行数等于1时就没有必要再继续执行了，在上面打印完之后就可以停止了 if(row == 1) break; //删除上面遍历的数组的第一行，然后旋转这个数组并返回 matrix = revert(matrix); //更新行数 row = matrix.length; &#125; //返回 return list; &#125; private int[][] revert(int[][] matrix)&#123; //拿到matrix的行数和列数 int rows = matrix.length; int cols = matrix[0].length; //因为我们要将原数组遍历过的第一行删除，然后旋转变成一个新的数组，所以先初始化一下这个新数组 int[][] newMatrix = new int[cols][rows-1]; //对这个新数组进行赋值 for(int j=cols-1;j&gt;=0;j--)&#123; for(int i=1;i&lt;rows;i++)&#123; newMatrix[cols-j-1][i-1] = matrix[i][j]; &#125; &#125; //返回新数组 return newMatrix; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题18-二叉树的镜像】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9818-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十八题。 题目描述 解题思路 我们或许还记得递归的终极思想是数学归纳法，我们思考递归的时候一定不要去一步一步看它执行了啥，只会更绕。我们牢牢记住，思考的方式是我们首先假设子问题都已经完美处理，我只需要处理一下最终的问题即可，子问题的处理方式与最终那个处理方式一样，但是问题规模一定要以1的进制缩小。最后给一个递归出口条件即可。 对于本题，首先假设root的左右子树已经都处理好了，即左子树自身已经镜像了，右子树自身也镜像了，那么最后一步就是交换左右子树，问题解决。所以我只需要将root.left和root.right交换即可。下面进入递归，就是处理子问题。子问题的处理方式与root一样，只是要缩小问题规模，所以只需要考虑root.left是什么情况，root.right是什么情况即可。 我的答案 1234567891011121314151617181920212223242526public class Solution &#123; public void Mirror(TreeNode root) &#123; reverseTree(root); &#125; private void reverseTree(TreeNode root)&#123; //为空则结束 if(root == null)&#123; return; &#125; //假设root两边的子树自己都已经翻转成功了，那么只需要再将左右子树互换一下就成功了 //交换root的左右子树 swap(root); //左右子树翻转自己去处理就行了，我们规定每个子树的root都跟最终的root处理方式一样即可 reverseTree(root.left); reverseTree(root.right); &#125; private void swap(TreeNode root)&#123; TreeNode node = null; node = root.left; root.left = root.right; root.right = node; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题17-树的子结构】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9817-%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十七题。 题目描述 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 解题思路 针对树的结构，肯定是要用递归了，本题我觉得还是比较难的，需要注意很多东西，但是理解上比较轻松，直接看代码，理解了就好了。 我的答案 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public boolean HasSubtree(TreeNode root1,TreeNode root2) &#123; boolean res = false; //都不为空的话再去判断，因为空树不是任意一个树的子树 if(root1 != null &amp;&amp; root2 != null)&#123; //相等，则进入另外一个函数递归判断 if(root1.val == root2.val)&#123; res = doTree1ContainsTree2(root1,root2); &#125; //上面个不符合条件的话，则继续找下一个相等的结点，再去遍历 if(!res)&#123; res = HasSubtree(root1.left,root2); &#125; if(!res)&#123; res = HasSubtree(root1.right,root2); &#125; &#125; return res; &#125; private boolean doTree1ContainsTree2(TreeNode root1,TreeNode root2)&#123; //递归出口1：子树已经判断完毕了则结束，表明整个过程都为true，最终返回true if(root2 == null)&#123; return true; &#125; //递归出口2：子树还没空，父树先走到空了，那么肯定是不是子树结构的 if(root1 == null)&#123; return false; &#125; //递归出口3：两个数的结点值不相等的话停止 if(root1.val != root2.val)&#123; return false; &#125; //递归 return doTree1ContainsTree2(root1.left,root2.left) &amp;&amp; doTree1ContainsTree2(root1.right,root2.right); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题16-合并两个排序的链表】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9816-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十六题。 题目描述 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 解题思路 跟归并排序的并过程很类似。理解了归并排序，这个就很简单了。 我的答案 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode Merge(ListNode list1,ListNode list2) &#123; if(list1 == null)&#123; return list2; &#125; if(list2 == null)&#123; return list1; &#125; //newHead是为了表示头节点用的 //node是为了穿针引线，将两个链表连起来 ListNode newHead = null; ListNode node = null; while(list1 != null &amp;&amp; list2 != null)&#123; //两个指针不停比较，把较小的用node串起来，并且焦较小的指针往后移动即可 if(list1.val &gt; list2.val)&#123; if(node == null)&#123; newHead = list2; node = list2; list2 = list2.next; &#125;else&#123; node.next = list2; node = node.next; list2 = list2.next; &#125; &#125;else&#123; if(node == null)&#123; newHead = list1; node = list1; list1 = list1.next; &#125;else&#123; node.next = list1; node = node.next; list1 = list1.next; &#125; &#125; &#125; //走到这里，最多只有一个链表还没遍历完，下面补全就可以了 if(list1 != null)&#123; node.next = list1; &#125; if(list2 != null)&#123; node.next = list2; &#125; return newHead; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题15-反转链表】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9815-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十五题。 题目描述 输入一个链表，反转链表后，输出新链表的表头。 解题思路 一开始学的时候看的答案就是这个方法，显然是要比递归好的，但是如果不理解的话，光靠背很容易出错，并且也不大背的上，如今重温这道题，其实是很简单的，我们下面用图示来阐述。 主要的思想是用两个指针，其中newHead指向的是反转成功的链表的头部，currentHead指向的是还没有反转的链表的头部： 初始状态是newHead指向null，currentHead指向的是第一个元素，一直往后遍历直到newHead指向最后一个元素为止： 下面展示的是其中某个时间点的指向细节： 理解了上面的图示，程序就呼之欲出了。 我的答案 123456789101112131415161718public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; ListNode newHead = null; ListNode currentHead = head; if(head == null || head.next == null)&#123; return head; &#125; while(currentHead != null)&#123; ListNode next = currentHead.next; currentHead.next = newHead; newHead = currentHead; currentHead = next; &#125; return newHead; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题14-链表中倒数第k个结点】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9814-%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十四题。 题目描述 输入一个链表，输出该链表中倒数第k个结点。 解题思路 做过这个题目或者了解过，思路一下就有了。要求倒数第k个结点，而这个单向链表只能next，所以往前找是不行的。怎么办呢？我们可以先派一个指针走k个结点，然后另一个指针从头开始，两个指针同时后移，当前面个指针到最后一个结点的下一个结点即null的时候，后面个指针恰好指向的就是倒数第k个结点。 我的答案 答案稍微有点繁琐了，但是我觉得思路清晰是最重要的，代码再精简是需要在思路清晰的基础上再加以磨练才行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; if(head == null)&#123; return null; &#125; //计算出链表的长度，判断k是否超出 int count = 0; ListNode tmp = head; while(tmp != null)&#123; count++; tmp = tmp.next; &#125; if(k &gt; count)&#123; return null; &#125; //找到第k个结点 ListNode node = head; while(k &gt; 0)&#123; node = node.next; k--; &#125; //一个结点指向head，一个结点指向第k个，两者同时后移 while(node != null)&#123; head = head.next; node = node.next; &#125; //此时ehad就是倒数第k个结点 return head; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题13-调整数组顺序使奇数位于偶数前面】]]></title>
    <url>%2F2019%2F03%2F07%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9813-%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十三题。 题目描述 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 解题思路 这个题目最简单的思路自然是新开一个数组来存。 12345678910111213141516171819202122public class Solution &#123; public void reOrderArray(int [] array) &#123; int[] temp = new int[array.length]; int count = 0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i] % 2 == 1)&#123; count++; &#125; &#125; int e = 0; for(int i=0;i&lt;array.length;i++)&#123; if(array[i] % 2 == 1)&#123; temp[e++] = array[i]; &#125;else&#123; temp[count++] = array[i]; &#125; &#125; for(int i=0;i&lt;temp.length;i++)&#123; array[i] = temp[i]; &#125; &#125;&#125; 不过确实有点low，应该不是出题者本意。我们可以在原地进行处理，基本思想同插入排序。 首先说明一下，本题用快排的话会非常复杂，几乎是做不出来。因为他要保证偶数与偶数，奇数与奇数相对的顺序不变。 要想保证偶数与偶数，奇数与奇数相对顺序不变，那么就不能简单地交换，办法是整体移动。我首先找到第一个偶数，然后从这个偶数开始找第一个奇数。两者不能直接交换，因为交换的话，在前面的偶数被调到了后面，不符合题意。 从偶数位置开始，依次往后挪一格，然后将第一个奇数放到当前偶数的位置。依次循环，到最后i或者j越界了说明已经全部调整好了。 我的答案 123456789101112131415161718192021222324252627282930313233343536373839public class Solution &#123; public void reOrderArray(int [] array) &#123; //非法判断 if(array.length == 0 || array == null)&#123; return; &#125; int i = 0 , j; while(i &lt; array.length)&#123; //找到第一个偶数停止 while(i &lt; array.length &amp;&amp; !isEven(array[i]))&#123; i++; &#125; //从偶数位置开始，找第一个奇数 j = i + 1; while(j &lt; array.length &amp;&amp; isEven(array[j]))&#123; j++; &#125; //这个j越界说明找不到奇数了，则后面没有奇数就结束了 if(j &lt; array.length)&#123; //从偶数位置开始到奇数位置整体向后移动一格，然后把这个奇数放到偶数位置即可 int tmp = array[j]; for(int j2 = j-1;j2 &gt;= i;j2--)&#123; array[j2+1] = array[j2]; &#125; array[i++] = tmp; &#125;else&#123; break; &#125; &#125; &#125; //判断是否为偶数 private boolean isEven(int n)&#123; if(n%2 == 0) return true; return false; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题12-数值的整数次方】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9812-%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十二题。 题目描述 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 解题思路 把两个问题考虑清楚就可以了。第一个问题是判断指数是不是小于0，这关系到要不要取倒数；第二个问题是判断底数是不是负数，这个时候，如果指数是偶数，则为正数，如果指数为奇数则为负数。 我们将问题简化，将底数设置为正数，设好标识符最后统一处理，我们的首要目标是计算出次方数，最后再判断上面的两种情况，对结果进行调整。 我的答案 12345678910111213141516171819202122232425262728293031323334353637public class Solution &#123; public double Power(double base, int exponent) &#123; if(exponent == 0)&#123; return 1; &#125; //先把指数全部搞成正数，后面再处理 int e = exponent; if(exponent &lt; 0)&#123; e = -exponent; &#125; //看看是不是奇数,如果是奇数并且base为负数，那么最终还是负数，其余情况都是正数 boolean flag = true; if(base % 2 == 0)&#123; flag = false; &#125; //计算次方的结果 double res = 1.0; while(e &gt; 0)&#123; res *= base; e--; &#125; //如果指数小于0，则要取倒数 if(exponent &lt; 0)&#123; res = 1/res; //base为负数并且指数为基数的时候，要取反 if(flag)&#123; res = 0-res; &#125; &#125; return res; &#125;&#125; 注：貌似有更快的方法，暂时先不考虑了。]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题11-二进制中1的个数】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9811-%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十一题。 题目描述 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 解题思路 这个题目求一个数的二进制形式的1的个数，比较简单的思路是：每次与1进行&amp;操作，看是不是1，是则奇数，无论如何都后移一位再判断。 123456789101112public class Solution &#123; public int NumberOf1(int n) &#123; int count = 0; while(n != 0)&#123; if((n &amp; 1)==1)&#123; count++; &#125; n = n &gt;&gt;&gt; 1; &#125; return count; &#125;&#125; 但是，如果输入时负数会陷入死循环，因为负数右移时，在最高位补得是1。那么就死循环了。 比较好的解法是：如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。 循环是判断n是不是为0了，只要不为0，就循环。一进循环说明n是有至少一个1的，二话不说，先count++; 比如：一个二进制数1100，减去1之后是1011。 两者一&amp;则变为1000，原来最右边的1就变成了0. 由于n不为0，则再次进入循环： 1000减去1是0111，两者一&amp;就是0000.退出循环。 我的答案 12345678910public class Solution &#123; public int NumberOf1(int n) &#123; int count = 0; while(n != 0)&#123; count++; n = n &amp; (n-1); &#125; return count; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题10-矩形覆盖】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%9810-%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第十题。 题目描述 我们可以用 2*1 的小矩形横着或者竖着去覆盖更大的矩形。请问用n个 2*1 的小矩形无重叠地覆盖一个 2*n 的大矩形，总共有多少种方法？ 解题思路 我的答案 1234567891011121314public class Solution &#123; public int RectCover(int target) &#123; if(target &lt;= 0)&#123; return 0; &#125; if(target == 1)&#123; return 1; &#125; if(target == 2)&#123; return 2; &#125; return RectCover(target-1) + RectCover(target-2); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题9-变态跳台阶】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%989-%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第九题。 题目描述 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 解题思路 这就是上一题的延申，上一题要么跳一个要么跳两个，这一题是从1个到n个，那么同样地，找出f(n)与f(n-1)的关系就好了。 f(1) = 1：一个台阶只有一种跳法，就是跳一个 f(2) = f(2-1) + f(2-2)：两个台阶有两种跳法，跳一个或者跳两个 ①f(n) = f(n-1) + f(n-2) + f(n-3) + … + f(n-(n-1)) + f(n-n) = f(0) + f(1) + f(2) + … + f(n-2) + f(n-1)：n个台阶就有n个跳法 根据上一个式子，我们可以得出： ②f(n-1) = f(0) + f(1)+f(2)+f(3) + … + f((n-1)-1) = f(0) + f(1) + f(2) + f(3) + … + f(n-2)：n-1个台阶有n-1个跳法 根据①②可以得出： f(n) = f(n-1) + f(n-1) =2*f(n-1) 这个问题就解决了。 我的答案 123456789101112public class Solution &#123; public int JumpFloorII(int target) &#123; if(target &lt;= 0)&#123; return 0; &#125;else if(target == 1)&#123; return 1; &#125; else&#123; return 2*JumpFloorII(target-1); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题8-跳台阶】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%988-%E8%B7%B3%E5%8F%B0%E9%98%B6%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第八题。 题目描述 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 解题思路 对于本题,前提只有 一次 1阶或者2阶的跳法。 如果两种跳法，1阶或者2阶，那么假定第一次跳的是一阶，那么剩下的是n-1个台阶，跳法是f(n-1); 假定第一次跳的是2阶，那么剩下的是n-2个台阶，跳法是f(n-2) 可以得出总跳法为: f(n) = f(n-1) + f(n-2) 然后通过实际的情况可以得出：只有一阶的时候 f(1) = 1 ,只有两阶的时候可以有 f(2) = 2 我的答案 1234567891011public class Solution &#123; public int JumpFloor(int target) &#123; if(target == 1)&#123; return 1; &#125;else if(target == 2)&#123; return 2; &#125;else&#123; return JumpFloor(target-1) + JumpFloor(target-2); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题7-斐波那契数列】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%987-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第七题。 题目描述 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 解题思路 最简单的解法就是递归了： 1234567891011public class Solution &#123; public int Fibonacci(int n) &#123; if(n == 0)&#123; return 0; &#125;else if(n == 1 || n == 2)&#123; return 1; &#125;else&#123; return Fibonacci(n-1) + Fibonacci(n-2); &#125; &#125;&#125; 但是递归存在重复子问题，所以时间复杂度较高，但是这里竟然通过了？斐波那契数列我认为最优的一个解法是用两个变量存储。 这题目用动态规划来解其实是不好的，至少逆向思考问题本身就是比较问难的，我觉得这种记忆化搜索对于这种问题是最好理解并且解是比较优的解。 我的答案 12345678910111213141516171819public class Solution &#123; public int Fibonacci(int n) &#123; if(n == 0)&#123; return 0; &#125; if(n == 1 || n == 2)&#123; return 1; &#125; int a = 1; int b = 1; int c = 0; for(int i=2;i&lt;n;i++)&#123; c = a + b; a = b; b = c; &#125; return c; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题6-旋转数组的最小数字】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%986-%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第六题。 题目描述 一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 解题思路 当我们看到关键字“非减排序的数组&quot;的时候，我们要想到这题目的最优解可能是用二分查找法。 我们知道，二分查找法适用于有序数组的查找，这里对这个非减排序的数组进行了旋转，那么旋转之后这个数组的一个明显特征是：前面的一段序列必然大于等于后面的一段序列。 这题目可以认为是二分查找法的变种题目，下面我们来具体分析一下编程思路。 我们可以找一个基准数，比如数组的最优一个元素target，我们比较中间元素比如叫做arr[mid]和这个target的大小： 如果arr[mid]&gt;target:那么表明截取的时候，后面比前面长，那么最小值必然存在于索引mid的后面 如果arr[mid]&lt;target:那么表明截取的时候，前面比后面长，那么最小值必然存在于索引mid的前面(此时应该包含mid对应的元素，因为极限情况比如只有两个元素，那么反转之后前面可能比后面大，如果不包含这个mid就错了) 如果arr[mid]=target:这个时候元素组比如为[0,1,1,1,1]，那么旋转之后可能为 [1,0,1,1,1] 或者[1,1,1,0,1]，不好判断，一个一个试。 我的答案 12345678910111213141516171819202122232425import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; if(array.length == 0)&#123; return 0; &#125; if(array.length == 1)&#123; return array[0]; &#125; int start = 0; int end = array.length - 1; while(start &lt;= end)&#123; int mid = start + (end - start)/2; if(array[mid] &gt; array[end])&#123; start = mid + 1; &#125;else if(array[mid] &lt; array[end])&#123; end = mid; &#125;else&#123; end--; &#125; &#125; return array[start]; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题5-用两个栈实现队列】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%985-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第五题。 题目描述 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 解题思路 队列是先进先出，栈是先进后出，如何用两个栈来实现这种先进先出呢？ 其实很简单，我们假设用stack1专门来装元素，那么直接stack1.pop肯定是不行的，这个时候stack2就要发挥作用了。 我们的规则是：只要stack2中有元素就pop，如果stack2为空，则将stack1中所有元素倒进satck2中，就是说，新元素只进stack1，元素出来只从stack2出来。 这样子，就能保证每次从stack2中pop出来的元素就是最老的元素了。 我的答案 12345678910111213141516171819202122232425262728import java.util.Stack;public class Solution&#123; //负责装元素 Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); //负责出元素 Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; stack1.push(node); &#125; //主要思想是：stack2有元素就pop，没有元素就将stack1中所有元素倒进来再pop public int pop() throws Exception&#123; if(!stack2.isEmpty())&#123; int node = stack2.pop(); return node; &#125;else&#123; if(stack1.isEmpty())&#123; throw new Exception("no valid element"); &#125; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; return stack2.pop(); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题4-重建二叉树】]]></title>
    <url>%2F2019%2F03%2F06%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%984-%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第四题。 题目描述 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 解题思路 因为是树的结构，一般都是用递归来实现。 用数学归纳法的思想就是，假设最后一步，就是root的左右子树都已经重建好了，那么我只要考虑将root的左右子树安上去即可。 根据前序遍历的性质，第一个元素必然就是root，那么下面的工作就是如何确定root的左右子树的范围。 根据中序遍历的性质，root元素前面都是root的左子树，后面都是root的右子树。那么我们只要找到中序遍历中root的位置，就可以确定好左右子树的范围。 正如上面所说，只需要将确定的左右子树安到root上即可。递归要注意出口，假设最后只有一个元素了，那么就要返回。 我的答案 12345678910111213141516171819202122232425262728293031323334import java.util.Arrays;public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; //数组长度为0的时候要处理 if(pre.length == 0)&#123; return null; &#125; int rootVal = pre[0]; //数组长度仅为1的时候就要处理 if(pre.length == 1)&#123; return new TreeNode(rootVal); &#125; //我们先找到root所在的位置，确定好前序和中序中左子树和右子树序列的范围 TreeNode root = new TreeNode(rootVal); int rootIndex = 0; for(int i=0;i&lt;in.length;i++)&#123; if(rootVal == in[i])&#123; rootIndex = i; break; &#125; &#125; //递归，假设root的左右子树都已经构建完毕，那么只要将左右子树安到root左右即可 //这里注意Arrays.copyOfRange(int[],start,end)是[)的区间 root.left = reConstructBinaryTree(Arrays.copyOfRange(pre,1,rootIndex+1),Arrays.copyOfRange(in,0,rootIndex)); root.right = reConstructBinaryTree(Arrays.copyOfRange(pre,rootIndex+1,pre.length),Arrays.copyOfRange(in,rootIndex+1,in.length)); return root; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题3-从尾到头打印链表】]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%983-%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第三题。 题目描述 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 解题思路 这一题给一个单向链表，然后你反向打印出来。其实最容易想到的就是两个方案，一个是将这个链表进行反置，然后依次打印即可。一个就是通过栈这个数据结构，先进后出，那么也可以反向打印出来。由于用栈比较简单，但是链表的反置稍微难一点并且重要，所以本文用反置链表的方式解决。 我的答案 1234567891011121314151617181920212223242526import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); if(listNode == null)&#123; return res; &#125; ListNode pre = null; ListNode curr = listNode; while(curr != null)&#123; ListNode next = curr.next; curr.next = pre; pre = curr; curr = next; &#125; //pre就是反向链表的头结点 while(pre != null)&#123; res.add(pre.val); pre = pre.next; &#125; return res; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题2-替换空格】]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%982-%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第二题。 题目描述 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 解题思路 由于java给的参数是StringBuilder，这在一定程度上给我们一定的提示，我们利用StringBuilder的charAt(i)和setCharAt(i,xxx)这两个函数可以轻易实现获取索引处元素以及对这个索引处元素进行赋值。 由于是将空格替换成%20，空格本身就算一个地方，所以我需要将原来的字符串进行扩充。扩充的长度也非常好算，就是原来的长度+空格个数*2即可。然后遍历原字符串，不是空格的就一个个搬到新长度对应的位置上，否则就塞入%20三个字符。 我的答案 123456789101112131415161718192021222324252627282930public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; //1.获取空格数量 int count = 0; for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i) == ' ')&#123; count++; &#125; &#125; //2.拿到原来字符串的长度，计算新的字符串长度 //并且我是从尾巴开始往前遍历，所以我需要获取起始的两个索引值 int oldLength = str.length(); int oldIndex = oldLength - 1; int newLength = oldLength + 2*count; int newIndex = newLength - 1; str.setLength(newLength); //3.从后往前遍历原字符串，赋值给“新”的字符串，不过是原地进行的，没有开辟新的空间 for(;oldIndex &gt;= 0 &amp;&amp; oldIndex &lt; newIndex;oldIndex--)&#123; if(str.charAt(oldIndex) == ' ')&#123; str.setCharAt(newIndex--,'0'); str.setCharAt(newIndex--,'2'); str.setCharAt(newIndex--,'%'); &#125;else&#123; str.setCharAt(newIndex--,str.charAt(oldIndex)); &#125; &#125; return str.toString(); &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试题1-二维数组中的查找】]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%89%91%E6%8C%87offer%2F%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%981-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE%E3%80%91%2F</url>
    <content type="text"><![CDATA[剑指offer第一题。 题目描述 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 解题思路 考察数组的特性，我们如果从右上角的元素开始找，比如我们要找4，比9小，那么直接删除列即可，因为这个列上的元素看到都大于9；那么我们这时锁定8，发现还是比他小，再删除一列；这时变成了2，发现4比他大，则删除行；这个是后就锁定了4，就是我们想要的答案。 规律：首先选取数组中右上角的数字。如果该数字等于要查找的数字，查找过程结束； 如果该数字大于要查找的数字，剔除这个数字所在的列：如果该数字小于要查找的数字，剔除这个数字所在的行。 也就是说如果要查找的数字不在数组的右上角，则每－次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 我的答案 12345678910111213141516public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int i=0,j=array[0].length-1; while(i&lt;=array.length-1 &amp;&amp; j&gt;-1)&#123; if(array[i][j] == target)&#123; return true; &#125;else if(array[i][j] &gt; target)&#123; j--; &#125;else&#123; i++; &#125; &#125; return false; &#125;&#125;]]></content>
      <tags>
        <tag>剑指offer题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法14-二叉搜索树]]></title>
    <url>%2F2019%2F03%2F04%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%9514-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文比较轻松简单，就是动态生成一个二叉搜索而已。 动态构建二叉搜索树 首先明确二叉搜索树基本性质，就是根节点的值一定比左孩子的值要大，一定比右孩子的值要小。(为了简单起见，假定元素都是不重复的) 如何动态生成一颗二叉搜索树呢？思路其实是很简单的，就是判断与根节点的左孩子和右孩子分别比较大小，一直到末梢就可以插入元素了。 先准备一个TreeNode： 下面就是对节点进行操作了： 我们来进行前序遍历和中序遍历以及后序遍历看看生成的二叉搜索树是否正确： 运行结果为： 123前序遍历为： 6 3 2 1 5 8 7 中序遍历为： 1 2 3 5 6 7 8 后序遍历为： 1 2 5 3 7 8 6 经过验证发现完全是正确的。并且我们发现，中序遍历后是一个有序的序列，说明二叉搜索树中序遍历之后有序。 二叉树天生递归，关于它的很多题目用递归就可以解决，后序会有一个单独的文章专门来刷关于二叉树的题目。这里先不赘述了。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法13-动态规划入门]]></title>
    <url>%2F2019%2F03%2F04%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%9513-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[在求解某个序列的最优解的时候，经常会用到动态规划来解决。动态规划的核心是找出一个关系式，并且借助于选或者不选找出两个迭代的路径进行比较，最终遍历整个序列之后也就得到了选或者不选中的最优解，也就是整个序列的最优解。由于还是比较复杂的，所以是对动态规划学习的一个入门之作。 什么是动态规划 动态规划算法通常用于求解具有某种最优性质的问题。在这类问题中，可能会有许多可行解。每一个解都对应于一个值，我们希望找到具有最优值的解。动态规划算法与分治法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。我们可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其结果填入表中。这就是动态规划法的基本思路。引自百度百科 我们以解决一个小例子入手。 例子 如图所示，横轴是时间，上面是一段一段的长方形，将其想象为工作时间，比如第一块，是1点到4点的工作，收益为5，第二块是3点到5点的工作，收益是1.一个人不可能同时做两份工作，即时间不能重叠，请问这个人如何安排做哪几个工作最终收益是最大的？ 这道题目如何思考呢？正常的思维可能是：假设我做第一个工作，那么我做完之后只能做第四个工作才不冲突，做完第四个再做第8个工作，但是这个不一定是最优解，那么我还需要判断各种情况，比如我第一个工作做完了可能会去做第6个工作，好像很混乱，无从下手了。 各位，现在转换一下思维，我们不从第一个开始考虑，我们从最后一个开始考虑。怎么考虑呢？ 对于第八份工作而言，我有两个选择，一个是做，一个是不做。不做就很简单，我再从第七份工作开始考虑。做的话，那么我下面就要考虑前面可以做的最好的工作了，我们发现第五份工作以及之前的工作都可以做做，那么就是说对前五个工作找到一个收益最大的方案就即刻。我们用OPT(i)函数表示第i个参数之前最优解的话，那么对于上述可以表示为： 其实根据这个情况，我们递推出一个通用一点的关系式： 对于这个prev(i)表示在选择当前i元素之后，可以考虑的最优的子集的第一个元素（这个元素是反着找的）。比如选了8，那么我前面能考虑的只能是1，2，3，4，5这几块，因为6和7都不能选的。即使3或者4可能不满足条件，但是我们只要找到最接近的5即可。 我们会发现这种问题是存在重叠子问题的，这个就是问题的关键，一个问题分解为若干子问题，为了避免重复的计算，是需要将这些子问题存储起来的，下次用到的时候直接去内存中查出来即可。 这个问题根据上面的表达式就可以求出最优的一个情况。对于这个例子我们只需要理解它的解体思想即可，下面我们真正实践一个题目。 小偷偷东西 这个问题大概就是说，一个小偷偷东西，但是它不能偷相邻的两家人的东西，这样会被抓住，只能隔着偷，请问，怎么偷才能利益最大化，因为每个房子的价值不一样，假如有两家，一家有100块，一家有一个亿，你选择不好，选了100的那家，那就亏大发了，一个亿的下次再来偷吧。 按照上面的思路，我们来思考这个问题，我们从最后一家开始考虑，选择就两个，偷或者不偷，偷了会怎么样，不偷会怎么样，其实关系式跟上面是差不多的，我偷了的话，比如偷的是n，那么下次只能从n-2开始考虑它的最大价值方案。不偷的话，就是考虑n-1的最大价值方案。 最简单的实现就是递归来实现，主要是要考虑一下函数的出口条件，即偷到第一家了是什么样，以及第一家之前没有人家了怎么办。代码如下： 1234567891011121314151617class Solution &#123; public int rob(int[] nums) &#123; return rob(nums,nums.length-1); &#125; private int rob(int[] nums,int n)&#123; if(n &lt; 0)&#123; return 0; &#125;else if(n == 0)&#123; return nums[0]; &#125;else&#123; int A = rob(nums,n-2) + nums[n]; int B = rob(nums,n-1); return A &gt; B ? A : B; &#125; &#125;&#125; 很可惜，虽然逻辑上正确，但是由于是递归，在数据规模大一点之后，就不行了，因为我们前面也说到，存在大量的重叠子问题，问题多一个输入，那么计算的规模就要乘上2，所以时间复杂度为2^n这个级别。所以此方案放在leetcode上会超时，下面就是想办法把他改成非递归的版本。 前面提到，计算出来的子问题完全可以存在一个地方，下次要用直接取。OK，用非递归版本的数组版本实现是： 123456789101112131415161718192021class Solution &#123; public int rob(int[] nums) &#123; if(nums.length &lt;= 0)&#123; return 0; &#125; if(nums.length == 1)&#123; return nums[0]; &#125; int[] tmp = new int[nums.length]; tmp[0] = nums[0]; tmp[1] = nums[0] &gt; nums[1] ? nums[0] : nums[1]; for(int i=2;i&lt;nums.length;i++)&#123; int A = tmp[i-2] + nums[i]; int B = tmp[i-1]; int max = A &gt; B ? A : B; tmp[i] = max; &#125; return tmp[nums.length-1]; &#125;&#125; 主体还是上面提到的那个关系，只是我们将子问题的结果放在了一个临时数组中。临时数组中存放的都是子问题的最优解。比如tmp[1]里面存放的就是前两个元素中最大值，即最优解。当计算前三个数中最优解的时候，我只要做一个选择，即要不要选择nums[2]这个元素，选择的话，那么我就从tmp[2-2]中得到最优解，加起来就是当前最优解，不选择的话，就从tmp[2-1]中选择最优解。依次下去，tmp中最后一个值就是整个序列中组合的最优解了。 这里注意一下，这里其实是记忆化搜索的思想来实现的，我们可以注意到，其实是自顶向下来看的，从第一个数字来一直推到最后。然而动态规划的思想是从底向上的，参见第一个递归版本的实现。我们先考虑的是最终的n，而不是考虑从0开始。所以在设计思想上是有所区别的，但是又是非常类似，有的人将他们归位一类，我想，它们在大多数场景下可以互换的化，可以认为都是广义上的DP算法吧，因为DP毕竟只是一种思想，正过来实现反过来实现也未尝不可。 我们用一个数组来存放子问题的最优解，大大降低了时间复杂度，leetcode上也顺利通过。其实这个是一种记忆化搜索的思想，上面这个用了一个数组，其实完全没有必要用数组，用两个变量即可： 123456789101112131415161718192021class Solution &#123; public int rob(int[] nums) &#123; if(nums.length &lt;= 0)&#123; return 0; &#125; if(nums.length == 1)&#123; return nums[0]; &#125; int a = nums[0]; int b = nums[0] &gt; nums[1] ? nums[0] : nums[1]; for(int i=2;i&lt;nums.length;i++)&#123; int A = a + nums[i]; int B = b; int max = A &gt; B ? A : B; a = b; b = max; &#125; return b; &#125;&#125; 时间复杂度不变，空间上占用的更少了。下面我们还是从底向上的思想来解决一下这个问题吧。所以我们还是从最后一个元素开始思考，从后往前找，一直找到第一个结束。 123456789101112131415161718192021class Solution &#123; public int rob(int[] nums) &#123; int n = nums.length; if(n &lt;= 0)&#123; return 0; &#125; if(n == 1)&#123; return nums[0]; &#125; int[] tmp = new int[n]; tmp[n-1] = nums[n-1]; for(int i=n-2;i&gt;=0;i--)&#123; for(int j=i;j&lt;n;j++)&#123; tmp[i] = Math.max(tmp[i],nums[j] + (j+2&lt;n?tmp[j+2]:0)); &#125; &#125; return tmp[0]; &#125;&#125; 运行下来的效果是要比上面的记忆化搜索效果差的，但是都通过了。 ⭐墙裂推荐： 动态规划 (第1讲) 动态规划 (第2讲) 部分图片截取于他的视频。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML标签学习]]></title>
    <url>%2F2019%2F03%2F04%2Ffront%2FHTML%E6%A0%87%E7%AD%BE%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[学习前端的第一步是学习html的基本标签，比较重要的标签并不多，多用用就好了。下面一一过一下，复习一下即可。 一、文本标签 p标签 &lt;p&gt;xxxx&lt;/p&gt;对文本可以进行段落划分。 斜体 &lt;i&gt;xxx&lt;/i&gt;或者&lt;em&gt;xxx&lt;/em&gt; 删除线 &lt;s&gt;xxx&lt;/s&gt;或者&lt;del&gt;xxx&lt;/del&gt; 加粗 &lt;strong&gt;xxx&lt;/strong&gt;或者&lt;b&gt;xxx&lt;/b&gt; 下划线 &lt;u&gt;xxx&lt;/u&gt;或者&lt;ins&gt;xxx&lt;/ins&gt; 二、图片 没什么好说的：&lt;img src=&quot;xxx&quot; alt=&quot;图裂了&quot; title=&quot;本图片提示信息&quot;&gt; 三、超链接 原地跳转新页面：&lt;a href=&quot;http://www.google.com&quot; target=&quot;_self&quot;&gt;超链接&lt;/a&gt; 新开页面跳转：&lt;a href=&quot;http://www.google.com&quot; target=&quot;_blank&quot;&gt;超链接&lt;/a&gt; 这里顺便说一下锚点定位。就是点一下页面移动到指定的地方开始显示。 &lt;a href=&quot;#hello&quot;&gt;移动到hello处&lt;/a&gt; &lt;h1 id=&quot;hello&quot;&gt;我是hello&lt;/h1&gt; 点一下就移动到对应的地方了。 四、列表 无序列表： 12345&lt;ul&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt;&lt;/ul&gt; 有序列表： 12345&lt;ol&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt;&lt;/ol&gt; 所谓的顺序其实只是后者显示了123而已，并不是说对内容进行排序。 自定义列表 12345678910&lt;dl&gt; &lt;dt&gt;标题1&lt;/dt&gt; &lt;dd&gt;1&lt;/dd&gt; &lt;dd&gt;2&lt;/dd&gt; &lt;dd&gt;3&lt;/dd&gt; &lt;dt&gt;标题2&lt;/dt&gt; &lt;dd&gt;4&lt;/dd&gt; &lt;dd&gt;5&lt;/dd&gt; &lt;dd&gt;6&lt;/dd&gt;&lt;/dl&gt; ⭐在webstorm中，我们这样一个一个打有点累，其实是有快捷键的。比如我写一个有5个&lt;li&gt;的&lt;ul&gt;，快捷写法是ul&gt;li*5再按tab即可。 比如我要五组这样的呢？(ul&gt;li*5)*5+tab即可。 五、表格 这个也没什么好说的 123456789101112&lt;table border="1px"&gt; &lt;tr&gt; &lt;td&gt;111&lt;/td&gt; &lt;td&gt;222&lt;/td&gt; &lt;td&gt;333&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;444&lt;/td&gt; &lt;td&gt;555&lt;/td&gt; &lt;td&gt;666&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 也可以有表头用&lt;th&gt;即可。下面注意一下，如果我们想要111一下占两列咋办呢？就是跟excel中差不多的单元格合并功能。 占两列：&lt;td colspan=&quot;2&quot;&gt;111&lt;/td&gt; 占两行：&lt;td rowspan=&quot;2&quot;&gt;111&lt;/td&gt; 六、表单 输入框：&lt;input type=&quot;text&quot;&gt; 往往下面这个是成对出现的：效果是点击label的提示语鼠标的焦点就会进入输入框了。 12&lt;label for="text"&gt;请输入：&lt;/label&gt;&lt;input id="text" type="text"&gt; 按钮：&lt;input type=&quot;button&quot; value=&quot;我是按钮&quot;&gt; 勾选框：&lt;input type=&quot;checkbox&quot;&gt; 单选按钮：&lt;input type=&quot;radio&quot;&gt; 比如男女是一组radio，如何只让用户选择一个呢？ 12&lt;input name="sex" type="radio" value="male"&gt;&lt;input name="sex" type="radio" value="female"&gt; 文本输入框：&lt;textarea name=&quot;&quot; id=&quot;&quot; cols=&quot;30&quot; rows=&quot;10&quot;&gt;&lt;/textarea&gt; 下拉框： 12345&lt;select name="" id=""&gt; &lt;option value=""&gt;1&lt;/option&gt; &lt;option value=""&gt;2&lt;/option&gt; &lt;option value=""&gt;3&lt;/option&gt;&lt;/select&gt; 七、name属性 表单提交，比如登陆功能，我们需要将输入的用户名和密码发送到服务器校验。那么后端如何接收这个参数呢？其实就是根据name来识别的，后端受到的数据形如username:xxx，那么后端就可以根据username这个name属性接受到xxx这个内容。 id只是标识这个标签，要唯一。两者不要混淆。 123456&lt;form action="服务器接口地址"&gt; &lt;input type="text" name="username"&gt; &lt;input type="password" name="password"&gt; &lt;input type="submit"&gt; &lt;input type="reset"&gt;&lt;/form&gt; 好了，关键的标签就全部说完了。其实猛然回首才发现，这不正是我们初学html的时候一个一个学习的标签吗?真是时光荏苒呐！下面就要进军CSS了。]]></content>
      <tags>
        <tag>前端基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注解-属性赋值和自动装配]]></title>
    <url>%2F2019%2F03%2F03%2Fspring%2F%E6%B3%A8%E8%A7%A3-%E5%B1%9E%E6%80%A7%E8%B5%8B%E5%80%BC%E5%92%8C%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[对于属性的赋值以及如何自动装配是我们在实际编程中经常要实现的功能，spring为我们提供了应有尽有的方式来实现，特别地方便，用好spring还是先用好spring给我们提供的注解开始。理解它们的不同之处也是面试中常问的点。 1. @Value赋值 给一个Person： 12345678@Data@AllArgsConstructor@NoArgsConstructor@ToStringpublic class Person &#123; private String name; private Integer age;&#125; 注册到容器中： 1234567@Configurationpublic class MainConfig3 &#123; @Bean public Person person()&#123; return new Person(); &#125;&#125; 启动查看一下： 12345678@Testpublic void test03()&#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig3.class); System.out.println("容器已经启动成功..."); Person person = applicationContext.getBean(Person.class); System.out.println(person);//Person(name=null, age=null) applicationContext.close();&#125; 那么在以前写配置文件的时候，我们可以在xml中给属性注入值。 1234&lt;bean id="person" class="com.swg.bean.Person"&gt; &lt;property name="age" value="10"/&gt; &lt;property name="name" value="张三"/&gt;&lt;/bean&gt; 那么现在的注解可以实现这样的功能吗？ @Value 12@Value("hello")private String name; @SpEL表达式 12@Value("#&#123;20-6&#125;")private Integer age; 2. @PropertySource加载外部配置文件 ${}注入属性文件的值 新建一个配置文件db.properties，在里面写: person.nickname = hello 首先在配置类中声明要引入的配置文件： @PropertySource(value = “classpath:/db.properties”) 然后再引入： 12@Value("$&#123;person.nickname&#125;")private String nickname; 以前在用xml的时候，是这样引入这个值的： 12345&lt;context:property-placeholder location="db.properties"/&gt;//然后下面就引入这个属性文件的值&lt;property name="nickname" value="$&#123;person.nickname&#125;"/&gt; 自动装配：spring利用依赖注入DI，完成对IOC容器中各个组件依赖关系赋值 3. @Autowired &amp; @Qualifier &amp; @Primary @Autowired：默认是按照类型去容器中找相应的组件。找到就赋值。 applicationContext.getBean(Person.class) 如果找到多个相同类型的组件，再将属性名(默认是小写字母开头的id或者用@Bean(&quot;xxx&quot;)中的xxx为属性名)作为组件的id去容器中查找。 applicationContext.getBean(“person”) 所以，最好名字不一样。 如果有多个相同类型的组件，可以用@Qualifier(&quot;xxx&quot;)可以指定装配的id，而不是属性名。 @Primary：让spring自动装配的时候，默认使用首选的bean；但是如果@Qualifier明确指定了要装配哪一个，还是以@Qualifier为准。 默认，如果容器中没有这个组件，那么@Autowired就会报错。那么可不可以有就装配，没有就算了呢？ @Autowired（required=false） 4. @Resource &amp; @Inject 这两个都是java规范的注解。 @Resource–JSR250 默认是按照组件名称进行装配的。没有能支持@Primary和@Autowired（required=false）这个功能。 @Inject–JSR330 还需要一个依赖：javax.inject 和@Autowired差不多，但是比@Autowired稍弱，虽然支持@Primary，但是没有required=false @Autowired是spring定义的，后两者都是java的规范。 那么这些自动装配功能的注解是如何实现的呢？原来是AutowiredAnnotationBeanPostProcessor来实现的。 5. 方法、构造器位置的自动装配 @Autowired能标注的位置：构造器、参数、方法、属性 写在属性上面： 12@Autowiredprivate Car car; 写在方法上： 123456//spring容器创建当前对象，就会调用方法，完成赋值；//方法使用的参数，自定义类型的值从ioc容器中获取，就是这里的参数car。@Autowiredpubllic void setCar(Car car)&#123; this.car = car;&#125; 写在构造器上： 因为注册到ioc容器的组件，容器启动的时候回调用无参构造器创建对象，然后在进行初始化赋值等操作。 123456//放在有参构造器上面，这样容器启动的时候就会调用这个有参构造器//构造器中要用的组件car，也是从容器中获取@Autowiredpubllic Boss(Car car)&#123; this.car = car;&#125; 如果当前类只有一个有参构造器，@Autowired是可以省略的。 写在参数前面： 1234//car都是从容器中获取的publlic void setCar(@Autowired Car car)&#123; this.car = car;&#125; 6. Aware注入Spring底层组件及原理 自定义组件想使用spring容器底层的一些组件，比如applicationContext或者beanFactory等。只需要实现xxxAware接口即可。在创建对象的时候，会调用接口规定的方法注入相关组件。 比如ApplicationContextAware： 123public interface ApplicationContextAware extends Aware &#123; void setApplicationContext(ApplicationContext var1) throws BeansException;&#125; 那么我们可以将它传进来的ApplicationContext保存一下： 123456789public class Snail implements ApplicationContextAware&#123; private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125;&#125; 7. @Profile环境搭建和根据环境注册bean spring为我们提供的可用根据当前环境，动态地激活和切换一系列组件的功能。 开发环境、测试环境、生产环境，可能用不同的数据源，那么不想改动很多代码的话，就可以用@Profile切换。 @Profile：指定组件在哪个环境的情况下才能被注册到容器中，不指定，在任何环境下都能注册这个组件。 加了@Profile的@Bean，只有这个环境被激活的时候才能注册到容器中，但是有一个默认注册的：@Profile(“default”) 那么如何指定某个环境注册到spring容器中呢？ 第一种方式：使用命令行动态参数 VM arguments: -Dspring.profiles.active=test 第二种方式：代码 1234567//1,创建一个applicationContext//2,设置需要激活的环境applicationContext.getEnvironment().setActiveProfiles("test","dev");//3,注册配置类applicationContext.register(MainConfigOfProfile.class);//4,启动刷新容器applicationContext.refresh(); 8. 总结 @Value赋值 @PropertySource加载外部配置文件 @Autowired &amp; @Qualifier &amp; @Primary &amp; @Resource &amp; @Inject以及@Autowired在方法、构造器、参数位置的自动装配 通过实现Aware接口可以注入Spring底层的一些组件 @Profile环境搭建和根据环境注册bean]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注解-生命周期]]></title>
    <url>%2F2019%2F03%2F03%2Fspring%2F%E6%B3%A8%E8%A7%A3-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[我们在之前说到过bean的生命周期，其中提到了很多初始化方法，搞得我们晕头晕脑，本文就是来解决这个问题，对bean生命周期中重要的几个初始化和销毁接口或注解进行消息阐述，使得对bean的生命周期理解更加轻松。 1. @Bean指定初始化和销毁方法 bean生命周期：bean创建----初始化----销毁的过程 容器管理bean的生命周期，我们可以自定义初始化和销毁方法，容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法。 指定初始化和销毁方法 用xml配置的方式，可以指定init-method和destory-method； 那么注解如何做到自定义的初始化和销毁方法呢？ 我们先来创建一个Dog的类: 12345678910111213public class Dog &#123; public Dog()&#123; System.out.println("Dog constructor...."); &#125; public void init()&#123; System.out.println("Dog init..."); &#125; public void destory()&#123; System.out.println("Dog destory..."); &#125;&#125; 写一个配置类来注册这个Dog： 12345678@Configurationpublic class MainConfigOfLifeCycle &#123; @Bean public Dog dog()&#123; return new Dog(); &#125;&#125; 先来启动容器： 12345@Testpublic void test01()&#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfigOfLifeCycle.class); System.out.println("容器已经启动成功...");&#125; 那么打印结果是： 12Dog constructor....容器已经启动成功... 那如何指定我们自定义的初始化和销毁方法呢？ 首先修改一下测试方法，增加一句关闭容器： 123456@Testpublic void test01()&#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfigOfLifeCycle.class); System.out.println("容器已经启动成功..."); applicationContext.close();&#125; 然后在@Bean注解上指定初始化方法和销毁方法： @Bean(initMethod = “init”,destroyMethod = “destory”) 再次启动，显示： 1234Dog constructor....Dog init...容器已经启动成功...Dog destory... 但是注意单例和多例的区别，现在我将其配置成多例，由于多例是每次访问才会创建bean，所以我们还需要访问一下。 最后的打印结果是： 12345容器已经启动成功...Dog constructor....五月 28, 2018 3:49:33 下午 org.springframework.context.annotation.AnnotationConfigApplicationContext doClose信息: Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@16f65612: startup date [Mon May 28 15:49:32 CST 2018]; root of context hierarchyDog init... 说明在多例的情况下，容器最后不会销毁这个bean。 ⭐总结一下： 注解如何指定bean的初始化和销毁：@Bean注解后面指定init-method和destory-method 初始化：对象创建完成之后，并赋值好，在调用初始化方法 销毁方法：单例：容器关闭的时候销毁；多例：容器不会管理这个bean，容器不会调用销毁方法 2. InitializingBean和DisposableBean 除了上一种用@Bean的方式来指定bean的初始化和销毁之外，spring还提供了另外的方法来实现。 初始化： 让Bean实现InitializingBean接口并且实现它的afterPropertiesSet方法，他的作用时机是：当一个BeanFactory创建之后并且所有的属性值已经被设置完成之后，可以调用这个方法来进行初始化的工作。 123public interface InitializingBean &#123; void afterPropertiesSet() throws Exception;&#125; 销毁： 让Bean实现DisposableBean接口并且实现destroy方法，他的作用时机是BeanFactory销毁的时候也将单实例bean给销毁掉。 123public interface DisposableBean &#123; void destroy() throws Exception;&#125; 示例： 123456789101112131415public class Cat implements InitializingBean,DisposableBean&#123; public Cat()&#123; System.out.println("cat constructor..."); &#125; @Override public void destroy() throws Exception &#123; System.out.println("cat destory..."); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("cat afterPropertiesSet init..."); &#125;&#125; 最后打印一下，发现达到了一样的效果： 1234cat constructor...cat afterPropertiesSet init...容器已经启动成功...cat destory... 3. @PostConstruct&amp;@PreDestory @PostConstruct：bean创建好并且赋值好属性值之后执行一些初始化工作 @PreDestory：在容器销毁bean之前通知我们进行清理工作 123456789101112131415public class Pig&#123; public Pig()&#123; System.out.println("pig constructor..."); &#125; @PostConstruct public void init()&#123; System.out.println("pig init..."); &#125; @PreDestroy public void destory()&#123; System.out.println("pig destory..."); &#125;&#125; 一样的效果。 4. BeanPostProcessor-后置处理器 这是一个接口，bean的后置处理器，在bean初始化前后进行一些处理工作，有两个方法，一个是初始化之前处理，一个是初始化之后处理。 具体的执行时机： postProcessBeforeInitialization是在bean实例生成之后，在任何的初始化方法之前（比如InitializingBean接口的afterPropertiesSet方法；比如init-method方法） postProcessAfterInitialization与上面个完全相反，在任何的初始化方法完成之后再调用。 12345public interface BeanPostProcessor &#123; Object postProcessBeforeInitialization(Object var1, String var2) throws BeansException; Object postProcessAfterInitialization(Object var1, String var2) throws BeansException;&#125; 示例： 我这里将上面的Dog，Cat，Pig全部用起来。pig用到init-destory和destory-method方法；cat实现InitializingBean,DisposableBean这两个接口；pig是实现@PostConstruct和@PreDestory这两个接口。 再加上后置处理器： 12345678910111213141516171819202122@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor&#123; /** * * @param bean 还未初始化的bean对象 * @param beanName 这个bean对象在容器中的名字 * @return 返回我们要用的bean实例对象，可以直接返回传进来的bean，也可以包装一下再返回 * @throws BeansException */ @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("postProcessBeforeInitialization..."+beanName+"=&gt;"+bean); return bean;//返回null的话，下面个方法就不会再执行了 &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("postProcessAfterInitialization..."+beanName+"=&gt;"+bean); return bean; &#125;&#125; 一起启动，看看是什么先后顺序： 1234567891011121314151617181920212223242526//1.首先是Dog对象创建//2.然后是在任何的初始化方法之前执行postProcessBeforeInitialization//3.然后初始化init//4.init初始化之后执行postProcessAfterInitializationDog constructor....postProcessBeforeInitialization...dog=&gt;com.swg.bean.Dog@157632c9Dog init...postProcessAfterInitialization...dog=&gt;com.swg.bean.Dog@157632c9//同理cat constructor...postProcessBeforeInitialization...cat=&gt;com.swg.bean.Cat@64c87930cat afterPropertiesSet init...postProcessAfterInitialization...cat=&gt;com.swg.bean.Cat@64c87930//同理pig constructor...postProcessBeforeInitialization...pig=&gt;com.swg.bean.Pig@4de5031fpig init...postProcessAfterInitialization...pig=&gt;com.swg.bean.Pig@4de5031f容器已经启动成功...五月 28, 2018 4:29:27 下午 org.springframework.context.annotation.AnnotationConfigApplicationContext doClose信息: Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@16f65612: startup date [Mon May 28 16:29:26 CST 2018]; root of context hierarchypig destory...cat destory...Dog destory... ⭐⭐⭐其实顺序是这样的：Constructor &gt; @BeanPostProcessor前置处理 &gt; @PostConstruct &gt; InitializingBean &gt; init-method &gt; @BeanPostProcessor后置处理 5. BeanPostProcessor原理 首先是创建IOC容器： 1AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfigOfLifeCycle.class); 进去之后是构造器： 12345public AnnotationConfigApplicationContext(Class... annotatedClasses) &#123; this(); this.register(annotatedClasses); this.refresh();//刷新IOC容器&#125; 这个refresh方法中有finishBeanFactoryInitialization这个方法： 12//初始化剩下的所有非懒记载的单例beanthis.finishBeanFactoryInitialization(beanFactory); finishBeanFactoryInitialization这个方法中有一个方法是： 12//真正初始化剩下的所有非懒记载的单例beanbeanFactory.preInstantiateSingletons(); 下面就是获取bean，获取不到就创建对象。 上面已经完成了对象的创建。下面就是进行属性赋值和初始化工作。 123456781、赋值先执行populateBean方法，是对bean进行属性赋值2、初始化//遍历得到容器中所有的BeanPostProcessor：挨个执行beforeInitialization,一旦返回null，后置处理器就不会再执行applyBeanPostProcessorsBeforeInitialization//初始化之前处理invokeInitMethods：执行初始化方法applyBeanPostProcessorsAfterInitialization//初始化之后处理 6. BeanPostProcessor在spring底层的使用 aop最基本的原理就是通过动态代理（jdk，cglib）来构造出一个代理对象，在容器创建bean的时候替换原来的bean。 是谁来创建这个代理对象呢？AnnotationAwareAspectJAutoProxyCreator，这个玩意就是BeanPostProcessor的某个子类。 关于Spring AOP的具体实现，还是比较复杂的，具体的代码以后再去研究，这里给出一个大概的步骤： @EnableAspectJAutoProxy 会注册一个AnnotationAwareAspectJAutoProxyCreator AnnotationAwareAspectJAutoProxyCreator是一个InstantiationAwareBeanPostProcessor 创建流程 registerBeanPostProcessors() 注册后置处理器，创建 AnnotationAwareAspectJAutoProxyCreator finishBeanFactoryInitialization 初始化剩下的单实例Bean 创建Bean和切面 AnnotationAwareAspectJAutoProxyCreator拦截创建过程 创建完Bean判断是否需要增强。通过BeanPostProcessorsAfterInitialization， wrapIfNecessary() 包装代理对象 执行目标方法 获取拦截器链（advisor包装为Interceptor） 递归调用拦截器链 前置通知、目标方法、后置通知、返回通知、异常通知 总结 执行初始化和销毁方法 通过@Bean指定init-destory和destory-method方法 通过让Bean实现InitializingBean（定义初始化逻辑）,DisposableBean（定义销毁前的逻辑） 通过使用JSR250规范中的@PostConstruct和@PreDestory来进行初始化工作和销毁之前的工作 BeanPostProcessor：bean的后置处理器，在bean初始化前后进行一些处理工作]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注解--组件注册]]></title>
    <url>%2F2019%2F03%2F03%2Fspring%2F%E6%B3%A8%E8%A7%A3--%E7%BB%84%E4%BB%B6%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[所有的组件都应该放进IOC容器中，组件之间的关系通过容器实现自动装配，也就是依赖注入。对于如何将组件注册到容器中，本文从使用的角度出发详细阐述配置文件和注解的实现方式。涉及的注解还是挺多的，不过还是需要记忆一下，尤其是设置bean作用域的注解，面试中被问到过如何设置为多例。 新建一个maven工程，引入spring-context依赖。 1. @Configuration &amp; @Bean给容器注册组件 以往的方式注册一个bean 新建一个实体类Person： 12345678@Data@AllArgsConstructor@NoArgsConstructor@ToStringpublic class Person &#123; private String name; private Integer age;&#125; 那么，我们可以在beans.xml中注册这个bean，给他赋值。 12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="person" class="com.swg.bean.Person"&gt; &lt;property name="age" value="10"/&gt; &lt;property name="name" value="张三"/&gt; &lt;/bean&gt;&lt;/beans&gt; 那么，我们就可以拿到张三这个人了： 1234567public class MainTest &#123; public static void main(String[] args) &#123; ApplicationContext applicationContext = new ClassPathXmlApplicationContext("beans.xml"); Person person = (Person) applicationContext.getBean("person"); System.out.println(person); &#125;&#125;//输出：Person(name=张三, age=10) 注解的方式注册bean 配置类 = 配置文件 @Configuration 告诉spring这是一个配置类 @Bean 给容器注册一个Bean，类型为返回值类型，id默认是方法名 1234567@Configurationpublic class MainConfig &#123; @Bean public Person person()&#123; return new Person("李四",20); &#125;&#125; 如何获取这个bean呢？ 123ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig.class);Person person = applicationContext.getBean(Person.class);System.out.println(person);//Person(name=李四, age=20) 我们还可以根据类型来获取这个bean在容器中名字是什么： 1234String[] names = applicationContext.getBeanNamesForType(Person.class);for(String name:names)&#123; System.out.println(name);//person&#125; 上面提到，id默认是方法名。如果我们修改MainConfig中的person这个方法名，果然打印结果也随着这个方法名改变而改变；也可以自己另外指定这个bean在容器中的名字：@Bean(“hello”)，那么这个bean的名字就变成了hello. 2. @ComponentScan自动扫描组件以及扫描规则 配置文件中配置包扫描时这样配置的： 12&lt;!--包扫描，只要标注了@Controller，@Service，@Repository，@Component，就会被自动扫描到加入到容器中--&gt;&lt;context:component-scan base-package="com.swg"/&gt; 现在用注解来实现这个功能： 只需要加上注解即可： 12@ComponentScan(value = "com.swg")//java8可以写多个@ComponentScan//java8以前虽然不能写多个，但是也可以实现这个功能，用@ComponentScans配置即可 我们增加BookController.java,BookService.java以及BookDao.java三个类，并且分别加上注解：@Controller，@Service，@Repository；那么包扫描就可以把这些类全部注册到IOC容器中了。 我们来打印一下目前所有注册到IOC容器的类的名称： 123456789@Testpublic void shouldAnswerWithTrue()&#123; ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig.class); String[] names = applicationContext.getBeanDefinitionNames(); for(String name:names)&#123; System.out.println(name); &#125;&#125; 输出结果： 123456789101112org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalRequiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactory//以上是spring IOC容器自身需要的组件，下面是我们自定义的组件mainConfig//主配置类，因为有注解@Configuration，而这个注解本身是有@Component的，所以也是一个beanbookController//@ControllerbookDao//@RepositorybookService//@Serviceperson//这是由自己@Bean注册进去的 上面的扫描路径是扫描所有的，有的时候我们需要排除掉一些扫描路径或者只扫描某个路径，如何做到呢？ 用excludeFilters来排除，里面可以指定排除规则，这里是按照ANNOTATION来排除，排除掉所有@Controller注解的类。classes也是个数组，可以排除很多。 123@ComponentScan(value = "com.swg",excludeFilters = &#123; @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)&#125;) 那么效果就是controller没有了，但是service和dao都在。 那如果我想只包含controller呢？ 123@ComponentScan(value = "com.swg", includeFilters = &#123; @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)&#125;,useDefaultFilters = false) 注意要useDefaultFilters = false，因为默认为true，就是扫描所有，不设置为false无效。 3. 自定义TypeFilter制定过滤规则 上面包扫描是按照FilterType.ANNOTATION规则来实现的，他还有其他几种规则： 12345678910public enum FilterType &#123; ANNOTATION,//注解，最常用 ASSIGNABLE_TYPE,//按照给定的类型，比如指定是BookService.class，那么只要是BookService这个类型就会被规则配置进来，子类或者实现类都可以 ASPECTJ,//ASPECTJ表达式，不常用 REGEX,//正则 CUSTOM;//自定义规则 private FilterType() &#123; &#125;&#125; 对于最后的CUSTOM，这里着重说一说怎么用。 首先是要求实现FilterType接口： 1234567891011121314151617181920212223public class MyTypeFilter implements TypeFilter&#123; /** * * @param metadataReader:读取到的当前正在扫描的类的信息 * @param metadataReaderFactory：可以获取到其他任何类信息 * @return * @throws IOException */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; //获取当前类注解信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); //获取当前正在扫描的类的类信息,可以获取子类，父类，接口等信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); //获取当前类资源（类的路径） Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println("----&gt;"+className); return false; &#125;&#125; 返回false，表示不匹配，返回true的就匹配。这里默认是false； 在mainConfig类中配置这个自定义的过滤规则： 123@ComponentScan(value = "com.swg", includeFilters = &#123;@ComponentScan.Filter(type = FilterType.CUSTOM,classes = &#123;MyTypeFilter.class&#125;)&#125;,useDefaultFilters = false) 那么此时输出： 12345678910111213141516----&gt;com.swg.AppTest----&gt;com.swg.bean.Person----&gt;com.swg.controller.BookController----&gt;com.swg.dao.BookDao----&gt;com.swg.FilterType.MyTypeFilter----&gt;com.swg.MainTest----&gt;com.swg.service.BookServiceorg.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalRequiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorymainConfigperson 就是显示了所有他处理的类，最后由于都返回fasle，那么那些controller，service都将被过滤掉。 下面指定通过一个： 123if(className.contains("er"))&#123; return true;&#125; 输出： 12345mainConfigpersonbookController//newmyTypeFilter//newbookService//new 4. @Scope-设置组件作用域 spring的bean默认是单实例，下面佐证一下： 123456789101112@Testpublic void test02()&#123; ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig2.class); String[] names = applicationContext.getBeanDefinitionNames(); for(String name:names)&#123; System.out.println(name); &#125; Object bean = applicationContext.getBean("person"); Object bean1 = applicationContext.getBean("person"); System.out.println(bean == bean1);//true&#125; 那么我们可以配置bean为多例吗？显然是可以的： 12345@Bean@Scope("prototype")public Person person()&#123; return new Person("李四",20);&#125; @Scope注解中有四个选项： prototype:多例 singleton:单例，默认 request:同一次请求创建一个实例 session:同一个session创建一个实例 着重看一下singleton和prototype，他们的加载时机？ ⭐⭐⭐singleton：IOC容器启动时调用方法创建对象放到IOC容器中，以后每次获取都直接从容器中拿，类似于map.get(); ⭐⭐⭐prototype:IOC容器启动时不会创建对象，而是在每次获取时才会调用方法创建对象；并且是新new出来的对象，都是不一样的。 5. @lazy-bean-懒加载 单实例bean，默认在容器启动时创建对象。 即只要执行了： 1ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig2.class); Person这个对象就会加载在容器中。测试一下： 12345678@Configurationpublic class MainConfig2 &#123; @Bean public Person person()&#123; System.out.println("创建对象Person");//容器启动的时候就会执行这个方法，创建Perosn对象 return new Person("李四",20); &#125;&#125; 懒加载：容器启动时不创建对象，第一次使用(获取)Bean创建对象。 123456789@Configurationpublic class MainConfig2 &#123; @Bean @Lazy public Person person()&#123; System.out.println("创建对象Person"); return new Person("李四",20); &#125;&#125; 这个时候，就不会在容器一启动的时候就加载了。那什么时候加载呢？ 我获取一下这个对象： 12ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig2.class);Object bean = applicationContext.getBean("person"); 这个时候，@Bean就被创建了。这就是懒加载。 6. @Conditional-按照条件注册bean 按照一定的条件进行判断，满足条件给容器注册bean。 先创建三个bean： 1234567891011121314151617@Configurationpublic class MainConfig2 &#123; @Bean public Person person()&#123; return new Person("李四",20); &#125; @Bean("bill") public Person person01()&#123; return new Person("Bill",60); &#125; @Bean("linus") public Person person02()&#123; return new Person("linus",50); &#125;&#125; 打印一下创建的bean： 123456789101112@Testpublic void test03()&#123; ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig2.class); String[] names = applicationContext.getBeanNamesForType(Person.class); for(String name:names)&#123; System.out.println("---&gt;"+name); &#125; Map&lt;String,Person&gt; types = applicationContext.getBeansOfType(Person.class); System.out.println(types);&#125; 打印结果： 1234---&gt;person---&gt;bill---&gt;linus&#123;person=Person(name=李四, age=20), bill=Person(name=Bill, age=60), linus=Person(name=linus, age=50)&#125; 那假设一个场景：如果系统是windows,给容器注册“bill”；如果系统是linux,给容器注册“linus”； 至于获取操作系统是什么，我们可以： 123ConfigurableEnvironment environment = (ConfigurableEnvironment) applicationContext.getEnvironment();String osName = environment.getProperty("os.name");System.out.println(osName);//wondows 7 那么我们如何根据条件来注册bean呢？ 123456789101112131415161718192021//判断是否是linux系统public class LinuxCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) &#123; //1.获取IOC使用的bean factory，这个factory就是创建对象并进行装配的工厂 ConfigurableListableBeanFactory factory = conditionContext.getBeanFactory(); //2.获取类加载器 ClassLoader classLoader = conditionContext.getClassLoader(); //3.获取当前环境信息 Environment environment = conditionContext.getEnvironment(); //4.获取到bean定义的注册类 BeanDefinitionRegistry registry = conditionContext.getRegistry();//如果是liunx系统，就让其注册进容器，windows也是如此 String osName = environment.getProperty("os.name"); if(osName.contains("Linux"))&#123; return true; &#125; return false; &#125;&#125; 12345678910111213141516171819@Configurationpublic class MainConfig2 &#123; @Bean public Person person()&#123; return new Person("李四",20); &#125; @Conditional(&#123;WindowsCondition.class&#125;)//传condition数组 @Bean("bill") public Person person01()&#123; return new Person("Bill",60); &#125; @Conditional(LinuxCondition.class) @Bean("linus") public Person person02()&#123; return new Person("linus",50); &#125;&#125; 那么运行结果可以预测到：由于我们是windows系统，所以linux的就不能注册进容器了。 1234Windows 7---&gt;person---&gt;bill&#123;person=Person(name=李四, age=20), bill=Person(name=Bill, age=60)&#125; 7. @Import-给容器中快速导入一个组件 上面所说得给容器注册组件的方式是： 包扫描+组件标注注解：@Controller，@Service，@Repository， @Component 比较方便，但是有局限性：如果是注册第三方包怎么办呢？ 有一种是：@Bean[导入第三方包里面的组件],对于简单的可用这样用 还有一种是：@Import，快速给容器导入一个组件 比如我随便新建一个类叫Dog，里面啥注解和内容都不写。默认他是不会导入进去的。但是我在webconfig类上增加注解： @Import(Dog.class) 那么再次打印出所有注册进容器的组件时，会出现 com.swg.bean.Dog 可见，@import注解可以方便快速地导入一个组件，并且id默认是组件的全类名 那如何导入多个呢？ @Import({Dog.class, Cat.class}) 8. @Import-使用ImportSelector 123456789101112131415/** * 自定义逻辑返回需要导入的组件 */public class MyImportSelector implements ImportSelector&#123; /** * * @param annotationMetadata 当前标注Import注解的类的所有注解信息 * @return 返回值就是导入到容器中的组件的全类名 */ @Override public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; return new String[]&#123;"com.swg.bean.Dog","com.swg.bean.Cat","com.swg.bean.pig"&#125;; &#125;&#125; 然后打上注解导入进来即可： @Import(MyImportSelector.class) 这里导入的实际上不是MyImportSelector.class这个类，而是他返回的组件全类名 9. @Import-使用ImportBeanDefinitionRegistrar 1234567891011121314151617181920212223public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /** * * @param annotationMetadata 当前类的注解信息 * @param registry beanDefinition注册类； * 把所有需要添加进容器的bean：调用BeanDefinitionRegistry.registerBeanDefinition * 来手工注册进来 * */ @Override public void registerBeanDefinitions(AnnotationMetadata annotationMetadata, BeanDefinitionRegistry registry) &#123; //判断这两个bean是否都已经存在于容器中 boolean definition = registry.containsBeanDefinition("com.swg.bean.Pig"); boolean definition2 = registry.containsBeanDefinition("com.swg.bean.Cat"); //如果两个bean都有，则注册一头牛 if(definition &amp;&amp; definition2)&#123; //指定bean定义信息 RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(Bull.class); //注册一个bean，指定bean的名字 registry.registerBeanDefinition("bull",rootBeanDefinition); &#125; &#125;&#125; 然后打上注解导入进来即可： @Import(MyImportBeanDefinitionRegistrar.class) 10. 使用FactoryBean注册组件 1234567891011121314151617181920212223242526public class AnimalFactory implements FactoryBean&#123; /** * @return 返回一个Pig对象，这个对象会添加到容器中 * @throws Exception */ @Override public Object getObject() throws Exception &#123; System.out.println("AnimalFactory...getObject()..."); return new Pig(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Pig.class; &#125; /** * 是单例吗？返回true表示是单例，在容器中保存一份 * false：表示是多例，每次获取都创建新的，每次调用getObject()这个方法 * @return */ @Override public boolean isSingleton() &#123; return true; &#125;&#125; 我们先将它用@Bean添加进容器看看： 1234@Beanpublic AnimalFactory animalFactory()&#123; return new AnimalFactory();&#125; 显示的id是animalFactory，我们根据这个id获取一下这个bean的类型： 12Object bean = applicationContext.getBean("animalFactory");System.out.println("bean的类型："+bean.getClass()); 结果显示： 12AnimalFactory...getObject()...bean的类型：class com.swg.bean.Pig 就是说，这个bean的类型就是getObject方法中返回的Pig对象。 那如果我们想获取这个工厂对象呢？也是可以的，id前面加上&amp;即可。 12Object bean = applicationContext.getBean("&amp;animalFactory");System.out.println("bean的类型："+bean.getClass());//bean的类型：class com.swg.bean.AnimalFactory 原因是在BeanFactory中定义了一个前缀&amp;，只要是以&amp;为前缀，表示拿FactoryBean本身。 12public interface BeanFactory &#123; String FACTORY_BEAN_PREFIX = "&amp;"; 11. 总结 给容器中注册组件： 包扫描+组件标注注解：@Controller，@Service，@Repository，@Component @Bean[导入第三方包里面的组件] @Import，快速给容器导入一个组件—重要 1).@Import(要导入到容器中的组件)；容器会自动注册这个组件，id默认是全类名 2).@ImportSelector：返回要导入的组件的全类名数组 3).@ImportBeanDefinitionRegistrar：手动注册bean到容器中 使用spring提供的@FactoryBean（工厂bean）来注册bean @Conditional按照条件注册bean—重要 @Scope作用域 懒加载，单例和多例是不一样的]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Bean]]></title>
    <url>%2F2019%2F03%2F02%2Fspring%2FSpring%20Bean%2F</url>
    <content type="text"><![CDATA[一提到Spring的IOC，那么里面的Bean基本就会被问到，我们也知道，Spring的任务就是对这些bean进行管理和装配，所以bean就是spring IOC处理的对象，如此关键的对象，我们需要了解它核心的两点：作用域和生命周期。 一、Spring Bean的作用域 singleton：Spring默认作用域，容器里拥有唯一的Bean实例 prototype：针对每个getBean请求，容器都会创建一个bean实例 request：会为每个HTTP请求创建一个Bean实例 session：会为每个session创建一个Bean实例 globalSession：会为每个全局Http Session创建一个Bean实例，该作用域仅对Portlet有效 对于这个问题面试中我也被问过，即spring中bean默认作用域，如何设置为多例。这个问题我在注解–组件注册这篇文章中会详细谈论。 二、Bean的生命周期 beanDefinition（容器启动阶段）只完成bean的定义，并未完成初始化。初始是通过beanFactory的getBean()时才进行的。 对于普通的Java对象，当new的时候创建对象，当它没有任何引用的时候被垃圾回收机制回收。而由Spring IoC容器托管的对象，它们的生命周期完全由容器控制。Spring中每个Bean的生命周期如下： 2.1 实例化Bean 对于BeanFactory容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。 容器通过获取BeanDefinition对象中的信息进行实例化。并且这一步仅仅是简单的实例化，并未进行依赖注入。 实例化对象被包装在BeanWrapper对象中，BeanWrapper提供了设置对象属性的接口，从而避免了使用反射机制设置属性。 2.2 设置对象属性（依赖注入） 实例化后的对象被封装在BeanWrapper对象中，并且此时对象仍然是一个原生的状态，并没有进行依赖注入。 紧接着，Spring根据BeanDefinition中的信息进行依赖注入。 并且通过BeanWrapper提供的设置属性的接口完成依赖注入。 2.3 注入Aware接口 紧接着，Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给bean。 2.4 BeanPostProcessor 当经过上述几个步骤后，bean对象已经被正确构造，但如果你想要对象被使用前再进行一些自定义的处理，就可以通过BeanPostProcessor接口实现。 该接口提供了两个函数： postProcessBeforeInitialzation( Object bean, String beanName ) 当前正在初始化的bean对象会被传递进来，我们就可以对这个bean作任何处理。 这个函数会先于InitialzationBean执行，因此称为前置处理。 所有Aware接口的注入就是在这一步完成的。 postProcessAfterInitialzation( Object bean, String beanName ) 当前正在初始化的bean对象会被传递进来，我们就可以对这个bean作任何处理。 这个函数会在InitialzationBean完成后执行，因此称为后置处理。 2.5 InitializingBean与init-method 当BeanPostProcessor的前置处理完成后就会进入本阶段。 InitializingBean接口只有一个函数： afterPropertiesSet() 这一阶段也可以在bean正式构造完成前增加我们自定义的逻辑，但它与前置处理不同，由于该函数并不会把当前bean对象传进来，因此在这一步没办法处理对象本身，只能增加一些额外的逻辑。 若要使用它，我们需要让bean实现该接口，并把要增加的逻辑写在该函数中。然后Spring会在前置处理完成后检测当前bean是否实现了该接口，并执行afterPropertiesSet函数。 2.6 DisposableBean和destroy-method 和init-method一样，通过给destroy-method指定函数，就可以在bean销毁前执行指定的逻辑。 对于上面的过程只能理解并且记忆，还是很容易被问到的，是spring的一个高频考点。或许你对这些所说的方法一脸懵逼，对于生命周期这一块，我对里面涉及的所有初始化以及销毁方法进行了汇总，详情见文章:注解-生命周期]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AOP基本使用和原理]]></title>
    <url>%2F2019%2F03%2F02%2Fspring%2FAOP%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%92%8C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[除了IOC之外，spring核心的东西就是AOP。主要的目标是实现关注业务逻辑，解耦非业务逻辑，比如比较典型的日志处理。将日志的处理划分出来，在运行时动态地添加到要拦截的接口方法上，对这个方法的执行前后以及发生异常时实现日志的监控。这种动态的功能是非常重要的功能，本文来介绍一下AOP最基本的使用。 一、什么是aop AOP（Aspect Oriented Programming），即面向切面编程（也叫面向方面编程，面向方法编程）。其主要作用是，在不修改源代码的情况下给某个或者一组操作添加额外的功能。像日志记录，事务处理，权限控制等功能，都可以用AOP来“优雅”地实现，使这些额外功能和真正的业务逻辑分离开来，软件的结构将更加清晰。AOP是OOP的一个强有力的补充。 二、先简单用一下spring aop 首先要导入依赖：spring-aspects 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.3.12.RELEASE&lt;/version&gt;&lt;/dependency&gt; 写一个业务逻辑类： 1234567public class MathCaculator &#123; public int div(int i,int j)&#123; return i/j; &#125;&#125; 我们要向在这个业务逻辑方法运行前，运行结束时，方法出现异常时都将日志文件打印。 定义一个日志切面类 切面类里面的方法需要动态感知MathCaculator.div运行到哪里。 通知方法： 前置通知(@Before)：logStart()：在目标方法div运行之前运行 后置通知(@After)：logEnd():在目标方法div运行结束之后运行 返回通知(@AfterReturning)：logReturn():在目标方法div运行正常返回之后运行 异常通知(@AfterThrowing)：logException():在目标方法div出现异常之后运行 环绕通知(@Around)：动态代理，手动推进目标方法运行 123456789101112131415161718192021222324252627282930@Aspectpublic class LogAspect &#123; @Pointcut("execution(public int com.swg.aop.MathCaculator.*(..))") public void pointCut()&#123;&#125; @Before("pointCut()") public void logStart(JoinPoint joinPoint)&#123; String methodName = joinPoint.getSignature().getName(); Object[] args = joinPoint.getArgs(); System.out.println("【@Before】...方法名：&#123;"+methodName+"&#125;...参数列表是-&gt;&#123;"+ Arrays.asList(args)+"&#125;"); &#125; @After("pointCut()") public void logEnd(JoinPoint joinPoint)&#123; System.out.println("【@After】...&#123;"+joinPoint.getSignature().getName()+"&#125;结束..."); &#125; @AfterReturning(value = "pointCut()",returning = "result") public void logReturn(JoinPoint joinPoint,Object result)&#123; System.out.println("【@AfterReturning】...&#123;"+joinPoint.getSignature().getName()+"&#125;正常返回，运行结果是&#123;"+result+"&#125;"); &#125; @AfterThrowing(value = "pointCut()",throwing = "exception") public void logException(JoinPoint joinPoint,Exception exception)&#123; System.out.println("【@AfterThrowing】...&#123;"+joinPoint.getSignature().getName()+"&#125;发生异常,异常信息是&#123;"+exception.getMessage()+"&#125;"); &#125;&#125; 切面类和业务逻辑类都加入到容器中 123456789101112131415@EnableAspectJAutoProxy@Configurationpublic class MainConfigOfAop &#123; @Bean public MathCaculator mathCaculator()&#123; return new MathCaculator(); &#125; @Bean public LogAspect aspect()&#123; return new LogAspect(); &#125;&#125; 容器启动 12345678@Testpublic void test01()&#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfigOfAop.class); System.out.println("容器已经启动成功..."); MathCaculator caculator = applicationContext.getBean(MathCaculator.class); caculator.div(6,2); applicationContext.close();&#125; 无异常的情况输出 12345容器已经启动成功...【@Before】...方法名：&#123;div&#125;...参数列表是-&gt;&#123;[6, 2]&#125;div...【@After】...&#123;div&#125;结束...【@AfterReturning】...div正常返回，运行结果是&#123;3&#125; 有异常的情况输出 12345容器已经启动成功...【@Before】...方法名：&#123;div&#125;...参数列表是-&gt;&#123;[6, 0]&#125;div...【@After】...&#123;div&#125;结束...【@AfterThrowing】...&#123;div&#125;发生异常,异常信息是&#123;/ by zero&#125; 主要是有三步： 将业务逻辑组件和切面类都加入到容器中，告诉spring哪个是切面类(@Aspect) 在切面类上的每一个通知方法上标注通知注释，告诉spring合适何地运行(切入点表达式) 开启基于注解的aop模式(@EnableAspectJAutoProxy) 三、基本原理 AOP是具有特定的应用场合的，它只适合那些具有横切逻辑的应用场合，如性能检测、访问控制、事务管理及日志纪录。 Spring AOP使用动态代理技术在运行期织入增强的代码，Spring AOP使用了两种代理机制：一种是基于JDK的动态代理；另一种是基于CGLib的动态代理。这两种机制就是AOP最根本的实现原理，面试中把这两者说清楚即可。 3.1 JDK动态代理 Java1.3后，Java提供了动态代理技术，运行开发者在运行期间创建接口的代理实例。JDK动态代理主要涉及java.lang.reflect包中的两个类：Proxy和InvocationHandler。InvocationHandler是一个接口，可以通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态地将横切逻辑和业务逻辑编织在一起。 具体请看笔记java基础之JDK动态代理 3.2 CGLib动态代理 CGLib采用底层的字节码技术，可以为一个类创建子类，在子类中采用方法拦截的技术拦截所有父类方法的调用并顺势织入横切逻辑。 需要注意的是，由于CGLib采用动态创建子类的方式生成代理对象，所以不能对目标类中的final或private方法进行代理。 3.3 为什么会有两种代理机制 JDK创建代理有一个限制，即它只能为接口创建代理实例，虽然面向接口编程是好的编程习惯，但有时候并不是必须的，这是JDK动态代理的局限性。 就性能来说，CGLib所创建的动态代理对象的性能比JDK所创建的动态代理对象的性能高差不多10倍，CGLib在创建代理对象时所话费的时间却比JDK动态代理大概多8倍，但是对于singleton的代理对象或者具有实例池的代理，因为无需频繁创建代理对象，所以比较合适采用CGLib动态代理技术，反之则适合采用JDK动态代理技术。]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOC的用法]]></title>
    <url>%2F2019%2F03%2F02%2Fspring%2FIOC%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[我们已经知道了IOC的基本思想，它用一种倒置的思想帮助我们实现高层建筑需要什么直接引入底层就行，而不需要关心底层的具体实现，因为具体实现已经交给了我们的IOC去实现了。了解了这些之后，光说不练肯定是不行的，下面我们来看看这种依赖倒置到底是如何去用的。首先介绍一下传统的方式，就是spring中经常用的方式。然后再介绍一下springBoot中是什么样子的。其实它们两是差不多的。 我们知道，要想用Bean，那么必然是要先注册进去才行。Spring 启动时读取应用程序提供的Bean配置信息，并在Spring容器中生成一份相应的Bean配置注册表，然后根据这张注册表实例化Bean，装配好Bean之间的依赖关系，为上层应用提供准备就绪的运行环境。 一、@Configuration 和 @Bean给容器注册组件 新建一个maven工程，引入spring-context依赖。 1.1 以往的方式注册一个bean 新建一个实体类Person： 12345678@Data@AllArgsConstructor@NoArgsConstructor@ToStringpublic class Person &#123; private String name; private Integer age;&#125; 那么，我们可以在beans.xml中注册这个bean，给他赋值。 12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="person" class="com.swg.bean.Person"&gt; &lt;property name="age" value="10"/&gt; &lt;property name="name" value="张三"/&gt; &lt;/bean&gt;&lt;/beans&gt; 那么，我们就可以拿到张三这个人了： 1234567public class MainTest &#123; public static void main(String[] args) &#123; ApplicationContext applicationContext = new ClassPathXmlApplicationContext("beans.xml"); Person person = (Person) applicationContext.getBean("person"); System.out.println(person); &#125;&#125;//输出：Person(name=张三, age=10) 1.2 注解的方式注册bean 配置类 = 配置文件 @Configuration 告诉spring这是一个配置类 @Bean 给容器注册一个Bean，类型为返回值类型，id默认是方法名 1234567@Configurationpublic class MainConfig &#123; @Bean public Person person()&#123; return new Person("李四",20); &#125;&#125; 如何获取这个bean呢？ 123ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig.class);Person person = applicationContext.getBean(Person.class);System.out.println(person);//Person(name=李四, age=20) 我们还可以根据类型来获取这个bean在容器中名字是什么： 1234String[] names = applicationContext.getBeanNamesForType(Person.class);for(String name:names)&#123; System.out.println(name);//person&#125; 上面提到，id默认是方法名。如果我们修改MainConfig中的person这个方法名，果然打印结果也随着这个方法名改变而改变；也可以自己另外指定这个bean在容器中的名字：@Bean(&quot;hello&quot;)，那么这个bean的名字就变成了hello. 1.3 springboot中用这种方式注册bean以及获取 springboot那就更简单了。 定义bean跟注解的方式是一样的，不再赘述。只是，我们来到它的启动类，直接可以获取到ApplicationContext，然后就可以直接获取到bean： 12345678910@SpringBootApplicationpublic class SpringbootdemoApplication &#123; public static void main(String[] args) &#123; ApplicationContext ctx = SpringApplication.run(SpringbootdemoApplication.class, args); Person person = (Person) ctx.getBean("person"); System.out.println(person.getName()); &#125;&#125; 我们点进run里面： 我们这里不深入探讨里面的实现细节，我们这里只要知道可以用ApplicationContext来接收即可。 只是，到现在，我们一直在用这个ApplicationContext，它到底是何方神圣？ 二、BeanFactory和ApplicationContext 我们知道，IOC会帮助我们完成对象的创建并将其送到服务对象即完成对象的绑定，即IOC要实现这两件事情： 对象的构建 对象的绑定 spring提供了两种类型的容器来实现对bean的管理，一个是BeanFactory,一个是ApplicationContext(可以认为是BeanFactory的扩展)，这两者是spring core中最核心的两个基础接口。下面我们将介绍这两种容器如何实现对对象的管理。 Spring 通过一个配置文件描述 Bean 及 Bean 之间的依赖关系，利用 Java 语言的反射功能实例化 Bean 并建立 Bean 之间的依赖关系。 Spring 的 IoC 容器在完成这些底层工作的基础上，还提供了 Bean 实例缓存、生命周期管理、 Bean 实例代理、事件发布、资源装载等高级服务。 BeanFactory 是 Spring 框架的基础设施，面向 Spring 本身； ApplicationContext 面向使用 Spring框架的开发者，几乎所有的应用场合我们都直接使用 ApplicationContext 而非底层的 BeanFactory。 我们先来看看 BeanFactory ，再来看看 ApplicationContext 。 2.1 BeanFactory BeanFactory 体系架构： BeanDefinitionRegistry： Spring 配置文件中每一个节点元素在 Spring 容器里都通过一个 BeanDefinition 对象表示，它描述了 Bean 的配置信息。而 BeanDefinitionRegistry 接口提供了向容器手工注册 BeanDefinition 对象的方法。 BeanFactory 接口位于类结构树的顶端 ，它最主要的方法就是 getBean(String beanName)，该方法从容器中返回特定名称的 Bean，BeanFactory 的功能通过其他的接口得到不断扩展. ListableBeanFactory：该接口定义了访问容器中 Bean 基本信息的若干方法，如查看Bean 的个数、获取某一类型 Bean 的配置名、查看容器中是否包括某一 Bean 等方法； HierarchicalBeanFactory：父子级联 IoC容器的接口，子容器可以通过接口方法访问父容器； 通过 HierarchicalBeanFactory 接口， Spring 的 IoC 容器可以建立父子层级关联的容器体系，子容器可以访问父容器中的 Bean，但父容器不能访问子容器的 Bean。Spring 使用父子容器实现了很多功能，比如在 Spring MVC 中，展现层 Bean 位于一个子容器中，而业务层和持久层的 Bean 位于父容器中。这样，展现层 Bean 就可以引用业务层和持久层的 Bean，而业务层和持久层的 Bean 则看不到展现层的 Bean。 ConfigurableBeanFactory：是一个重要的接口，增强了 IoC 容器的可定制性，它定义了设置类装载器、属性编辑器、容器初始化后置处理器等方法； AutowireCapableBeanFactory：定义了将容器中的 Bean按某种规则（如按名字匹配、按类型匹配等）进行自动装配的方法； SingletonBeanRegistry：定义了允许在运行期间向容器注册单实例 Bean 的方法； ⭐⭐⭐其中，我们最关心的是BeanDefinition、BeanDefinitionRegistry、 BeanFactory。注意，BeanDefinition,它完成了Bean的生成过程。BeanDefinitionRegistry是将定义好的bean，注册到容器中。BeanFactory 是一个bean工厂类，从容器中可以取到任意定义过的bean。 Bean的生成大致可以分为两个阶段：容器启动阶段和bean实例化阶段： 它是面向spring管理bean的最核心的一个接口，但是作为使用者，我们往往更关心的是ApplicationContext. 2.2 ApplicationContext ApplicationContext 由 BeanFactory 派生而来，提供了更多面向实际应用的功能。 在BeanFactory 中，很多功能需要以编程的方式实现，而在 ApplicationContext 中则可以通过配置的方式实现。 ApplicationContext 继承了 HierarchicalBeanFactory 和 ListableBeanFactory 接口，在此基础上，还通过多个其他的接口扩展了 BeanFactory 的功能： ClassPathXmlApplicationContext：默认从类路径加载配置文件 FileSystemXmlApplicationContext：默认从文件系统中装载配置文件 ApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。实现了 ApplicationListener 事件监听接口的 Bean 可以接收到容器事件 ， 并对事件进行响应处理 。 在 ApplicationContext 抽象实现类AbstractApplicationContext 中，我们可以发现存在一个 ApplicationEventMulticaster，它负责保存所有监听器，以便在容器产生上下文事件时通知这些事件监听者。 MessageSource：为应用提供 i18n 国际化消息访问的功能； ResourcePatternResolver ： 加载资源文件 LifeCycle：该接口是 Spring 2.0 加入的，该接口提供了 start()和 stop()两个方法，主要用于控制异步处理过程。在具体使用时，该接口同时被 ApplicationContext 实现及具体 Bean 实现， ApplicationContext 会将 start/stop 的信息传递给容器中所有实现了该接口的 Bean，以达到管理和控制 JMX、任务调度等目的。 ConfigurableApplicationContext 扩展于 ApplicationContext，它新增加了两个主要的方法： refresh()和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力。在应用上下文关闭的情况下调用 refresh() 即可启动应用上下文，在已经启动的状态下，调用 refresh()则清除缓存并重新装载配置信息，而调用close()则可关闭应用上下文。这些接口方法为容器的控制管理带来了便利，但作为开发者，我们并不需要过多关心这些方法。 2.3 两者的区别 BeanFactory是Spring框架的基础设施，面向Spring ApplicationContext面向使用Spring框架的开发者 BeanFactory可以理解为汽车的发动机，ApplicationContext可以理解为比较完整的一辆汽车。 三、@ComponentScan自动扫描组件以及扫描规则 每次都用@Configure和@Bean着实太麻烦了，有没有什么办法可以自动地装载为Bean呢？答案就是这个自动扫描的注解。下面来看看是如何使用的。 配置文件中配置包扫描时这样配置的： 12&lt;!--包扫描，只要标注了@Controller，@Service，@Repository，@Component，就会被自动扫描到加入到容器中--&gt;&lt;context:component-scan base-package=&quot;com.swg&quot;/&gt; 现在用注解来实现这个功能，只需要加上注解@ComponentScan即可： 12@ComponentScan(value = "com.swg")//java8可以写多个@ComponentScan//java8以前虽然不能写多个，但是也可以实现这个功能，用@ComponentScans配置即可 表示自动扫描com.swg包路径下以及子目录下所有的Bean，装载进容器中。 上面的扫描路径是扫描所有的，有的时候我们需要排除掉一些扫描路径或者只扫描某个路径，如何做到呢？ 用excludeFilters来排除，里面可以指定排除规则，这里是按照ANNOTATION来排除，排除掉所有@Controller注解的类。classes也是个数组，可以排除很多。 123@ComponentScan(value = "com.swg",excludeFilters = &#123; @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)&#125;) 那么效果就是controller没有了，但是service和dao都在。 那如果我想只包含controller呢？ 123@ComponentScan(value = "com.swg", includeFilters = &#123; @ComponentScan.Filter(type = FilterType.ANNOTATION,classes = Controller.class)&#125;,useDefaultFilters = false) 注意要useDefaultFilters = false，因为默认为true，就是扫描所有，不设置为false无效。 关于springboot这里强调一下， 启动方法上的注解中已经默认有了扫描的注解，默认扫描的范围是这个启动类所在的路径及其子路径。]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOC基本原理]]></title>
    <url>%2F2019%2F03%2F02%2Fspring%2FIOC%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[学习spring的原理，第一步就是要理解IOC的基本原理，而IOC的重要实现方式是DI，本文了解为什么要有IOC这种思想，它到底帮助我们解决了什么问题，它的优势又是什么。 依赖倒置原则 要了解控制反转( Inversion of Control ), 我觉得有必要先了解软件设计的一个重要思想：依赖倒置原则（Dependency Inversion Principle ）。 什么是依赖倒置原则？假设我们设计一辆汽车：先设计轮子，然后根据轮子大小设计底盘，接着根据底盘设计车身，最后根据车身设计好整个汽车。这里就出现了一个“依赖”关系：汽车依赖车身，车身依赖底盘，底盘依赖轮子。 这样的设计看起来没问题，但是可维护性却很低。假设设计完工之后，上司却突然说根据市场需求的变动，要我们把车子的轮子设计都改大一码。这下我们就蛋疼了：因为我们是根据轮子的尺寸设计的底盘，轮子的尺寸一改，底盘的设计就得修改；同样因为我们是根据底盘设计的车身，那么车身也得改，同理汽车设计也得改——整个设计几乎都得改！ 注意，我们这里的轮胎是底层，汽车是高层，在整体高层架构确定的情况下，底层根据客户的需求进行改动是很正常的，但是这种模式改动的话，牵一发而动全身，不仅浪费极大时间，在成千上万的类的情况下，也无法进行逐一改动了。 我们现在换一种思路。我们先设计汽车的大概样子，然后根据汽车的样子来设计车身，根据车身来设计底盘，最后根据底盘来设计轮子。这时候，依赖关系就倒置过来了：轮子依赖底盘， 底盘依赖车身， 车身依赖汽车。 这时候，上司再说要改动轮子的设计，我们就只需要改动轮子的设计，而不需要动底盘，车身，汽车的设计了。 这就是依赖倒置原则——把原本的高层建筑依赖底层建筑“倒置”过来，变成底层建筑依赖高层建筑。高层建筑决定需要什么，底层去实现这样的需求，但是高层并不用管底层是怎么实现的。这样就不会出现前面的“牵一发动全身”的情况。 从软件设计的角度来看，改变轮胎这种类的实现是比较简单的，但是有的人会问：如果我要改汽车样式咋办呢？这不跟上面一样了。注意，汽车是高层建筑，这个东西相当于初期的软件架构，如果到后期要改动这个，将相当于处于架构设计不合理，那么整个项目会出现推倒重来的现象，这不是spring所能解决的事情了。 控制反转（Inversion of Control） 就是依赖倒置原则的一种代码设计的思路。具体采用的方法就是所谓的依赖注入（Dependency Injection）。其实这些概念初次接触都会感到云里雾里的。说穿了，这几种概念的关系大概如下： 为了理解这几个概念，我们还是用上面汽车的例子。只不过这次换成代码。我们先定义四个Class，车，车身，底盘，轮胎。然后初始化这辆车，最后跑这辆车。代码结构如下： 这样，就相当于上面第一个例子，上层建筑依赖下层建筑——每一个类的构造函数都直接调用了底层代码的构造函数。假设我们需要改动一下轮胎（Tire）类，把它的尺寸变成动态的，而不是一直都是30。我们需要这样改： 由于我们修改了轮胎的定义，为了让整个程序正常运行，我们需要做以下改动： 由此我们可以看到，仅仅是为了修改轮胎的构造函数，这种设计却需要修改整个上层所有类的构造函数！在软件工程中，这样的设计几乎是不可维护的——在实际工程项目中，有的类可能会是几千个类的底层，如果每次修改这个类，我们都要修改所有以它作为依赖的类，那软件的维护成本就太高了。 所以我们需要进行控制反转（IoC），及上层控制下层，而不是下层控制着上层。我们用依赖注入（Dependency Injection）这种方式来实现控制反转。所谓依赖注入，就是把底层类作为参数传入上层类，实现上层类对下层类的“控制”。这里我们用构造方法传递的依赖注入方式重新写车类的定义： 这里我们再把轮胎尺寸变成动态的，同样为了让整个系统顺利运行，我们需要做如下修改： 看到没？这里我只需要修改轮胎类就行了，不用修改其他任何上层类。这显然是更容易维护的代码。不仅如此，在实际的工程中，这种设计模式还有利于不同组的协同合作和单元测试：比如开发这四个类的分别是四个不同的组，那么只要定义好了接口，四个不同的组可以同时进行开发而不相互受限制；而对于单元测试，如果我们要写Car类的单元测试，就只需要Mock一下Framework类传入Car就行了，而不用把Framework, Bottom , Tire 全部 new 一遍再来构造 Car 。 这里我们是采用的构造函数传入的方式进行的依赖注入。其实还有另外两种方法：Setter传递和接口传递。这里就不多讲了，核心思路都是一样的，都是为了实现控制反转。 看到这里你应该能理解什么控制反转和依赖注入了。那什么是控制反转容器(IoC Container)呢？其实上面的例子中，对车类进行初始化的那段代码发生的地方，就是控制反转容器。 显然你也应该观察到了，因为采用了依赖注入，在初始化的过程中就不可避免的会写大量的new。这里IoC容器就解决了这个问题。这个容器可以自动对你的代码进行初始化，你只需要维护一个Configuration（可以是xml可以是一段代码），而不用每次初始化一辆车都要亲手去写那一大段初始化的代码。这是引入IoC Container的第一个好处。 IoC Container的第二个好处是：我们在创建实例的时候不需要了解其中的细节。在上面的例子中，我们自己手动创建一个车instance时候，是从底层往上层new的： 这个过程中，我们需要了解整个Car/Framework/Bottom/Tire类构造函数是怎么定义的，才能一步一步new注入。而IoC Container在进行这个工作的时候是反过来的，它先从最上层开始往下找依赖关系，到达最底层之后再往上一步一步new（有点像深度优先遍历）： 这里IoC Container可以直接隐藏具体的创建实例的细节，在我们来看它就像一个工厂： 我们就像是工厂的客户。我们只需要向工厂请求一个Car实例，然后它就给我们按照Config创建了一个Car实例。我们完全不用管这个Car实例是怎么一步一步被创建出来。 实际项目中，有的Service Class可能是十年前写的，有几百个类作为它的底层。假设我们新写的一个API需要实例化这个Service，我们总不可能回头去搞清楚这几百个类的构造函数吧？IoC Container的这个特性就很完美的解决了这类问题——因为这个架构要求你在写class的时候需要写相应的Config文件，所以你要初始化很久以前的Service类的时候，前人都已经写好了Config文件，你直接在需要用的地方注入这个Service就可以了。这大大增加了项目的可维护性且降低了开发难度。 整理自：Sevenvidia的回答]]></content>
      <tags>
        <tag>spring面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法12-排序总结]]></title>
    <url>%2F2019%2F03%2F01%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%9512-%E6%8E%92%E5%BA%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[至此，经典的排序算法的原理全部过了一遍，其中没有说明希尔排序，不过这个并不是重点，我们重点掌握的应该是快排，其次是归并和堆排，最后是插入排序和冒泡排序，在后面就是非比较的排序。本文对它们的特性做一个简单的总结。算法复杂度已经说明，所以不再赘述。 1.稳定性说明 定义：相同的数字排序前后相对位置不变。 1.冒泡排序：可以做到稳定，因为冒泡排序的思想是每次将大的数往下沉。如果我们设置每次相等也交换，那么就不是稳定的；反之，我们就可以做到稳定。 2.选择排序：不可以，考虑下面这个数组：5，5，5，5，1；那么我们选择一个最小的与第0号元素交换。那么就变成：1，5，5，5，5；此时，第一个5越过后面所有的5跑到了最后面。 3.插入排序：可以做到稳定，插入排序的基本思想是前面已经排好序，后面个数与前面已排好序的比较，小于的话就交换。相等的时候是不需要交换的。 4.归并排序：可以做到稳定，因为主要思想是分治，在merge的时候，两边数组进行比较，凑成一个排序的数组。那么我们只要设置：相等的时候，一直先考虑左边(或者右边)就可以。 5.快速排序：一般不可以，这个就很明显了，我们以三路快排为例，partition的过程中，随机选择一个数，每次形成的数组是&lt;x =x &gt;x这三种，显然中间的x就会打乱顺序。 6.堆排序：很显然不可以。因为每次的建堆过程中，即heapInsert过程中，由于新加入的大的值要上浮，那么就可以调换其中的顺序。比如 6 4 4，此时插入一个5建堆，那么显然第一个4要跟5交换，那么第一个4就跑到了第二个4的后面。 2.工程中的综合算法 如果是很长的基本类型数组，考虑快排；如果是很长的自定义类型的数组，考虑归并排序；如果数组很短(&lt;60)，考虑插入排序，因为常数项小。 因为基本类型相等的是没有差异的，不需要注意原始顺序，所以直接用快排； 但是自定义类型的个体是有差异的，需要对自定义类型中的属性进行判断，关系到顺序。 3.排序问题的补充 归并排序的额外空间复杂度可以变成O(1)，但是非常难，不需要掌握。 快速排序可以做到稳定性问题，但是非常难，不需要掌握。 面试中碰到一个问题：奇数放在数组左边，偶数放在数组右边，还要求原始的相对次序是不变的。空间复杂度为O(1)，时间复杂度为O(N). 针对这个问题，其实是很难的，因为一个数不是奇数就是偶数，属于01问题，那么就跟快排的partition过程是类似的，就是小于等于某个数放左边，大于某个数放右边，那么也是01问题，就是说，荷兰国旗问题是做不到稳定的。但是快速排序做到稳定性是很难的，所以这道题目其实是比较难做到的。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法11-排序之计数排序、桶排序、基数排序]]></title>
    <url>%2F2019%2F03%2F01%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%9511-%E6%8E%92%E5%BA%8F%E4%B9%8B%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F%E3%80%81%E6%A1%B6%E6%8E%92%E5%BA%8F%E3%80%81%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[在之前我们介绍的都是比较排序算法，在结果中各元素的次序都基于输入元素间的比较。而任何比较排序算法在最坏情况下都要用 O(NlgN) 此比较来排序。而非基于比较的排序，如计数排序，桶排序，和在此基础上的基数排序，则可以突破O(NlogN)时间下限。但要注意的是，非基于比较的排序算法的使用都是有条件限制的，例如元素的大小限制，相反，基于比较的排序则没有这种限制(在一定范围内)。但并非因为有条件限制就会使非基于比较的排序算法变得无用，对于特定场合有着特殊的性质数据，非基于比较的排序算法则能够非常巧妙地解决。 计数排序 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 123456789101112131415161718//maxVal为传入的数组的最大值private int[] countSort(int[] array,int maxVal)&#123; // 1.根据最大值可以确定辅助数组的长度 int[] helper = new int[maxVal+1]; //2.遍历array数组，统计每个元素出现的次数，记录在辅助数组对应索引处 for(int i=0;i&lt;array.length;i++)&#123; helper[array[i]]++; &#125; //3.遍历辅助数组，覆盖原数组 int index = 0; for(int i=0;i&lt;maxVal+1;i++)&#123; while (helper[i] &gt; 0)&#123; array[index++] = i; helper[i]--; &#125; &#125; return array;&#125; 桶排序 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序. 找出待排序数组中的最大值max、最小值min 我们使用动态数组 ArrayList 作为桶，桶里放的元素也用 ArrayList 存储。桶的数量为(max-min)/arr.length+1 遍历数组 arr，计算每个元素 arr[i] 放的桶 每个桶各自排序 遍历桶数组，把排序好的元素放进输出数组 123456789101112131415161718192021222324252627282930313233343536373839404142private static int[] bucketSort(int[] arr)&#123; //1.确定出数组的最大值和最小值 int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; for(int i=0;i&lt;arr.length;i++)&#123; max = Math.max(arr[i],max); min = Math.min(arr[i],min); &#125; //2.根据最大值和最小值确定桶的数量，并且初始化每个桶 int buctetSize = (max-min)%arr.length + 1; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucket = new ArrayList&lt;&gt;(buctetSize); for(int i=0;i&lt;buctetSize;i++)&#123; bucket.add(new ArrayList&lt;&gt;()); &#125; //3.类似于hashmap，将其元素放到对应下标的桶中 for(int i=0;i&lt;arr.length;i++)&#123; int num = (arr[i] - min)%buctetSize; bucket.get(num).add(arr[i]); &#125; //4.对每个桶中的元素都要进行排序 for(int i=0;i&lt;buctetSize;i++)&#123; Collections.sort(bucket.get(i)); &#125; System.out.println(bucket.toString()); //5.遍历所有的桶，类似于计数排序一样覆盖原数组得到有序的序列 int arrIndex = 0; for(int i=0;i&lt;buctetSize;i++)&#123; int index = 0; int sum = bucket.get(i).size(); while(sum &gt; 0)&#123; arr[arrIndex++] = bucket.get(i).get(index++); sum--; &#125; &#125; return arr;&#125; 基数排序 基数排序(Radix Sort)是桶排序的扩展，它的基本思想是：将整数按位数切割成不同的数字，然后按每个位数分别比较。 通过基数排序对数组{53, 3, 542, 748, 14, 214, 154, 63, 616}，它的示意图如下： 在上图中，首先将所有待比较数值统一为统一位数长度，接着从最低位开始，依次进行排序。 按照个位数进行排序。 按照十位数进行排序。 按照百位数进行排序。 排序后，数列就变成了一个有序序列。 在理解了基本的思想之后，下面以一个简单的例子辅助理解程序思想。 首先我们有以下这个数组： 1int[] arrays = &#123;6, 4322, 432, 344, 55 &#125;; 现在我们有10个桶子，每个桶子下能装载arrays.length个数字 1int[][] buckets = new int[arrays.length][10]; 效果如下： 第一趟分配与回收:将数组的每个个位数进行分配到不同的桶子上 分配完之后，我们按照顺序来进行回收：得到的结果应该是这样子的：{4322,432,344,55,6} 第二趟分配与回收:将数组的每个十位数进行分配到不同的桶子上(像6这样的数，往前边补0) 分配完之后，我们按照顺序来进行回收：得到的结果应该是这样子的：{6,4322,432,344,55} 第三趟分配与回收:将数组的每个百位数进行分配到不同的桶子上(像6、55这样的数，往前边补0) 分配完之后，我们按照顺序来进行回收：得到的结果应该是这样子的：{6,55,4322,344,432} 第四趟分配与回收:将数组的每个百位数进行分配到不同的桶子上(像6、55，344，432这样的数，往前边补0) 分配完之后，我们按照顺序来进行回收：得到的结果应该是这样子的：{6,55,344,432,4322} 理解了上面，代码也就非常容易理解了： 获取这个数组的最大值，这里用递归来实现一下： 12345678private static int getMax(int[] arr,int L,int R)&#123; if(L == R)&#123; return arr[L]; &#125; int a = arr[L]; int b = getMax(arr,L+1,R); return a &gt; b ? a : b;&#125; 基数排序： 12345678910111213141516171819202122232425262728public static int[] radixSort(int[] arr) &#123; //求得数组最大值 int max = getMax(arr,0,arr.length-1); //最大数的位数就是我们要分配的次数 for(int i = 1 ; max / i &gt; 0 ; i *= 10)&#123; //构造arr.length行，10列的二维数组 int[][] buckets = new int[arr.length][10]; //求数组每个位，如个位，十位等，根据该位的数字放到对应的二维数组里 for(int j=0;j&lt;arr.length;j++)&#123; int num = (arr[j]/i)%10; buckets[j][num] = arr[j]; &#125; //一次放完之后，就要回收起来放进原来的数组中，等待下一次的重新分配 int index = 0; for(int k=0;k&lt;10;k++)&#123; for(int j=0;j&lt;arr.length;j++)&#123; if(buckets[j][k] != 0)&#123; arr[index++] = buckets[j][k]; &#125; &#125; &#125; &#125; return arr;&#125; 基数排序要理解起来并不困难，不过值得注意的是：基数排序对有负数和0的数列难以进行排序 因此，往往有0和负数的数组一般我们都不用基数来进行排序 基数排序的要点就两个： 分配：按照元素的大小来放入不同的桶子里 回收：将桶子里的元素按桶子顺序重新放到数组中 重复…两个步骤]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法10-堆排序]]></title>
    <url>%2F2019%2F02%2F28%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%9510-%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[堆排序的重要性在于它涉及到二叉堆这个数据结构，面试中曾经被问过堆这个数据结构，那么堆其实是一个完全二叉树，它里面含有好几种类型的堆，其中我们比较关注的是二叉堆，它分为大顶堆和小顶堆，是非常常用的一种数据结构。所以我觉得面试中问到堆这个数据结构的时候可以往二叉堆上进行靠拢，然后回答问题。 1. 前言 在学习堆排序之前，有必要把一些数据结构方面的知识理一下，要不然会有点乱。 首先看一下一个大的分类： 因此堆是一种特殊的树，并且是特殊的完全二叉树。对于堆排序中的堆通常是指二叉堆。二叉堆分为大根堆和小根堆。其中每个节点的值小于等于其左、右孩子的值，这样的堆称为小根堆；其中每个节点的值大于等于其左、右孩子的值，这样的堆称为大根堆；这里注意二叉堆和二叉搜索树的区别。 那么理清这个关系之后，我们知道了，我们主要的研究对象是二叉堆这个数据结构。 2. 二叉堆的结构 回顾一下完全二叉树的定义。 如下图，每一层都是从左向右摆放节点，每个节点都是摆满两个子节点后才向右移动到下一个节点，一层摆满后向下移动一层，直到摆放完所有数字。这样得到的二叉树就是完全二叉树，中间有任何缺失的节点就不能称为完全二叉树。 二叉堆是一种完全二叉树，他们的区别是： 二叉堆是一颗完全二叉树，完全二叉树有一个非常重要的性质：即完全二叉树只用数组而不需要指针就可以表示。优势在于数组表示的话内存是紧凑排列的，不会有太多的内存碎片，并且数组对于随机访问是很快的，基于数组下标即可。 3. 二叉堆与优先队列 什么是优先队列，队列我们是比较熟悉的，是一种先进先出的数据结构，在优先队列中，出队的顺序与入队的顺序无关了，而是与优先级有关。即优先级越高，越早出队。 优先队列到底有什么实际应用呢？比如一个电商网站搞特卖或抢购，用户登录下单提交后，考虑这个时间段用户访问下单提交量很大，通常表单提交到服务器后端后，后端程序一般不直接进行扣库存处理，将请求放到队列列，异步消费处理，用普通队列是FIFO的，这里有个需求是，用户会员级别高的，可以优先抢购到商品，可能这个时间段的级别较高的会员用户下单时间在普通用户之后，这个时候使用优先队列代替普通队列，基本能满足我们的需求。 优先队列就是依靠二叉堆来实现的。优先队列需要支持两种操作： 删除最小（最大）元素 插入元素 为什么要用堆来实现优先队列？ 优先队列所需要实现的两种操作，不同于队列和栈，它需要一个有序的元素序列，但不要求全部有序，只需要从这些元素中找到最大（或最小）的一个元素。而堆刚好满足这个条件。 队列，栈都是用数组或者链表来实现的，针对优先队列，用数组和链表实现也是可以的，在队列较小，大量使用两种操作之一时，或者所操作的元素的顺序已知时，用数组和链表十分有用，但是，在最坏的情况下，优先队列用这两张方法实现所需的时间却是线性的。而用堆在最坏情况下的时间则是对数级别。 由于我们比较关注的是立即拿到最大或者最小的元素，然后高效地删除和插入。这些都依赖于堆的内部算法实现，下面我们就来看看大顶堆为例的插入和删除操作原理。 4. 堆的算法 我们用N+1长度的数组来表示一个大小为N的堆，我们不会使用[0],堆元素会被保存于[1]-[N-1]中。 4.1 大顶堆的插入(上浮) 123456789/* * k:当前插入元素的位置，相应地k/2就是其父结点的位置 */private void swim(int k)&#123; while(k&gt;1 &amp;&amp; less(k/2,k))&#123; swap(k/2,k); k = k/2; &#125;&#125; 4.2 大顶堆的删除 1234//伪代码1. 获取根结点2. 将根结点与最后一个结点交换3. 恢复堆的有序性... 显然现在看来该二叉树虽然是一个完全二叉树，但是它并不符合最大堆的相关定义，我们的目的是要在删除完成之后，该完全二叉树依然是最大堆。因此就需要我们来做一些相关的操作！ 1234567891011121314/* * k:当前被删除元素的位置(若删除根节点,则k=1)，相应地2*k就是其左子结点的位置 */private void sink(int k)&#123; while(2*k &lt; N)&#123; int j = 2 * k; if(j &lt; N &amp;&amp; less(j,j+1)) j++; if(!less(k,j)) break; swap(k,j); k = j; &#125;&#125; 5. 堆排 在了解了堆的基本操作之后，我们将目标先转到堆排序上，这才是本章研究的重点。 时间复杂度为O(logN)，额外空间复杂度为O(1); 我们知道，数组可以对应到一个完全二叉树。 堆:大根堆和小根堆。堆就是一个完全二叉树。 大根堆：完全二叉树中任何一个子树的最大值就是其头部节点对应的值。 那么，数组已经是一个完全二叉树，而下面的任务就是：将一个数组变成大根堆。 构建一个大根堆的复杂度是log1+log2+…+logi = O(N) 第一次构建完大根堆之后，还不是有序的。堆排序的主要思路是：每次将最后一个数与第一个数交换，就是完全二叉树的最后一个数与根节点进行交换。由于根节点已经是最大的数，所以我们就可以不要再管它。我们再将0~n-1下标的所有数进行调整，也调整为大顶堆，然后重复上面的动作。这样，不停地把当前大顶堆的最大数调整到后面，一直到最后，整个数组就是有序的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class HeapSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;1,1,3,1,3,2,2,4,1,3&#125;; heapSort(arr); for (int anArr : arr) &#123; System.out.print(anArr + " "); &#125; &#125; private static void heapSort(int[] arr)&#123; if(arr == null || arr.length &lt; 2)&#123; return; &#125; //第一次调整为大顶堆 for(int i=0;i&lt;arr.length;i++)&#123; heapInsert(arr,i); &#125; int heapSize = arr.length; //第一个数与最后一个数交换，最后一个数就是最大的值，前面的数再调整为新的大顶堆 //这样每次都将当前最大的数从数组的后面依次往前排，排到最后整个数组升序 swap(arr,0,--heapSize); while(heapSize &gt; 0)&#123; heapify(arr,0,heapSize); swap(arr,0,--heapSize); &#125; &#125; //如果当前的值是大于父节点的，就与父节点交换，使得大的数上浮 private static void heapInsert(int[] arr, int index) &#123; while(arr[index] &gt; arr[(index-1)/2])&#123; swap(arr,index,(index-1)/2); index = (index - 1)/2; &#125; &#125; //每次将最后一个数与第一个数交换后，我们要重新构建大顶堆，主要是将当前第一个数与自己的子节点进行比较，如果小于当前的子节点，则交换；否则不变，已经是大顶堆 private static void heapify(int[] arr,int index,int heapSize)&#123; //拿到当前节点的左子节点 int left = index * 2 + 1; //左子节点下标不能越界 while(left &lt; heapSize)&#123; //在右子节点也不越界的情况下，选出左右子节点中的较大者 int largest = left+1 &lt; heapSize &amp;&amp; arr[left+1] &gt; arr[left] ? left + 1 : left; //当前节点与左右子节点的最大者再比较 largest = arr[largest] &gt; arr[index] ? largest : index; //如果最大值就是当前节点，说明当前节点的值是大于左右两个子节点的，不需要交换，跳出循环 if(largest == index)&#123; break; &#125; //这个时候说明当前节点是小于某一个子节点的，那么就要进行交换，并且更新当前节点的坐标为子节点的而坐标，再更新左子节点，做下一次循环的比较 swap(arr,largest,index); index = largest; left = index * 2 + 1; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125; 堆这个数据结构是非常重要的，因为他的动态调整的时间复杂度为logN，是非常低的。比如经典问题：快速在一串数字流中快速找到中位数等。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法9-快速排序]]></title>
    <url>%2F2019%2F02%2F28%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%959-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[上一篇介绍了归并排序，本文介绍快速排序，顾名思义，应该是综合性能最好的排序了。在具体实现上，往上也有很多的版本，虽然大体思想一致，但是我觉得掌握其中一种最实用的方式就够了，本文的快排思想基于荷兰国旗问题演变，即所谓的三路快排，对于重复元素较多的场景是非常适合的，对于普通场景来说，性能也不弱。 1. 荷兰国旗问题 在研究快速排序之前，我们先来研究一下一个经典问题：荷兰国旗问题，我们的目标是给出一个num，将原来的数组中的值按照下面的规则进行排列：比num小的全放到num的左边，比bum大的全部放在右边，中间全是等于num的值。类似于荷兰国旗的三色旗。 具体的算法思想看代码注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 荷兰国旗问题 * 给定一个数组arr，和一个数num，请把小于num的数放在数组的左边， * 等于num的数放在数组的中间，大于num的数放在数组的右边。 */public class helan_flag_question_solve &#123; public static void main(String[] args) &#123; int[] arr = &#123;1,1,3,1,3,2,2,1,1,3&#125;; solve(arr,0,arr.length-1,2); for(int i=0;i&lt;arr.length;i++)&#123; System.out.print(arr[i] + " "); &#125; &#125; /** * 主要思想是：初始化的坐标分别为-1和N，就是取超出数组的范围，当前坐标是从0开始 * 分为三种情况：如果当前等于num，那么指针后移一格即可 * 如果当前小于num，那么就将当前和less+1交换位置，并且当前指针后移一格 * 如果当前大于num，那么就将当前和more-1交换位置，当前位置不变继续判断、 * 为什么与前面交换当前指针就要后移一格，但是与后面交换不用后移呢？ * 我们知道，curr扫过的地方，curr当前指向的和前面的数肯定都是小于num的了，所以需要后移一个判断下一个元素 * 但是从后面交换过来的，我们不知道这个交换过来的元素比num小还是大，所以对这个元素还需要判断一下 */ private static void solve(int[] arr,int L,int R,int num)&#123; int less = L-1; int more = R+1; int curr = L; while(curr &lt; more)&#123; if(arr[curr] &lt; num)&#123; swap(arr,++less,curr++); &#125;else if(arr[curr] &gt; num)&#123; swap(arr,--more,curr); &#125;else&#123; curr++; &#125; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125; 这道题目一定一定要注意边界问题。下面的快排其实就是对荷兰国旗问题的递归操作。因此要想理解快排，需要先掌握荷兰国旗问题。 2. 快排 快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 时间复杂度为O(N*logN),额外空间复杂度O(logN); 因为要记录划分区域的边界，所以需要一定的空间。这里划分的空间与二分的次数有关，所以需要O(logN)。 123456789101112131415161718192021222324252627282930private void quick_sort(int[] input,int low,int high)&#123; if(low &lt; high)&#123; int[] p = partion(input,low,high); quick_sort(input,0,p[0]-1); quick_sort(input,p[1]+1,high); &#125;&#125;private int[] partion(int[] input,int L,int R)&#123; int less = L - 1; int more = R + 1; int curr = L; int num = input[L]; while(curr &lt; more)&#123; if(input[curr] &lt; num)&#123; swap(input,++less,curr++); &#125;else if(input[curr] &gt; num)&#123; swap(input,curr,--more); &#125;else&#123; curr++; &#125; &#125; return new int[]&#123;less,more&#125;;&#125;private void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;&#125; 快排存在的一个问题是：可能划分出来两边数组很偏，排序效率就会变差。可以用随机快排进行改进。 思路：随机选一个数与数组最后一个数交换。 1swap(arr,L+(int)(Math.random()*(R-L+1)),R); 相比于经典快排，这个优化的快排的优点在于：每一次partition之后，就可能会揪出一串的相等数字，然后左边全是小于这个数，右边都是大于这个数。而经典快排每次只找出一个数字来，左边是小于等于这个数，右边是大于这个数。很显然，优化后的快排要快一点。 这种从荷兰国旗演变过来的快排，对于重复元素较多的时候是非常有利的，因此这种是我比较喜欢的一种写法，这也意味着快排的实现上有一些差异，但是主要的思想是一致的，即分治处理。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法8-归并排序]]></title>
    <url>%2F2019%2F02%2F27%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%958-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[从本文开始就要介绍O(nlogn)复杂度级别的排序算法了，首先登场的是归并排序，这个排序可以解决一些问题，会在文章的后面给出，并且是一个经典的分治思想，即先分隔再合并，将复杂的大问题瓦解为小问题，将若干小问题解决了之后大问题也就迎刃而解了。下面我们来学习一下归并排序的基本原理。 1. 原理 归并排序（MERGE-SORT）是利用归并的思想实现的排序方法，该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案&quot;修补&quot;在一起，即分而治之)。 复杂度为(nlogN),这里采用自顶向下和递归来完成的。 归并排序的原理是，先把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 归并的前提是先把要排序的序列分为若干个字序列，然后才归并。在拆分数列的时候，就要用到拆分，直到不能再拆为止。 如一个数列{9,8,7,6,5,4,3,2,1} 先分成{9,8,7,6,5}和{4,3,2,1} 然后再分成{9,8,7}和{6,5}和{4,3}和{2,1} 然后再分{9,8}、{6}、{5}、{4}、{3}、{2}、{1} 然后再合并起来，小在的前面，大的在后面，没有比较的在后面填充数列。 具体如何合并的呢？下面展示的最后的一步合并过程： 我们注意到，归并排序是需要额外的空间来辅助的。动态图为： 2. 代码 2.1 左右分开 12345678910111213public static void sort(int[] a, int low, int high) &#123; //int mid = (low + high) / 2; int mid = low + (high - low)/2; if (low &lt; high) &#123; //左边归并排序，使得左子序列有序 sort(a, low, mid); //右边归并排序，使得右子序列有序 sort(a, mid + 1, high); //将两个有序子数组合并操作 merge(a, low, mid, high); &#125;&#125; 2.2 合并过程 1234567891011121314151617181920212223242526public static void merge(int[] a, int low, int mid, int high) &#123; int[] temp = new int[high - low + 1]; int i = low;// 左指针 int j = mid + 1;// 右指针 int k = 0;//临时指针 // 把较小的数先移到新数组中 while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (a[i] &lt; a[j]) &#123; temp[k++] = a[i++]; &#125; else &#123; temp[k++] = a[j++]; &#125; &#125; // 把左边剩余的数移入数组 while (i &lt;= mid) &#123; temp[k++] = a[i++]; &#125; // 把右边边剩余的数移入数组 while (j &lt;= high) &#123; temp[k++] = a[j++]; &#125; // 把新数组中的数覆盖原数组 for (int k2 = 0; k2 &lt; temp.length; k2++) &#123; a[k2 + low] = temp[k2]; &#125;&#125; 对于它的理解，一句话就是先对半分，分到不能分为止，然后再倒过来将卡擦分开的两组数进行比较合并成有序序列，最终逐渐合并成有序序列。 3. 归并排序应用1–小和问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * 归并排序的应用 * * 在一个数组中，每一个数左边比当前数小的数累加起来，叫做这个数组的小和。求一个数组的小和。 * * 例子： * [1,3,4,2,5] * 1左边比1小的数，没有； * 3左边比3小的数，1； * 4左边比4小的数，1、3； * 2左边比2小的数，1； * 5左边比5小的数，1、3、4、2； * 所以小和为1+1+3+1+1+3+4+2=16 */public class MergeSortApply1 &#123; public static void main(String[] args) &#123; int[] arr= &#123;1,3,4,2,5&#125;; System.out.println(merge_sort(arr)); &#125; public static int merge_sort(int[] arr)&#123; if(arr == null || arr.length &lt; 2)&#123; return 0; &#125; return sortProcess(arr,0,arr.length-1); &#125; private static int sortProcess(int[] arr, int low, int high) &#123; if(low == high)&#123; return 0; &#125; int mid = low + (high-low)/2; return sortProcess(arr,low,mid) + sortProcess(arr,mid+1,high) + merge(arr,low,mid,high);&#125; private static int merge(int[] arr, int low, int mid, int high) &#123; int[] help = new int[high-low+1]; int k = 0; int p1 = low; int p2 = mid+1; int count = 0; while(p1 &lt;= mid &amp;&amp; p2 &lt;= high)&#123; //核心的是增加这一句，当发现arr[p1] &lt; arr[p2]时 //那么p2后面的数必然都大于它，所以这一次合并过程中 //p1位置比(high-p2+1)这些位置都小，那么针对这个p1位置的数字，一次性全部累计起来即可 count += arr[p1] &lt; arr[p2] ? (high-p2+1)*arr[p1] : 0; help[k++] = arr[p1] &lt; arr[p2] ? arr[p1++] : arr[p2++]; &#125; while(p1 &lt;= mid)&#123; help[k++] = arr[p1++]; &#125; while(p2 &lt;= high)&#123; help[k++] = arr[p2++]; &#125; for(int ii=0;ii&lt;help.length;ii++)&#123; arr[ii+low] = help[ii]; &#125; return count; &#125;&#125; 4. 归并排序应用2–逆序对问题 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 归并排序的应用 * * 在一个数组中，左边的数如果比右边的数大，则折两个数构成一个逆序对，请打印所有逆序对的数量。 */public class MergeSortApply2 &#123; public static void main(String[] args) &#123; int[] arr= &#123;1,2,3,4,5,6,7,0&#125;; System.out.println(merge_sort(arr)); &#125; public static int merge_sort(int[] arr)&#123; if(arr == null || arr.length &lt; 2)&#123; return 0; &#125; return sortProcess(arr,0,arr.length-1); &#125; private static int sortProcess(int[] arr, int low, int high) &#123; if(low == high)&#123; return 0; &#125; int mid = low + (high-low)/2; return sortProcess(arr,low,mid) + sortProcess(arr,mid+1,high) + merge(arr,low,mid,high);&#125; private static int merge(int[] arr, int low, int mid, int high) &#123; int[] help = new int[high-low+1]; int k = 0; int p1 = low; int p2 = mid+1; int count = 0; while(p1 &lt;= mid &amp;&amp; p2 &lt;= high)&#123; //归并的过程中发现前面大于后面的话就算一组 count += arr[p1] &gt; arr[p2] ? (high-p2+1) : 0; help[k++] = arr[p1] &lt; arr[p2] ? arr[p1++] : arr[p2++]; &#125; while(p1 &lt;= mid)&#123; help[k++] = arr[p1++]; &#125; while(p2 &lt;= high)&#123; help[k++] = arr[p2++]; &#125; for(int ii=0;ii&lt;help.length;ii++)&#123; arr[ii+low] = help[ii]; &#125; return count; &#125;&#125;]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法7-基本排序之冒泡、选择、插入]]></title>
    <url>%2F2019%2F02%2F27%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%957-%E5%9F%BA%E6%9C%AC%E6%8E%92%E5%BA%8F%E4%B9%8B%E5%86%92%E6%B3%A1%E3%80%81%E9%80%89%E6%8B%A9%E3%80%81%E6%8F%92%E5%85%A5%2F</url>
    <content type="text"><![CDATA[排序算法毋庸置疑，是最重要最重要的基础算法，真正的实际应用中，往往是几种排序算法的组合，因为没有完美的算法，只有适合的算法。学好算法的第一步应该是熟练手写出基本的排序算法，本文应该被放在一篇文章，但是命运的巧合，我还是选择了递归。因为排序算法就摆在那，思想比较清晰，理解上没有难度，但是递归也摆在那，好像简单但是又无从下手。本文先从复杂度比较高但是比较简单的几种排序算法入手。这几种都是O(n^2)的时间复杂度。 1. 冒泡排序 冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法的基本步骤： 比较相邻的元素。如果第一个比第二个大(注意相等不要交换，所谓冒泡是稳定的排序)，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 动态图： 12345678910111213141516171819202122232425262728293031/** * 冒泡排序 */public class BubbleSort &#123; private static void bubble_sort(int[] arr)&#123; if(arr == null || arr.length &lt; 2)&#123; return; &#125; /** * 冒泡排序整体思路：每一趟的比较，都会将最大的一个数排到最后面 * 0。。。。。。。n-1 第一趟一直比较到最后一个，把最大的放到对后面 * 0。。。。。n-2 第二趟比较的数组长度会减少一个，因为最大的已经确定了 * 0。。。。n-3 第三趟比较的就再少两个，因为两个最大的已经确定了 */ for(int i=arr.length-1;i&gt;0;i--)&#123; for(int j=0;j&lt;i;j++)&#123; //两两比较交换 if(arr[j] &gt; arr[j+1])&#123; swap(arr,j,j+1); &#125; &#125; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125; 2. 选择排序 选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：不断地在未排序序列中找到最小元素，交换到数组的最前面。 123456789101112131415161718192021222324252627/** * 选择排序 */public class SelectSort &#123; private static void select_sort(int[] arr)&#123; /** * 基本思想是：每一趟都将最小值的索引确定好，然后放到前 * 所以每一趟结束之后，前面是已经排好序的 */ for(int i=0;i&lt;arr.length;i++)&#123; int min = i; for(int j=i+1;j&lt;arr.length;j++)&#123; if(arr[j] &lt; arr[min])&#123; min = j; &#125; &#125; swap(arr,min,i); &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125;&#125; 3. 插入排序 它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 12345678910111213141516171819202122232425262728293031323334353637/** * 插入排序 */public class InsertSort &#123; public static void main(String[] args) &#123; int[] arr= &#123;3,1,54,32,23,3,6&#125;; insert_sort(arr); for (int i=0;i&lt;arr.length;i++)&#123; System.out.print(arr[i] + " "); &#125; &#125; private static void insert_sort(int[] arr)&#123; /** * 插入排序主要思想是：每一趟都保证当前索引前的所有元素都小于当前索引 * 比如【5,4,3,2,1】，那么第一趟是【4,5,3,2,1】，第二趟是【3,4,5,2,1】 * 第三趟是【2,3,4,5,1】，第四趟是【1,2,3,4,5】 * * * 其实这是优化后的方法，简单的插入排序是这样子的： * for(int i=1;i&lt;arr.length;i++)&#123; * for(int j=i; j&gt;0 &amp;&amp; array[j-1]&gt;array[j]; j--)&#123; * swap(array, j, j-1); * &#125; * &#125; * 这里的优化是考虑到原始方法要不断地进行交换，其实是没有必要的，直接赋值就好了 */ for(int i=1;i&lt;arr.length;i++)&#123; int tmp = arr[i]; int j; for(j=i;j&gt;0 &amp;&amp; arr[j-1] &gt; tmp ;j--)&#123; arr[j] = arr[j-1]; &#125; arr[j] = tmp; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法6-关于二叉树的经典面试题分析]]></title>
    <url>%2F2019%2F02%2F27%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%956-%E5%85%B3%E4%BA%8E%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[掌握对树的基本操作是很重要的，这里所谓的操作是指对树的遍历，以及对树的构造等等。下面通过一些题目来好好研究研究。由于篇幅、时间以及精力有限，本文着重提取两种题型进行分析，都是高频面试问题。 问题1 这是一道比较常见的题目，虽然难度是medium，但是也没有那么难，这个题目主要是要求我们根据前序遍历和中序遍历构造出整棵树。 基本的思路是： 也就是说，前序遍历的第一个元素必然是整棵树的头节点，那么我在中序遍历找到头节点的位置后，就可以根据中序遍历的特点，前面的都是左子树，后面的都是右子树。找到了这一个，下面就让计算机递归去找，所以问题的关键就是第一步的缩小范围。无需关心构造树的细节。 我的解题方案是： 123456789101112131415161718192021222324252627282930313233class Solution &#123; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; //6.递归的停止条件，最后考虑，先考虑下面的一般情况 if(preorder.length == 0)&#123; return null; &#125; //1.根据前序遍历的结果，第一个元素就是树的root int rootVal = preorder[0]; //2.根据root的值去inorder中去找，题目规定这个序列是没有重复元素的 int rootIndex = 0; for(int i=0;i&lt;inorder.length;i++)&#123; if(inorder[i] == rootVal)&#123; rootIndex = i; break; &#125; &#125; //3.找到了之后，我们就可以确定root的左子树和右子树的所有元素了 TreeNode root = new TreeNode(rootVal); //4.下面就交给计算机了，我们只要考虑第一次的缩小规模，即root的左子树是什么范围，递归下去，相信它一定可以给我们一个正确的root的左子树 //这个范围的确定也是很简单的，根据前序遍历和中序遍历的关系就可以获得 //不过额外需要注意的是Arrays.copyOfRange是一个[)的结果集，需要注意以下边界 root.left = buildTree(Arrays.copyOfRange(preorder,1,1+rootIndex),Arrays.copyOfRange(inorder,0,rootIndex)); //递归下去，相信它一定可以给我们一个正确的root的右子树 root.right = buildTree(Arrays.copyOfRange(preorder,1+rootIndex,preorder.length),Arrays.copyOfRange(inorder,rootIndex+1,inorder.length)); //5.返回root，构造完毕 return root; &#125;&#125; 问题2 根据前序和后序构建的二叉树不唯一，理由是前序与后序都没有明确规定节点间的父子关系，例如下图所示： 本题比较人性化，要求只要输出其中一种可能性即可。还是可以根据一般的思路，采用递归思想，对于每一个先序序列，划分出对应的根节点、左子树、右子树范围即可自上而下构建出二叉树。 例如对于上例中的先序序列[1,2,4,5,3,6,7]，第一个节点一定为根节点，第2到第i个节点为左子树，第i+1到最后一个节点为右子树，那么问题就可以简化为：如何确定左右子树分界点？ 对于这个简化过后的问题，从后序遍历序列上很容易得到答案： 根据上图的思路，就可以写代码啦： 1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public TreeNode constructFromPrePost(int[] pre, int[] post) &#123; // if(pre.length == 0)&#123; return null; &#125; //数组还有元素，则取出第一个元素作为root TreeNode root = new TreeNode(pre[0]); //数组长度为1 的时候直接返回即可 if(pre.length == 1)&#123; return new TreeNode(pre[0]); &#125; //找到左子树根节点在后序遍历中的位置，找到之后，元素前面的都是左子树元素，后面除了最后一个元素都是右子树元素 int leftRootIndex = 0; int leftRootVal = pre[1]; for(int i=0;i&lt;post.length;i++)&#123; if(post[i] == leftRootVal)&#123; leftRootIndex = i; break; &#125; &#125; //递归构造 root.left = constructFromPrePost(Arrays.copyOfRange(pre,1,leftRootIndex+2), Arrays.copyOfRange(post,0,leftRootIndex+1)); root.right = constructFromPrePost(Arrays.copyOfRange(pre,leftRootIndex+2,pre.length), Arrays.copyOfRange(post,leftRootIndex+1,post.length-1)); return root; &#125;&#125; 问题3 到现在为止，我们对于中序排序的规则已经很熟悉，下面图示: 我们从这个图上可以看到，找下一个节点是可以分为几种情况的。 第一种情况，就是一个节点有右子树。比如要求节点B的下一个节点，其实是找到它的右子树的最左孩子，就是G节点。 第二种情况，就是一个节点没有右子树，此时又可以分为两种情况。 对于G这个节点来说，没有右子节点了，它的父亲节点是E，G是E的左子节点，即E的左子节点是G，那么G的下一个节点就是E。 对于E这个节点来说，也没有右子节点，它的父亲节点是B，此时E是B的右子节点，根据实际情况来说，E的下一个节点绝对不是B，因为E是B的右子节点，根据中序遍历的规则，此时肯定是先遍历B再遍历E，所以B肯定在E的前面，而不是后面，所以我们还需要再往上找父亲节点，此时B的父亲节点为A，B为A的左子节点，此时根据实际情况，A就是我们要找的E的下一个节点。 所以，对于一个没有右子节点的节点来说，只需要判断它有没有父节点并且是不是父节点的左子节点，是的话，就找到了，不是则要不断地向上找。 如果一直找到根还是找不到，像节点F，那就返回null，因为实际上F节点就是中序遍历的最后一个节点，没有所谓的下一个节点了。 将上面所述转换为图示为： 总之，我们不关心当前节点的左子节点，因为它不在我们的考虑范围内，它必定出现在当前节点的前面。 我们主要就是考虑有没有右子节点，或者没有右子节点的话就考虑父亲节点。有右子节点比较简单，一直找最左边的子节点即可。但是没有右子节点的时候，就需要去查询父亲节点了。理解了这些，程序也就呼之欲出了： 1234567891011121314151617181920212223242526272829public class Solution &#123; public TreeLinkNode GetNext(TreeLinkNode pNode) &#123; if(pNode == null)&#123; return null; &#125; //1.判断当前节点是否有右子节点，有则去里面找 if(pNode.right != null)&#123; return firstInRightTree(pNode); //2.没有右子节点，就需要去父节点找 &#125;else&#123; //3.直到找到符合条件的父节点为止，跳出循环时pNode的父节点符合条件，这个父节点就是我们要的东西 while(pNode.next != null &amp;&amp; pNode.next.left != pNode)&#123; pNode = pNode.next; &#125; return pNode.next; &#125; &#125; //到右子树中找符合条件的，显然就是找最最最左边的子节点即可 private TreeLinkNode firstInRightTree(TreeLinkNode pNode)&#123; TreeLinkNode curr = pNode.right; while(curr.left != null)&#123; curr = curr.left; &#125; return curr; &#125;&#125; 这里对比较常见的树的一些算法题进行了分析，关于树的题目还有很多，并且很多重要的题目也还每设计到，后面有时间整理一下leetcode上比较经典的二叉树的算法题。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法5-二分搜索树]]></title>
    <url>%2F2019%2F02%2F26%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%955-%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[数据结构中有很多树的结构，其中包括二叉树、二叉搜索树、2-3树、红黑树等等。普通的二叉树其实没什么好讲的，就是最多只有两个孩子的树，而二叉搜索树赋予了它一些额外的条件，使得它有了使用的价值，例如根据它的性质，那么中序遍历出来的结果恰好就是有序的结果，故本文着重说明二叉搜索树。 一、二叉树 二叉树是数据结构中一种重要的数据结构，也是树表家族最为基础的结构。它本身对里面的数据是没有说明要求的，只是个数要满足二叉树的每个结点至多只有二棵子树(不存在度大于2的结点)，二叉树的子树有左右之分，次序不能颠倒。二叉树的第i层至多有2的(i-1)次方个结点；深度为k的二叉树至多有2的k次方-1个结点； 二、满二叉树和完全二叉树 一张图就可以看出它们的区别了： 三、二叉搜索树概念 二叉查找树定义：又称为是二叉排序树（Binary Sort Tree）或二叉搜索树,不需要是一棵完全二叉树。具有以下性质： 若左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值； 左、右子树也分别为二叉排序树； 没有键值相等的节点。 二叉查找树的性质：对二叉查找树进行中序遍历，即可得到有序的数列。 二叉查找树的时间复杂度：它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。 二叉查找树的高度决定了二叉查找树的查找效率。 四、树的定义 就是说如果我要定义一个二叉树，那么这个Node如何定义呢？其实很简单，无非就是left,right,val这三个变量而已，也有可能是key和value这种类型，这个定义是在《算法4》上看到的，二叉搜索树判断大小的依据就是这个key.不必对这个纠结. 123456789101112131415161718192021//二分搜索树//由于Key需要能够进行比较，所以需要extends Comparable&lt;Key&gt;public class BST&lt;Key extends Comparable&lt;Key&gt;, Value&gt; &#123; // 树中的节点为私有的类, 外界不需要了解二分搜索树节点的具体实现 private class Node &#123; private Key key; private Value value; private Node left, right; public Node(Key key, Value value) &#123; this.key = key; this.value = value; left = right = null; &#125; &#125; private Node root; // 根节点 private int count; // 树种的节点个数 &#125; 五、插入新节点 查看以下动画演示了解插入新节点的算法思想：（其插入过程充分利用了二分搜索树的特性） 例如待插入数据60，首先与根元素41比较，大于根元素，则与其右孩子再进行比较，大于58由于58无右孩子，则60为58的右孩子，过程结束。（注意其递归过程） 判断node节点是否为空，为空则创建节点并将其返回（ 判断递归到底的情况）。 若不为空，则继续判断根元素的key值是否等于根元素的key值：若相等则直接更新value值即可。若不相等，则根据其大小比较在左孩子或右孩子部分继续递归直至找到合适位置为止。、 代码实现(递归实现)： 1234567891011121314151617181920212223242526272829// 向二分搜索树中插入一个新的(key, value)数据对// 返回的是最后插入完成之后二叉树的根public void insert(Key key, Value value)&#123; root = insert(root, key, value);&#125;//********************//* 二分搜索树的辅助函数//********************// 向以node为根的二分搜索树中, 插入节点(key, value), 使用递归算法// 返回插入新节点后的二分搜索树的根private Node insert(Node node, Key key, Value value)&#123; //递归的终止条件 if( node == null )&#123; count ++; return new Node(key, value); &#125; if( key.compareTo(node.key) == 0 ) node.value = value; else if( key.compareTo(node.key) &lt; 0 ) node.left = insert( node.left , key, value); else node.right = insert( node.right, key, value); return node;&#125; 六、二分搜索树的查找 123456789101112131415161718// 查看二分搜索树中是否存在键keypublic boolean contain(Key key)&#123; return contain(root, key);&#125;// 查看以node为根的二分搜索树中是否包含键值为key的节点, 使用递归算法private boolean contain(Node node, Key key)&#123; if( node == null ) return false; if( key.compareTo(node.key) == 0 ) return true; else if( key.compareTo(node.key) &lt; 0 ) return contain( node.left , key ); else // key &gt; node-&gt;key return contain( node.right , key );&#125; 七、二分搜索树的遍历 这块内容其实我想删除的，但是吧，这一段对树的前中后序遍历的动态图是非常不错的，对理解树的遍历是非常有利的，所以保留在这里。下面进入正文。 遍历分为前序遍历、中序遍历以及后序遍历三种，如何理解其遍历顺序呢？ 对于每个节点而言，可能会有左、右两个孩子，所以分成下图中3个点，每次递归过程中会经过这3个点。 前序遍历：先访问当前节点，再依次递归访问左右子树 中序遍历：先递归访问左子树，再访问自身，再递归访问右子树 后续遍历：先递归访问左右子树，再访问自身节点 下面分别来看看是如何遍历的。 7.1 前序遍历 我们注意看，先找到28的第一个点，然后将28返回，下面看有没有左儿子，有就先来到左儿子的节点，然后将16弹出… 最终的打印结果: 7.2 中序遍历 最终的打印结果: 7.3 后序遍历 最终打印结果： 7.4 递归代码实现 1234567891011121314151617181920212223242526272829// 对以node为根的二叉搜索树进行前序遍历, 递归算法private void preOrder(Node node)&#123; if( node != null )&#123; System.out.println(node.key); preOrder(node.left); preOrder(node.right); &#125;&#125;// 对以node为根的二叉搜索树进行中序遍历, 递归算法private void inOrder(Node node)&#123; if( node != null )&#123; inOrder(node.left); System.out.println(node.key); inOrder(node.right); &#125;&#125;// 对以node为根的二叉搜索树进行后序遍历, 递归算法private void postOrder(Node node)&#123; if( node != null )&#123; postOrder(node.left); postOrder(node.right); System.out.println(node.key); &#125;&#125; 针对非递归的写法，推荐用第二篇文章中说明的方式，那种方式具有较好的通用性。当然了，此时应该认识到学好递归的重要性了。 八、层序遍历 8.1 算法思想 层序遍历即一层一层地向下遍历，查看以下动画： 查看以上动画，实现其过程需要引入先进先出的“队列”数据结构，首先将28入队，第一层遍历完毕，可进行操作，将28出队并打印。遍历第二层16、30依次入队，再出队进行打印操作，依次类推。 8.2 代码实现 123456789101112131415161718// 二分搜索树的层序遍历public void levelOrder()&#123; // 我们使用LinkedList来作为我们的队列 LinkedList&lt;Node&gt; q = new LinkedList&lt;Node&gt;(); q.add(root); while( !q.isEmpty() )&#123; Node node = q.remove(); System.out.println(node.key); if( node.left != null ) q.add( node.left ); if( node.right != null ) q.add( node.right ); &#125;&#125; 九、局限性来源 它的局限性来源于哪？注意其二分搜索树的创建，如下图所示，同样的数据，可以对应不同的二分搜索树。 如上图，第一种创建情况可能是大部分人心中设想，但是第二种情况也是符合二分搜索树的特征，如此一来，二分搜索树可能退化成链表。二分搜索树的查找过程是与其高度相关，此时高度为n，时间复杂度为O(n^2)。 十、初识红黑树 其实二分搜索树的性能总体而言还是十分优异的，它所有的有关操作时间复杂度为O(n)，出现以上情况的概率很小，但如果创建时其数据都是有序的，那么就会令人担忧了。也许你会想到快速排序中也有此问题，不过它通过随机获取标志点的方法解决了此问题。 所以类似以上解决办法，将其顺序打乱再插入到二分搜索树即可？这是一个解决办法，但是需要一开始获取所有数据，其实这些数据是慢慢流入系统的，所以在创建其过程中才会发现数据是否几乎有序。 为了解决此问题，可以改造二叉树的实现，使得其无法退化成链表—–平衡二叉树，它有左右两棵子树，并且其高度差不会超过1，因此可以保证其高度一定是 logn 级别的，此概念的经典实现就是红黑树。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法4-二分查找算法]]></title>
    <url>%2F2019%2F02%2F26%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%954-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[二分查找是比较常见的查找算法，但是它需要一个条件就是数组有序，因此当面试中听到有序数组这个关键词的时候，不妨往二分查找法想一想，或许它就是解开问题的钥匙。 算法思想： 注意该算法的前提条件：有序数组。想查找元素value，先查看数组中间元素值v与value的大小，若相等则刚好，否则根据比较结果选择左、右半部分再次寻找。 时间复杂度： 整个查找过程可构成一棵树，时间复杂度为O(logn)。 问题1 给定一个有序的数组，查找value是否在数组中，不存在返回-1。 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class BinarySearch &#123; /* * arr:数组 * n:数组数据长度 * target:就是要查找的被返回的值 * while循环迭代的方式实现二分查找 */ public static int binarySearch(int arr[], int n, int target)&#123; // 在arr[l...r]之中查找target int lo = 0, hi = n-1; while( lo &lt;= hi )&#123; //int mid = (l + r)/2;防止极端情况下的整形溢出，使用下面的逻辑求出mid int mid = lo + (hi-lo)/2; if( arr[mid] == target ) return mid; if( arr[mid] &gt; target ) hi = mid - 1; else lo = mid + 1; &#125; return -1; &#125; /* * 递归的方式实现二分查找 */ public static int binarySearch2(int arr[], int lo, int hi, int target)&#123; if( lo &gt; hi ) return -1; int mid = lo + (hi-lo)/2; if( arr[mid] == target ) return mid; else if( arr[mid] &gt; target ) return binarySearch2(arr, lo, mid-1, target); else return binarySearch2(arr, mid+1, hi, target); &#125; &#125; 问题2 这就是一道经典的用二分查找解决的问题。下面给出解题答案： 12345678910111213141516171819202122232425262728293031class Solution &#123; public int[] searchRange(int[] nums, int target) &#123; int[] res = new int[2]; int left = 0; int right = nums.length-1; while(left &lt;= right)&#123; int mid = left + (right - left) / 2 ; if(nums[mid] &lt; target)&#123; left = mid + 1; &#125;else if(nums[mid] &gt; target)&#123; right = mid - 1; &#125;else&#123; //这边相等的话，就要找它周围的数字是否相等，直到找到一个区间为止 int low = mid; int high = mid; while(low - 1 &gt;= 0 &amp;&amp; nums[low-1] == target)&#123; low--; &#125; while(high + 1 &lt;= nums.length-1 &amp;&amp; nums[high+1] == target)&#123; high++; &#125; res[0] = low; res[1] = high; return res; &#125; &#125; res[0] = -1; res[1] = -1; return res; &#125;&#125; 其实我的思路很简单，就是在找到符合条件的mid之后，我就尝试在mid的两边再去找是否有相等的数字，由于是递增的数组，所以很好判断。 问题3 这是《剑指offer》上的一道题目，原本的数列一个非递减的序列，这里在中间咔了一刀变成两截，并且颠倒，那么就被划成了两段非递减的序列，并且前面的非递减数列要比后面的非递减数列要大于等于。所以，是有一定的规律的，这里还是推荐使用二分查找，只是是二分查找法的变体了。 当然了这个题目的暴力解法其实已经很简单了，就是从头开始遍历，只要出现一个数比前面一个数小，那么这个数就是原来序列的最前面的数，那么其实就是最小的数。 而二分查找在比较极端的条件下，比如元素都相等，可能就会退化为O(n)复杂度，但是如果原来的数列是一个严格递增的数列，那么还是快一点的。因为缩小的范围比较快。 12345678910111213141516171819202122232425import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; int left = 0; int right = array.length-1; while(left &lt;= right)&#123; int mid = left + (right - left) / 2; if(array[mid] &gt; array[right])&#123; //最小的元素一定在mid后面 left = mid + 1; &#125;else if(array[mid] &lt; array[right])&#123; //最小的元素在mid或者mid之前，注意这里有个坑：如果待查询的范围最后只剩两个数，那么mid 一定会指向下标靠前的数字 //比如 array = [4,6] //array[low] = 4 ;array[mid] = 4 ; array[high] = 6 ; //如果high = mid - 1，就会产生错误， 因此high = mid right = mid; &#125;else&#123; //出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1] //此时最小数字不好判断在mid左边还是右边,这时只好一个一个试 right--; &#125; &#125; return array[left]; &#125;&#125; 二分查找法最主要的注意点就是边界，一定要注意边界的选取，这直接影响了程序的实现细节。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法3-循环控制]]></title>
    <url>%2F2019%2F02%2F26%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%953-%E5%BE%AA%E7%8E%AF%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[在第一篇文章中为了说明递归如何写，所以对于链表的操作都是用递归来写的，我们发现递归写起来比较简洁，但是执行的过程有点复杂，并且往往在实际的算法中都是要将递归改成循环来做，可以一定程度上减少开销提高性能。下面我们来看看循环如何实现的。 链表的反转 需要验证准确性的话，可以去leetcode上去做这道题，题号为206.这道题还是比较经典的。代码如下： 123456789101112131415161718192021/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; ListNode newHead = null; ListNode currentNode = head; while(currentNode != null)&#123; ListNode next = currentNode.next; currentNode.next = newHead; newHead = currentNode; currentNode = next; &#125; return newHead; &#125;&#125; 一开始学的时候看的答案就是这个方法，显然是要比递归好的，但是如果不理解的话，光靠背很容易出错，并且也不大背的上，如今重温这道题，其实是很简单的，我们下面用图示来阐述。 主要的思想是用两个指针，其中newHead指向的是反转成功的链表的头部，currentHead指向的是还没有反转的链表的头部： 初始状态是newHead指向null，currentHead指向的是第一个元素，一直往后遍历直到newHead指向最后一个元素为止： 下面展示的是其中某个时间点的指向细节： 理解了上面的图示，程序就呼之欲出了。 删除链表节点 题目为：给一个数值，找到链表中这个等于这个数的所有节点并且删除。效果如下，比如给的数是2，则表示删除链表中所有为2的节点。 这个题目也是非常地经典，面试中经常会看到。我们务必要掌握。 这个其实有两种解题思路，比较简单的是增加一个虚拟的头节点。 123456789101112131415161718192021222324class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; //构造一个虚拟的头节点，指向head ListNode dummy = new ListNode(0); dummy.next = head; //用一个指针指向虚拟头节点，因为虚拟头节点还要表示去重后的链表的头节点 ListNode curr = dummy; //进入循环，看虚拟头节点下一个节点 while(curr.next != null)&#123; //如果下一个节点不为空并且值是等于val的，那么就说明要删除掉这个节点 //所谓的删除，只是改变指针，使得这个要删除的节点没有任何引用即可，java会自动回收它 if(curr.next.val == val)&#123; ListNode delNode = curr.next; curr.next = delNode.next; &#125;else&#123; //说明值不等于val，那么就后移一个即可 curr = curr.next; &#125; &#125; //返回头节点 return dummy.next; &#125;&#125; 这种实现的方式相对来说比较简单。大体的解决思路为： 另一种是比较特殊的处理方式，不需要虚拟的头节点就可以实现。 123456789101112131415161718192021222324class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; //对于比较特殊的，就是head也与val相等的情况，需要出一下 while(head != null &amp;&amp; head.val == val)&#123; head = head.next; &#125; //走到这，还需要判断一下head是否为null，因为有可能这个链表全部都等于val //那么经过上一步之后这个链表已经为null了，那么就不需要进入下一步了 if(head == null)&#123; return null; &#125; //此时链表开头的重复元素全部剔除了，下面就是普通的后续的元素，循环判断删除即可 ListNode prev = head; while(prev.next != null)&#123; if(prev.next.val == val)&#123; ListNode delNode = prev.next; prev.next = delNode.next; &#125;else&#123; prev = prev.next; &#125; &#125; return head; &#125;&#125; 没错，代码大体是相似的，特殊的处理在于一开始的节点的值与val相等的处理，所以我们需要先处理一下head以及head的后面连续的都是等于val的节点，直到处理到不为val的节点为止，即把开头相等的节点全部剔除掉，下面再继续循环判断是否相等。 关于链表的题目还有很多，由于链表数据结构比较简单，但是算法并不简单，所以面试中经常会被问道，需要好好准备一下。后面会进行相应的总结。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法2-汉诺塔问题]]></title>
    <url>%2F2019%2F02%2F26%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%952-%E6%B1%89%E8%AF%BA%E5%A1%94%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[为什么还要再来说说递归问题，因为数据结构中二叉树是比较重要也是比较难的数据结构，它的结构是天生递归的，所以对于二叉树的很多操作都可以用递归来实现，因此递归这一关能尽量理解是最好的，本章从汉诺塔的问题出发，来看看递归的实现原理。 汉诺塔问题 这个问题估计大多数人都是知道的，汉诺塔（又称河内塔）问题是源于印度一个古老传说的益智玩具。大梵天创造世界的时候做了三根金刚石柱子，在一根柱子上从下往上按照大小顺序摞着64片黄金圆盘。大梵天命令婆罗门把圆盘从下面开始按大小顺序重新摆放在另一根柱子上。并且规定，在小圆盘上不能放大圆盘，在三根柱子之间一次只能移动一个圆盘。 抽象为数学问题：如下图所示，从左到右有A、B、C三根柱子，其中A柱子上面有从小叠到大的n个圆盘，现要求将A柱子上的圆盘移到C柱子上去，期间只有一个原则：一次只能移到一个盘子且大盘子不能在小盘子上面，求移动的步骤和移动的次数。 其实核心的思想已经在上篇文章中说明了，就是数学归纳法的思想，就拿简单又不失一般性的三个盘子先说事。 其实我们发现，最核心的一个状态就是： 就是说，我们已经有了中间B这个符合条件的2个盘子的情况，那么我只需要将这两个想办法将这B上两个盘子放到C上就结束了。 同理，更多的盘子n，我就是想办法将n-1个符合条件的盘子放到第n个盘子上不就可以了。大概的思想如下： OK，下面展示程序： 123456789101112static void hanoi(int n,char A,char B,char C)&#123; if(n == 1)&#123; System.out.println(A+" -&gt; "+C); &#125;else &#123; //上面n-1个盘子从A通过C想办法移到B上，对应上图的第一行图示 hanoi(n-1,A,C,B); //将A剩下的盘子移到C上 System.out.println(A+" -&gt; "+C); //这个n-1个盘子再想办法从B通过A移到C上，对应上如的第三行图示 hanoi(n-1,B,A,C); &#125;&#125; 先拿2测试一下： 123public static void main(String[] args) &#123; hanoi(2,'A','B','C');&#125; 结果为： 123A -&gt; BA -&gt; CB -&gt; C 是符合我们的预期的。并且尝试更多的时候，按照它一步一步是正确的。 如果理解了上面说的，那么这个程序是非常好理解的。但是真正想深入进递归里面，一旦多起来还是比较复杂的。其实我们可以这样理解： 一个小朋友坐在第10排，他的作业本被小组长扔到了第1排，小朋友要拿回他的作业本，可以怎么办？他可以拍拍第9排小朋友，说：“帮我拿第1排的本子”，而第9排的小朋友可以拍拍第8排小朋友，说：“帮我拿第1排的本子”…如此下去，消息终于传到了第1排小朋友那里，于是他把本子递给第2排，第2排又递给第3排…终于，本子到手啦！这就是递归，拍拍小朋友的背可以类比函数调用，而小朋友们都记得要传消息、送本子，是因为他们有记忆力，这可以类比栈。 更严谨一些，递归蕴含的思想其实是数学归纳法：为了求解问题p（n），首先解决基础情形p（1），然后假定p（n-1）已经解决，在此基础上若p（n）得解，那所有问题均得解。这也启发我们：使用递归，切忌纠结中间步骤，因为这样做的代价是手动推理中间的若干步骤，而这些脏活，应该是计算机给我们干的! 所以理解递归还是不能太纠结具体的过程，这样只会更加地糊涂，我们注重的应该是思想以及写递归的一些注意事项，比如对于参数的确定，停止条件以及每次都要缩小范围并且都是以1这个段位缩小，不要跨段不要跨段。 递归与栈 虽然说我们不能纠结于递归的过程，但是递归与栈关系紧密，区别只是这个栈是计算机系统栈帮我们实现，而迭代是我们自己控制栈来实现，两者的基本思想都是栈，那么我们就来探讨探讨递归与栈。这个问题也是我之前面试被问过的一个问题，希望再这里能有个比较清晰的理解。 前面说过，树是一个天然递归的数据结构，这里拿二叉树的前序遍历作为分析点，并且拿最简单的三个节点的二叉树作为示例。 对于树这个数据结构暂且不多说，我们如果用递归的方式来实现前序遍历还是非常简单的。 12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; if (root != null)&#123; res.add(root.val); preorderTraversal(root.left); preorderTraversal(root.right); &#125; return res; &#125;&#125; 这个直接放到leetcode上竟然也是可以通过的，说明对这个递归答案还是认可的。不过这里想说明一下递归与栈的关系，所以需要详细说明一下它里面是如何通过系统栈来进行调用的。（前序遍历：144号，中序遍历84号，后序遍历145号）。 当然了，我们也知道递归相当于一个函数调用另一个子函数，它是自己再调用自己而已，递归借助了系统栈自己来实现的。我们这里以遍历最简单的二叉树为例： 当执行到第一个preorder的时候，此时系统栈里面已经标志一下前面两句执行完毕，还剩下遍历右孩子的操作。此时就是已经遍历了1这个节点，下面就是准备进入1的左孩子即2这个节点的遍历。 此时进入递归重新执行preorder，那么此时又将这个重新执行的函数的参数压到栈顶： 此时2这个节点已经打印出来了，此时又要进入preorder重新执行了，再将2的左孩子放进preorder进行遍历，此时为null，那么就会直接结束preorder函数，返回来继续执行，此时go 2-R还没有执行，那么就是看看2的右孩子，也是null，那么此时关于2的节点的孩子们都遍历完毕了，就会出栈，回到一开始继续执行go 1-R。同理再去遍历1的右孩子们。 我们从上面的过程中知道，系统栈会保留递归调用的时候调用方的参数以及执行情况，等递归返回的时候，就可以将现场恢复并且继续执行。我们还明确，比如对于节点1来说，系统栈的处理方式是：先count 1即打印1，然后递归访问左孩子go 1-L，最后递归访问右孩子go 1-R，那么我们也可以用栈来模拟这个过程，那么压栈的过程必然是go 1-R–go 1-L–count 1，这样根据后进先出的原则，出来的顺序正好是count 1–go 1-L–go 1-R。 根据这个思路，我们完全可以将上面的递归程序改成用栈来实现。 首先将root入栈，并且标识为go，下面进入循环判断栈是否为空，不为空则进入循环。 首先是判断如果当前节点为go，则表示进行入栈操作，这里首先演示的是前序遍历，所以入栈的顺序是右孩子(go 1-R)–左孩子(go 1-L)–自身(print 1)，此时栈中已经有了这三个信息，那么进入下一次循环，首先出栈的就是自身(print 1)，那么则打印，继续循环，此时出栈的是左孩子(go 1-L)，因为是go所以要添加左右孩子，但是它没有左右孩子，则直接将print 2压入栈中，那么下次循环就会打印出来。最后同理就是右孩子出栈，跟2一样没有左右子孩子则直接将print 3压入栈中下次循环打印出来。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Command&#123; String str; TreeNode node; public Command(String str,TreeNode node)&#123; this.str = str; this.node = node; &#125;&#125;class Solution &#123; public static List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null)&#123; return res; &#125; Stack&lt;Command&gt; stack = new Stack&lt;&gt;(); stack.push(new Command("go",root)); while (!stack.isEmpty())&#123; Command command = stack.pop(); //遇到go则按照顺序入栈 if (command.str.equals("go"))&#123; //首先压入的是当前节点的右子节点 if(command.node.right != null)&#123; stack.push(new Command("go",command.node.right)); &#125; //然后压入的是当前节点的左子节点 if(command.node.left != null)&#123; stack.push(new Command("go",command.node.left)); &#125; //最后压入的是当前节点，准备打印 stack.push(new Command("print",command.node)); &#125;else &#123; //等于`print`的节点则打印出来 System.out.println(command.node.val); res.add(command.node.val); &#125; &#125; return res; &#125;&#125; 有的小伙伴可能会发现这个写法好像跟主流的写法不大一样，后面介绍二叉树的话会介绍一下主流的写法是什么，但是这个写法是比较通用的。原因在于这个写法是真正模拟了系统栈的执行流程，思路会比较清晰一点，并且它具有通用性，如果我想改为中序遍历或者后续遍历是非常简单的，只需要简单地调整if (command.str.equals(&quot;go&quot;))里面的顺序即可。 至此，简单地昭示了系统栈的执行流程，阐明了递归与栈之间的关系，并且用自己的栈来模拟系统栈写出了非递归的版本。递归在树这种数据结构中是随处可见的，应该对它重视起来。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法1-递归入门]]></title>
    <url>%2F2019%2F02%2F25%2Falgorithms-basic%2F%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%951-%E9%80%92%E5%BD%92%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[算法入门系列以递归开头，我们知道，递归的编码往往是比较简单的，但是递归的思想往往又是难以理解。在写完这篇笔记之后仍然无法得递归之要领，不过对于如何写递归是有了一定得章法，一句话就是用数据归纳法，先尝试n得情况，再去考虑0或者1得情况，并且保证范围在逐渐缩小并且一定可以结束，下面我们来详细说一说递归。 一、什么是递归 我们可以把”递归“比喻成”查字典“，当你查一个词，发现这个词的解释中某个词仍然不懂，于是你开始查这第二个词。 可惜，第二个词里仍然有不懂的词，于是查第三个词，这样查下去，直到有一个词的解释是你完全能看懂的，那么递归走到了尽头，然后你开始后退，逐个明白之前查过的每一个词，最终，你明白了最开始那个词的意思。（摘自知乎的一个回答） 从程序本身来看，就是一个函数直接或间接调用自身的一种方法，它通常把一个大型复杂的问题层层转化为一个与原问题相似的规模较小的问题来求解。 我们这里以计算阶乘为切入点： 1234567int Factorial(int n)&#123; if (n == 0) return 1; return n * Factorial(n - 1);&#125; 我们以上述代码为例，取 n=3，则过程如下： 第 1~4 步，都是入栈过程，Factorial(3)调用了Factorial(2)，Factorial(2)又接着调用Factorial(1)，直到Factorial(0)； 第 5 步，因 0 是递归结束条件，故不再入栈，此时栈高度为 4，即为我们平时所说的递归深度； 第 6~9 步，Factorial(0)做完，出栈，而Factorial(0)做完意味着Factorial(1)也做完，同样进行出栈，重复下去，直到所有的都出栈完毕，递归结束。 可以看出来，递归的本质就是由一个系统栈不停地保存每一层调用的方法及其参数，直到遇到终止条件为止，一层一层地结束返回，但是当层数过深的时候就有可能出现stack overflow这样的栈溢出错误。 也可以看出来，每一个递归程序都可以把它改写为非递归版本。但是并不是每个递归程序都是那么容易被改写为非递归的。某些递归程序比较复杂，其入栈和出栈非常繁琐，给编码带来了很大难度，而且易读性极差，所以条件允许的情况下，推荐使用递归。 二、如何思考递归 在初学递归的时候, 看到一个递归实现, 我们总是难免陷入不停的验证之中，比如上面提及的阶乘，求解Factorial(n)时，我们总会情不自禁的发问，Factorial(n-1)可以求出正确的答案么？接着我们就会再用Factorial(n-2)去验证，，，不停地往下验证直到Factorial(0)。 对递归这样的不适应，和我们平时习惯的思维方式有关。我们习惯的思维是：已知Factorial(0)，乘上 1 就等于Factorial(1)，再乘以 2 就等于Factorial(2)，，，直到乘到 n。 因此，递归和我们的思维方式正好相反。这就会给我们编程造成相当大的思维干扰。 其实，递归的数学思想是数学归纳法： 如果下面这两点是成立的，我们就知道这个递归对于所有的 n 都是正确的。 1）当 n=0,1 时，结果正确； 2）假设递归对于 n 是正确的，同时对于 n+1 也正确。 在递归中，我们通常把第 1 点称为终止条件，因为这样更容易理解，其作用就是终止递归，防止递归无限地运行下去。 对于第二点就是假定如果n-1的情况是正确的，那么n的情况就是正确的，然后再假定n-2的情况是正确的，那么n-1的情况也是正确的，那么就会一直推导到特殊情况比如0的时候，这个时候是正确的，那么前面所有的都是正确的。 从而达到了上面说的，将一个复杂的问题一层一层地转化为相似的小规模的问题，这样，解决了小规模问题之后一层一层地就可以返回来求出复杂的问题。 根据数学归纳法，其实我们还可以归纳出编写递归程序的一些准则： 严格定义递归函数作用，包括参数、返回值、side-effect 先一般再特殊 每次调用必须缩小问题规模 每次问题规模缩小程度必须为1 这里简单地再解释一下，第一条中注意的是side-effect，这些是一些存储状态的变量，比如一些全局的变量来控制递归里面的一些逻辑等等。 考虑问题的时候可以从特殊的问题来考虑，但是在编写递归程序的时候，最好是先考虑一般的场景，最后再来看特殊的场景从而终止递归。并且每次缩小程度为1，不能为2或者3等。 好了，其实递归真正说起来好像也就那回事，但是真正用好确实是需要大量的训练，即递归的思维训练。 三、递归的方式创建单向链表 首先定义一下链表类： 1234567891011121314151617181920212223242526272829303132public class Node &#123; private final int val; private Node next; public Node(int val)&#123; this.val = val; next = null; &#125; public int getVal() &#123; return val; &#125; public Node getNext() &#123; return next; &#125; public void setNext(Node next) &#123; this.next = next; &#125; //打印链表 public static void print(Node head)&#123; while (head != null)&#123; System.out.print(head.val+"-&gt;"); head = head.next; &#125; System.out.print("null"); System.out.println(""); System.out.println("======================================"); &#125;&#125; 假设有一个数组：1，2，3，4，5，目标是将它们转换为链表。 正常的思维是：创建节点node1，node2，然后node1的next指向node2，依次类推直到最后。但是用递归的时候就不要这么想了，我们的思维方式变为： 假设2，3，4，5已经组装好了，那么我只需要再将1插到这个组装好的链表的最前面即可： 那么，首先我们确定函数参数，返回值等： 123public Node createLinkedList(List&lt;Integer&gt; values)&#123; &#125; 那么，首先，我取出1作为firstNode，然后我这个1指向的是后面已经排好的(2-&gt;3-&gt;4-&gt;5)这个链表，此时不就成功了嘛！ 12345678910public Node createLinkedList(List&lt;Integer&gt; values)&#123; //获取数组第一个元素 Node firstNode = new Node(values.get(0)); //获取后面排好的链表的头节点 Node fisrtSubNode = createLinkedList(values.subList(1,values.size())); //第一个元素指向后面排好的链表的头节点即可 firstNode.setNext(fisrtSubNode); //返回整个链表的头节点 return firstNode;&#125; 此时，我们只要知道：createLinkedList(values.subList(1,values.size()));这个就可以帮助我们拼接成我们需要的(2-&gt;3-&gt;4-&gt;5)这个链表，下面我只需要将1指向这个链表就可以成功了。我们仔细来看这个函数，就是调用自身，即递归，里面参数是：除了第一个元素外剩余的元素，并且每一层就剔除掉第一个元素，那么此时还需要一个终止条件，就是这个数组已经没有元素了就停止。所以完整的程序是： 1234567891011121314public Node createLinkedList(List&lt;Integer&gt; values)&#123; //5.递归停止的条件就是没有元素了 if (values.size() == 0)&#123; return null; &#125; //1.获取数组第一个元素 Node firstNode = new Node(values.get(0)); //2.获取后面排好的链表的头节点，用subList来截取，逐渐地截短使得问题的规模变小 Node fisrtSubNode = createLinkedList(values.subList(1,values.size())); //3.第一个元素指向后面排好的链表的头节点即可 firstNode.setNext(fisrtSubNode); //4.返回整个链表的头节点 return firstNode;&#125; 我们可以想象，一层一层地剔除第一个元素，过程是(2-&gt;3-&gt;4-&gt;5)，(3-&gt;4-&gt;5)，(4-&gt;5)，（5)，()，此时为空了停止，就会返回到上一层执行的地方，即Node fisrtSubNode = null这一句，此时，上一层的firstNode就是5，那么继续执行就是5.setNext(null)，return 5，这一层又结束了返回到上一层，上一层此时firstNode=4，那么fisrtSubNode = 5，然后继续执行就是4.setNext(5)，最后return 4，此时链表已经是4-&gt;5-&gt;null了，再重复以上的过程直到2-&gt;3-&gt;4-&gt;5-&gt;null，此时回到最上层，即一开始的地方，即假定1后面就是拼接好的链表，此时确实也是拼接好的。所以这里用的就是数学归纳法的思想。 四、递归的方式反转单向链表 链表的反转效果为： 我们跟上面一样考虑，考虑一般的情况，那就是1后面的所有元素已经全部反转好了，初始状态为： 此时假定后面全部反转好了： 下面只需要将2指向1，并且1指向null即可： 123456789public Node reverseLinkedList(Node head)&#123; if(head == null || head.getNext() == null)&#123; return head; &#125; Node newHead = reverseLinkedList(head.getNext()); head.getNext().setNext(head); head.setNext(null); return newHead;&#125; 六、递归存在的问题 比如计算斐波那契数列： 1234567public int fib(int index)&#123; if(index==1||index==2)&#123; return 1; &#125;else&#123; return fib(index-1)+fib(index-2); &#125;&#125; 在这个递归里做了冗余的工作，如图，我们在f4里面已经计算了f2，可是f3里有同样计算了f2，以此类推那些冗余的工作，在数值比较小的情况下，计算机还是可以接受的。但是，当求解的数值比较大，它是成指数级增长的，所以不要再递归中做重复的工作。 下一节通过经典得汉诺塔问题再来探讨一下递归的思想。]]></content>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记12-分布式锁]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B012-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[分布式锁在分布式系统中是非常常见的，redis以及ZK都可以实现分布式锁，在文章Curator从实战的层面进行了实际的分布式锁的实现，具体看这个文章即可。下面是再唠叨唠叨。 redis实现分布式锁 之前，在redis实现分布式锁实现过一个基于redis的分布式锁，用来保证一个系统去定时关单。 这上面两个例子，一个是针对多个系统修改同一个资源，一个是面临高并发下订单时控制同一时间只能有一个用户拿到锁然后下订单（可能是多个系统，比如是订单服务器和库存服务器，当然，为了解耦和提高速度，那么可以把写订单表这个逻辑用MQ异步出去），首先我要判断库存是不是真的够，那么这个时候我就要用分布式锁控制，防止两个用户同时来查库存数量然后都觉得自己可以下订单（极端情况，库存只有1，那么这两个用户同时查到为1，那么都认为自己可以下订单咋办？），其他的用户必须等到这个用户释放锁或者超时才可以再拿到锁再去执行操作。这样，有效地解决了商品的超卖问题。 优点：实现简单，吞吐量十分客观，对于高并发情况应付自如，自带超时保护，对于网络抖动的情况也可以利用超时删除策略保证不会阻塞所有流程。但是redis存在一些问题： 单点问题：因为redis一般都是单实例使用，那么对于单点问题，可以做一个主从。当然主从切换的时候也是不可用的，因为主从同步是异步的，可能会并发问题。如果对于主从还是不能保证可靠性的话，可以上Redis集群，对于Redis集群，因为使用了类一致性Hash算法，虽然不能避免节点下线的并发问题(当前的任务没有执行完，其他任务就开始执行)，但是能保证Redis是可用的。可用性的问题是出了问题之后的备选方案，如果我们系统天天都出问题还玩毛啊，对于突发情况牺牲一两个请求还是没问题的。 锁删除失败：分布式锁基本都有这个问题，可以对key设置失效时间。这个超时时间需要把控好，过大那么系统吞吐量低，很容易导致超时。如果过小那么会有并发问题，部分耗时时间比较长的任务就要遭殃了。 redis集群的同步策略是需要时间的，有可能A线程setNX成功后拿到锁，但是这个值还没有更新到B线程执行setNX的这台服务器，那就会产生并发问题。 zookeeper实现分布式锁 Zookeeper是一个分布式一致性协调框架，主要可以实现选主、配置管理和分布式锁等常用功能，因为Zookeeper的写入都是顺序的，在一个节点创建之后，其他请求再次创建便会失败，同时可以对这个节点进行Watch，如果节点删除会通知其他节点抢占锁。 Zookeeper实现分布式锁虽然是比较重量级的，但实现的锁功能十分健全，由于Zookeeper本身需要维护自己的一致性，所以性能上较Redis还是有一定差距的。 “惊群”就是在一个节点删除的时候，大量对这个节点的删除动作有订阅Watcher的线程会进行回调，这对Zk集群是十分不利的。所以需要避免这种现象的发生。 为了解决“惊群“问题，我们需要放弃订阅一个节点的策略，那么怎么做呢？详细看这里：https://www.jianshu.com/p/5d12a01018e1 最后想说明一点，其实对于Zookeeper的一些常用功能是有一些成熟的包实现的，像Curator。Curator的确是足够牛逼，不仅封装了Zookeeper的常用API，也包装了很多常用Case的实现。形如： 123456789101112InterProcessMutex lock = new InterProcessMutex(client, lockPath);if ( lock.acquire(maxWait, waitUnit) ) &#123; try &#123; // do some work inside of the critical section here &#125; finally &#123; lock.release(); &#125;&#125; 具体的时间在分布式电商项目-码码购中用Curator实现分布式锁实现了某些场景的需求。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记11-Apache Curator客户端的使用（二）]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B011-Apache%20Curator%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文来继续来看Apache Curator客户端的使用！ zk-watcher应用实例之模拟统一更新N台节点的配置文件 zookeeper有一个比较常见的应用场景就是统一管理、更新分布式集群环境中每个节点的配置文件，我们可以在代码中监听集群中的节点，当节点数据发生改变时就同步到其他节点上。如下图： 因为我们使用的json作为节点存储的数据格式，所以需要准备一个工具类来做json与pojo对象的一个转换，也就是所谓的反序列化。创建一个 JsonUtils 类，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class JsonUtils &#123; // 定义jackson对象 private static final ObjectMapper MAPPER = new ObjectMapper(); /** * 将对象转换成json字符串。 * &lt;p&gt;Title: pojoToJson&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @param data * @return */ public static String objectToJson(Object data) &#123; try &#123; String string = MAPPER.writeValueAsString(data); return string; &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 将json结果集转化为对象 * * @param jsonData json数据 * @param beanType 对象中的object类型 * @return */ public static &lt;T&gt; T jsonToPojo(String jsonData, Class&lt;T&gt; beanType) &#123; try &#123; T t = MAPPER.readValue(jsonData, beanType); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 将json数据转换成pojo对象list * &lt;p&gt;Title: jsonToList&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * @param jsonData * @param beanType * @return */ public static &lt;T&gt;List&lt;T&gt; jsonToList(String jsonData, Class&lt;T&gt; beanType) &#123; JavaType javaType = MAPPER.getTypeFactory().constructParametricType(List.class, beanType); try &#123; List&lt;T&gt; list = MAPPER.readValue(jsonData, javaType); return list; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 需要额外的依赖： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt;&lt;/dependency&gt; 然后创建客户端类，客户端类就是用来监听集群中的节点的。由于是模拟，所以这里的部分代码是伪代码。客户端类我们这里创建了三个，因为集群中有三个节点，由于代码基本上是一样的，每个客户端分别监听watch事件，所以这里只贴出客户端_1的代码。如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class Client_1 &#123; public CuratorFramework client = null; public static final String zkServerIp = "192.168.190.128:2181"; // 初始化重连策略以及客户端对象并启动 public Client_1() &#123; RetryPolicy retryPolicy = new RetryNTimes(3, 5000); client = CuratorFrameworkFactory.builder() .connectString(zkServerIp) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace("workspace").build(); client.start(); &#125; // 关闭客户端 public void closeZKClient() &#123; if (client != null) &#123; this.client.close(); &#125; &#125; // public final static String CONFIG_NODE = "/super/testNode/redis-config"; public final static String CONFIG_NODE_PATH = "/super/testNode"; public final static String SUB_PATH = "/redis-config"; public static CountDownLatch countDown = new CountDownLatch(1); // 计数器 public static void main(String[] args) throws Exception &#123; Client_1 cto = new Client_1(); System.out.println("client1 启动成功..."); // 开启子节点缓存 final PathChildrenCache childrenCache = new PathChildrenCache(cto.client, CONFIG_NODE_PATH, true); childrenCache.start(StartMode.BUILD_INITIAL_CACHE); // 添加子节点监听事件 childrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; // 监听节点的数据更新事件 if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_UPDATED)) &#123; String configNodePath = event.getData().getPath(); if (configNodePath.equals(CONFIG_NODE_PATH + SUB_PATH)) &#123; System.out.println("监听到配置发生变化，节点路径为:" + configNodePath); // 读取节点数据 String jsonConfig = new String(event.getData().getData()); System.out.println("节点" + CONFIG_NODE_PATH + "的数据为: " + jsonConfig); // 从json转换配置 RedisConfig redisConfig = null; if (StringUtils.isNotBlank(jsonConfig)) &#123; redisConfig = JsonUtils.jsonToPojo(jsonConfig, RedisConfig.class); &#125; // 配置不为空则进行相应操作 if (redisConfig != null) &#123; String type = redisConfig.getType(); String url = redisConfig.getUrl(); String remark = redisConfig.getRemark(); // 判断事件 if (type.equals("add")) &#123; System.out.println("\n-------------------\n"); System.out.println("监听到新增的配置，准备下载..."); // ... 连接ftp服务器，根据url找到相应的配置 Thread.sleep(500); System.out.println("开始下载新的配置文件，下载路径为&lt;" + url + "&gt;"); // ... 下载配置到你指定的目录 Thread.sleep(1000); System.out.println("下载成功，已经添加到项目中"); // ... 拷贝文件到项目目录 &#125; else if (type.equals("update")) &#123; System.out.println("\n-------------------\n"); System.out.println("监听到更新的配置，准备下载..."); // ... 连接ftp服务器，根据url找到相应的配置 Thread.sleep(500); System.out.println("开始下载配置文件，下载路径为&lt;" + url + "&gt;"); // ... 下载配置到你指定的目录 Thread.sleep(1000); System.out.println("下载成功..."); System.out.println("删除项目中原配置文件..."); Thread.sleep(100); // ... 删除原文件 System.out.println("拷贝配置文件到项目目录..."); // ... 拷贝文件到项目目录 &#125; else if (type.equals("delete")) &#123; System.out.println("\n-------------------\n"); System.out.println("监听到需要删除配置"); System.out.println("删除项目中原配置文件..."); &#125; // TODO 视情况统一重启服务 &#125; &#125; &#125; &#125; &#125;); countDown.await(); cto.closeZKClient(); &#125;&#125; 完成以上代码的编写后，将所有的客户类都运行起来。然后到zookeeper服务器上，进行如下操作： 12345678910111213141516171819202122232425262728293031323334353637[zk: localhost:2181(CONNECTED) 14] set /workspace/super/testNode/redis-config &#123;&quot;type&quot;:&quot;add&quot;,&quot;url&quot;:&quot;ftp://192.168.10.123/config/redis.xml&quot;,&quot;remark&quot;:&quot;add&quot;&#125;cZxid = 0xc00000039ctime = Mon Apr 30 01:43:47 CST 2018mZxid = 0xc00000043mtime = Mon Apr 30 01:52:35 CST 2018pZxid = 0xc00000039cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 75numChildren = 0[zk: localhost:2181(CONNECTED) 15] set /workspace/super/testNode/redis-config &#123;&quot;type&quot;:&quot;update&quot;,&quot;url&quot;:&quot;ftp://192.168.10.123/config/redis.xml&quot;,&quot;remark&quot;:&quot;update&quot;&#125;cZxid = 0xc00000039ctime = Mon Apr 30 01:43:47 CST 2018mZxid = 0xc00000044mtime = Mon Apr 30 01:53:46 CST 2018pZxid = 0xc00000039cversion = 0dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 81numChildren = 0[zk: localhost:2181(CONNECTED) 16] set /workspace/super/testNode/redis-config &#123;&quot;type&quot;:&quot;delete&quot;,&quot;url&quot;:&quot;&quot;,&quot;remark&quot;:&quot;delete&quot;&#125; cZxid = 0xc00000039 ctime = Mon Apr 30 01:43:47 CST 2018mZxid = 0xc00000045mtime = Mon Apr 30 01:54:06 CST 2018pZxid = 0xc00000039cversion = 0dataVersion = 3aclVersion = 0ephemeralOwner = 0x0dataLength = 44numChildren = 0[zk: localhost:2181(CONNECTED) 17] 如上，从三个客户端的控制台输出信息可以看到，三个节点都进行了同样操作，触发了同样的watch事件，这样就可以完成统一的配置文件管理。 curator之acl权限操作与认证授权 我们先演示在创建节点时设置acl权限，现在/workspace/super只有如下节点： 12[zk: localhost:2181(CONNECTED) 27] ls /workspace/super[xxxnode, testNode] 然后新建一个 CuratorAcl 类，关于acl权限的概念以及部分API代码都在之前的zk原生API使用一文中介绍过了，所以这里就不赘述了。编写代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class CuratorAcl &#123; // Curator客户端 public CuratorFramework client = null; // 集群模式则是多个ip private static final String zkServerIps = "192.168.190.128:2181,192.168.190.129:2181,192.168.190.130:2181"; public CuratorAcl() &#123; RetryPolicy retryPolicy = new RetryNTimes(3, 5000); client = CuratorFrameworkFactory.builder().authorization("digest", "user1:123456a".getBytes()) // 认证授权，登录用户 .connectString(zkServerIps) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace("workspace").build(); client.start(); &#125; public void closeZKClient() &#123; if (client != null) &#123; this.client.close(); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 实例化 CuratorAcl cto = new CuratorAcl(); boolean isZkCuratorStarted = cto.client.isStarted(); System.out.println("当前客户的状态：" + (isZkCuratorStarted ? "连接中" : "已关闭")); String nodePath = "/super/testAclNode/testOne"; // 自定义权限列表 List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); Id user1 = new Id("digest", AclUtils.getDigestUserPwd("user1:123456a")); Id user2 = new Id("digest", AclUtils.getDigestUserPwd("user2:123456b")); acls.add(new ACL(ZooDefs.Perms.ALL, user1)); acls.add(new ACL(ZooDefs.Perms.READ, user2)); acls.add(new ACL(ZooDefs.Perms.DELETE | ZooDefs.Perms.CREATE, user2)); // 创建节点，使用自定义权限列表来设置节点的acl权限 byte[] nodeData = "child-data".getBytes(); cto.client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).withACL(acls).forPath(nodePath, nodeData); cto.closeZKClient(); boolean isZkCuratorStarted2 = cto.client.isStarted(); System.out.println("当前客户的状态：" + (isZkCuratorStarted2 ? "连接中" : "已关闭")); &#125;&#125; 运行该类，然后到zookeeper服务器上，通过命令行进行如下操作： 123456789101112[zk: localhost:2181(CONNECTED) 19] ls /workspace/super/testAclNode [testOne][zk: localhost:2181(CONNECTED) 20] getAcl /workspace/super/testAclNode&apos;world,&apos;anyone: cdrwa[zk: localhost:2181(CONNECTED) 21] getAcl /workspace/super/testAclNode/testOne&apos;digest,&apos;user1:TQYTqd46qVVbWpOd02tLO5qb+JM=: cdrwa&apos;digest,&apos;user2:CV4ED0rE6SxA3h/DN/WyScDMbCs=: r&apos;digest,&apos;user2:CV4ED0rE6SxA3h/DN/WyScDMbCs=: cd 如上，可以看到，创建的全部节点的acl权限都是我们设置的自定义权限。 最后我们再来演示如何修改一个已存在的节点的acl权限，修改 CuratorAcl 类中的main方法代码如下： 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 实例化 CuratorAcl cto = new CuratorAcl(); boolean isZkCuratorStarted = cto.client.isStarted(); System.out.println("当前客户的状态：" + (isZkCuratorStarted ? "连接中" : "已关闭")); String nodePath = "/super/testAclNodeTwo/testOne"; // 自定义权限列表 List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); Id user1 = new Id("digest", AclUtils.getDigestUserPwd("user1:123456a")); Id user2 = new Id("digest", AclUtils.getDigestUserPwd("user2:123456b")); acls.add(new ACL(ZooDefs.Perms.READ | ZooDefs.Perms.CREATE | ZooDefs.Perms.ADMIN, user1)); acls.add(new ACL(ZooDefs.Perms.READ | ZooDefs.Perms.DELETE | ZooDefs.Perms.CREATE, user2)); // 设置指定节点的acl权限 cto.client.setACL().withACL(acls).forPath(nodePath); cto.closeZKClient(); boolean isZkCuratorStarted2 = cto.client.isStarted(); System.out.println("当前客户的状态：" + (isZkCuratorStarted2 ? "连接中" : "已关闭"));&#125; 运行该类，然后到zookeeper服务器上，通过命令行进行如下操作： 123456[zk: localhost:2181(CONNECTED) 31] getAcl /workspace/super/testAclNodeTwo/testOne&apos;digest,&apos;user1:TQYTqd46qVVbWpOd02tLO5qb+JM=: cra&apos;digest,&apos;user2:CV4ED0rE6SxA3h/DN/WyScDMbCs=: cdr[zk: localhost:2181(CONNECTED) 32] 可以看到，成功修改了该节点的acl权限。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记10-Apache Curator客户端的使用（一）]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B010-Apache%20Curator%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文来看看Apache Curator客户端的使用吧！ 一、前言 对于上一章中应用的java 原生API来操作节点。来看看他的不足： 超时重连，不支持自动，需要手动操作 watcher注册一次后会失效 不支持递归创建节点 对于Apache Curator开源客户端，具有以下的优点： Apache的开源项目，值得信赖 解决watcher的注册一次就失效的问题 API更加简单易用 提供更多解决方案并且实现简单，比如分布式锁 提供常用的zookeeper工具类 编程风格更爽 本篇文章为上半部分，主要学习了一下自动重连、创建节点、查询节点数据和子节点、删除和修改节点数据。还有就是用nodeCache以及PathChildrenCache缓存节点数据来解决注册一次就失效的问题。 二、使用 首先新建一个maven工程，我这里直接新建了一个SpringBoot工程，依赖： 12345678910111213141516&lt;!--curator--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.11&lt;/version&gt;&lt;/dependency&gt; 三、连接&amp;自动重连 配置完依赖后，我们就可以来写一个简单的demo测试与zookeeper服务端的连接。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class CuratorConnect &#123; public CuratorFramework client = null; private static final String zkServerPath = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"; public CuratorConnect()&#123; /** * （推荐） * 同步创建zk示例，原生api是异步的 * 这一步是设置重连策略 * * 构造器参数： * curator链接zookeeper的策略:ExponentialBackoffRetry * baseSleepTimeMs：初始sleep的时间 * maxRetries：最大重试次数 * maxSleepMs：最大重试时间 */ RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,5); client = CuratorFrameworkFactory.builder() .connectString(zkServerPath) .sessionTimeoutMs(10*1000) .retryPolicy(retryPolicy) .build(); client.start(); &#125; private void closeZKClient()&#123; if(client != null)&#123; client.close(); &#125; &#125; public static void main(String[] args) &#123; CuratorConnect curatorConnect = new CuratorConnect(); boolean isZkClientStart = curatorConnect.client.isStarted(); System.out.println("客户端是否打开:"+isZkClientStart); curatorConnect.closeZKClient(); isZkClientStart = curatorConnect.client.isStarted(); System.out.println("客户端是否打开:"+isZkClientStart); &#125;&#125; curator连接zookeeper服务器时有自动重连机制，而curator的重连策略有五种。 第一种就是上面提到的： 123456789101112/** * （推荐） * 同步创建zk示例，原生api是异步的 * 这一步是设置重连策略 * * 构造器参数： * curator链接zookeeper的策略:ExponentialBackoffRetry * baseSleepTimeMs：初始sleep的时间 * maxRetries：最大重试次数 * maxSleepMs：最大重试时间 */RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 5); 第二种，可设定重连n次： 123456789/** * （推荐） * curator链接zookeeper的策略:RetryNTimes * * 构造器参数： * n：重试的次数 * sleepMsBetweenRetries：每次重试间隔的时间 */RetryPolicy retryPolicy = new RetryNTimes(3, 5000); 第三种，只会重连一次： 123456789/** * （不推荐） * curator链接zookeeper的策略:RetryOneTime * * 构造器参数： * sleepMsBetweenRetry:每次重试间隔的时间 * 这个策略只会重试一次 */RetryPolicy retryPolicy2 = new RetryOneTime(3000); 第四种，永远重连： 1234/** * 永远重试，不推荐使用 */RetryPolicy retryPolicy3 = new RetryForever(retryIntervalMs) 第五种，可设定最大重试时间： 123456789/** * curator链接zookeeper的策略:RetryUntilElapsed * * 构造器参数： * maxElapsedTimeMs:最大重试时间 * sleepMsBetweenRetries:每次重试间隔 * 重试时间超过maxElapsedTimeMs后，就不再重试 */RetryPolicy retryPolicy4 = new RetryUntilElapsed(2000, 3000); 四、zk命名空间以及创建节点 zookeeper的命名空间就类似于我们平时使用Eclipse等开发工具的工作空间一样，我们该连接中所有的操作都是基于这个命名空间的。curator提供了设置命名空间的方法，这样我们任何的连接都可以去设置一个命名空间。设置了命名空间并成功连接后，zookeeper的根节点会多出一个以命名空间名称所命名的节点。然后我们在该连接的增删查改等操作都会在这个节点中进行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CuratorCreateNode &#123; // Curator客户端 public CuratorFramework client = null; // 集群模式则是多个ip private static final String zkServerIps = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"; public CuratorCreateNode() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 5); // 实例化Curator客户端 client = CuratorFrameworkFactory.builder() // 使用工厂类来建造客户端的实例对象 .connectString(zkServerIps) // 放入zookeeper服务器ip .sessionTimeoutMs(10000).retryPolicy(retryPolicy) // 设定会话时间以及重连策略 .namespace("workspace").build(); // 设置命名空间以及开始建立连接 // 启动Curator客户端 client.start(); &#125; // 关闭zk客户端连接 private void closeZKClient() &#123; if (client != null) &#123; this.client.close(); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 实例化 CuratorCreateNode curatorConnect = new CuratorCreateNode(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 创建节点 String nodePath = "/super/testNode"; // 节点路径 byte[] data = "this is a test data".getBytes(); // 节点数据 String result = curatorConnect.client.create().creatingParentsIfNeeded() // 创建父节点，也就是会递归创建 .withMode(CreateMode.PERSISTENT) // 节点类型 .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) // 节点的acl权限 .forPath(nodePath, data); System.out.println(result + "节点，创建成功..."); Thread.sleep(1000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 运行该类，控制台输出信息如下： 123当前客户端的状态：连接中.../super/testNode节点，创建成功...当前客户端的状态：已关闭... 五、修改节点以及删除节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class CuratorConnect &#123; // Curator客户端 public CuratorFramework client = null; // 集群模式则是多个ip private static final String zkServerIps = "192.168.190.128:2181,192.168.190.129:2181,192.168.190.130:2181"; public CuratorConnect() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 5); // 实例化Curator客户端 client = CuratorFrameworkFactory.builder() // 使用工厂类来建造客户端的实例对象 .connectString(zkServerIps) // 放入zookeeper服务器ip .sessionTimeoutMs(10000).retryPolicy(retryPolicy) // 设定会话时间以及重连策略 .namespace("workspace").build(); // 设置命名空间以及开始建立连接 // 启动Curator客户端 client.start(); &#125; // 关闭zk客户端连接 private void closeZKClient() &#123; if (client != null) &#123; this.client.close(); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 节点路径 String nodePath = "/super/testNode"; // 更新节点数据 byte[] newData = "this is a new data".getBytes(); Stat resultStat = curatorConnect.client.setData().withVersion(0) // 指定数据版本 .forPath(nodePath, newData); // 需要修改的节点路径以及新数据 System.out.println("更新节点数据成功，新的数据版本为：" + resultStat.getVersion()); // 删除节点 curatorConnect.client.delete() .guaranteed() // 如果删除失败，那么在后端还是会继续删除，直到成功 .deletingChildrenIfNeeded() // 子节点也一并删除，也就是会递归删除 .withVersion(resultStat.getVersion()) .forPath(nodePath); Thread.sleep(1000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 六、查询节点相关信息 1.获取某个节点的数据 1234567891011121314151617181920212223242526272829...public class CuratorConnect &#123; ... public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 节点路径 String nodePath = "/super/testNode"; // 读取节点数据 Stat stat = new Stat(); byte[] nodeData = curatorConnect.client.getData().storingStatIn(stat).forPath(nodePath); System.out.println("节点 " + nodePath + " 的数据为：" + new String(nodeData)); System.out.println("该节点的数据版本号为：" + stat.getVersion()); Thread.sleep(1000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 2.获取某个节点下的子节点列表，现有一个节点的子节点列表如下： 123[zk: localhost:2181(CONNECTED) 33] ls /workspace/super/testNode[threeNode, twoNode, oneNode][zk: localhost:2181(CONNECTED) 34] 123456789101112131415161718192021222324252627282930...public class CuratorConnect &#123; ... public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 节点路径 String nodePath = "/super/testNode"; // 获取子节点列表 List&lt;String&gt; childNodes = curatorConnect.client.getChildren().forPath(nodePath); System.out.println(nodePath + " 节点下的子节点列表："); for (String childNode : childNodes) &#123; System.out.println(childNode); &#125; Thread.sleep(1000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 3.查询某个节点是否存在 12345678910111213141516171819202122232425262728293031...public class CuratorConnect &#123; ... public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 节点路径 String nodePath = "/super/testNode"; // 查询某个节点是否存在，存在就会返回该节点的状态信息，如果不存在的话则返回空 Stat statExist = curatorConnect.client.checkExists().forPath(nodePath); if (statExist == null) &#123; System.out.println(nodePath + " 节点不存在"); &#125; else &#123; System.out.println(nodePath + " 节点存在"); &#125; Thread.sleep(1000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 七、curator之usingWatcher curator在注册watch事件上，提供了一个usingWatcher方法，使用这个方法注册的watch事件和默认watch事件一样，监听只会触发一次，监听完毕后就会销毁，也就是一次性的。而这个方法有两种参数可选，一个是zk原生API的Watcher接口的实现类，另一个是Curator提供的CuratorWatcher接口的实现类，不过在usingWatcher方法上使用哪一个效果都是一样的，都是一次性的。 新建一个 MyWatcher 实现类，实现 Watcher 接口。代码如下： 12345678910111213/** * @program: zookeeper-connection * @description: zk原生API的Watcher接口实现 * @author: 01 * @create: 2018-04-28 13:41 **/public class MyWatcher implements Watcher &#123; // Watcher事件通知方法 public void process(WatchedEvent watchedEvent) &#123; System.out.println("触发watcher，节点路径为：" + watchedEvent.getPath()); &#125;&#125; 新建一个 MyCuratorWatcher 实现类，实现 CuratorWatcher 接口。代码如下： 12345678910111213/** * @program: zookeeper-connection * @description: Curator提供的CuratorWatcher接口实现 * @author: 01 * @create: 2018-04-28 13:40 **/public class MyCuratorWatcher implements CuratorWatcher &#123; // Watcher事件通知方法 public void process(WatchedEvent watchedEvent) throws Exception &#123; System.out.println("触发watcher，节点路径为：" + watchedEvent.getPath()); &#125;&#125; 1234567891011121314151617181920212223242526public class CuratorConnect &#123; ... public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 节点路径 String nodePath = "/super/testNode"; // 添加 watcher 事件，当使用usingWatcher的时候，监听只会触发一次，监听完毕后就销毁 curatorConnect.client.getData().usingWatcher(new MyCuratorWatcher()).forPath(nodePath); // curatorConnect.client.getData().usingWatcher(new MyWatcher()).forPath(nodePath); Thread.sleep(100000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 运行该类，然后到zookeeper服务器上修改/super/testNode节点的数据： 12345678910111213[zk: localhost:2181(CONNECTED) 35] set /workspace/super/testNode new-datacZxid = 0xb00000015ctime = Sat Apr 28 20:59:57 CST 2018mZxid = 0xb0000002bmtime = Sat Apr 28 21:40:58 CST 2018pZxid = 0xb0000001ccversion = 3dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 8numChildren = 3[zk: localhost:2181(CONNECTED) 36] 修改完成后，此时控制台输出内容如下，因为workspace是命名空间节点，所以不会被打印出来： 1触发watcher，节点路径为：/super/testNode 八、curator之nodeCache一次注册N次监听 想要实现watch一次注册n次监听的话，我们需要使用到curator里的一个NodeCache对象。这个对象可以用来缓存节点数据，并且可以给节点添加nodeChange事件，当节点的数据发生变化就会触发这个事件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051...public class CuratorConnect &#123; ... public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 节点路径 String nodePath = "/super/testNode"; // NodeCache: 缓存节点，并且可以监听数据节点的变更，会触发事件 final NodeCache nodeCache = new NodeCache(curatorConnect.client, nodePath); // 参数 buildInitial : 初始化的时候获取node的值并且缓存 nodeCache.start(true); // 获取缓存里的节点初始化数据 if (nodeCache.getCurrentData() != null) &#123; System.out.println("节点初始化数据为：" + new String(nodeCache.getCurrentData().getData())); &#125; else &#123; System.out.println("节点初始化数据为空..."); &#125; // 为缓存的节点添加watcher，或者说添加监听器 nodeCache.getListenable().addListener(new NodeCacheListener() &#123; // 节点数据change事件的通知方法 public void nodeChanged() throws Exception &#123; // 防止节点被删除时发生错误 if (nodeCache.getCurrentData() == null) &#123; System.out.println("获取节点数据异常，无法获取当前缓存的节点数据，可能该节点已被删除"); return; &#125; // 获取节点最新的数据 String data = new String(nodeCache.getCurrentData().getData()); System.out.println(nodeCache.getCurrentData().getPath() + " 节点的数据发生变化，最新的数据为：" + data); &#125; &#125;); Thread.sleep(200000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125; 运行该类后，我们到zookeeper服务器上，对/super/testNode节点进行如下操作： 12345678910111213141516171819202122232425262728[zk: localhost:2181(CONNECTED) 2] set /workspace/super/testNode change-data cZxid = 0xb00000015ctime = Sat Apr 28 20:59:57 CST 2018mZxid = 0xb00000037mtime = Sat Apr 28 23:49:42 CST 2018pZxid = 0xb0000001ccversion = 3dataVersion = 6aclVersion = 0ephemeralOwner = 0x0dataLength = 11numChildren = 3 [zk: localhost:2181(CONNECTED) 3] set /workspace/super/testNode change-agin-datacZxid = 0xb00000015ctime = Sat Apr 28 20:59:57 CST 2018mZxid = 0xb00000038mtime = Sat Apr 28 23:50:01 CST 2018pZxid = 0xb0000001ccversion = 3dataVersion = 7aclVersion = 0ephemeralOwner = 0x0dataLength = 16numChildren = 3[zk: localhost:2181(CONNECTED) 8] delete /workspace/super/testNode[zk: localhost:2181(CONNECTED) 9] create /workspace/super/testNode test-dataCreated /workspace/super/testNode[zk: localhost:2181(CONNECTED) 10] 此时控制台输出内容如下： 1234567当前客户端的状态：连接中...节点初始化数据为：new-data/super/testNode 节点的数据发生变化，最新的数据为：change-data/super/testNode 节点的数据发生变化，最新的数据为：change-agin-data获取节点数据异常，无法获取当前缓存的节点数据，可能该节点已被删除/super/testNode 节点的数据发生变化，最新的数据为：test-data当前客户端的状态：已关闭... 从控制台输出的内容可以看到，只要数据发生改变了都会触发这个事件，并且是可以重复触发的，而不是一次性的。 九、curator之PathChildrenCache子节点监听 使用NodeCache虽然能实现一次注册n次监听，但是却只能监听一个nodeChanged事件，也就是说创建、删除以及子节点的事件都无法监听。如果我们要监听某一个节点的子节点的事件，或者监听某一个特定节点的增删改事件都需要借助PathChildrenCache来实现。从名称上可以看到，PathChildrenCache也是用缓存实现的，并且也是一次注册n次监听。当我们传递一个节点路径时是监听该节点下的子节点事件，如果我们要限制监听某一个节点，只需要加上判断条件即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364...public class CuratorConnect &#123; ... public static void main(String[] args) throws Exception &#123; // 实例化 CuratorConnect curatorConnect = new CuratorConnect(); // 获取当前客户端的状态 boolean isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); // 父节点路径 String nodePath = "/super/testNode"; // 为子节点添加watcher // PathChildrenCache: 监听数据节点的增删改，可以设置触发的事件 final PathChildrenCache childrenCache = new PathChildrenCache(curatorConnect.client, nodePath, true); /** * StartMode: 初始化方式 * POST_INITIALIZED_EVENT：异步初始化，初始化之后会触发事件 * NORMAL：异步初始化 * BUILD_INITIAL_CACHE：同步初始化 */ childrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE); // 列出子节点数据列表，需要使用BUILD_INITIAL_CACHE同步初始化模式才能获得，异步是获取不到的 List&lt;ChildData&gt; childDataList = childrenCache.getCurrentData(); System.out.println("当前节点的子节点详细数据列表："); for (ChildData childData : childDataList) &#123; System.out.println("\t* 子节点路径：" + new String(childData.getPath()) + "，该节点的数据为：" + new String(childData.getData())); &#125; // 添加事件监听器 childrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent event) throws Exception &#123; // 通过判断event type的方式来实现不同事件的触发 if (event.getType().equals(PathChildrenCacheEvent.Type.INITIALIZED)) &#123; // 子节点初始化时触发 System.out.println("\n--------------\n"); System.out.println("子节点初始化成功"); &#125; else if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_ADDED)) &#123; // 添加子节点时触发 System.out.println("\n--------------\n"); System.out.print("子节点：" + event.getData().getPath() + " 添加成功，"); System.out.println("该子节点的数据为：" + new String(event.getData().getData())); &#125; else if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_REMOVED)) &#123; // 删除子节点时触发 System.out.println("\n--------------\n"); System.out.println("子节点：" + event.getData().getPath() + " 删除成功"); &#125; else if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_UPDATED)) &#123; // 修改子节点数据时触发 System.out.println("\n--------------\n"); System.out.print("子节点：" + event.getData().getPath() + " 数据更新成功，"); System.out.println("子节点：" + event.getData().getPath() + " 新的数据为：" + new String(event.getData().getData())); &#125; &#125; &#125;); Thread.sleep(200000); // 关闭客户端 curatorConnect.closeZKClient(); // 获取当前客户端的状态 isZkCuratorStarted = curatorConnect.client.isStarted(); System.out.println("当前客户端的状态：" + (isZkCuratorStarted ? "连接中..." : "已关闭...")); &#125;&#125;]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记9-原生Java API使用]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B09-%E5%8E%9F%E7%94%9FJava%20API%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文来介绍原生的API来操作ZK节点，从而引出下节要说明的Curator客户端。 环境准备 新建一个普通的java项目即可，然后引入一些jar包的依赖： 代码在code-for-chapter9 客户端与ZK建立连接 123456789101112131415161718192021222324252627282930313233343536373839404142import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;public class ZKConnect implements Watcher &#123; public static void main(String[] args) throws IOException &#123; String serverPath = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"; /** * serverPath:可以是一个ip，也可以是多个ip * sessionTimeout：超时时间，心跳收不到，就超时 * watcher：通知事件，如果有对应的事件发生触发，则客户端会收到通知 * canBeReadOnly：可读，当这个物理机节点断开以后，还是可以读到数据的，只是不能写 * 此时数据被读取到的可能是旧数据，此处建议设置为false * sessionId：会话的ID * sessionPassword：会话密码 当会话消失以后，可以一句sessionId和sessionPasswd重新获取会话 */ ZooKeeper zk = new ZooKeeper(serverPath,5*1000,new ZKConnect()); System.out.println("客户端开始连接zk...，连接状态为:"+zk.getState()); /** * 休息一段时间，保证让节点状态看到 */ try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("连接状态为:"+zk.getState()); &#125; @Override public void process(WatchedEvent watchedEvent) &#123; System.out.println("接受到的watch通知："+watchedEvent); &#125;&#125; 会话重连机制 主要的思路就是上一个程序注释中所描述:用上一次连接的sessionId和sessionPasswd这两个参数代入到下次连接中，就可以重新获取上一次的连接了。 下面就是不断地看seesionId来判断是否为同一个连接。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ZKConnectSessionWatcher implements Watcher &#123; public static void main(String[] args) throws IOException &#123; String serverPath = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"; long sessionId = 999L; byte[] sessionPasswd = null; /** * 第一次连接 */ ZooKeeper zk = new ZooKeeper(serverPath,5*1000,new ZKConnectSessionWatcher()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //如果状态时已连接了，就获取sessionId if (zk.getState().equals(ZooKeeper.States.CONNECTED))&#123; sessionId = zk.getSessionId(); System.out.println(sessionId); String ssid = "0x" + Long.toHexString(sessionId); //对sessionId经过64位编码之后的值，也就是dump命令查出来的sessionId值 System.out.println(ssid); sessionPasswd = zk.getSessionPasswd(); &#125; /** * 第2次连接，会话重连，那么用sessionID和password来进行来重新获取连接 */ System.out.println("会话重连..."); ZooKeeper zkSession = new ZooKeeper(serverPath,5*1000,new ZKConnectSessionWatcher(),sessionId,sessionPasswd); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("重连之后的sessionId为："+zkSession.getSessionId()); &#125; @Override public void process(WatchedEvent watchedEvent) &#123; System.out.println("接受到的watch通知："+watchedEvent); &#125;&#125; 节点的增删改查 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121public class ZKNodeOperator implements Watcher &#123; final static String serverPath = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"; private ZooKeeper zooKeeper = null; public ZooKeeper getZooKeeper()&#123; return zooKeeper; &#125; public void setZooKeeper(ZooKeeper zooKeeper)&#123; this.zooKeeper = zooKeeper; &#125; public ZKNodeOperator()&#123;&#125; public ZKNodeOperator(String connectionString,int sessionTimeout)&#123; try &#123; zooKeeper = new ZooKeeper(connectionString,sessionTimeout,new ZKNodeOperator()); &#125; catch (IOException e) &#123; e.printStackTrace(); if(zooKeeper != null)&#123; try &#123; zooKeeper.close(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; ZKNodeOperator zkServer = new ZKNodeOperator(serverPath,5*1000); //1. 同步创建节点 /** * zkServer.syncCreateZkNode(path,data,acls); * path：节点路径 * data：节点数据 * acls：节点权限，有Id ANYONE_ID_UNSAFE = new Id("world", "anyone");和Id AUTH_IDS = new Id("auth", ""); */ /* try &#123; zkServer.getZooKeeper().create("/testNode1","123".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125;*/ //2. 异步创建节点 /** * path：节点路径 * data：节点数据 * acls：权限 * mode：持久类型还是其他类型 * callback：异步的回调函数 * ctx：回调内容 */ /* String ctx = "&#123;'create':'success'&#125;"; zkServer.getZooKeeper().create("/testNode2","456".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT,new CreateCallBack(),ctx); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ //3. 同步修改节点 /* try &#123; zkServer.getZooKeeper().setData("/testNode1","update123".getBytes(),0);//版本必须要对 Thread.sleep(2000); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ //4. 异步修改节点 /* String ctx = "&#123;'update':'success'&#125;"; zkServer.getZooKeeper().setData("/testNode1","update123456".getBytes(),1,new UpdateCallBack(),ctx); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ //5. 同步删除节点 /* try &#123; zkServer.getZooKeeper(). delete("/testNode1",2); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125;*/ //6. 异步删除数据 String ctx = "&#123;'delete':'success'&#125;"; zkServer.getZooKeeper().delete("/testNode1",0,new DeleteCallBack(),ctx); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void process(WatchedEvent watchedEvent) &#123; System.out.println("watch被触发..."+watchedEvent); &#125;&#125; countDownLatch 上面学习了对于节点的增删改，还差一个查，这里先学习一下countDownLatch： demo的场景是：有一个监控中心，监控很多地方的调度中心的情况，每检查一个，就返回一个状态，直到所有的调度中心都检查完。 代码在文件夹countdownlatchdemo. 这个玩意就是一个计数器。引入这个玩意，是为了配合下面的案例，使得线程能挂起，我们可以测试数据变化一下，然后触发watcher,拿到变化后的值，然后主线程执行结束。 获取父节点数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class ZKGetNodeData implements Watcher &#123; private ZooKeeper zooKeeper; final static String serverPath = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"; private static Stat stat = new Stat(); private static CountDownLatch countDownLatch = new CountDownLatch(1); public ZKGetNodeData()&#123;&#125; public ZKGetNodeData(String connectionString,int sessionTimeout)&#123; try &#123; zooKeeper = new ZooKeeper(connectionString,sessionTimeout,new ZKGetNodeData()); &#125; catch (IOException e) &#123; e.printStackTrace(); if(zooKeeper != null)&#123; try &#123; zooKeeper.close(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException, KeeperException &#123; ZKGetNodeData zkServer = new ZKGetNodeData(serverPath,5*1000); byte[] resByte = zkServer.getZooKeeper().getData("/hello",true,stat); String result = new String(resByte); System.out.println("当前值:"+result); countDownLatch.await(); &#125; @Override public void process(WatchedEvent event) &#123; try &#123; if(event.getType() == Event.EventType.NodeDataChanged)&#123; ZKGetNodeData zkServer = new ZKGetNodeData(serverPath,5*1000); byte[] resByte = zkServer.getZooKeeper().getData("/hello",true,stat); String result = new String(resByte); System.out.println("更改后的值："+result); System.out.println("版本："+stat.getVersion()); countDownLatch.countDown(); &#125;else if(event.getType() == Event.EventType.NodeCreated)&#123; &#125;else if(event.getType() == Event.EventType.NodeChildrenChanged)&#123; &#125;else if(event.getType() == Event.EventType.NodeDeleted)&#123; &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; public ZooKeeper getZooKeeper() &#123; return zooKeeper; &#125; public void setZooKeeper(ZooKeeper zooKeeper) &#123; this.zooKeeper = zooKeeper; &#125;&#125; 获取子节点数据 基本同上，见代码ZKGetChildrenList 判断节点是否存在 基本同上，见代码ZKNodeExist。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记8-典型应用场景详解]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B08-%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本章对于zk的应用场景和解决方案进行详细的介绍。 一、从应用程序的角度看ZooKeeper ZNode之间的层级关系就像文件系统的目录结构一样，但和传统的磁盘文件系统不同的是全量数据都存储在内存中，以此来实现提高服务器吞吐、减少延迟的目的，从这一点来说应用只应该存储控制信息和配置信息到ZNode，而不应该用它来存储大量数据。 ZNode可以分为持久节点和临时节点两类。持久节点是指一旦该ZNode被创建了，除非主动进行删除操作，这个节点就会一直存在；而临时节点的生命周期会和客户端会话绑定在一起，一旦客户端会话失效其所创建的所有临时节点都会被删除。 ZK还支持客户端创建节点时指定一个特殊的SEQUENTIAL属性，这个节点被创建的时候ZK会自动在其节点名后面追加上一个整形数字，该数字是一个由服务端维护的自增数字，以此实现创建名称自增的顺序节点。 二、监听器Watcher Watcher是ZK中很重要的特性，ZK允许用户在指定节点上注册一些Watcher，在该节点相关特定事件（比如节点添加、删除、子节点变更等）发生时Watcher会监听到，ZK服务端会将事件通知到感兴趣的客户端上去，该机制是ZK实现分布式协调服务的重要特性。 通知的时候服务端只会告诉客户端一个简单的事件（通知状态、事件类型、节点路径）而不包含具体的变化信息（如原始数据及变更后的数据），客户端如要具体信息再次主动去重新获取数据；此外，无论是服务端还是客户端，只要Watcher被触发ZK就会将其删除，因此在Watcher的使用上需要反复注册，这样轻量的设计有效减轻了服务端压力，如果Watcher一直有效，节点更新频繁时服务端会不断向客户端发送通知，对网络及服务端性能影响会非常大。 三、典型应用场景 3.1 数据发布/订阅（以Dubbo注册中心为例） Dubbo是阿里集团开源的分布式服务框架，致力于提供高性能和透明化的远程服务调用解决方案和基于服务框架展开的完整SOA服务治理方案。 其中服务自动发现是最核心的模块之一，该模块提供基于注册中心的目录服务，使服务消费方能够动态的查找服务提供方，让服务地址透明化，同时服务提供方可以平滑的对机器进行扩容和缩容，其注册中心可以基于提供的外部接口来实现各种不同类型的注册中心，例如数据库、ZK和Redis等。接下来看一下基于ZK实现的Dubbo注册中心。 /dubbo: 这是Dubbo在ZK上创建的根节点。 /dubbo/com.foo.BarService: 这是服务节点，代表了Dubbo的一个服务。 /dubbo/com.foo.BarService/Providers: 这是服务提供者的根节点，其子节点代表了每个服务的真正提供者。 /dubbo/com.foo.BarService/Consumers: 这是服务消费者的根节点，其子节点代表了没一个服务的真正消费者 Dubbo基于ZK实现注册中心的工作流程： 服务提供者：在初始化启动的时候首先在/dubbo/com.foo.BarService/Providers节点下创建一个子节点，同时写入自己的url地址，代表这个服务的一个提供者。 服务消费者：在启动的时候读取并订阅ZooKeeper上/dubbo/com.foo.BarService/Providers节点下的所有子节点，并解析所有提供者的url地址类作为该服务的地址列表，开始发起正常调用。同时在Consumers节点下创建一个临时节点，写入自己的url地址，代表自己是BarService的一个消费者 监控中心：监控中心是Dubbo服务治理体系的重要一部分，它需要知道一个服务的所有提供者和订阅者及变化情况。监控中心在启动的时候会通过ZK的/dubbo/com.foo.BarService节点来获取所有提供者和消费者的url地址，并注册Watcher来监听其子节点变化情况 所有服务提供者在ZK上创建的节点都是临时节点，利用的是临时节点的生命周期和客户端会话绑定的特性，一旦提供者机器挂掉无法对外提供服务时该临时节点就会从ZK上摘除，这样服务消费者和监控中心都能感知到服务提供者的变化。 3.2 命名服务 命名服务也是分布式系统中比较常见的一类场景，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象，其中较为常见的是一些分布式服务框架中的服务地址列表，通过使用命名服务客户端应用能够指定名字来获取资源的实体、服务地址和提供者的信息等。 3.3 Master选举 当前系统的mater节点挂了怎么办？那就要重新选出一个master来。 ZK创建节点时有一个重要的特性，利用ZK的强一致性能够很好的保证在分布式高并发情况下节点的创建一定能够保证全局唯一，即ZK会保证客户端无法重复创建一个已经存在的数据节点。也就是说同时有多个客户端请求创建同一个节点最终一定只有一个客户端能够请求创建成功，利用这个特性就能很容易的在分布式环境中进行Master选举了。 进行Master选举时客户端启动后可以向ZK请求创建一个临时节点，例如/master_election/master。在多个客户端创建时只有一个能创建成功，那么这个创建成功的客户端所在的机器就成为了Master。 同时其他没有创建成功的客户端都可以在节点/master_election上注册一个子节点变更的Watcher来监控当前Master是否在线，一旦发现Master挂了临时节点会被删除，其它客户端会收到通知，开始重新进行Master选举。 3.4 分布式锁 分布式锁是控制分布式系统之间同步访问共享资源的一种方式。如果不同系统或同一系统不同机器之间共享了同一资源，那访问这些资源时通常需要一些互斥手段来保证一致性，这种情况下就需要用到分布式锁了。 接下来看下使用ZK如何实现排他锁。排他锁的核心是如何保证当前有且只有一个事务获得锁，并且锁被释放后所有等待获取锁的事务能够被通知到。 和Master选举类似，在需要获取排他锁时，所有客户端都会试图在/exclusive_lock下创建临时子节点/exclusive_lock/lock，最终只有一个客户端能创建成功，该客户端就获取到了锁。 同时没有获取到锁的客户端需要到/exclusive_lock节点上注册一个子节点变更的Watcher监听，用于实时监听lock节点的变更情况。 /exclusive_lock/lock是一个临时节点，在一下两种情况下都有可能释放锁： 当获取锁的客户端挂掉，ZK上的该节点会被删除 正常执行完业务逻辑之后客户端会主动将自己创建的临时节点删除。 无论在什么情况下删除了lock临时节点ZK都会通知在/exclusive_lock节点上注册了子节点变更Watcher监听的客户端，重新发起锁的获取。 3.5 分布式屏障 分布式屏障，举个栗子，在大规模分布式并行计算的场景下，最终的合并计算需要基于很多并行计算的子结果来进行，即系统需要满足特定的条件，一个队列的元素必须都聚齐之后才能进行后续处理，否则一直等待。看下如何用ZK来支持这种场景。 开始时/queue_barrier是一个存在的节点，数据内容赋值为一个数字n来代表满足条件的机器总数，例如n=10表示只有当/queue_barrier节点下的子节点数量达到10后才会打开屏障继续处理。 然后所有的客户端都会到/queue_barrier节点下创建一个临时节点，如/queue_barrier/192.168.0.1。创建完节点之后根据以下步骤来确定执行顺序: 1、调用获取节点数据的api获取/queue_barrier节点的内容：10 2、调用获取子节点总数的api获取/queue_barrier下的所有子节点，并且注册对子节点变更的Watcher监听 3、统计子节点个数 4、如果子节点个数小于10则继续等待，否则打开屏障继续处理 5、接收到Watcher通知后，重复步骤2]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记7-ZK的基本操作以及权限控制]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B07-ZK%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[说了那么久的理论，但是其实对于大多数人来说，会用才是王道，在会用的基础上再去深入探讨原理可能是最佳的实践方式，但是对于笔记来说，先原理再实践可能更为稳妥，本文介绍zk最基本的一些操作，比如查看节点、增加一个节点、Watch机制以及ACL权限控制等基本命令，最后还介绍了一下为了方便而产生的四字命令。 基本数据模型 树形结构，每个节点成为znode，它可以有子节点，也可以有数据 临时节点和永久节点，临时节点在客户端断开后消失 每个zk节点都有各自的版本号，可以通过命令行来显示节点信息 每当节点数据发生变化，那么该节点的版本号会累加（乐观锁） 删除/修改过时节点，版本号不匹配则会报错 每个zk节点存储的数据不宜过大，几k即可 节点可以设置acl，可以通过权限来限制用户的访问 zk的作用 master选举，保证集群是高可用的 统一配置文件管理，即只需要部署一台服务器，则可以把相同的配置文件同步更新到其他所有服务器 发布与订阅，dubbo发布者把数据存在znode上，订阅者可以读取这个数据 分布式锁 集群管理，集群中保证数据的强一致性 zk的基本操作 ls / 显示根节点名称 ls2 / 显示了根节点的状态信息（stat也可以看状态） get / 拿出节点的数据和信息 create [-s] [-e] path data acl 创建节点，如果是默认创建，则是非顺序的、 持久的节点。加上-e则是临时节点；加上-s表示顺序节点 【注1】：如果是持久节点，状态信息中的ephemeralOwner=0x0；临时节点的这个属性，是后面一串比较长的字符 【注2】：客户端断开连接了，一段时间之后，那么临时节点就会消失（主要是有个时效，超出这个时间还不收到来自客户端的心跳包则才认定客户端挂了） 【注3】：在加上-s后，创建的节点会重命名为一个累加的名称 set path newData 每次修改值后dataVersion数据版本号会增1 【注4】：如何实现乐观锁？set path data version,就是说带上版本号，如果这个版本不对应，那么就修改失败 delete path version 删除节点 watcher机制 针对每个节点的操作，都会有一个监督者watcher 当监控的某个对象(znode)发生了变化，则触发watcher事件 zk中watcher是一次性的，触发后立即销毁（用其他的开源客户端开源让其不会销毁，重复触发） 父节点以及他的子孙们的 增 删 改 都能够触发其watcher 针对不同类型的事件，触发的watcher事件也不同： （子）节点创建事件 （子）节点删除事件 （子）节点数据变化事件 通过get path [watch]或者stat path [watch]或者ls path [watch]都可以设置watcher 父节点 增 删 改 操作触发watcher 子节点 增 删 改 操作触发watcher 【创建父节点触发】：NodeCreated 12345678[zk: localhost:2181(CONNECTED) 24] stat /hello watch Node does not exist: /hello[zk: localhost:2181(CONNECTED) 25] create /hello worldWATCHER::Created /helloWatchedEvent state:SyncConnected type:NodeCreated path:/hello 【修改父节点数据触发】：NodeDataChanged 12345678910[zk: localhost:2181(CONNECTED) 26] stat /hello watch[zk: localhost:2181(CONNECTED) 27] get /helloworld[zk: localhost:2181(CONNECTED) 28] set /hello lalalaWATCHER::cZxid = 0x300000011WatchedEvent state:SyncConnected type:NodeDataChanged path:/helloctime = Sat Dec 08 20:00:53 CST 2018 【删除父节点触发】：NodeDeleted 1234567[zk: localhost:2181(CONNECTED) 32] get /hello watch[zk: localhost:2181(CONNECTED) 33] delete /helloWATCHER::[zk: localhost:2181(CONNECTED) 34]WatchedEvent state:SyncConnected type:NodeDeleted path:/hello 【创建子节点触发】：ls为父节点设置watcher，创建子节点触发NodeChildrenChanged 123456789101112[zk: localhost:2181(CONNECTED) 52] ls /[zookeeper][zk: localhost:2181(CONNECTED) 53] create /hello worldCreated /hello[zk: localhost:2181(CONNECTED) 54] ls /hello watch[][zk: localhost:2181(CONNECTED) 55] create /hello/helloson worldsonWATCHER::Created /hello/hellosonWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/hello 【删除子节点触发】：ls为父节点设置watcher，删除子节点触发NodeChildrenChanged 123456789[zk: localhost:2181(CONNECTED) 56] ls /hello[helloson][zk: localhost:2181(CONNECTED) 57] ls /hello watch[helloson][zk: localhost:2181(CONNECTED) 58] delete /hello/hellosonWATCHER::[zk: localhost:2181(CONNECTED) 59]WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/hello 【更新子节点触发】：ls为父节点设置watcher，更新子节点不触发事件 1234567891011121314151617181920[zk: localhost:2181(CONNECTED) 59] ls /hello[][zk: localhost:2181(CONNECTED) 60] create /hello/helloson worldsonCreated /hello/helloson[zk: localhost:2181(CONNECTED) 61] ls /hello watch [helloson][zk: localhost:2181(CONNECTED) 62] set /hello/helloson worldsonhahahacZxid = 0x300000020ctime = Sat Dec 08 20:15:05 CST 2018mZxid = 0x300000021mtime = Sat Dec 08 20:16:06 CST 2018pZxid = 0x300000020cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 14numChildren = 0[zk: localhost:2181(CONNECTED) 63] get /hello/hellosonworldsonhahaha 就算是设置成ls /hello/helloson watch也不行。只有这样才可以触发watcher: 123456789[zk: localhost:2181(CONNECTED) 72] get /swg/swgson watch8080[zk: localhost:2181(CONNECTED) 74] set /swg/swgson 7070WATCHER::cZxid = 0x300000024WatchedEvent state:SyncConnected type:NodeDataChanged path:/swg/swgsonctime = Sat Dec 08 20:18:53 CST 2018 为什么更新子节点的时候ls不行，但是get就行呢？客户端要想收到更新子节点内容的消息的话，不能通过子节点的事件来触发，必须把子节点当作父节点来做。然而ls path [watch] ：查询指定路径下的子节点所以是针对的子节点，所以不能触发这个事件；而get path [watch] : 查询指定节点中的数据，如果节点中有数据或者stat path [watch] : 查询指定节点的一些描述信息这些直接是操作这个节点，把这个节点当作是父节点，所以能起作用。 watcher使用场景 Watcher是ZK中很重要的特性，ZK允许用户在指定节点上注册一些Watcher，在该节点相关特定事件（比如节点添加、删除、子节点变更等）发生时Watcher会监听到，ZK服务端会将事件通知到感兴趣的客户端上去，该机制是ZK实现分布式协调服务的重要特性。 通知的时候服务端只会告诉客户端一个简单的事件（通知状态、事件类型、节点路径）而不包含具体的变化信息（如原始数据及变更后的数据），客户端如要具体信息再次主动去重新获取数据；此外，无论是服务端还是客户端，只要Watcher被触发ZK就会将其删除，因此在Watcher的使用上需要反复注册，这样轻量的设计有效减轻了服务端压力，如果Watcher一直有效，节点更新频繁时服务端会不断向客户端发送通知，对网络及服务端性能影响会非常大。 比如统一资源配置。 ACL 权限控制列表 针对节点可以设置相关读写等权限，目的是为了保障数据安全性 权限permissions可以指定不同的权限范围以及角色 getAcl：获取某个节点的acl权限信息 setAcl：设置某个节点的acl权限信息 addauth：注册某个用户，要把某个用户的用户名和密码输入到系统中进行注册，用户才能登陆。 默认权限： 123[zk: localhost:2181(CONNECTED) 76] getAcl /swg&apos;world,&apos;anyone: cdrwa ACL构成：zk的acl通过[scheme🆔permissions]来构成权限列表，其中scheme指采用的某种权限机制；id指允许访问的用户；permissions指权限组合字符串 scheme：主要是四种 world：world下只有一个id，即只有一个用户，也就是anyone，那么组合的写法就是world:anyone:[permissions] auth：代表认证登陆，需要注册用户有权限就可以，形式为auth:user:password:[permissions],密码是明文 degest：需要对密码加密才能访问，组合形式为digest:username:BASE64(SHA(password)):[permissions]，密码是加密的 ip：当设置为ip指定的ip地址，此时限制ip进行访问，比如ip:192.168.1.1:[permissions] super：代表超级管理员，拥有所有权限 id：代表允许访问的用户 permissions： c：create,创建当前节点的子节点权限 r：read,获取当前节点或者子节点列表 w：write,设置当前节点的数据 d：delete，删除子节点 a：admin，是比较高的权限，可以去设置和修改权限，即拥有分配权限的权限 world:anyone:cdrwa：对于默认权限，我们可以修改他的权限字符串，如setAcl path world:anyone:crwa auth和digest：先addauth digest username:password（明文密码）注册用户,然后setAcl path auth:username:password（明文密码）:cdrwa就可以设置ACL了。再去getAcl path查询到的密码时加密后的。 digest：要先退出刚才的auth的账号，直接重启当前客户端即可。setAcl path digest:username:password（密文密码）:cdrwa。再去getAcl path查询到的密码时加密后的。此时访问、删除、创建节点比如get path是需要登陆的，即先addauth digest username:password（明文密码）登陆。 ip：setAcl path ip:192.168.1.1:cdrwa super：最高权限，修改zkServer.sh增加super管理员，重启zkServer.sh。到bin目录下修改zkServer.sh增加配置： 找到这一行： 1nohup $JAVA &quot;-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;&quot; &quot;-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;&quot; 在后面继续添加： 1&quot;-Dzookeeper.DigestAuthenticationProvider.superDigest=username:xQJmxLMiHGwaqBvst5y6rkB6HQs=&quot; ACL常用使用场景 开发/测试环境分离，开发者无权限操作测试库的节点，只能看。比如分为开发节点和测试节点。 生产环境上控制指定ip的服务可以访问相关节点防止混乱 zk四字命令 zk可以通过它自身提供的简写命令来和服务器进行交互 需要使用到 nc 命令，yum install nc echo [command] | nc [ip] [port] 【stat】查看zk的状态信息，以及是单机还是集群状态：echo stat | nc ip或者localhost 2181 【ruok】查看当前zkServer是否启动，正常返回imok：echo ruok | nc ip 2181 【dump】列出未经处理的会话和临时节点：echo dump | nc ip 2181 【conf】查看服务配置 【cons】展示连接到服务器的客户端信息 【envi】环境变量，显示jdk和zk等环境变量的信息 【mntr】监控zk健康信息 【wchs】展示watch的信息 【wchc】与【wchp】：分别展示session与watch及path与watcher的信息，默认这两个命令是不能访问的，需要将他们列入白名单才行]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记6-zk安装和集群搭建]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B06-zk%E5%AE%89%E8%A3%85%E5%92%8C%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[本章会回顾一下ZAB协议，然后介绍一下集群搭建的配置文件。 前言 回顾一下ZAB协议。Zab协议包括两个核心： 第一，原子广播 客户端提交事务请求时Leader节点为每一个请求生成一个事务Proposal，将其发送给集群中所有的Follower节点，收到过半Follower的反馈后开始对事务进行提交。 这也导致了Leader崩溃后可能会出现数据不一致的情况，ZAB使用了崩溃恢复来处理数字不一致问题； 消息广播使用了TCP协议进行通讯所有保证了接受和发送事务的顺序性。广播消息时Leader节点为每个事务Proposal分配一个全局递增的ZXID（事务ID），每个事务Proposal都按照ZXID顺序来处理； Leader节点为每一个Follower节点分配一个队列按事务ZXID顺序放入到队列中，且根据队列的规则FIFO来进行事务的发送。 Follower节点收到事务Proposal后会将该事务以事务日志方式写入到本地磁盘中，成功后反馈Ack消息给Leader节点。 收到过半ACK反馈之后，同时向所有的Follower节点广播Commit消息，Follower节点收到Commit后开始对事务进行提交； 第二，Master选举 里面用的算法叫做：Fast Leader Election。 epoch：选举轮数,即周期，就是之前说的逻辑时钟logicClock Zxid: Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增的计数器，针对客户端每一个事务请求，计数器加 1；而高 32 位则代表 Leader 周期 epoch 的编号，每个当选产生一个新的 Leader 服务器，就会从这个 Leader 服务器上取出其本地日志中最大事务的ZXID，并从中读取 epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。 成为leader的条件 选epoch最大的 epoch相等，选 zxid 最大的 epoch和zxid都相等，选择server id最大的（就是我们配置zoo.cfg中的myid） 选举的基本步骤 每个从节点都向其他节点发送选自身为Leader的Vote投票请求，等待回复； 从节点接受到的Vote如果比自身的大（ZXID更新）时则投票，并更新自身的Vote，否则拒绝投票； 每个从节点中维护着一个投票记录表，当某个节点收到过半的投票时，结束投票并把该从节点选为Leader，投票结束； 具体一点的步骤： 1、发起一轮投票选举，推举自己作为leader，通知所有的服务器，等待接收外部选票。 2、只要当前服务器状态为LOOKING，进入循环，不断地读取其它Server发来的通知、进行比较、更新自己的投票、发送自己的投票、统计投票结果，直到leader选出或出错退出。具体做法： 2.1 如果发送过来的逻辑时钟大于目前的逻辑时钟，那么说明这是更新的一次选举投票，此时更新本机的逻辑时钟（logicalclock），清空投票箱（因为已经过期没有用了）调用totalOrderPredicate函数判断对方的投票是否优于当前的投票（见下面代码），是的话用对方推荐的leader更新下一次的投票，否则使用初始的投票（投自己），调用sendNotifications() 通知所有服务器我的选择，跳到2.4。 2.2 如果对方处于上轮投票，不予理睬，回到2。 2.3 如果对方也处于本轮投票，调用totalOrderPredicate函数判断对方的投票是否优于当前的投票，是的话更新当前的投票，否则使用初始的投票（投自己）并新生成notification消息放入发送队列。调用sendNotifications() 通知所有服务器我的选择。 2.4 将收到的投票放入自己的投票箱中。 2.5 调用计票器的containsQuorum函数，判断所推荐的leader是否得到集群多数人的同意，如果得到多数人同意，那么还需等待一段时间，看是否有比当前更优的提议，如果没有，则认为投票结束。根据投票结果修改自己的状态。以上任何一条不满足，则继续循环。 关于totalOrderPredicate: 123456789101112131415161718192021protected boolean totalOrderPredicate(long newId, long newZxid, long newEpoch, long curId, long curZxid, long curEpoch) &#123; LOG.debug("id: " + newId + ", proposed id: " + curId + ", zxid: 0x" + Long.toHexString(newZxid) + ", proposed zxid: 0x" + Long.toHexString(curZxid)); // 使用计票器判断当前server的权重是否为0 if(self.getQuorumVerifier().getWeight(newId) == 0)&#123; return false; &#125; // 通过Epoch、zxid、id来比较两个候选leader return ((newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId))))); &#125; 总结起来就是一句话：若干个节点，第一次都是投给自己；后面就是，尽量向数据最新的节点靠拢，可以理解为：每个节点贫富有差距，富有的节点让贫穷的节点投自己一票，那么贫穷的节点会接受，反之不行，那么最先拿到超过一半的贫穷的节点的投票，就成为leader。（贫穷与富有都是相对的，越富有越可能成为leader）。 zk集群搭建 首先准备三份解压文件，每一份中都新建一个叫data的文件夹：里面新建一个叫做myid的文件，第一个写1，后面递增。 每一份中配置文件改为zoo.cfg。zk1对应的zoo.cfg: 12345678910111213141516171819202122232425262728293031323334353637383940# The number of milliseconds of each tick 心跳检测时间tickTime=2000# The number of ticks that the initial # synchronization phase can take# 集群启动后，相互连接，如果在initLimit*tickTime时间内没有连接成功，那么认为连接失败initLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgement# Masrer和slave之间或者slave和slave之间的数据同步时间，在syncLimit*tickTime是按内没有返回一个ACk，则# 认为该节点宕机，如果是Master宕机了，就要重新选举了syncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.# 数据存放的目录，这是我新建的dataDir=D:/zookeeper1-3.4.10/data# dataLogDir = xx# the port at which the clients will connect# 客户端连接集群的端口号clientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1# 第一个端口是数据同步的端口号 第二个端口是选举的端口号server.1=127.0.0.1:2887:3887server.2=127.0.0.1:2888:3888server.3=127.0.0.1:2889:3889 后面一次递增这个clientPort和data文件夹位置。 依次启动即可。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记5-ZAB协议]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B05-ZAB%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[在zookeeper中其实使用的ZAB协议来实现数据的一致性，并且主要依靠的是leader和follower这两种角色控制数据的一致性，而leader是里面最重要的一个角色，它是主要负责写操作的节点，然后与其他的follower进行数据同步，所以我们也要保证leader宕机的时候要快速选举出新的leader并且进行数据恢复。 一、前言 ZooKeeper是一个分布式协调服务，可用于服务发现、分布式锁、分布式领导选举、配置管理等。 这一切的基础，都是ZooKeeper提供了一个类似于Linux文件系统的树形结构（可认为是轻量级的内存文件系统，但只适合存少量信息，完全不适合存储大量文件或者大文件），同时提供了对于每个节点的监控与通知机制。 既然是一个文件系统，就不得不提ZooKeeper是如何保证数据的一致性的。本节将将介绍ZooKeeper如何保证数据一致性，如何进行领导选举，以及数据监控/通知机制的语义保证。 二、ZAB-原子广播（重点） ZooKeeper集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种： Leader： 一个ZooKeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其它服务器。 Follower： 一个ZooKeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的读请求，同时会将写请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票。 Observer： 角色与Follower类似，但是无投票权。 为了保证写操作的一致性与可用性，ZooKeeper专门设计了一种名为原子广播（ZAB）的支持崩溃恢复的一致性协议。基于该协议，ZooKeeper实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。 根据ZAB协议，所有的写操作都必须通过Leader完成，Leader写入本地日志后再复制到所有的Follower节点。 一旦Leader节点无法工作，ZAB协议能够自动从Follower节点中重新选出一个合适的替代者，即新的Leader，该过程即为领导选举。该领导选举过程，是ZAB协议中最为重要和复杂的过程。 1、写Leader 通过Leader进行写操作流程如下图所示： 由上图可见，通过Leader进行写操作，主要分为五步： 客户端向Leader发起写请求 Leader将写请求以Proposal的形式发给所有Follower并等待ACK Follower收到Leader的Proposal后返回ACK Leader得到过半数的ACK（Leader对自己默认有一个ACK）后向所有的Follower和Observer发送Commmit Leader将处理结果返回给客户端 这里要注意： Leader并不需要得到Observer的ACK，即Observer无投票权 Leader不需要得到所有Follower的ACK，只要收到过半的ACK即可，同时Leader本身对自己有一个ACK。上图中有4个Follower，只需其中两个返回ACK即可，因为(2+1) / (4+1) &gt; 1/2 Observer虽然无投票权，但仍须同步Leader的数据从而在处理读请求时可以返回尽可能新的数据 2、写Follower/Observer 通过Follower/Observer进行写操作流程如下图所示： 从上图可见： Follower/Observer均可接受写请求，但不能直接处理，而需要将写请求转发给Leader处理 除了多了一步请求转发，其它流程与直接写Leader无任何区别 3、读操作 Leader/Follower/Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。 由于处理读请求不需要服务器之间的交互，Follower/Observer越多，整体可处理的读请求量越大，也即读性能越好。 在整个消息广播过程中，Leader服务器会为每个事务请求生成对应的Proposal来进行广播，并且在广播事务Proposal之前，Leader服务器会首先为这个事务Proposal分配一个全局单调递增的唯一ID，我们称之为事务ID(即ZXID)。由于ZAB协议需要保证每一个消息严格的因果关系，因此必须将每一个事务Proposal按照其ZXID的先后顺序进行排序和处理。 具体的，在消息广播过程中，Leader服务器会为每个Follower服务器都各自分配一个单独的队列，然后将需要广播的事务Proposal依次放入这些队列中取，并且根据FIFO策略进行消息发送。每一个Follower服务器在接收到这个事务Proposal之后，都会首先将其以事务日志的形式写入本地磁盘中，并且成功写入后反馈给Leader服务器一个Ack相应。当Leader服务器接收到过半数Follower的Ack响应后，就会广播一个Commit消息给所有的Follower服务器以通知其进行事务提交，同时Leader自身也会完成对事务的提交，而每个Follower服务器在接收到Commit消息后，也会完成对事务的提交。 然而，在这种简化的二阶段提交模型下，无法处理Leader服务器崩溃退出而带来的数据不一致问题，因此ZAB协议添加了崩溃恢复模式来解决这个问题。另外，整个消息广播协议是基于有FIFO特性的TCP协议来进行网络通信的，因此很容易地保证消息广播过程中消息接收和发送的顺序性。 在ZAB协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出一个新的Leader服务器。因此，ZAB协议需要一个高效且可靠的Leader选举算法，从而确保能够快速选举出新的Leader。同时，Leader选举算法不仅仅需要让Leader自己知道其自身已经被选举为Leader，同时还需要让集群中的所有其他服务器也快速地感知到选举产生的新的Leader服务器。崩溃恢复主要包括Leader选举和数据恢复两部分，下面将详细讲解Leader选举和数据恢复流程。 三、支持的领导选举算法 在3.4.10版本中，默认值为3，也即基于TCP的FastLeaderElection。另外三种算法已经被弃用，并且有计划在之后的版本中将它们彻底删除而不再支持。 何时触发选举？ 选举Leader不是随时选举的，毕竟选举有产生大量的通信，造成网络IO的消耗。因此下面情况才会出现选举： 集群启动 服务器处于寻找Leader状态 当服务器处于LOOKING状态时，表示当前没有Leader，需要进入选举流程 崩溃恢复 Leader宕机 网络原因导致过半节点与Leader心跳中断 下面学习一下FastLeaderElection的原理。 四、名词解释 1、myid 每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群唯一的ID（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其ID与hostname必须一一对应，如下所示。在该配置文件中，server.后面的数据即为myid: 123server.1=zoo1:2888:3888server.2=zoo2:2888:3888server.3=zoo3:2888:3888 第1个端口是通信和数据同步端口，默认是2888 第2个端口是投票端口，默认是3888 数小的向数大的发起TCP连接。比如有3个节点，myid文件内容分别为1,2,3。zk集群的tcp连接顺序是1向2发起TCP连接，2向3发起TCP连接。如果有n个节点，那么tcp连接顺序也以此类推。这样整个zk集群就会连接起来 2、zxid 类似于RDBMS中的事务ID，用于标识一次更新操作的Proposal ID。为了保证顺序性，该zxid必须单调递增。因此ZooKeeper使用一个64位的数来表示，高32位是Leader的epoch，从1开始，每次选出新的Leader，epoch加一。低32位为该epoch内的序号，每次epoch变化，都将低32位的序号重置。这样保证了zxid的全局递增性。 3、服务器状态 LOOKING 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举。 FOLLOWING 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁。 LEADING 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳。 OBSERVING 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票。 4、选票数据结构 每个服务器在进行领导选举时，会发送如下关键信息： logicClock 每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的第多少轮投票 state 当前服务器的状态 self_id 当前服务器的myid self_zxid 当前服务器上所保存的数据的最大zxid vote_id 被推举的服务器的myid vote_zxid 被推举的服务器上所保存的数据的最大zxid 五、leader的判定标准 数据新旧程度，只有拥有最新数据的节点才能有机会成为Leader，通过zxid的大小来表示数据的新，zxid越大代表数据越新 myid:集群启动时，会在data目录下配置myid文件，里面的数字代表当前zk服务器节点的编号.当zk服务器节点数据一样新时， myid中数字越大的就会被选举成Leader 当集群中已经有Leader时，新加入的节点不会影响原来的集群 投票数量，只有得到集群中多半的投票，才能成为Leader，多半即：n/2+1,其中n为集群中的节点数量 六、Leader选举流程 1、自增选举轮次 ZooKeeper规定所有有效的投票都必须在同一轮次中。每个服务器在开始新一轮投票时，会先对自己维护的logicClock进行自增操作。 2、发送初始化选票 每个服务器最开始都是通过广播把票投给自己。 4、更新选票 根据选票logicClock -&gt; vote_zxid -&gt; vote_id依次判断 4.1 判断选举轮次收到外部投票后，首先会根据投票信息中所包含的logicClock来进行不同处理： 外部投票的logicClock &gt; 自己的logicClock： 说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的logicClock更新为收到的logicClock，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去; 外部投票的logicClock &lt; 自己的logicClock： 当前服务器直接忽略该投票，继续处理下一个投票; 外部投票的logickClock = 自己的： 进行下一步的进行选票PK。 4.2 选票PK是基于(self_id, self_zxid)与(vote_id, vote_zxid)的对比： 若logicClock一致，则对比二者的vote_zxid。 若外部投票的vote_zxid比较大，则将自己的票中的vote_zxid与vote_myid更新为收到的票中的vote_zxid与vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(self_myid, self_zxid)相同的选票，则直接覆盖 若二者vote_zxid一致，则比较二者的vote_myid。 若外部投票的vote_myid比较大，则将自己的票中的vote_myid更新为收到的票中的vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱 5、统计选票 如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。 6、更新服务器状态 投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。 七、图示Leader选举流程 说明： 图中箭头上的(1,1,0) 三个数依次代表 该选票的服务器的LogicClock（即投票轮数）; 被推荐的服务器的myid (即vote_myid); 被推荐的服务器的最大事务ID(即vote_zxid)； (1, 1)表示： 投票服务器myid(即self_myid) 被推荐的服务器的myid (即vote_myid) 所以(1,1,0)在这里的意思是：第一轮投票中，投给server 1，并且自己的最大事务ID都是0(这里可能会比较乱，ZXID可用这样理解：前32位是年号，比如万历年间；后32位是多少年，比如万历15年)，我们这里先不考虑年号的更迭，就假设这个投票发生在万历15年这一年，并且只考虑第一轮投票。即(1,vote_id,0)，所以暂时只考虑中间个数字。后面接受外部选票的时候，我们只要关注中间个数字即可，比如(1,2,0)说明是投给server 2的。 这里的示例只考虑第一轮，并且ZXID就是0. 第一步：自增选票轮次&amp;初始化选票&amp;发送初始化选票 首先，三台服务器自增选举轮次将LogicClock=1；然后初始化选票，清空票箱；最后发起初始化投票给自己将各自的票通过广播的形式投个自己并保存在自己的票箱里。 所以都是自己投给自己一票(1,1,0),(1,2,0),(1,3,0) 投完票之后的状态时(1,1),(2,2),(3,3) 第二步：接受外部投票&amp;更新选票 以Server 1 为例，分别经历 Server 1 PK Server 2 和 Server 1 PK Server 3 过程 Server 1 PK Server 2 Server 1 接收到Server 2的选票(1,2,0) 表示投给server 2. 这时Server 1将自身的选票轮次和Server 2 的选票轮次比较，发现LogicClock=1相等，接着Server 2比较比较最大事务ID，发现也zxid=0也相等，最后比较各自的myid，发现Server 2的myid=2 大于自己的myid=1； 根据选票PK规则，Server 1将自己的选票由 (1, 1) 更正为 (1, 2)，表示选举Server 2为Leader，然后将自己的新选票 (1, 2)广播给 Server 2 和 Server 3，同时更新票箱子中自己的选票并保存Server 2的选票，至此Server 1票箱中的选票为(1, 2) 和 (2, 2)； Server 2收到Server 1的选票同样经过轮次比较和选票PK后确认自己的选票保持不变，并更新票箱中Server 1的选票由(1, 1)更新为(1, 2)，注意此次Server 2自己的选票并没有改变所有不用对外广播自己的选票。 此时便认为已经选出了Leader。但是这里可能会等一会看看有没有最优的情况，可能就会来到下面一步。 Server 1 PK Server 3 Server 1 接收到Server 3的选票(1,3,0) 表示投给server 3. 根据Server 1 PK Server 2的流程类推，Server 1自己的选票由(1, 2)更新为(1, 3), 同样更新自己的票箱并广播给Server 2 和 Server 3； Server 2再次接收到Server 1的选票(1, 3)时经过比较后根据规则也要将自己的选票从(1, 2)更新为(1, 3), 并更新票箱里自己的选票和Server 1的选票，同时向Server 1和 Server 3广播； 同理 Server 2 和 Server 3也会经历上述投票过程，依次类推，Server 1 、Server 2 和Server 3 在俩俩之间在经历多次选举轮次比较和选票PK后最终确定各自的选票。 最后更新服务器状态： 选票确定后服务器根据自己票箱中的选票确定各自的角色和状态，票箱中超过半数的选票投给自己的则为Leader，更新自己的状态为LEADING，否则为Follower角色，状态为FOLLOWING，成为Leader的服务器要主动向Follower发送心跳包，Follower做出ACK回应，以维持他们之间的长连接。 https://dbaplus.cn/news-141-1875-1.html https://www.jianshu.com/p/3fec1f8bfc5f]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记4-Zookeeper介绍]]></title>
    <url>%2F2019%2F02%2F23%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B04-Zookeeper%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[关于zookeeper最基本的介绍已经在笔记一中介绍了。不再赘述。ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。本文再来稍微深入一点介绍一下zookeeper的一些特性。 1、 配置维护：在分布式系统中，一般会把服务部署到n台机器上，服务配置文件都是相同的，如果配置文件的配置选项发生了改变，那我们就得一台一台的去改动。这时候zookeeper就起作用了，可以把zk当成一个高可用的配置存储器，把这样配置的事情交给zk去进行管理，将集群的配置文件拷贝到zookeeper的文件系统的某个节点上，然后用zk监控所有分布式系统里的配置文件状态，一旦发现有配置文件发生了变化，那么每台服务器同步zk的配置文件，zk同时保证同步操作的原子性，确保每个服务器的配置文件都能被更新。 2、 命名服务：在分布式应用中，通常需要一个完整的命名规则，既能够产生唯一的名称又便于人识别和记住。Zk就提供了这种服务，类似于域名和ip之间对应关系，域名容易记住，通过名称来获取资源和服务的地址，提供者等信息。 3、 分布式锁：分布式程序分布在不同主机上的进程对互斥资源进行访问的时候需要加锁。这样理解：很多分布式系统有多个服务窗口，但是某个时刻只让一个服务去干活，当这台服务器出问题的时候锁释放，里脊fail over到另外的服务。举例子，比如去某个地方办理证件的时候，只能有一个窗口对你服务，如果这个窗口的柜员有急事走了，那么系统或者经理给你指定另外一个窗口继续服务。 4、 集群管理：分布式集群中，经常会由于各种原因，比如硬件故障，网络问题，有些节点挂掉、有些节点加进来。这个时候机器需要感知到变化，然后根据变化做出对应的决策，那么zk就实现了类似这种集群的管理。 一、特性 顺序一致性： 从同一个客户端发起的事务请求，最终将会严格按照其发起顺序被应用到ZooKeeper中。 原子性： 所有事务请求的结果在集群中所有机器上的应用情况是一致的，也就是说要么整个集群所有集群都成功应用了某一个事务，要么都没有应用，一定不会出现集群中部分机器应用了该事务，而另外一部分没有应用的情况。 单一视图： 无论客户端连接的是哪个ZooKeeper服务器，其看到的服务端数据模型都是一致的。 可靠性： 一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更。 实时性： 通常人们看到实时性的第一反应是，一旦一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。 二、集群角色 最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入以下三种角色： Leader Follower Observer zookeeper-server status可以看当前节点的ZooKeeper是什么角色。 ZooKeeper默认只有Leader和Follower两种角色，没有Observer角色。 ZooKeeper集群的所有机器通过一个Leader选举过程来选定一台被称为『Leader』的机器，Leader服务器为客户端提供读和写服务。 Follower和Observer都能提供读服务，不能提供写服务。两者唯一的区别在于，Observer机器不参与Leader选举过程，也不参与写操作的『过半写成功』策略，因此Observer可以在不影响写性能的情况下提升集群的读性能。 系统模型如图所示： 三、关于ZooKeeper的一些重要概念 ZooKeeper本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper就能正常服务） 为了保证高可用，最好是以集群形态来部署ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么ZooKeeper本身仍然是可用的。 ZooKeeper将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。 ZooKeeper是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提交数据节点监听服务。 只要半数以上节点存活，ZooKeeper就能正常服务，这也解释了为什么集群个数一般是奇数。 我们知道在Zookeeper中Leader选举算法采用了Zab协议。Zab核心思想是当多数Server写成功，则任务数据写成功。 ①如果有3个Server，则最多允许1个Server挂掉。 ②如果有4个Server，则同样最多允许1个Server挂掉。 既然3个或者4个Server，同样最多允许1个Server挂掉，那么它们的可靠性是一样的，所以选择奇数个ZooKeeper Server即可，这里选择3个Server。 四、会话 Session是指客户端会话，在讲解客户端会话之前，我们先来了解下客户端连接。在ZooKeeper中，一个客户端连接是指客户端和ZooKeeper服务器之间的TCP长连接。ZooKeeper对外的服务端口默认是2181，客户端启动时，首先会与服务器建立一个TCP连接，从第一次连接建立开始，客户端会话的生命周期也开始了，通过这个连接，客户端能够通过心跳检测和服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能通过该连接接收来自服务器的Watch事件通知。 Session的SessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在SessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于sessionID 是Zookeeper会话的一个重要标识，许多与会话相关的运行机制都是基于这个sessionID的，因此，无论是哪台服务器为客户端分配的sessionID，都务必保证全局唯一。 五、数据节点 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 六、Watcher Watcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 七、版本 在前面我们已经提到，Zookeeper的每个ZNode上都会存储数据，对应于每个ZNode，Zookeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个ZNode的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 cversion（当前ZNode的ACL版本）。 Zookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于UNIX文件系统的权限控制。 CREATE: 创建子节点的权限。 READ: 获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE: 删除子节点的权限。 ADMIN: 设置节点ACL的权限。 注意：CREATE 和 DELETE 都是针对子节点的权限控制。 下一节先说说ZAB协议。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记3-paxos算法]]></title>
    <url>%2F2019%2F02%2F22%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B03-paxos%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[有人说一千个人就有一千个paxos算法理解，算法本身晦涩难懂，如何快速生动理解paxos核心要点一直是一个老大难的问题，本文集结多篇文章精华，算是理顺了其中的门道。 一、paxos解决了什么问题 在上一章节中，我们着重提到了解决一致性问题的两种协议：2PC和3PC。但是在分布式环境中，这两种协议都无法真正实现一致性(分布式的一致性问题其实主要是指分布式系统中的数据一致性问题。所以，为了保证分布式系统的一致性，就要保证分布式系统中的数据是一致的。) 分布式系统中的节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。基于消息传递通信模型的分布式系统，不可避免的会发生以下错误： 进程可能会慢、被杀死或者重启 消息可能会延迟、丢失、重复 （在基础 Paxos 场景中，先不考虑可能出现消息篡改即拜占庭错误的情况） ⭐Paxos 算法解决的问题是在一个可能发生上述异常的分布式系统中如何就某个值达成一致，保证不论发生以上任何异常，都不会破坏决议的一致性。 那么我们可用形象地理解为：Paxos可以说是一个民主选举的算法——大多数节点的决定会成个整个集群的统一决定。任何一个点都可以提出要修改某个数据的提案，是否通过这个提案取决于这个集群中是否有超过半数的节点同意。取值一旦确定将不再更改，并且可以被获取到(不可变性，可读取性)。 二、典型场景 一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。 所以，paxos算法主要解决的问题就是如何保证分布式系统中各个节点都能执行一个相同的操作序列。 上图中，C1是一个客户端，N1、N2、N3是分布式部署的三个服务器，初始状态下N1、N2、N3三个服务器中某个数据的状态都是S0。当客户端要向服务器请求处理操作序列：op1–op2–op3时（op表示operation） 如果想保证在处理完客户端的请求之后，N1、N2、N3三个服务器中的数据状态都能从S0变成S1并且一致的话（或者没有执行成功，还是S0状态），就要保证N1、N2、N3在接收并处理操作序列op1–op2–op3时，严格按照规定的顺序正确执行opi，要么全部执行成功，要不就全部都不执行。 所以，针对上面的场景，paxos解决的问题就是如何依次确定不可变操作opi的取值，也就是确定第i个操作什么，在确定了opi的内容之后，就可以让各个副本执行opi操作。 三、paxos算法 1、算法涉及的主要角色： Proposer：提议者，提出议案(同时存在一个或者多个，他们各自发出提案) Acceptor：接受者，收到议案后选择是否接受 Learner：最终决策学习者，只学习正确的决议 Client：产生议题者，发起新的请求 主要的角色就是“提议者”和“接受者”。先有提议，再来表决。（注：实际应用中，可以将一堆服务器任意指定角色，一部分做“提议者”、一部分做“接受者”，也可以指定特定的服务器做“提议者”，剩下的都是“接受者”。） Proposer就像Client的代理人，由Proposer拿着Client的议题去向Acceptor提议，让Acceptor来做出决策。 这幅图表示的是角色之间的逻辑关系，每一种角色就代表了一种节点类型。在物理部署环节，可以把每一种角色都部署在一台物理机器上，也可以组合任何两种或者多种角色部署在一台物理机器上，甚至于，把这四种角色都部署在同一台物理机器上也是可以的。 2、关于这两阶段的理解 从一个故事说起。 从前，在国王Leslie Lamport的统治下，有个黑暗的希腊城邦叫paxos。城邦里有3类人， 决策者 提议者 群众 虽然这是一个黑暗的城邦但是很民主，按照议会民主制的政治模式制订法律，群众有什么建议和意见都可以写提案交给提议者，提议者会把提案交给决策者来决策，决策者有奇数个，为什么要奇数个？很简单因为决策的方式很无脑，少数服从多数。最后决策者把刚出炉的决策昭告天下，群众得知决策结果。 等一下，那哪里黑暗呢？问题就出在“提议者会把提案交给决策者来决策”，那么多提案决策者先决策谁的？谁给的钱多就决策谁的。 那这样会有几个问题，决策者那么多，怎么保证最后决策的是同一个提案，以及怎么保证拿到所有提议者中最高的报价。 聪明又贪婪的决策者想到了一个办法：分两阶段报价 第一阶段： 决策者接受所有比他当前持有报价高的报价，且不会通知之前报价的人 提议者给所有(一半以上即可)决策者报价，若有人比自己报价高就加价，有半数以上决策者接受自己报价就停止报价。 一旦某个提议者收到了所有决策者中一半以上的人同意的回复。就会进入第二阶段。 第二阶段： 提议者去找收过自己钱的大佬签合同，这里有3种情况： 很多大佬收了别人更高的价，达不到一半人数了，只好回去拿钱继续贿赂，回到第一阶段重新升级; 大佬收到的最高报价是自己的，美滋滋，半数以上成功签合同，提案成功; 提议者回去拿钱回来继续贿赂的时候发现合同已经被签了且半数以上都签了这个提案，不干了，赶快把自己的提案换成已经签了的提案，再去提给所有大佬，看看能不能分一杯羹遇见还没签的大佬。 最后一步就是让所有节点知道这个过半通过的提议是什么，从而达到最终的一致。 三、深化理解 假设有两个“提议者”和三个“接受者”。下面这一坨的内容一开始如果看不明白不要紧，立即转到下面的图示过程，看懂图示再回过头来就会理解了。 怎么明确意见领袖呢？通过编号。每个“提议者”在第一阶段先报个号，谁的号大，谁就是意见领袖。如果不好理解，可以想象为贿选。每个提议者先拿着钞票贿赂一圈“接受者”，谁给的钱多，第二阶段“接受者”就听谁的。意见领袖理解为贿赂中胜出的“提议者”即可。 有个跟选举常识不一样的地方，就是每个“提议者”不会执着于让自己的提议通过，而是每个“提议者”会执着于让提议尽快达成一致意见。所以，为了这个目标，如果“提议者”在贿选的时候，发现“接受者”已经接受过前面意见领袖的提议了，即便“提议者”贿选成功，也会默默的把自己的提议改为前面意见领袖的提议。所以一旦贿赂成功，胜出的“提议者”再提出提议，提议内容也是前面意见领袖的提议（这样，在谋求尽早形成多数派的路上，又前进了一步）。 钱的多少很重要，如果钱少了，无论在第一还是第二阶段“接受者”都不会鸟你，直接拒绝。 上面讲到，如果“提议者”在贿选时，发现前面已经有意见领袖的提议，那就将自己的提议默默改成前面意见领袖的提议。这里有一种情况，如果你是“提议者”，在贿赂的时候，“接受者1”跟你说“他见过的意见领袖的提议是方案1”，而“接受者2”跟你说“他见过的意见领袖提议是方案2”，你该怎么办？这时的原则也很简单，还是：钱的多少很重要！你判断一下是“接受者1”见过的意见领袖有钱，还是“接受者2”见过的意见领袖有钱？如何判断呢？因为“接受者”在被“提议者”贿赂的时候，自己会记下贿赂的金额。所以当你贿赂“接受者”时，一旦你给的贿赂多而胜出，“接受者”会告诉你两件事情：a.前任意见领袖的提议内容（如果有的话），b.前任意见领袖当时贿赂了多少钱。这样，再面对刚才的情景时，你只需要判断一下“接受者1”和“接受者2”告诉你的信息中，哪个意见领袖当时给的钱多，那你就默默的把自己的提议，改成那个意见领袖的提议。 在整个选举过程中，每个人谁先来谁后到，“接受者”什么时间能够接到“提议者”的信息，是完全不可控的。所以很可能一个意见领袖已经产生了，但是由于这个意见领袖的第二阶段刚刚开始，绝大部分“接受者”还没有收到这个意见领袖的提议。结果，这时突然冲进来了一个新的土豪“提议者”，那么这个土豪“提议者”也是有机会让自己的提议胜出的！这时就形成了一种博弈：a.上一个意见领袖要赶在土豪“提议者”贿赂到“接受者”前，赶到“接受者”面前让他接受自己的提议，否则会因为自己的之前贿赂的钱比土豪少而被拒绝。b.土豪“提议者”要赶在上一个意见领袖将提议传达给“接受者”前，贿赂到“接受者”，否则土豪“提议者”即便贿赂成功，也要默默的将自己的提议改为前任意见领袖的提议。这整个博弈的过程，最终就看这两个“提议者”谁的进展快了。但最终一定会有一个意见领袖，先得到多数“接受者”的认可，那他的提议就胜出了。这一块不理解就看下面的分解说明。 1）首先“提议者1”贿赂了3个“接受者” 2）3个“接受者”记录下贿赂金额，因为目前只有一个“提议者”出价，因此$1就是最高的了，所以“接受者”们返回贿赂成功。此外，因为没有任何先前的意见领袖提出的提议，因此“接受者”们告诉“提议者1”没有之前接受过的提议（自然也就没有上一个意见领袖的贿赂金额了）。 假如此时由于贿赂的人数超过了一半，那么第一阶段成功，准备进入第二阶段，就是正式签合同，签超过半数的合同才真正表示本轮成功。 3）“提议者1”发现有超过一半人接受了自己的贿赂，下面就要真正发起提议了，先向“接受者1”提出了自己的提议：1号提议，并告知自己之前已贿赂$1。 4）“接受者1”检查了一下，目前记录的贿赂金额就是$1，于是接受了这一提议，并把1号提议记录在案。 5）在“提议者1”向“接受者2”“接受者3”发起提议前，土豪“提议者2”出现，他开始用$2贿赂“接受者1”与“接受者2”。 6）“接受者1”与“接受者2”立刻被收买，将贿赂金额改为$2。但是，不同的是：“接受者1”告诉“提议者2”,之前我已经接受过1号提议了，同时1号提议的“提议者”贿赂过$1；而，“接受者2”告诉“提议者2”，之前没有接受过其他意见领袖的提议，也没有上一个意见领袖的贿赂金额。 7）这时，“提议者1”回过神来了，他向“接受者2”和“接受者3”发起1号提议，并带着信息“我前期已经贿赂过$1”。 8）“接受者2”“接受者3”开始答复：“接受者2”检查了一下自己记录的贿赂金额，然后表示，已经有人出价到$2了，而你之前只出到$1，不接受你的提议，再见。但“接受者3”检查了一下自己记录的贿赂金额，目前记录的贿赂金额就是$1，于是接受了这一提议，并把1号提议记录在案。 9）到这里，“提议者1”已经得到两个接受者的赞同，已经得到了多数“接受者”的赞同。于是“提议者1”确定1号提议最终通过。 10）此时“提议者2”发现1号提议已经被通过了，为了最快达成一致，那么他就默默地将自己的提议也改为与1号提议一致，然后开始向“接受者1”“接受者2”发起提议（提议内容仍然是1号提议），并带着信息：之前自己已贿赂过$2。 11）这时“接受者1”“接受者2”收到“提议者2”的提议后，照例先比对一下贿赂金额，比对发现“提议者2”之前已贿赂$2，并且自己记录的贿赂金额也是$2，所以接受他的提议，也就是都接受1号提议。 12）于是，“提议者2”也拿到了多数派的意见，最终通过的也是1号提议。 回到上面的第5）步，如果“提议者2”第一次先去贿赂“接受者2”“接受者3”会发生什么？那很可能1号提议就不会成为最终选出的提议。因为当“提议者2”先贿赂到了“接受者2”“接受者3”，那等“提议者1”带着议题再去找这两位的时候，就会因为之前贿赂的钱少（$1&lt;$2）而被拒绝。所以，这也就是刚才讲到可能存在博弈的地方：a.“提议者1”要赶在“提议者2”贿赂到“接受者2”“接受者3”之前，让“接受者2”“接受者3”接受自己的意见，否则“提议者1”会因为钱少而被拒绝；b.“提议者2”要赶在“提议者1”之前贿赂到“接受者”，否则“提议者2”即便贿赂成功，也要默默的将自己的提议改为“提议者1”的提议。但你往后推演会发现，无论如何，总会有一个“提议者”的提议获得多数票而胜出。 五、总结 好啦，故事到这里基本讲述完了，咱们来总结一下，其实Paxos算法就下面这么几个原则： Paxos算法包括两个阶段：第一个阶段主要是贿选，还没有提出提议；第二个阶段主要根据第一阶段的结果，明确接受谁的提议，并明确提议的内容是什么（这个提议可能是贿选胜出“提议者”自己的提议，也可能是前任意见领袖的提议，具体是哪个提议，见下面第3点原则）。 编号（贿赂金额）很重要，无论在哪个阶段，编号（贿赂金额）小的，都会被鄙视（被拒绝）。 在第一阶段中，一旦“接受者”已经接受了之前意见领袖的提议，那后面再来找这个“接受者”的“提议者”，即便在贿赂中胜出，也要被洗脑，默默将自己的提议改为前任意见领袖的提议，然后他会在第二阶段提出该提议（也就是之前意见领袖的提议，以力争让大家的意见趋同）。如果“接受者”之前没有接受过任何提议，那贿选胜出的“提议者”就可以提出自己的提议了。 还有一个问题需要考量，假如proposer A发起ID为n的提议，在提议未完成前proposer B又发起ID为n+1的提议，在n+1提议未完成前proposer C又发起ID为n+2的提议…… 如此acceptor不能完成决议、形成活锁(livelock)，虽然这不影响一致性，但我们一般不想让这样的情况发生。解决的方法是从proposer中选出一个leader，提议统一由leader发起。 如何浅显易懂地解说 Paxos 的算法？ 共识算法之：Paxos 讲一个关于paxos的故事 浅显易懂地解读Paxos算法 分布式系统理论进阶 - Paxos]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记2-2PC&3PC]]></title>
    <url>%2F2019%2F02%2F22%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B02-2PC%263PC%2F</url>
    <content type="text"><![CDATA[很多人对解决分布式系统中一致性问题的难度还没有一个直观的感受，本节详细讲解分布式系统中一致性面临的种种挑战，并且详细说明2PC和3PC这些比较简单的解决方案的原理和存在的问题，为后面引出zk的解决方案做铺垫。 一、前言 分布式系统中，一致性问题是一个比较重要的问题，zookeeper解决的就是分布式系统的一致性问题。下面我们从一致性问题、特定条件下解决一致性问题的两种方法(2PC、3PC)入门，了解最基础的分布式系统理论。 二、一致性 何为一致性问题？简单而言，一致性问题就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。这个问题在我们的日常生活中也很常见，比如牌友怎么商定几点在哪打几圈麻将. 假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性： 全认同(agreement): 所有N个节点都认同一个结果 值合法(validity): 该结果必须由N个节点中的节点提出 可结束(termination): 决议过程在一定时间内结束，不会无休止地进行下去 有人可能会说，决定什么时候在哪搓搓麻将，4个人商量一下就ok，这不很简单吗？ 但就这样看似简单的事情，分布式系统实现起来并不轻松，因为它面临着上一节所说的这些问题： 消息传递异步无序(asynchronous): 现实网络不是一个可靠的信道，存在消息延时、丢失，节点间消息传递做不到同步有序(synchronous) 节点宕机(fail-stop): 节点持续宕机，不会恢复 节点宕机恢复(fail-recover): 节点宕机一段时间后恢复，在分布式系统中最常见 网络分化(network partition): 网络链路出现问题，将N个节点隔离成多个部分 拜占庭将军问题(byzantine failure): 节点或宕机或逻辑失败，甚至不按套路出牌抛出干扰决议的信息 假设现实场景中也存在这样的问题，我们看看结果会怎样： 第一种情况： 我: 老王，今晚7点老地方，搓够48圈不见不散！ …… (第二天凌晨3点) 隔壁老王: 没问题！ // 消息延迟 我: …… 第二种情况： 我: 小张，今晚7点老地方，搓够48圈不见不散！ 小张: No …… (两小时后……) 小张: No problem！ // 宕机节点恢复 我: …… 第三种情况： 我: 老李头，今晚7点老地方，搓够48圈不见不散！ 老李: 必须的，大保健走起！ // 拜占庭将军 (这是要打麻将呢？还是要大保健？还是一边打麻将一边大保健……) 还能不能一起愉快地玩耍… 正如上节所说，在分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）中最多只能满足两个。那么，第三个是一定要满足的了。那么强一致性和高可用必定不能同时满足。 对于一致性，2PC、3PC是相对简单的解决强一致性问题的协议，下面我们就来了解2PC和3PC。 三、协调者 在分布式系统中，每一个机器节点虽然都能明确的知道自己执行的事务是成功还是失败，但是却无法知道其他分布式节点的事务执行情况。因此，当一个事务要跨越多个分布式节点的时候（比如，淘宝下单流程，下单系统和库存系统可能就是分别部署在不同的分布式节点中），为了保证该事务可以满足ACID，就要引入一个协调者（Cooradinator）。其他的节点被称为参与者（Participant）。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务进行提交。 四、2PC(二阶提交) 顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)。 在阶段1中，协调者发起一个提议，分别问询各参与者是否接受。只要有一个参与者没有准备好就中止。 值得注意的是，二阶段提交协议的第一阶段准备阶段不仅仅是回答YES or NO，还是要执行事务操作的，只是执行完事务操作，并没有进行commit还是roolback。也就是说，一旦事务执行之后，在没有执行commit或者roolback之前，资源是被锁定的。这会造成阻塞。 1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息(用于失败时的回滚)和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 在阶段2中，协调者根据参与者的反馈，提交或中止事务，如果参与者全部同意则提交，只要有一个参与者不同意就中止。释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源) 问题： 1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题） 3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这会导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。 二阶段无法解决的问题：协调者在发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 为了解决这些问题，衍生出了对2PC的改进3PC。我们接下来看看3PC是如何解决这些问题的。 五、3PC(三阶段提交) 3PC最关键要解决的是单点和阻塞。 所以3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。在第一阶段，只是询问所有参与者是否可可以执行事务操作，并不在本阶段执行事务操作。当协调者收到所有的参与者都返回YES时，在第二阶段才执行事务操作，然后在第三阶段在执行commit或者rollback。 5.1 CanCommit阶段 3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 但是此时不执行事务的操作，也就时说不会锁住资源。 5.2 PreCommit阶段 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。此时会执行事务操作和将undo和redo信息记录到事务日志中。如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 5.3 doCommit阶段 加入协调接收到所有参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ） 相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。 但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 六、总结 了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper笔记1-CAP和BASE理论]]></title>
    <url>%2F2019%2F02%2F22%2Fzookeeper%2FZookeeper%E7%AC%94%E8%AE%B01-CAP%E5%92%8CBASE%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[随着应用规模的迅速扩张，单台机器的部署已经难以支撑用户大规模、高并发的请求了， 因此服务化、集群化、分布式概念应运而生。 然而， 集群的维护和多节点应用程序的协作运行远比单机模式复杂，需要顾及到的细节问题实在太多，比如说同一分配置在多台机器上的同步， 客户端程序实时感知服务机状态，应用与应用之间的公共资源的互斥访问等等一系列的问题。zookeeper能够给我们非常完美的解决这些问题，zookeeper天生的就是为解决分布式协调服务这个问题而来。 一、前言 学习zookeeper才算是真正跨进分布式这个大门。比较经典的应用是可以作为dubbo推荐的注册中心。 首先，我们必须要明确几个我们之前可能不会遇到的但是在分布式系统中又很常见的问题： 网络相当可靠 延迟为0 传输带宽是无限的 网络相当安全 拓扑结构不会改变 必须要有一个管理员 传输成本为0 网络同质化 总结起来，分布式系统中最常出现的问题是：通信异常，表现为网络通信成功，失败和超时；节点故障，包括宕机和OOM。 二、Zookeeper是什么 官方解释： 它是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 上面的解释有点抽象，简单来说zookeeper=文件系统+监听通知机制。我们这里拿比较简单的分布式应用配置管理为例来说明。 假设我们的程序是分布式部署在多台机器上，如果我们要改变程序的配置文件，需要逐台机器去修改，非常麻烦，现在把这些配置全部放到zookeeper上去，保存在 zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 zookeeper 的通知，然后从 zookeeper 获取新的配置信息应用到系统中。 三、Zookeeper设计目的 1、最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 2、可靠性：具有简单、健壮、良好的性能，如果消息被到一台服务器接受，那么它将被所有的服务器接受。 3、实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 4、等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。 5、原子性：更新只能成功或者失败，没有中间状态。 6、顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。 四、CAP理论 指的是在一个分布式系统中，不可能同时满足Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这三个基本需求，最多只能满足其中的两项。 1、一致性： 指数据在多个副本之间是否能够保持一致的特性。当执行数据更新操作后，仍然保证系统数据处于一致的状态。 2、可用性（高可用）： 系统提供的服务必须一直处于可用的状态。对于用户的每一个操作请求总是能够在“有限的时间内”返回结果。这个有限时间是系统设计之初就指定好的系统运行指标。返回的结果指的是系统返回用户的一个正常响应结果，而不是“out ot memory error”之类的系统错误信息。 3、分区容错性（数据分片）： 分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。组成分布式系统的每个节点的加入与退出都可以看成是一个特殊的网络分区。 一个分布式系统无法同时满足这三个条件，只能满足两个，意味着我们要抛弃其中的一项，如下图所示： 1、CA，放弃P：将所有数据都放在一个分布式节点上。这同时放弃了系统的可扩展性。 2、CP，放弃A：一旦系统遇到故障时，受影响的服务器需要等待一段时间，在恢复期间无法对外提供正常的服务。 3、AP，放弃C：这里的放弃一致性是指放弃数据强一致性，而保留数据的最终一致性。系统无法实时保持数据的一致，但承诺在一个限定的时间窗口内，数据最终能够达到一致的状态。 对于分布式系统而言，分区容错性是一个最基本的要求，因为分布式系统中的组件必然需要部署到不通的节点，必然会出现子网络，在分布式系统中，网络问题是必定会出现的异常。因此分布式系统只能在C（一致性）和A（可用性）之间进行权衡。 五、BASE理论 Basically Available（基本可用）、Soft-state（ 软状态/柔性事务）、Eventual Consistency（最终一致性）。是基于CAP定理演化而来，是对CAP中一致性和可用性权衡的结果。 核心思想：即使无法做到强一致性，但每个业务根据自身的特点，采用适当的方式来使系统达到最终一致性。 1、基本可用： 指分布式系统在出现故障的时候，允许损失部分可用性，保证核心可用。但不等价于不可用。比如：搜索引擎0.5秒返回查询结果，但由于故障，2秒响应查询结果；网页访问过大时，部分用户提供降级服务，等。 2、软状态： 软状态是指允许系统存在中间状态，并且该中间状态不会影响系统整体可用性。即允许系统在不同节点间副本同步的时候存在延时。 3、最终一致性： 系统中的所有数据副本经过一定时间后，最终能够达到一致的状态，不需要实时保证系统数据的强一致性。最终一致性是弱一致性的一种特殊情况。 BASE理论面向的是大型高可用可扩展的分布式系统，通过牺牲强一致性来获得可用性。ACID是传统数据库常用的概念设计，追求强一致性模型。]]></content>
      <tags>
        <tag>zookpeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot通用知识深入--切面、异常、单元测试]]></title>
    <url>%2F2019%2F02%2F21%2Fmiscellany%2FSupringBoot%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86%E6%B7%B1%E5%85%A5--%E5%88%87%E9%9D%A2%E3%80%81%E5%BC%82%E5%B8%B8%E3%80%81%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[对于小白来说，下面的知识都是满满的干货，值得好好学习，具体的视频是学习的廖师兄的Spring Boot进阶之Web进阶，值得一看，下面是笔记总结。 一、面向切面编程 访问 localhost:8080/dev/add 日志打印结果是： 二、校验和返回结果封装 1、首先是在student类中对username增加注解：@NotNull(message = &quot;用户名不能为空&quot;) 2、在增加一个学生的方法上进行参数的验证： 3、在http工具上输入 http://localhost:8080/dev/add?username=hello&amp;age=10 返回正确信息，即学生的json流：{“id”:5,“username”:“hello”,“age”:10} 如果不传username，即必填的那一项不给，则发生异常： http://localhost:8080/dev/add?age=10 返回信息为：用户名不能为空 4、我们会发现，正确返回是新添加学生的json格式，错误返回就是一个字符串，这样对于前台来说是无法处理的，所以需要一个封装类来包装一下。 5、下面对controller层进行改造，返回统一的格式。 6、controller曾进行结果封装的时候，发现代码重复，进行优化。 新建一个工具类，用来封装结果。 继而改造controller: 结果与上面一致。 三、统一异常处理 如果service层业务逻辑是： 1、我们第一个想到的方案可能是给每一种情况加上一个标记，controller层根据标记的不同进行不同的返回处理： service层方法： 相应的controller层为： 对于逻辑比较简单的情况下，是可以到达我们的预期效果，但是一旦业务量逻辑复杂度高一点，就会非常地混乱。解决方案是统一异常处理。 2、加上异常处理 service层处理为: controller层处理为: 返回结果为： 显然格式都是不统一的，解决方案是对默认的exception返回信息再进行一次封装。 3、修改exception返回信息格式： 新建一个handle类： 但是对于这种方式，如果我想让上小学的学硕状态码为100,上初中的学生的状态码为101，就无法实现了。解决方案：自定义异常。 4、自定义异常 新建一个异常类： 在service层抛出异常为throw new StudentException(100,&quot;还在上小学&quot;)。 在刚才写的默认异常处理类中进行判断处理： 这样，当属于小学生时，状态码为100,当属于初中生时，状态码为101，就区分开了。当不属于这个异常的异常，就会抛出未知错误。这样也不好，最好用日志将未知错误打印出来。 5、系统异常(默认异常处理): 到现在仍然存在一些问题：状态码与状态信息都是自己在程序中临时定义的，是不规范的行为，需要有一个地方统一管理，便于区分和防止混乱重复。 6、枚举定义异常状态 改造service层： 这样就完成了异常的统一管理。 四、springboot测试 1、对于service层的测试 创建：方法名右击—go to–test 2、对于controller层的api的测试]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10.Sleuth服务追踪]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F10.Sleuth%E6%9C%8D%E5%8A%A1%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[在以前，所有的东西放在一起打包，放到服务器上也就不管了，但是到了分布式场景下，可能一个请求要经过十几个服务的周转，如果不进行链路的追踪，这些对于我们来说都是透明的，那么哪个节点存在问题或者存在超时隐患我们都是很难知道的，因此服务追踪是我们必须要做的一样事情。 一、为什么需要进行分布式链路追踪springcloud-sleuth呢？ 随着分布式系统越来越复杂，你的一个请求发过发过去，各个微服务之间的跳转，有可能某个请求某一天压力太大了，一个请求过去没响应，一个请求下去依赖了三四个服务，但是你去不知道哪一个服务出来问题，这时候我是不是需要对微服务进行追踪呀？监控一个请求的发起，从服务之间传递之间的过程，我最好记录一下，记录每一个的耗时多久，一旦出了问题，我们就可以针对性的进行优化，是要增加节点，减轻压力，还是服务继续拆分，让逻辑更加简单点呢？这时候springcloud-sleuth集成zipkin能帮我们解决这些服务追踪问题。 二、创建工程 本工程的Spring Boot的版本为1.5.8，Spring Cloud版本为Dalston.RELEASE。包含了eureka-server工程，作为服务注册中心，eureka-server的创建过程这里不重复；zipkin-server作为链路追踪服务中心，负责存储链路数据；gateway-service作为服务网关工程，负责请求的转发,同时它也作为链路追踪客户端，负责产生数据，并上传给zipkin-service；user-service为一个应用服务，对外暴露API接口，同时它也作为链路追踪客户端，负责产生数据。 1.构建eureka-server 这个服务就比较简单了，就是eureka的服务端，用来注册服务。见代码eureka-server. 2.构建zipkin-server 依赖： 12345678910111213141516171819202122232425262728&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--eureka服务端--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--zipkin服务端--&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--显示的页面--&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 主函数上打上两个注解开启ZipkinServer的功能： 12@EnableEurekaClient@EnableZipkinServer 配置文件上也很简单，注册到eureka上和指定好启动的端口即可。 123456789eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/server: port: 9411spring: application: name: zipkin-server 3.构建user-service 新建一个工程，取名为user-service，作为应用服务，对外暴露API接口. 依赖： 12345678910111213141516171819202122&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 打上eureka注解。配置文件为： 1234567891011121314eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/server: port: 8080spring: application: name: user-service zipkin: base-url: http://localhost:9411 sleuth: sampler: percentage: 1.0 Zipkin Server地址为http://localhost:9411。 spring.sleuth.sampler.percentage为1.0,即100%的概率将链路的数据上传给Zipkin Server，在默认的情况下，该值为0.1. 测试的api为： 123456789@RestController@RequestMapping("/user")public class UserController &#123; @GetMapping("/hello") public String hi()&#123; return "hello user!"; &#125;&#125; 4.构建gateway-service 新建一个名为gateway-service工程，这个工程作为服务网关，将请求转发到user-service，作为Zipkin客户端，需要将链路数据上传给Zipkin Server，同时它也作为Eureka Client。 依赖： 12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 注解： 12@EnableEurekaClient@EnableZuulProxy 配置文件： 123456789101112131415161718192021eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/server: port: 8081spring: application: name: gateway-service sleuth: sampler: percentage: 1.0 zipkin: base-url: http://localhost:9411zuul: routes: api-a: path: /user-api/** serviceId: user-service 以“/user-api/**”开头的Uri请求，转发到服务名为 user-service 的服务 三、演示效果 完整的项目搭建完毕，依次启动eureka-server、zipkin-server、user-service、gateway-service。在浏览器上访问http://localhost:8081/user-api/user/hi 访问http://localhost:9411，即访问Zipkin的展示界面，界面显示如图所示： 这个界面主要用来查找服务的调用情况，可以根据服务名、开始时间、结束时间、请求消耗的时间等条件来查找。点击“Find Traces”按钮，界面如图所示。从图可知服务的调用情况，比如服务调用时间、服务的消耗时间，服务调用的链路情况。 点击Dependences按钮，可以查看服务的依赖关系，在本案例中，gateway-service将请求转发到了user-service，它们的依赖关系如图： 参考文章：Spring Cloud Sleuth进阶实战]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[9.Stream消息驱动]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F9.Stream%E6%B6%88%E6%81%AF%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[对于MQ我们并不陌生，市面上也有形形色色的MQ，各有各的特点，原理上多多少少有一点区别，那么在进行MQ的更换的时候可能会出现代码的大片修改，这显然是很不好的，有没有一种方式来屏蔽掉这种MQ间的差异呢？这就引出了今天讨论的主角：Spring Cloud Stream. 一、为什么需要SpringCloud Stream消息驱动呢？ 比方说我们用到了RabbitMQ和Kafka，由于这两个消息中间件的架构上的不同，像RabbitMQ有exchange，kafka有Topic，partitions分区，这些中间件的差异性导致我们实际项目开发给我们造成了一定的困扰，我们如果用了两个消息队列的其中一种， 后面的业务需求，我想往另外一种消息队列进行迁移，这时候无疑就是一个灾难性的，一大堆东西都要重新推倒重新做，因为它跟我们的系统耦合了，这时候springcloud Stream给我们提供了一种解耦合的方式。 Spring Cloud Stream由一个中间件中立的核组成。应用通过Spring Cloud Stream插入的input(相当于消费者consumer，它是从队列中接收消息的)和output(相当于生产者producer，它是从队列中发送消息的。)通道与外界交流。 通道通过指定中间件的Binder实现与外部代理连接。业务开发者不再关注具体消息中间件，只需关注Binder对应用程序提供的抽象概念来使用消息中间件实现业务即可。 Binder: 通过定义绑定器作为中间层，实现了应用程序与消息中间件(Middleware)细节之间的隔离。通过向应用程序暴露统一的Channel通道，使得应用程序不需要再考虑各种不同的消息中间件的实现。当需要升级消息中间件，或者是更换其他消息中间件产品时，我们需要做的就是更换对应的Binder绑定器而不需要修改任何应用逻辑 。甚至可以任意的改变中间件的类型而不需要修改一行代码。目前只提供了RabbitMQ和Kafka的Binder实现。 Springcloud Stream还有个好处就是像Kafka一样引入了一点分区的概念，像RabbitMQ不支持分区的队列，你用了SpringCloud Stream技术，它就会帮RabbitMQ引入了分区的特性，SpringCloud Stream就是天然支持分区的，我们用起来还是很方便的。 二、简单的演示 首先我们要新建三个项目，分别是spring-cloud-stream，spring-cloud-stream1，spring-cloud-stream2，其中spring-cloud-stream作为生产者进行发消息模块，spring-cloud-stream-1，spring-cloud-stream-2作为消息接收模块。 第一步： 对于这三个项目，都要引入依赖： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.swg&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-producer&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-cloud-stream-producer&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-dependencies&lt;/artifactId&gt; &lt;version&gt;Elmhurst.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 第二步： 对于spring-cloud-stream，是信息的生产方，配置文件为： 12345678910111213141516171819202122server: port: 8081spring: application: name: producer cloud: stream: kafka: binder: #Kafka的消息中间件服务器 brockers: localhost:9092 #Zookeeper的节点，如果集群，后面加,号分隔 zk-nodes: localhost:2181 #如果设置为false,就不会自动创建Topic 有可能你Topic还没创建就直接调用了。 auto-create-topics: true bindings: #这里用stream给我们提供的默认output，后面会讲到自定义output output: #消息发往的目的地 destination: stream-demo #消息发送的格式，接收端不用指定格式，但是发送端要 content-type: text/plain 这里指定了kafka和zk的地址，最终的是，指定了我们要用的output，就是消息的生产通道，这里用的是自定义。下面指定了消息的目的地。 发送的程序SendService： 123456789101112@EnableBinding(Source.class)public class SendService &#123; @Autowired private Source source; public void sendMsg(String msg)&#123; source.output().send(MessageBuilder.withPayload(msg).build()); &#125;&#125; 这里就是接口一个字符串，然后把消息通过output发送到指定的目的地。@EnableBinding(Source.class)这个注解给我们绑定消息通道的，Source是Stream给我们提供的，可以点进去看源码，可以看到output和input,这和配置文件中的output，input对应的。 最后就是浏览器传入值： 1234567891011121314@RestControllerpublic class HelloController &#123; @Autowired private SendService sendService; @RequestMapping("/send/&#123;msg&#125;") public void send(@PathVariable("msg") String msg)&#123; System.out.println("发送了。。。"+msg); sendService.sendMsg(msg); &#125;&#125; 第三步： 消息的发送端已经搞好了，下面配置消息的消费端。配置文件为： 12345678910111213141516server: port: 8082spring: application: name: consumer_1 cloud: stream: kafka: binder: brockers: localhost:9092 zk-nodes: localhost:2181 auto-create-topics: true bindings: #input是接收，注意这里不能再像前面一样写output了 input: destination: stream-demo 接收的service： 12345678910//消息接受端，stream给我们提供了Sink,Sink源码里面是绑定input的，要跟我们配置文件的imput关联的。@EnableBinding(Sink.class)public class RecieveService &#123; @StreamListener(Sink.INPUT)//其实就是@StreamListener(“input”)，监听这个通道有没有消息过来，有就消费 public void recieve(Object payload)&#123; System.out.println("====="+payload); &#125;&#125; 第四步 启动kafka和zk，上一章已经讲过了如何启动。打开浏览器输入对应的url进行测试，看两个消费端是否都可以打印出来传入的信息。 总结 其实上面的一个流程就可以概括为下面一张图： 一个是产品的生产者，一个是商店消费者，生产者将产品通过通道，这里就是Source中默认的output，将产品发送到binder中，给他一个topic，告诉消费者这个消息的名字(位置，这里是stream-demo)。消费者通过监听Sink中的默认的input通道来看看有没有自己感兴趣的消息。 12345678910111213public interface Source &#123; String OUTPUT = "output"; @Output("output") MessageChannel output();&#125;public interface Sink &#123; String INPUT = "input"; @Input("input") SubscribableChannel input();&#125; 到这里，最基本的一个消息的生产消费流程就走完了。 三、自定义消息通道 到现在为止，我们进行了一个简单的消息发送和接收，用的是Stream给我们提供的默认Source，Sink，接下来我们要自己进行自定义，这种方式在工作中还是用的比较多的，因为我们要往不同的消息通道发消息， 必然不能全都叫input,output的，那样的话就乱套了，因此首先自定义一个接口，如下： 123456789/** * Created by cong on 2018/5/28. */public interface MySource &#123; @Output("myOutput") MessageChannel myOutput();&#125; 其实就是将默认的Source里的output改个名字而已，十分简单，在配置文件中指定通道的地方就要响应地修改为我们自己定义的通道就可以了。 12345678910111213141516171819202122server: port: 7888spring: application: name: producer cloud: stream: kafka: binder:#Kafka的消息中间件服务器 brockers: localhost:9092#Zookeeper的节点，如果集群，后面加,号分隔 zk-nodes: localhost:2181#如果设置为false,就不会自动创建Topic 有可能你Topic还没创建就直接调用了。 auto-create-topics: true bindings:#自定义output myOutput:#消息发往的目的地 destination: stream-demo#消息发送的格式，接收端不用指定格式，但是发送端要 content-type: text/plain 下面发送消息的时候，就是注入我们自己定义的source即可： 1234567891011@EnableBinding(MySource.class)public class SendService &#123; @Autowired private MySource source; public void sendMsg(String msg)&#123; source.myOutput().send(MessageBuilder.withPayload(msg).build()); &#125;&#125; 消费者同样如此，就是改一下Sink的名字即可。 四、消息中转站 这是一个比较有意思的功能，就是消息经过中间一个加工一下，再传给下一个消费者。就是一个链式的调用。 那么，我们这里改造一下spring-cloud-stream-consumer-1为spring-cloud-stream-trans: 第一步： 12345678910111213141516171819server: port: 7889spring: application: name: consumer_1 cloud: stream: kafka: binder: brockers: localhost:9092 zk-nodes: localhost:2181 auto-create-topics: true bindings:#input是接收，注意这里不能再像前面一样写output了 input: destination: stream-demo #进行消息中转处理后，在进行转发出去 output: destination: stream-demo-trans 第二步：接着在新建一个消息中转类，代码如下： 12345678910111213141516package hjc.consumer;import org.springframework.cloud.stream.annotation.EnableBinding;import org.springframework.cloud.stream.messaging.Processor;import org.springframework.integration.annotation.ServiceActivator;@EnableBinding(Processor.class)public class TransFormService &#123; @ServiceActivator(inputChannel = Processor.INPUT,outputChannel = Processor.OUTPUT) public Object transform(Object payload)&#123; System.out.println("消息中转站："+payload); return payload; &#125;&#125; 记得要将拷贝过来的工程中的SendService里面的注解全部删除掉，要不然程序启动会报错。 接着要修改消息中转站发送消息出去的接收端springcloud-stream2的配置,input的值改为stream-demo-trans即可。 五、消息分组 我们都是一端发消息，两个消息接受者都接收到了，但是有时候有些业务场景我只想让其中一个消息接收者接收到消息，那么该怎么办呢？ Group，如果使用过 Kafka 的读者并不会陌生。Spring Cloud Stream 的这个分组概念的意思基本和 Kafka 一致。微服务中动态的缩放同一个应用的数量以此来达到更高的处理能力是非常必须的。对于这种情况，同一个事件防止被重复消费， 只要把这些应用放置于同一个 group 中，就能够保证消息只会被其中一个应用消费一次。不同的组是可以消费的，同一个组内会发生竞争关系，只有其中一个可以消费。 修改消费段的配置文件，将他们都配置到同一个组下面： 123456789101112131415161718server: port: 7889spring: application: name: consumer_1 cloud: stream: kafka: binder: brockers: localhost:9092 zk-nodes: localhost:2181 auto-create-topics: true bindings: #input是接收，注意这里不能再像前面一样写output了 input: destination: stream-demo #分组的组名 group: group 可以看到springcloud-stream1和springcloud-stream2是属于同一组的。springcloud-stream模块的发的消息只能被springcloud-stream1或springcloud-stream2其中一个接收到，这样避免了重复消费。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[8.Bus消息总线]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F8.Bus%E6%B6%88%E6%81%AF%E6%80%BB%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[为了实现依次redresh自动刷新所有服务的配置文件，所以需要引入消息总线进行消息的通知，本文主要采用kafka作为消息队列来实现，当然了，rabbitMQ也是比较简单的。 一、前言 注：本篇文章是基于spring boot 2.x，主要参考 SpringCloud实战8-Bus消息总线 这篇文章而写。但是他的版本比较低，有一两个坑需要注意。 上一篇我们讲到，我们如果要去更新所有微服务的配置，在不重启的情况下去更新配置，只能依靠spring cloud config了，但是，是我们要一个服务一个服务的发送post请求，我们能受的了吗？ 虽然这比之前的没配置中心好多了，那但是我们如何继续避免挨个挨个的向服务发送Post请求来告知服务你的配置信息改变了，需要及时修改内存中的配置信息呢？ 这时候我们就不要忘记消息队列的发布订阅模型。让所有为服务来订阅这个事件，当这个事件发生改变了，就可以通知所有微服务去更新它们的内存中的配置信息。这时Bus消息总线就能解决，你只需要在springcloud Config Server端发出refresh，就可以触发所有微服务更新了。 如下架构图所示： 根据此图我们可以看出利用Spring Cloud Bus做配置更新的步骤: 1、提交代码触发post给客户端A发送bus/refresh 2、客户端A接收到请求从Server端更新配置并且发送给Spring Cloud Bus 3、Spring Cloud bus接到消息并通知给其它客户端 4、其它客户端接收到通知，请求Server端获取最新配置 5、全部客户端均获取到最新的配置 Spring Cloud Bus除了支持RabbitMQ的自动化配置之外，还支持现在被广泛应用的Kafka。在本文中，我们将搭建一个Kafka的本地环境，并通过它来尝试使用Spring Cloud Bus对Kafka的支持，实现消息总线的功能。 二、Kafka Kafka使用Scala实现，被用作LinkedIn的活动流和运营数据处理的管道，现在也被诸多互联网企业广泛地用作为数据流管道和消息系统。 Kafak架构图如下: Kafka是基于消息发布/订阅模式实现的消息系统，其主要设计目标如下： 消息持久化：以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。 高吞吐：在廉价的商用机器上也能支持单机每秒100K条以上的吞吐量 分布式：支持消息分区以及分布式消费，并保证分区内的消息顺序 跨平台：支持不同技术平台的客户端（如：Java、PHP、Python等） 实时性：支持实时数据处理和离线数据处理 伸缩性：支持水平扩展 Kafka中涉及的一些基本概念： Broker：Kafka集群包含一个或多个服务器，这些服务器被称为Broker。 Topic：逻辑上同Rabbit的Queue队列相似，每条发布到Kafka集群的消息都必须有一个Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个Broker上，但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） Partition：Partition是物理概念上的分区，为了提供系统吞吐率，在物理上每个Topic会分成一个或多个Partition，每个Partition对应一个文件夹（存储对应分区的消息内容和索引文件）。 Producer：消息生产者，负责生产消息并发送到Kafka Broker。 Consumer：消息消费者，向Kafka Broker读取消息并处理的客户端。 Consumer Group：每个Consumer属于一个特定的组（可为每个Consumer指定属于一个组，若不指定则属于默认组），组可以用来实现一条消息被组内多个成员消费等功能。 可以从kafka的架构图看到Kafka是需要Zookeeper支持的，你需要在你的Kafka配置里面指定Zookeeper在哪里，它是通过Zookeeper做一些可靠性的保证，做broker的主从，我们还要知道Kafka的消息是以topic形式作为组织的，Producers发送topic形式的消息， Consumer是按照组来分的，所以，一组Consumers都会接收同样的topic形式的消息。在服务端，它还做了一些分片，那么一个Topic可能分布在不同的分片上面，方便我们拓展部署多个机器，Kafka是天生分布式的。 首先是要下载对应的kafka：https://www.apache.org/dyn/closer.cgi?path=/kafka/2.1.0/kafka_2.11-2.1.0.tgz 解压之后进去/bin/windows/目录下： 首先启动zookeeper: 1.\zookeeper-server-start.bat D:\kafka_2.11-2.1.0\config\zookeeper.properties 如果出现错误： 123命令语法不正确。错误: 找不到或无法加载主类 Files\Java\jdk1.8.0_121\lib\dt.jar;C:\ProgramPS D:\kafka_2.11-2.1.0\bin\windows&gt; .\zookeeper-server-start.bat D:\kafka_2.11-2.1.0\config/zookeeper.properties 方法是： 首先我们进到下载好的Kafka目录中kafka_2.11-1.1.0\bin\windows 下编辑kafka-run-class.bat如下： 找到这条配置 如下： set COMMAND=%JAVA% %KAFKA_HEAP_OPTS% %KAFKA_JVM_PERFORMANCE_OPTS% %KAFKA_JMX_OPTS% %KAFKA_LOG4J_OPTS% -cp %CLASSPATH% %KAFKA_OPTS% %* 可以看到%CLASSPATH%没有双引号， 因此用双引号括起来，不然启动不起来的，报你JDK没安装好，修改后如下： set COMMAND=%JAVA% %KAFKA_HEAP_OPTS% %KAFKA_JVM_PERFORMANCE_OPTS% %KAFKA_JMX_OPTS% %KAFKA_LOG4J_OPTS% -cp “%CLASSPATH%” %KAFKA_OPTS% %* 后面启动kafka: 1.\kafka-server-start.bat D:\kafka_2.11-2.1.0\config\server.properties 这两者配置文件直接默认即可。启动成功之后，就放那吧！ 消息总线 在上一章的spring-cloud-config-server以及client继续集成。spring boot版本是2.0.3.RELEASE. 第一步：spring cloud config服务端和客户端(搞两个客户端)都要引入kafka依赖，以config server端为例： 12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--Eureka client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--config server--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--kafka MQ--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-kafka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 第二步：配置文件(服务端) 123456789101112131415161718192021server: port: 8085spring: application: name: config-server cloud: config: server: git: uri: https://github.com/sunweiguo/spring-cloud-config-center.git username: sunweiguo password: xxxxxxxxxeureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eurekamanagement: endpoints: web: exposure: include: bus-refresh 注意，在1.x版本中一般配置 1234#是否需要权限拉取，默认是true,如果不false就不允许你去拉取配置中心Server更新的内容management: security: enabled: false 而在2.x版本中已经把这个改掉了。所以要注意。不配置的话会报错405.可以尝试去掉，用postman测试一把。 第三步：添加注解 在服务端和客户端的启动函数上都增加一条注解@RefreshScope ok,至此，集成完毕。消息总线的功能就有了。 启动全部工程。修改git上的内容。然后发现客户端都没更新。下面启动postman来对config server发送一条post请求： 1localhost:8085/actuator/bus-refresh 再刷新浏览器，就会发现所有的客户端都自动更新了。我们也可以指定要刷新的客户端具体实例或者通配符符合的客户端。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7.Config分布式配置管理]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F7.Config%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[关于集中的配置管理已经在 10.天气预报系统-集中化配置 中详细介绍了。本文为了方便引出问题，所以重新介绍一下它的基本使用。 服务端 第一步：新建一个工程，引入相关依赖 12345678910111213141516171819202122232425&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--Eureka client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--config--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 第二步：添加注解 123456789@SpringBootApplication@EnableDiscoveryClient@EnableConfigServerpublic class SpringCloudConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudConfigServerApplication.class, args); &#125;&#125; 第三步：配置文件 12345678910111213141516server: port: 8085spring: application: name: config-server cloud: config: server: git: uri: https://github.com/sunweiguo/spring-cloud-config-center.git username: sunweiguo password: *********eureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eureka 第四步：在这个仓库里新建两个文件cloud-config-dev.properties和cloud-config-test.properties，里面有一些简单内容：name=sunweiguo-dev和name=sunweiguo-test 第四步：启动配置的服务端 浏览器输入：http://localhost:8085/cloud-config-dev.properties，如果可以看到里面配置的内容，那么就配置成功了。 客户端 第一步：新建一个sb项目，引入依赖 1234567891011121314151617181920212223242526272829&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--Eureka client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--健康监控包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 第二步：添加注解使其注册到eureka上，不再赘述 第三步：配置文件 注意，这里必须是bootstrap.yml，他的优先级别比application.yml要高。 123456789101112131415161718192021server: port: 8086# 这个cloud-config要与git上的文件名一致，文件的命名规则在开头的链接中已经详细介绍了spring: application: name: cloud-config cloud: config: profile: dev discovery: service-id: config-server # 致命服务端 enabled: trueeureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eurekamanagement: endpoints: web: exposure: include: &quot;*&quot; # 这是一个2.x的坑，需要配置这个暴露所有端点，否则refresh报404 第四步：写一个简单的controller来显示 12345678910111213@RestController@RefreshScopepublic class TestController &#123; @Value("$&#123;name&#125;") private String name; @RequestMapping("/test") public String test()&#123; return "hello."+this.name; &#125;&#125; 第五步：在浏览器中输入对应的url看是否显示正确，下面就是在git上修改一下对应的文件内容。我们会发现，服务端上面已经更新了，但是客户端没有更新。那么就需要我们手动去触发他更新 第六步：打开postman工具。输入Url：localhost:8086/actuator/refresh 如果返回： 1234[ &quot;config.client.version&quot;, &quot;name&quot;] 那么表示更新成功。此时再刷新之前的页面，会发现客户端的内容已经同步过来了。 总结 我们实现了将配置文件放在git上集中管理，但是在修改中心仓库中的配置后，客户端是没有更新的，后来我们用更新包手动去更新，虽然不用重启服务，但是还是比较麻烦。那么，如何避免，对每个服务发送post请求去更新呢？ 其实很简单，就是利用发布订阅的原理，利用消息来通知从而触发更新，是比较好的方式。下一节介绍消息总线。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.Zuul网关服务]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F6.Zuul%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[在 9.天气预报系统-API网关 中已经对API网关进行了详细的介绍。基础的概念就不再赘述了。下面着重看一下zuul中如何实现过滤器。 基本使用 第一步:创建一个springboot工程，引入依赖： 123456789101112131415161718192021222324252627282930313233343536&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--Eureka client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--zuul--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 第二步：在启动函数上增加注解： 123456789@SpringBootApplication@EnableDiscoveryClient@EnableZuulProxypublic class SpringCloudEurekaZuulApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudEurekaZuulApplication.class, args); &#125;&#125; 第三步：配置文件： 1234567891011121314spring: application: name: api-gatewayeureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eurekazuul: routes: service-hi: path: /service-hi/**/ serviceId: service-hiserver: port: 8084 主要就是配置路由规则，以service-hi为前缀的都去serivce-hi为名字的服务下找对应的映射。 这样，就完成了基本的整合。启动其他的项目：spring-cloud-eureka-server-1,spring-cloud-eureka-server-2,spring-cloud-eureka-client-1,spring-cloud-eureka-client-1. 在我的spring-cloud-eureka-client-1有一个路径叫localhost:8080/test返回一个字符串。那么此时用网关来访问的话应该是：localhost:8084/service-hi/test. 过滤器 新建一个类，假设我们要检验token，如果token为空，就返回未授权。 TokenFilter： 12345678910111213141516171819202122232425262728293031323334353637public class TokenFilter extends ZuulFilter &#123; //pre 在路由表寻找路由时生效 //routing 找到路由开始进行请求转发时生效 //error 出错时生效 //post routing或者error之后最后阶段生效 @Override public String filterType() &#123; return "pre"; &#125; //越小，优先级越高 @Override public int filterOrder() &#123; return 0; &#125; //true表示过滤器生效 @Override public boolean shouldFilter() &#123; return true; &#125; @Override public Object run() throws ZuulException &#123; RequestContext context = RequestContext.getCurrentContext(); HttpServletRequest request = context.getRequest(); String token = request.getParameter("token"); if(token == null)&#123; context.setSendZuulResponse(false); context.setResponseStatusCode(401); context.setResponseBody("unsutherized"); return null; &#125; return null; &#125;&#125; 要将其注册到spring： 1234@Beanpublic TokenFilter tokenFilter()&#123; return new TokenFilter();&#125; 再次访问localhost:8084/service-hi/test时，显示unsutherized，当访问localhost:8084/service-hi/test?token=123时访问通过。 如果在run()中出现异常，但是这个异常不能抛出来显示在页面上。那么如何在页面上如何显示呢？在catch的代码中这样写： 123context.set(error.status_code,401);context.set("error.exception",e);context.set("error.message","提示信息");]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.Hystrix请求合并]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F4.Hystrix%E8%AF%B7%E6%B1%82%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[在微服务架构中，我们将一个项目拆分成很多个独立的模块，这些独立的模块通过远程调用来互相配合工作，但是，在高并发情况下，通信次数的增加会导致总的通信时间增加，同时，线程池的资源也是有限的，高并发环境会导致有大量的线程处于等待状态，进而导致响应延迟，为了解决这些问题，我们需要来了解Hystrix的请求合并。 一、请求合并 hystrix中的请求合并，就是利用一个合并处理器，将对同一个服务发起的连续请求合并成一个请求进行处理(这些连续请求的时间窗默认为10ms)，在这个过程中涉及到的一个核心类就是HystrixCollapser。 请求不合并之前： 下图展示了在未使用HystrixCollapser请求合并器之前的线程使用情况。可以看到当服务消费者同时对USER-SERVICE的/users/{id}接口发起了五个请求时，会向该依赖服务的独立线程池中申请五个线程来完成各自的请求操作。 而在使用了HystrixCollapser请求合并器之后，相同情况下的线程占用如下图所示。由于同一时间发生的五个请求处于请求合并器的一个时间窗内，这些发向/users/{id}接口的请求被请求合并器拦截下来，并在合并器中进行组合，然后将这些请求合并成一个请求发向USER-SERVICE的批量接口/users?ids={ids}，在获取到批量请求结果之后，通过请求合并器再将批量结果拆分并分配给每个被合并的请求。从图中我们可以看到以来，通过使用请求合并器有效地减少了对线程池中资源的占用。所以在资源有效并且在短时间内会产生高并发请求的时候，为避免连接不够用而引起的延迟可以考虑使用请求合并器的方式来处理和优化。 本着简单的目的，所以非注解的方式我就没做。具体参考文章：Spring Cloud中Hystrix的请求合并 1.1 服务提供者 首先是需要服务提供者提供两个接口，一个是返回集合，模拟在同一时间多个线程同时来请求书籍信息。 12345678910@RequestMapping("/getbooks")public List&lt;Book&gt; books(String ids) &#123; System.out.println("ids&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;" + ids); ArrayList&lt;Book&gt; books = new ArrayList&lt;&gt;(); books.add(new Book("《李自成》", 55, "姚雪垠", "人民文学出版社")); books.add(new Book("中国文学简史", 33, "林庚", "清华大学出版社")); books.add(new Book("文学改良刍议", 33, "胡适", "无")); books.add(new Book("ids", 22, "helloworld", "haha")); return books;&#125; 1.2 服务消费者 首先在BookService中添加两个方法用来调用服务提供者提供的接口，如下： 1234567891011@HystrixCollapser(batchMethod = "test11",collapserProperties = &#123;@HystrixProperty(name ="timerDelayInMilliseconds",value = "100")&#125;)public Future&lt;Book&gt; test10(Long id) &#123; return null;&#125;@HystrixCommandpublic List&lt;Book&gt; test11(List&lt;Long&gt; ids) &#123; System.out.println("test9---------"+ids+"Thread.currentThread().getName():" + Thread.currentThread().getName()); Book[] books = restTemplate.getForObject("http://SERVICE-HI/getbooks?ids=&#123;1&#125;", Book[].class, StringUtils.join(ids, ",")); return Arrays.asList(books);&#125; 在test10方法上添加@HystrixCollapser注解实现请求合并，用batchMethod属性指明请求合并后的处理方法，collapserProperties属性指定其他属性:为请求合并器设置了时间延迟属性，合并器会在该时间窗内收集获取单个Book的请求并在时间窗结束时进行合并组装成单个批量请求。 1.3 调用消费服务 下面就是直接调用test10即可： 123456789101112131415161718@RequestMapping("/books")public void test7() throws ExecutionException, InterruptedException &#123; HystrixRequestContext context = HystrixRequestContext.initializeContext(); Future&lt;Book&gt; f1 = bookService.test10(1l); Future&lt;Book&gt; f2 = bookService.test10(2l); Future&lt;Book&gt; f3 = bookService.test10(3l); Book b1 = f1.get(); Book b2 = f2.get(); Book b3 = f3.get(); Thread.sleep(3000); Future&lt;Book&gt; f4 = bookService.test10(4l); Book b4 = f4.get(); System.out.println("b1&gt;&gt;&gt;"+b1); System.out.println("b2&gt;&gt;&gt;"+b2); System.out.println("b3&gt;&gt;&gt;"+b3); System.out.println("b4&gt;&gt;&gt;"+b4); context.close();&#125; 1.4 运行结果 123456test9---------[3, 1, 2]Thread.currentThread().getName():hystrix-BookService-6test9---------[4]Thread.currentThread().getName():hystrix-BookService-7b1&gt;&gt;&gt;com.swg.springcloudeureka.Book@5a901d44b2&gt;&gt;&gt;com.swg.springcloudeureka.Book@3305e39b3&gt;&gt;&gt;com.swg.springcloudeureka.Book@2282567cb4&gt;&gt;&gt;com.swg.springcloudeureka.Book@52e7b4ea 前三个请求会进行合并，即1，2，3三个id被合并到一个list中传给test11，第四个请求会单独执行. 二、总结 请求合并的优点小伙伴们已经看到了，多个请求被合并为一个请求进行一次性处理，可以有效节省网络带宽和线程池资源，但是，有优点必然也有缺点，设置请求合并之后，本来一个请求可能5ms就搞定了，但是现在必须再等10ms看看还有没有其他的请求一起的，这样一个请求的耗时就从5ms增加到15ms了，不过，如果我们要发起的命令本身就是一个高延迟的命令，那么这个时候就可以使用请求合并了，因为这个时候时间窗的时间消耗就显得微不足道了，另外高并发也是请求合并的一个非常重要的场景。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.Hystrix请求熔断服务降级]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F3.Hystrix%E8%AF%B7%E6%B1%82%E7%86%94%E6%96%AD%E6%9C%8D%E5%8A%A1%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[基本的介绍和基本的演示都已经在11.天气预报系统-熔断机制中说明和演示了。下面来说点不一样的东西吧！ 一、前言 比如拿我上一章中的代码，我们将spring-eureka-ribbon升级为spring-cloud-ribbon-hystrix: 无非是引入依赖，主函数添加一个允许熔断的声明。 12345678910111213141516@Servicepublic class HelloServiceImpl implements IHelloServie &#123; @Autowired RestTemplate restTemplate; @Override @HystrixCommand(fallbackMethod = "helloFallBack") public String hiService(String name) &#123; return restTemplate.getForEntity("http://SERVICE-HI/hello?name="+name,String.class).getBody(); &#125; public String helloFallBack(String name)&#123; return "&lt;font color='red'&gt;error&lt;/font&gt;"; &#125;&#125; 注意，我们的fallBack方法的参数必须与被注解的方法的参数一致，否则会报错。 启动项目：spring-cloud-eureka-server-1,spring-cloud-eureka-server-2,spring-cloud-eureka-client-1,spring-cloud-eureka-client-2,spring-cloud-ribbon-hystrix. 然后关闭一个client，看是否触发服务降级，显示红色的error. 二、用法进阶-实现异步消费 我们不用@HystrixCommand注解来实现上面个功能。代码见spring-cloud-eureka-ribbon-hystrix-02 1234567891011121314@RestControllerpublic class HelloController &#123; @Autowired private RestTemplate restTemplate; @RequestMapping("hello") public String hello()&#123; HelloServiceCommand command = new HelloServiceCommand("hello",restTemplate); String res = command.execute(); return res; &#125;&#125; 其中核心的HelloServiceCommand: 12345678910111213141516171819public class HelloServiceCommand extends HystrixCommand&lt;String&gt; &#123; private RestTemplate restTemplate; public HelloServiceCommand(String commandGroupKey,RestTemplate restTemplate) &#123; super(HystrixCommandGroupKey.Factory.asKey(commandGroupKey)); this.restTemplate = restTemplate; &#125; @Override protected String run() throws Exception &#123; return restTemplate.getForEntity("http://SERVICE-HI/hello?name='swg'",String.class).getBody(); &#125; @Override protected String getFallback() &#123; return "&lt;font color='red'&gt;error&lt;/font&gt;"; &#125;&#125; 这样就可以用代码实现上面的注解的功能。 我们可以发现，run()方法里调用其他的服务（多个），如果是串行执行，那么时间是所有服务执行时间之和。那么，有没有办法使他并行执行呢？达到一种NIO的效果。 NIO的两个实现方式：Future将来式和Callable回调式，这里使用将来式。 其实核心就是Future&lt;String&gt; future = command.queue();，让这个方法自己另开一个线程去默默执行，本线程还继续往下，等我想到结果的时候，再去调用String res = future.get();阻塞地获取结果，如果结果已经准备好了，那么就直接拿到。 在代码spring-cloud-eureka-ribbon-hystrix-03中的controller中进行测试： 12345678910111213@RequestMapping("hello")public String hello() throws ExecutionException, InterruptedException &#123; HelloServiceCommand command = new HelloServiceCommand("hello",restTemplate); long now = System.currentTimeMillis(); Future&lt;String&gt; future = command.queue(); System.out.println("start"); long end = System.currentTimeMillis(); System.out.println(end - now); String res = future.get(); long last = System.currentTimeMillis()-end; System.out.println(last); return res;&#125; 测试的目的就是看调用command.queue()之后会不会阻塞本线程的执行，我们让被restTemplate调用的方法睡眠一会： 1234567@RequestMapping("hello")public String hello(@RequestParam("name")String name) throws InterruptedException &#123; System.out.println("方法开始执行。。。"); Thread.sleep(800); System.out.println("方法执行结束。。。"); return "hi "+ name +",you are from " + port;&#125; 结果打印： 123start1817 就是说将执行其他服务这个操作异步到了另外一个线程中执行，本线程立即执行下面的逻辑。这样，提高了效率。 好了，用非注解的方式来实现了一下NIO的实现方式，那么肯定还是使用注解比较方便，那么基于注解的话，我们如何实现future的方式来执行呢？代码spring-cloud-eureka-ribbon-hystrix-04 我们只需要将原来的HelloServiceImpl中的hiService方法改为： 12345678910111213import com.netflix.hystrix.contrib.javanica.command.AsyncResult;@Override@HystrixCommand(fallbackMethod = "helloFallBack")public String hiService(String name) throws ExecutionException, InterruptedException &#123; Future&lt;String&gt; future = new AsyncResult&lt;String&gt;() &#123; @Override public String invoke() &#123; return restTemplate.getForEntity("http://SERVICE-HI/hello?name="+name,String.class).getBody(); &#125; &#125;; return future.get();&#125; 就可以实现这种异步的方式调用了。 三、观察者模式来实现 订阅者来监听自己感兴趣的事件，可以实现多个请求集中处理。 代码不想去搞了… 四、总结 这里主要是实现熔断的功能，一开始的@HystrixCommand+fallback，到后面自己用HystrixCommand代码实现的，再到后来用Future来实现异步消费，到最后介绍的用观察者模式来实现的方式HystrixObserverCommand。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.Ribbon客户端负载均衡]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F2.Ribbon%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[在微服务架构中，业务都会被拆分成一个独立的服务，服务与服务的通讯是基于http restful的。Spring cloud有两种服务调用方式，一种是ribbon+restTemplate，另一种是feign。在这一篇文章首先讲解下基于ribbon+rest。 一、前言 Spring Cloud Ribbon 是一个基于 HTTP 和 TCP 的客户端负载均衡工具，它基于 Netflix Ribbon 实现。 通过 Spring Cloud 的封装， 可以让我们轻松地将面向服务的 REST 模板请求自动转换成客户端负载均衡的服务调用。Spring Cloud Ribbon 虽然只是一个工具类框架，它不像服务注册中心、 配置中心、 API 网关那样需要独立部署， 但是它几乎存在于每一个Spring Cloud 构建的微服务和基础设施中。 因为微服务间的调用，API 网关的请求转发等内容实际上都是通过Ribbon 来实现的，包括后续我们将要介绍的 Feign, 它也是基于 Ribbon实现的工具。 二、Ribbon ribbon是一个负载均衡客户端，可以很好的控制htt和tcp的一些行为。Feign默认集成了ribbon。 而之前用的nginx来实现负载均衡，他是一种服务端的负载均衡。 集成Ribbon也是比较简单的。新建一个项目spring-cloud-eureka-ribbon 2.1 引入依赖： 1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.2 然后就是注册一个专门负载均衡的一个客户端： 1234567891011121314@SpringBootApplication@EnableDiscoveryClientpublic class SpringCloudEurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudEurekaApplication.class, args); &#125; @Bean @LoadBalanced RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 在工程的启动类中,通过@EnableDiscoveryClient向服务中心注册；并且向程序的ioc注入一个bean: restTemplate;并通过@LoadBalanced注解表明这个restRemplate开启负载均衡的功能。 2.3 配置文件 就是向注册中心集群注册而已。所以，上章中提到的Eureka两个服务端要启动起来。 123456789server: port: 8082spring: application: name: eureka-ribboneureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eureka 2.4 测试的controller 这个ribbon来接收请求，然后由他来决定转发到哪个服务。 所以这里要启动起来两个服务，让我来实际调用，我们就给他一个最简单的功能：打印出来端口。 1234567891011@RestControllerpublic class HelloController &#123; @Value("$&#123;server.port&#125;") private String port; @RequestMapping("hello") public String hello(@RequestParam("name")String name)&#123; return "hi "+ name +",you are from " + port; &#125;&#125; 然后在我的ribbon中写一个controller作为一个统一的入口： 123456789101112@RestControllerpublic class HelloController &#123; @Autowired private IHelloServie helloServie; @RequestMapping("hello") public String hello(@RequestParam("name")String name)&#123; return helloServie.hiService(name); &#125;&#125; 具体的service就是用刚才定义的restTemplate来根据服务实例的名称去发起调用： 1234567891011@Servicepublic class HelloServiceImpl implements IHelloServie &#123; @Autowired RestTemplate restTemplate; @Override public String hiService(String name) &#123; return restTemplate.getForObject("http://SERVICE-HI/hello?name="+name,String.class); &#125;&#125; 2.5 页面测试 浏览器输入http://localhost:8082/hello?name=sunweiguo，不停地刷新，我们会看到轮流显示： hi sunweiguo,you are from 8080和hi sunweiguo,you are from 8081这两句，说明负载均衡已经生效，并且算法是轮询。当然，我们也可以用其他的负载均衡算法。这里就不做演示了。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.Eureka服务治理]]></title>
    <url>%2F2019%2F02%2F21%2Fspring-cloud-modules%2F1.Eureka%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%2F</url>
    <content type="text"><![CDATA[在上一系列中，我们以最简单的业务场景急速入门了微服务，当然了，忽略了很多的细节，作为入门是可以的，但是真正使用的时候，还是需要了解很多额外的使用知识，所以本系列就孕育而生，即在上一系列的基础上继续强化，加深对spring cloud的使用理解。本章首先还是介绍eureka服务注册和发现组件，因为它是微服务的基石。 关于Eureka的基本使用，在另外一篇文章中详细介绍了：7.天气预报系统-微服务的注册和发现，不再赘述。本章学习如何搭建一个eureka高可用集群。 一、前言 首先来看看Eureka高可用的架构。 其中，续约是指：服务端维持一个有过期时间的服务列表，当客户端访问一次，我就刷新一下过期时间。 1234# 90秒不来续约就剔除lease-expiration-duration-in-seconds: 90# 每隔30秒自动续约一次lease-renewal-interval-in-seconds: 30 在客户端也可以设置一个定时任务，每隔多长时间去请求一下服务器，刷新一下服务列表： 12# 默认每隔30去请求一下服务器registry-fetch-interval-seconds: 30 二、总结 Eureka包含两个组件：Eureka Server 和 Eureka Client，它们的作用如下： Eureka Client是一个Java客户端，用于简化与Eureka Server的交互； Eureka Server提供服务发现的能力，各个微服务启动时，会通过Eureka Client向Eureka Server进行注册自己的信息（例如网络信息），Eureka Server会存储该服务的信息； 微服务启动后，会周期性地向Eureka Server发送心跳（默认周期为30秒）以续约自己的信息。如果Eureka Server在一定时间内没有接收到某个微服务节点的心跳，Eureka Server将会注销该微服务节点（默认90秒）； 每个Eureka Server同时也是Eureka Client，多个Eureka Server之间通过复制的方式完成服务注册表的同步； Eureka Client会缓存Eureka Server中的信息。即使所有的Eureka Server节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者。 三、集群配置 Server1端配置文件： 1234567891011server: port: 8761eureka: instance: hostname: eureka client: service-url: defaultZone: http://localhost:8762/eurekaspring: application: name: eureka-server Server2端配置文件： 1234567891011server: port: 8762eureka: instance: hostname: eureka client: service-url: defaultZone: http://localhost:8761/eurekaspring: application: name: eureka-server 就是说两个server互相注册。形成一个server集群。 client1配置文件： 123456789server: port: 8080spring: application: name: eureka-clienteureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eureka client2也是如此。 123456789server: port: 8081spring: application: name: eureka-clienteureka: client: service-url: defaultZone: http://localhost:8761/eureka,http://localhost:8762/eureka 分别启动之后，我们会看到效果： 关于本系列的所有代码，将按照1-10的章节依次存放在：https://github.com/sunweiguo/swgBook-for-spring-cloud/tree/master/spring-cloud-modules 后面不再赘述。]]></content>
      <tags>
        <tag>springcloud组件系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11.天气预报系统-熔断机制]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F11.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E7%86%94%E6%96%AD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[这是学习的第十一篇文章，某个服务一旦出现不可用，可能会牵连整个系统造成雪崩，或者有的时候资源需要聚集在核心业务上，非核心业务就可以适当地关闭，此时就是需要有一种机制来实现系统保护和服务降级的功能。本章介绍hystrix组件。 一、定义 保护系统的一种方式，当请求超出阈值，把真实的服务接口断开，可能只是返回给你一个默认值。这样，掐断了自己的服务，又可以给用户一个响应。 对该服务的调用执行熔断，对于后续请求，不再继续调用该目标服务，而是直接返回，从而可以快速释放资源。 熔断器好处：系统稳定、减少性能损耗、及时响应、阈值可配置 熔断这一概念来源于电子工程中的断路器（Circuit Breaker）。在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。 这种牺牲局部，保全整体的措施就叫做熔断。 如果不采取熔断措施，我们的系统会怎样呢？我们来看一个栗子。当前系统中有A，B，C三个服务，服务A是上游，服务B是中游，服务C是下游。它们的调用链如下： 一旦下游服务C因某些原因变得不可用，积压了大量请求，服务B的请求线程也随之阻塞。线程资源逐渐耗尽，使得服务B也变得不可用。紧接着，服务A也变为不可用，整个调用链路被拖垮。 像这种调用链路的连锁故障，叫做雪崩。 正所谓刮骨疗毒，壮士断腕。在这种时候，就需要我们的熔断机制来挽救整个系统。 开启熔断:在固定时间窗口内，接口调用超时比率达到一个阈值，会开启熔断。进入熔断状态后，后续对该服务接口的调用不再经过网络，直接执行本地的默认方法，达到服务降级的效果。 熔断恢复:熔断不可能是永久的。当经过了规定时间之后，服务将从熔断状态回复过来，再次接受调用方的远程调用。 二、熔断和降级 在股票市场，熔断这个词大家都不陌生，是指当股指波幅达到某个点后，交易所为控制风险采取的暂停交易措施。相应的，服务熔断一般是指软件系统中，由于某些原因使得服务出现了过载现象，为防止造成整个系统故障，从而采用的一种保护措施，所以很多地方把熔断亦称为过载保护。 大家都见过女生旅行吧，大号的旅行箱是必备物，平常走走近处绰绰有余，但一旦出个远门，再大的箱子都白搭了，怎么办呢？常见的情景就是把物品拿出来分分堆，比了又比，最后一些非必需品的就忍痛放下了，等到下次箱子够用了，再带上用一用。而服务降级，就是这么回事，整体资源快不够了，忍痛将某些服务先关掉，待渡过难关，再开启回来。 降级白话理解：比如在公司 遇到贵宾要来 就把一些不重要的常规接待暂停 把这些资源供给招待贵宾 之前有个淘宝的分享 比如双11 把订单评论和收藏等功能在这一天暂停 把这些资源分给其它关键服务 比如下单 所以从上述分析来看，两者其实从有些角度看是有一定的类似性的： 目的很一致，都是从可用性可靠性着想，为防止系统的整体缓慢甚至崩溃，采用的技术手段； 最终表现类似，对于两者来说，最终让用户体验到的是某些功能暂时不可达或不可用； 粒度一般都是服务级别，当然，业界也有不少更细粒度的做法，比如做到数据持久层（允许查询，不允许增删改）； 自治性要求很高，熔断模式一般都是服务基于策略的自动触发，降级虽说可人工干预，但在微服务架构下，完全靠人显然不可能，开关预置、配置中心都是必要手段； 而两者的区别也是明显的： 触发原因不太一样，服务熔断一般是某个服务（下游服务）故障引起，而服务降级一般是从整体负荷考虑； 管理目标的层次不太一样，熔断其实是一个框架级的处理，每个微服务都需要（无层级之分），而降级一般需要对业务有层级之分（比如降级一般是从最外围服务开始） 三、Spring Cloud Hystrix Spring Cloud Hystrix是基于Netflix的开源框架Hystrix实现，该框架实现了服务熔断、线程隔离等一系列服务保护功能。对于熔断机制的实现，Hystrix设计了三种状态： 熔断关闭状态（Closed）：服务没有故障时，熔断器所处的状态，对调用方的调用不做任何限制。 熔断开启状态（Open）：在固定时间窗口内（Hystrix默认是10秒），接口调用出错比率达到一个阈值（Hystrix默认为50%），会进入熔断开启状态。进入熔断状态后，后续对该服务接口的调用不再经过网络，直接执行本地的fallback方法。 半熔断状态（Half-Open）：在进入熔断开启状态一段时间之后（Hystrix默认是5秒），熔断器会进入半熔断状态。所谓半熔断就是尝试恢复服务调用，允许有限的流量调用该服务，并监控调用成功率。如果成功率达到预期，则说明服务已恢复，进入熔断关闭状态；如果成功率仍旧很低，则重新进入熔断关闭状态。 集成Hystrix也是很简单的： demo的改造的基础是eureka-client-feign,将其改造为eureka-client-feign-hystrix 1、引入依赖： 12345&lt;!--Hystrix--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 2、添加注解@EnableCircuitBreaker，启用Hystrix 12345678910@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients@EnableCircuitBreakerpublic class EurekaClientFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaClientFeignApplication.class, args); &#125;&#125; 3、在controller方法上增加注解@HystrixCommand 12345678910111213141516@RestControllerpublic class TestController &#123; @Autowired private CityClient cityClient; @GetMapping("cities") @HystrixCommand(fallbackMethod = "defaultCities") public String getData()&#123; String res = cityClient.listCity(); return res; &#125; public String defaultCities()&#123; return "City Data Server is down!"; &#125;&#125; 4、测试 启动eureka和城市数据服务，再启动本服务，是正常的。 那么，我们将城市数据服务关闭，看看有没有返回我们指定的默认值。 四、改造本系统 在demo中，用@HystrixCommand注解中的熔断时执行的方法来实现异常情况下的默认返回。现在我们要改造msa-weather-report-eureka-feign-gateway，将其改造为msa-weather-report-eureka-feign-gateway-hystrix，我们用新的方式，直接在DataClient这个接口里面声明触发熔断时回调的类DataClientFallback.class。 123456789101112131415@FeignClient(name = "msa-eureka-client-zuul",fallback = DataClientFallback.class)public interface DataClient &#123; /** * 获取城市列表 */ @RequestMapping("city/cities") List&lt;City&gt; listCity() throws Exception; /** * 根据城市ID获取天气 */ @RequestMapping("data/weather/cityId/&#123;cityId&#125;") WeatherResponse getDataByCityId(@PathVariable("cityId") String cityId);&#125; 具体这个回调的类里面时这样写的： 123456789101112131415161718@Componentpublic class DataClientFallback implements DataClient &#123; @Override public List&lt;City&gt; listCity() throws Exception &#123; List&lt;City&gt; cityList = new ArrayList&lt;&gt;(); City city = new City(); city.setCityId(&quot;101190101&quot;); city.setCityName(&quot;默认的南京&quot;); cityList.add(city); return cityList; &#125; @Override public WeatherResponse getDataByCityId(String cityId) &#123; return null; &#125;&#125; 也就是说，如果城市数据服务挂了，就默认返回一下我这里设置的城市；如果获取天气信息的服务挂了，我们就直接返回null; 那么，我们就相当于在feign中启用hystrix，就需要在配置文件中增加配置： 123feign: hystrix: enabled: true 因为如果根据城市id获取天气信息的服务不可用时，我们默认直接返回null，显示页面啥都不显示时不好的，所以我们需要在前端判断一下： 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!--不为空时--&gt;&lt;div th:if="$&#123;reportModel.report&#125; != null"&gt; &lt;div class="row"&gt; &lt;h1 class="text-success" th:text="$&#123;reportModel.report.city&#125;"&gt;城市名称&lt;/h1&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;p&gt; 空气质量指数：&lt;span th:text="$&#123;reportModel.report.aqi&#125;"&gt;&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;p&gt; 当前温度：&lt;span th:text="$&#123;reportModel.report.wendu&#125;"&gt;&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;p&gt; 温馨提示：&lt;span th:text="$&#123;reportModel.report.ganmao&#125;"&gt;&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;div class="card border-info" th:each="forecast : $&#123;reportModel.report.forecast&#125;"&gt; &lt;div class="card-body text-info"&gt; &lt;p class="card-text" th:text="$&#123;forecast.date&#125;"&gt;日期&lt;/p&gt; &lt;p class="card-text " th:text="$&#123;forecast.type&#125;"&gt;天气类型&lt;/p&gt; &lt;p class="card-text" th:text="$&#123;forecast.high&#125;"&gt;最高温度&lt;/p&gt; &lt;p class="card-text" th:text="$&#123;forecast.low&#125;"&gt;最低温度&lt;/p&gt; &lt;p class="card-text" th:text="$&#123;forecast.fengxiang&#125;"&gt;风向&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;!--为空时，说明熔断器生效，直接显示提示信息--&gt;&lt;div th:if="$&#123;reportModel.report&#125; == null"&gt; &lt;div class="row"&gt; &lt;p&gt; 天气数据API服务暂不可用！ &lt;/p&gt; &lt;/div&gt;&lt;/div&gt; 下面就来测试一把吧！ 首先时完全正常的情况，各个服务都可用： 启动如下服务：redis,weather-sureka-server,msa-weather-city-eureka,msa-weather-collection-eureka-feign-gateway,msa-weather-data-eureka,msa-weather-report-eureka-feign-gateway-hystrix,msa-eureka-client-zuul这六个服务： 正常的话，就会看到之前的页面：http://localhost:8083/report/cityId/101190101 城市数据服务不可用，熔断器生效： 关闭城市数据服务msa-weather-city-eureka，造成服务不可用的现象。看页面显示是否只有我塞进去的假数据。 天气数据服务不可用，熔断器生效： 关闭天气数据API服务msa-weather-data-eureka.看页面是否显示服务暂不可用的提示信息。 报了一个空指针错误，原因是msa-weather-report-eureka-feign-gateway-hystrix中WeatherReportServiceImpl中的方法原来是这样写的： 1234567891011@Service@Slf4jpublic class WeatherReportServiceImpl implements IWeatherReportService &#123; @Autowired private WeatherClient weatherClient; @Override public Weather getDataByCityId(String cityId) &#123; return weatherClient.getDataByCityId(cityId).getData(); &#125;&#125; 显然，要做一下判空操作，否则是不能调用getData()这个方法的。 12345678910111213141516@Service@Slf4jpublic class WeatherReportServiceImpl implements IWeatherReportService &#123; @Autowired private DataClient dataClient; @Override public Weather getDataByCityId(String cityId) &#123; WeatherResponse res = dataClient.getDataByCityId(cityId); Weather weather = null; if(res != null)&#123; weather = res.getData(); &#125; return weather; &#125;&#125; 这样子，重新启动天气预报UI服务。就可以看到效果啦！ 这样，本系统集成hystrix就成功了。]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10.天气预报系统-集中化配置]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F10.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E9%9B%86%E4%B8%AD%E5%8C%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[这是学习的第十篇文章，服务拆分之后，配置文件就必然随着这些拆分的服务分散在各个服务器上，这对运营是一个灾难，用一个集中化的方式统一进行配置文件的配置与修改是必要的。本章介绍spring config的基本使用，但是还是存在一些小问题，在后面的实战环节中会解决。 一、背景 随着线上项目变的日益庞大，每个项目都散落着各种配置文件，如果采用分布式的开发模式，需要的配置文件随着服务增加而不断增多。某一个基础服务信息变更，都会引起一系列的更新和重启，运维苦不堪言也容易出错。配置中心便是解决此类问题的灵丹妙药。 我们需要一个外部的、集中化的一个配置中心。 二、配置分类 按配置的来源划分 主要有源代码、文件、数据库连接、远程调用等 按配置的环境划分 主要有开发环境、测试环境、预发布环境、生产环境等。 按配置的集成阶段划分 编译时、打包时和运行时 按配置的加载方式划分 启动加载和动态加载 三、Spring Cloud Config 在我们了解spring cloud config之前，我可以想想一个配置中心提供的核心功能应该有什么 提供服务端和客户端支持 集中管理各环境的配置文件 配置文件修改之后，可以快速的生效 可以进行版本管理 支持大的并发查询 支持各种语言 Spring Cloud Config可以完美的支持以上所有的需求。 Spring Cloud Config项目是一个解决分布式系统的配置管理方案。它包含了Client和Server两个部分，server提供配置文件的存储、以接口的形式将配置文件的内容提供出去，client通过接口获取数据、并依据此数据初始化自己的应用。Spring cloud使用git或svn存放配置文件，默认情况下使用git. Server端 注册到eureka的实例名：weather-config-server 1、添加依赖 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2、配置文件 1234567891011121314151617spring: application: name: weather-config-server cloud: config: server: git: uri: https://github.com/sunweiguo/spring-cloud-config-center search-paths: config-repo username: sunweiguo password: xxxeureka: client: service-url: defaultZone: http://localhost:8761/eurekaserver: port: 8086 config-repo这个文件夹是由自己在github上创建的。在这个目录下新建一个文件：weather-config-client-dev.properties,里面的内容为auther=oursnail.cn(随便写点东西以供测试) 仓库中的配置文件会被转换成web接口，访问可以参照以下的规则： /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties 我这里的weather-config-client-dev.properties,它的application是weather-config-client，profile是dev。client会根据填写的参数来选择读取对应的配置。 3、启动类 启动类添加@EnableConfigServer，激活对配置中心的支持 123456789@SpringBootApplication@EnableDiscoveryClient@EnableConfigServerpublic class WeatherEurekaClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(WeatherEurekaClientApplication.class, args); &#125;&#125; 到此server端相关配置已经完成 4、测试 访问 http://localhost:8086/auther/dev 返回： 12345678910&#123;&quot;name&quot;: &quot;auther&quot;,&quot;profiles&quot;: [&quot;dev&quot;],&quot;label&quot;: null,&quot;version&quot;: &quot;ef1a6baeddce01d3956ba2a7181f66721959a10c&quot;,&quot;state&quot;: null,&quot;propertySources&quot;: []&#125; 我们可以读到auther里的内容，说明服务端配置成功。 四、Client端 注册到eureka的实例名：weather-config-client 1、添加依赖 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2、配置文件 需要配置两个配置文件，application.properties和bootstrap.properties application.properties如下： 1234spring.application.name=weather-config-clientserver.port=8087eureka.client.service-url.defaultZone: http://localhost:8761/eureka bootstrap.properties如下： 1234spring.cloud.config.name=weather-config-clientspring.cloud.config.profile=devspring.cloud.config.uri=http://localhost:8086/spring.cloud.config.label=master spring.application.name：对应{application}部分 spring.cloud.config.profile：对应{profile}部分 spring.cloud.config.label：对应git的分支。如果配置中心使用的是本地存储，则该参数无用 spring.cloud.config.uri：配置中心的具体地址,就是server端地址 特别注意：上面这些与spring-cloud相关的属性必须配置在bootstrap.properties中，config部分内容才能被正确加载。因为config的相关配置会先于application.properties，而bootstrap.properties的加载也是先于application.properties。 测试： 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class WeatherEurekaClientApplicationTests &#123; @Value("$&#123;auther&#125;") private String auther; @Test public void contextLoads() &#123; Assert.assertEquals("oursnail.cn",auther); &#125;&#125; 如果测试通过，那么获取内容成功。 但是我们通过网页的方式进行测试，我们会发现修改了github上的内容后，网页上的内容是不能立即刷新的。这比较头疼，可以通过一些途径去解决。这个自动刷新问题会在后面的实战项目中实现。 12345678910@RestControllerpublic class HelloController &#123; @Value("$&#123;auther&#125;") private String auther; @GetMapping("/hello") public String hello()&#123; return auther; &#125;&#125;]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[9.天气预报系统-API网关]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F9.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-API%E7%BD%91%E5%85%B3%2F</url>
    <content type="text"><![CDATA[这是学习的第九篇文章，服务的注册、发现和消费都解决了，小系统就可以跑起来了，但是对于复杂的系统，一个统一的入口是必要的，下面将介绍网关是什么，它的重要性，并且介绍zuul组件的使用。 一、背景 理论上，客户端可以直接向微服务发送请求，每个微服务都有一个公开的URL，该URL将映射到微服务的负载均衡器，由它负责在可用实例之间分发请求。 但是我们知道在微服务架构风格中，一个大应用被拆分成为了多个小的服务系统提供出来，这些小的系统他们可以自成体系，也就是说这些小系统可以拥有自己的数据库，框架甚至语言等，这些小系统通常以提供 Rest Api 风格的接口来被 H5, Android, IOS 以及第三方应用程序调用。 但是在UI上进行展示的时候，我们通常需要在一个界面上展示很多数据，这些数据可能来自于不同的微服务中，举个例子。 在一个电商系统中，查看一个商品详情页，这个商品详情页包含商品的标题，价格，库存，评论等，这些数据对于后端来说可能是位于不同的微服务系统之中，可能我后台的系统是这样来拆分我的服务的： 产品服务 - 负责提供商品的标题，描述，规格等。 价格服务 - 负责对产品进行定价，价格策略计算，促销价等。 库存服务 - 负责产品库存。 评价服务 - 负责用户对商品的评论，回复等。 现在，商品详情页需要从这些微服务中拉取相应的信息，问题来了: 由于我们使用的服务系统架构，所以没办法像传统单体应用一样依靠数据库的 join 查询来得到最终结果，那么如何才能访问各个服务呢？这里就会引出以下几个问题： 1. 客户端需求和微服务暴露的细粒度 API 不匹配 经常有一个业务调用很多个服务，假如客户端发送许多请求，这在公网上可能会很低效，而且会使客户端代码变得更复杂。 2. 服务使用的协议不是 Web 友好的 有的服务可能使用二进制 RPC（比如 thrift），有的服务可能使用 AMQP 消息传递协议。不管哪种协议都不是浏览器友好或防火墙友好的，最好是内部使用。在防火墙之外，应用程序应该使用诸如 HTTP 和 WebSocket 之类的协议。 3. 难重构 随着时间推移可能想要更改系统划分成服务的方式。例如，合并两个服务或者将一个服务拆分成两个或更多服务。如果客户端与微服务直接通信，那么执行这类重构就很困难。 由于以上问题，客户端与微服务直接通信很少是合理的，更好的方法是使用 API 网关，由 API 网关作为后端服务系统的唯一入口。它封装了系统内部架构，为每个客户端提供一个定制的 API 。由它负责服务请求路由、组合及协议转换。有的 API 网关还有其它职责，如身份验证、监控、负载均衡、缓存等。 二、API 网关 API网关是一个服务器，是系统的唯一入口。从面向对象设计的角度看，它与外观模式类似。API网关封装了系统内部架构，为每个客户端提供一个定制的API。它可能还具有其它职责，如身份验证、监控、负载均衡、缓存、请求分片与管理、静态响应处理。 API网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。通常，网关也是提供REST/HTTP的访问API。服务端通过API-GW注册和管理服务。 单节点网关 Backends for frontends 网关 三、API 网关的优缺点 1. 优点 封装了应用程序的内部结构。客户端只需要同网关交互，而不必调用特定的服务（统一API入口）。API 网关为每一类客户端提供了特定的 API ，从而减少客户端与应用程序间的交互次数，简化客户端代码的处理（集合多个API）。 另外，可以避免内部信息泄露给外部。可以为微服务添加额外的安全层。支持混合通信协议。降低构建微服务的复杂性。 2. 缺点 增加了一个必须开发、部署和维护的高可用组件。还有一个风险是 API 网关变成了开发瓶颈。为了暴露每个微服务，开发人员必须更新 API 网关。API 网关的更新过程要尽可能地简单，否则为了更新网关，开发人员将不得不排队等待。不过，虽然有这些不足，但对于大多数现实世界的应用程序而言使用 API 网关是合理的。（在架构上需要额外考虑更多编排和管理；路由逻辑配置要进行统一的管理；可能引发单点故障） 四、参考实现方案 以上列出在 DIY 这个 API 网关时需要考虑的点，以及参考的技术实现。下面是几种目前比较流行的 API 网关搭建的技术方案供参考，后续文章将给出这些方案搭建的例子 1）Nginx + Lua实现负载均衡、限流、服务发现等功能 2）使用 spring cloud 技术栈，其中 zuul 就是用作 API 网关的 3）Mashape 的开源 API 网关 Kong 本次，使用zuul作为API网关。 五、Zuul 功能：认证、压测、金丝雀测试、动态路由、负载削减、安全、静态相应处理… 注意：因为我到目前为止，springboot用的版本是2.1.x，但是呢，集成zuul的时候报错，查了一下，是zuul还不支持2.1.x的版本，所以我将demo:weather-eureka-client=zuul降级到了2.0.3版本。启动成功并且测试成功 首先是准备拿出之前的两个项目：weather-eureka-server和weather-eureka-client，启动，一个地址是8671，一个地址我设定为8081,基于weather-eureka-client新建一个项目：weather-eureka-client-zuul，改动如下： 首先将springboot版本降到2.0.x版本。在启动类上增加注解：@EnableZuulProxy，在yml文件中新增： 12345zuul: routes: hi: path: /hi/** serviceId: weather-eureka-client 这里的含义是：定义一个名字叫做hi的路由规则（自定义），我们访问/hi/**这个路径的时候，就会转发到weather-eureka-client这个服务下的**路径。 比如我这里的weather-eureka-client有一个controller路径为&quot;hello&quot;，调用localhost:8081/hello就可以返回一个字符串。那么有了zuul配置之后，我可以访问localhost:8082/hi/hello也可以访问到这个路径了。 改造本系统： 新建一个项目：msa-eureka-client-zuul。主要是定义网关的路由。 123456789101112131415161718spring: application: name: msa-eureka-client-zuuleureka: client: service-url: defaultZone: http://localhost:8761/eureka# 一个是msa-weather-data-eureka，一个是msa-weather-ciy-eurekazuul: routes: city: path: /city/**/ serviceId: msa-weather-city-eureka data: path: /data/**/ serviceId: msa-weather-data-eurekaserver: port: 8085 ok，下面我们就修改msa-weather-collection-cureka-feign和msa-weather-report-feign.复制为新的项目：msa-weather-collection-cureka-feign-zuul和msa-weather-report-feign-zuul 以msa-weather-report-feign-zuul为例，其实他依托于两个服务:msa-weather-data-eureka和msa-weather-ciy-eureka，这两个我们只需要写在一个接口内，调用网关里定义的路由即可： 12345678910111213141516@FeignClient("msa-eureka-client-zuul")public interface DataClient &#123; /** * 获取城市列表 */ @RequestMapping("city/cities") List&lt;City&gt; listCity() throws Exception; /** * 根据城市ID获取天气 */ @RequestMapping("data/weather/cityId/&#123;cityId&#125;") WeatherResponse getDataByCityId(@PathVariable("cityId") String cityId);&#125; 这样子，这个DataClient就取代了之前的cityClient和WeatherClient。改造完成。 测试无问题。]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[8.天气预报系统-微服务的消费]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F8.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%B6%88%E8%B4%B9%2F</url>
    <content type="text"><![CDATA[这是学习的第八篇文章，在解决了服务注册和发现两个问题之后，就要解决服务消费问题了。本节介绍feign的使用。 1.发现模式 直连模式： 直接去连接某个url，比较简单粗暴，但是不能实现负载均衡和高可用，使用比较少。 客户端发现模式： 服务实例启动后，将自己的位置信息提交到服务注册表 客户端从服务注册表进行查询，来获取可用的服务实例 客户端自行使用负载均衡算法从多个服务实例中选择一个 服务端发现模式： 负载均衡的实现在服务端。而客户端发现模式的负载均衡由客户端来实现。 2.服务的消费者 Apache HttpClient：这个比较简单，不再赘述。 Ribbon: 基于客户端负载均衡工具。可以基于Http或者Tcp实现负载均衡。 直接根据服务的名字来消费，具体是连到哪一个具体的ip去消费是不用管的，因为他已经在客户端上做了一定的负载均衡算法，由他的算法来决定。 Febin: Feign是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用Feign 注解和JAX-RS注解。Feign支持可插拔的编码器和解码器。Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。 我们先来搞个demo测试一把！ 3.Demo for Feign 首先，我们之前的工作中已经由了一个Eureka server，再拿一个叫做msa-weather-city-server的服务来测试。这个服务的主要功能是获取城市信息。 3.1 引入依赖、添加注解 首先引入feign依赖，注意这里有个坑，我一开始没有指定版本号，死活无法导入@EnableFeignClients这个注解： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;&lt;/dependency&gt; 在程序的启动类ServiceFeignApplication ，加上@EnableFeignClients注解开启Feign的功能： 123456789@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class EurekaClientFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaClientFeignApplication.class, args); &#125;&#125; 3.2 配置文件 1234567891011121314spring: application: name: eureka-weather-feigneureka: client: service-url: defaultZone: http://localhost:8761/eurekafeign: client: config: feignName: connectTimeout: 5000 readTimeout: 5000 3.3 定义feign接口 定义一个feign接口，通过@ FeignClient（“服务名”），来指定调用哪个服务。比如在代码中调用了msa-weather-city-eureka服务的“/cities”接口来获取所有的城市列表，代码如下： 12345@FeignClient("msa-weather-city-eureka")public interface CityClient &#123; @GetMapping("/cities") String listCity();&#125; 3.4 定义API来供浏览器调用 12345678910@RestControllerpublic class TestController &#123; @Autowired private CityClient cityClient; @GetMapping("cities") public String getData()&#123; String res = cityClient.listCity(); return res; &#125;&#125; 这样，启动服务中心Eureka server和服务提供方msa-weather-city-server以及本消费服务。再浏览器中访问对应的url：http://localhost:8080/cities就可以调用到msa-weather-city-server提供的服务。 至此，demo演示完毕。 4.用Feign继续完善天气项目 有三个TODO项： 数据采集微服务在天气数据同步任务中，依赖于城市数据API微服务 天气预报微服务查询天气信息，依赖于天气数据API微服务 天气预报微服务提供的城市列表，依赖于城市数据API微服务 那么我们可以看出来，需要去集成Feign去消费的微服务只有两个：msa-weather-collection-eureka和msa-weather-report-eureka。我们将其改造为：msa-weather-collection-eureka-feign和msa-weather-report-eureka-feign. 这里就以msa-weather-collection-eureka为例，步骤基本与demo一样。首先是引入依赖，然后加上注解开启Feign功能。新建一个接口，还是获取城市列表。我只要指定好那个城市列表的微服务的名字和路径，就可以获取到了。不清楚直接看代码即可。 那么在全部改好之后，我们启动这五个项目。但是我们要注意，先启动weather-eureka-server，来提供注册的服务。然后启动城市数据服务，因为天气数据采集要用到他。然后启动天气数据采集服务。然后一次启动天气数据API服务和天气预报UI显示服务。 那么我们访问天气预报UI对应的URL,以南京为例： http://localhost:8083/report/cityId/101190101，如果功能是正常的，标识微服务改造初步成功。]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7.天气预报系统-微服务的注册和发现]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F7.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[这是学习的第七篇文章，首先要解决的问题是服务注册和发现。本节介绍eureka的使用。 1.什么是spring cloud Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。 微服务是可以独立部署、水平扩展、独立访问（或者有独立的数据库）的服务单元，spring cloud就是这些微服务的大管家，采用了微服务这种架构之后，项目的数量会非常多，spring cloud做为大管家需要管理好这些微服务，自然需要很多小弟来帮忙。 解决了分布式系统中的一些问题:配置管理、服务注册、服务发现、断路器、智能路由、负载均衡、服务间调用、一次性令牌、全局锁、领导选举、控制总线、思维导图、分布式会话、集群状态、分布式消息。。。 2.spring cloud &amp; spring boot SpringBoot是构建spring cloud架构的基石 3.spring cloud子项目 参考这篇文章：springcloud(一)：大话Spring Cloud 4.Eureka 4.1.服务中心 服务中心又称注册中心，管理各种服务功能包括服务的注册、发现、熔断、负载、降级等，比如dubbo admin后台的各种功能。 有了服务中心调用关系会有什么变化，画几个简图来帮忙理解. 项目A调用项目B 12graph LR项目A--&gt;项目B 有了服务中心之后，任何一个服务都不能直接去掉用，都需要通过服务中心来调用 12graph LR项目A--&gt;注册中心再去访问项目B 由于各种服务都注册到了服务中心，就有了去做很多高级功能条件。比如几台服务提供相同服务来做均衡负载；监控服务器调用成功率来做熔断，移除服务列表中的故障点；监控服务调用时间来对不同的服务器设置不同的权重等等。 4.2.Eureka Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现。 Eureka Server 作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用 Eureka 的客户端连接到 Eureka Server，并维持心跳连接。 这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。 Spring Cloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。 Eureka由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。 其中有三个角色： Eureka Server：提供服务注册和发现 Service Provider服务提供方，将自身服务注册到Eureka，从而使服务消费方能够找到 Service Consumer：服务消费方，从Eureka获取注册服务列表，从而能够消费服务 4.3.Eureka Server 新建一个springboot项目。spring cloud已经帮我实现了服务注册中心，我们只需要很简单的几个步骤就可以完成。 演示的springboot版本是最新的&lt;version&gt;2.1.0.RELEASE&lt;/version&gt;,springcloud也是最新的&lt;version&gt;Finchley.RELEASE&lt;/version&gt; 1、pom中添加依赖 12345678910111213141516171819202122232425262728293031&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--Eureka server--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2、添加启动代码中添加@EnableEurekaServer注解 12345678@SpringBootApplication@EnableEurekaServerpublic class WeatherEurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(WeatherEurekaServerApplication.class, args); &#125;&#125; 3、配置文件 在默认设置下，该服务注册中心也会将自己作为客户端来尝试注册它自己，所以我们需要禁用它的客户端注册行为，在application.yml： 12345678910server: port: 8761eureka: instance: hostname: localhost client: register-with-eureka: false fetch-registry: false service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ eureka.client.register-with-eureka ：表示是否将自己注册到Eureka Server，默认为true。 eureka.client.fetch-registry ：表示是否从Eureka Server获取注册信息，默认为true。 eureka.client.serviceUrl.defaultZone ：设置与Eureka Server交互的地址，查询服务和注册服务都需要依赖这个地址。默认是http://localhost:8761/eureka ；多个地址可使用 , 分隔。 启动工程后，访问：http://localhost:8761/ ，可以看到下面的页面，其中还没有发现任何服务 4.4.Eureka Client 基本与上一个是类似的。 1@EnableDiscoveryClient 主要的配置文件： 1234567spring: application: name: weather-eureka-clienteureka: client: service-url: defaultZone: http://localhost:8761/eureka 这样同时启动Eureka Server和Eureka Client两个工程。在网站中输入localhost://8761就可以看到注册到Eureka Server的实例了。 5.本门实战 将之前的四个微服务改造为eureka的客户端。 即将 mas-weather-collection-server mas-weather-report-server mas-weather-data-server mas-weather-city-server 改为： mas-weather-collection-eureka mas-weather-report-eureka mas-weather-data-eureka mas-weather-city-eureka 改造过程十分简单，就是引入依赖，修改配置即可。 同时启动这四个微服务客户端和一个eureka服务端。 我们可以看到： 电脑要爆炸了~~~~~~ 下面，这些微服务之间可以相互访问了。]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.天气预报系统-拆分本系统]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F6.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E6%8B%86%E5%88%86%E6%9C%AC%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[这是学习的第六篇文章，先初步拆分本系统为四个服务，还没有进行服务治理等，只是简单的业务拆分，为下一步做准备。 天气数据采集微服务 这个微服务专门提供数据采集和定时更新功能，将数据存储在redis中。 该服务的核心service中的方法是：syncDataByCityId，就是根据cityId来将数据同步进redis。 代码：https://github.com/sunweiguo/swgBook-for-spring-cloud/tree/master/spring-cloud-weather-action/06/msa-weather-collection-server 天气数据API 这个服务专门来提供天气数据的查询功能。 将前端页面以及定时、城市相关的代码全部剔除。只留下两个API： 123WeatherResponse getDataByCityId(String cityId);WeatherResponse getDataByCityName(String cityName); 代码：https://github.com/sunweiguo/swgBook-for-spring-cloud/tree/master/spring-cloud-weather-action/06/msa-weather-data-server 天气预报微服务 本服务的主要功能为：用户通过浏览器来访问，可以返回一个天气预报的界面。 就将redis和定时任务相关的都删掉。我们只需要一个接口： 1Weather getDataByCityId(String cityId); 因为展示数据需要用到城市信息，但是此时还没有，所以需要自己去模拟一些数据去显示。 代码：https://github.com/sunweiguo/swgBook-for-spring-cloud/tree/master/spring-cloud-weather-action/06/msa-weather-report-server 城市数据API 本服务只提供城市列表数据功能。 1List&lt;City&gt; listCity() throws Exception; 有的需要填充一些假数据之后，都可以独立运行。 代码：https://github.com/sunweiguo/swgBook-for-spring-cloud/tree/master/spring-cloud-weather-action/06/msa-weather-city-server]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.天气预报系统-服务拆分和业务建模]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F5.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E6%9C%8D%E5%8A%A1%E6%8B%86%E5%88%86%E5%92%8C%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[这是学习的第五篇文章，在进行真正的业务拆分之前，我们来认识一下什么是SOA架构，什么是微服务。确定本系统拆分的几个部分。 单体架构 我们熟悉的单体MVC架构： 用户&lt;—&gt;表示层&lt;—&gt;业务层&lt;—&gt;数据访问层&lt;—&gt;数据库 单块结构的优缺点： 优点 缺点 功能划分清楚 功能仍然太大 层次关系良好 支付周期变长 每一层独立 升级风险高 部署简单 维护成本增加 技术单一 可伸缩性差 用人成本低 监控困难 单体架构如何转化为微服务 什么是SOA？ SOA是一种设计方法，其中包含多个服务，而服务之间通过配合最终会提供一系列功能。一个服务通常以独立的形式存在于操作系统进程中。服务之间通过网络调用，而非采用进程内调用的方式进行通信。 所以，SOA只是一种架构设计模式，SOAP，REST，RPC是根据这种设计模式构建出来的规范。其中SOAP通俗理解就是http+xml的形式，REST就是http+json的形式，RPC是基于socket的形式。dubbo就是典型的RPC框架，而SpringCloud就是遵守REST规范的生态系统。 SOA VS 微服务 话说1979年，又是一个春天，莆田乡下的赤脚医生吴大牛被改革的春风吹的心潮澎湃，说干就干，吴大牛趁着夜色朦胧找大队支书汇报了汇报思想，第二天就承包了村卫生室，开启了自己的在医疗圈的传奇历程。 乡村诊所大家都知道，没什么复杂的东东，房子只有一间，一个大柜台中间隔开，一半是诊疗兼候诊区，一半是药房，看病就直接找医生，如果前面有人就自己找个位子坐下，排队等一会，秩序倒也井然，看完病了医生直接给抓药，然后下一个继续，也不需要护士和药剂师，吴大牛一个人全部包办。 辛辛苦苦忙碌了十年，时间来到了八九年，又是一个春天，昔日的单身汉吴大牛已成为十里八乡的知名人物，媳妇娶上了不说，家里还增加了一对双胞胎儿子，二层的小洋房也甚是气派。可是也有烦心事，尽管乡村诊所扩大到了两间，媳妇还偶尔能去帮帮忙，但是医生还是只有自己一个，天天从早忙到晚挣的都是一份钱，想多挣点怎么办？吴大牛日思夜想，还真给他想出来一招，怎么办，扩大规模，多招几个医生一起干。原来吴大牛只能治头疼脑热和跌打损伤，现在新招了一个医科大学的毕业生刘小明专治感冒发烧，又从邻村请来了老大夫李阿花专治妇科病，现在一个普通的小诊所就变成了有三个独立科室加一个公共药房（吴大牛媳妇负责）的小医院了，吴大牛是外科主任兼院长，收入那可比之前翻了三番。人逢喜事精神爽，大牛院长请县里的书法名家为新医院书写了牌匾–“博爱医院”，挑了一个黄道吉日正式挂了上去。 一晃十年过去了，又是一个春天，吴大牛的博爱医院已经发展到了内科外科妇科五官科骨科生殖科六个科室，每个科室3到5名医生不等，也耗费巨资购进了血夜化验B超等先进仪器，大牛院长也早已脱离了医疗一线，成为了专职的管理者，但是医院的大事小事大家都找他，就这三十多号员工搞的他每天是焦头烂额，想再扩大规模实在是有心无力了。要说还是大学生有水平，老部下刘小明给大牛院长献了一计，把各个科室独立出去，让各个科室主任自己管理，大牛院长只管科室之间的协调和医院发展的大事，这样既能调动基层的积极性，又能把大牛院长解放出来扩大生产抓大事谋大事，岂不妙哉？就这样，博爱医院的新一轮改革轰轰烈烈的展开了。 又是一个十年，又是一个春天，大牛院长已成为本地知名的企业家，博爱医院也发展到了二十三个科室数百名员工，发展中也出现了新问题，由于各个科室独立挂号、收费、化验，有的科室整天忙忙碌碌效益好，有的科室就相对平庸些，连分到的各种检查仪器都不能满负荷运行，整个医院养了不少闲人。这时候大牛院长视野也开阔了，请来了管理专家进行了顶层设计，把原来分散到各个科室的非核心服务全部收归集中管理，把原来二十三个挂号窗口整合为十个，二十三个收费窗口整合为八个，集中布设在一楼大厅为全院服务，还把分散在各个科室的检查仪器集中起来成立独立的检验科，也为全院服务，这样人人有活干，整个医院的服务能力又上了一个新台阶，这轮改革后博爱医院通过了各级部门的鉴定成为了远近驰名的三甲医院，吴大牛也换身一变成为了博爱集团的CEO兼董事长，下一步就准备IPO上市了。 说到这里大家可能有点糊涂，这个跟微服务有嘛关系？在孙老师看来，大牛诊所的1.0阶段就相当于软件开发的单体结构，一个程序员打天下，从头编到尾，很难做大做强。大牛诊所的2.0阶段就相当于软件开发的垂直结构，各科室按照业务划分，很容易横向扩展。博爱医院的1.0阶段就相当于软件开发的SOA结构，除了药房（数据库）外各个服务独立提供（科室主任负责），但需要大牛院长（ESB总线）来协调。博爱医院的2.0阶段就相当于软件开发的微服务结构，公共服务院内共享，科室主任管理功能弱化（只管医生业务），优点是扩容方便，哪个部门缺人直接加，不用看上下游，资源利用率高，人员和设备效率高。为什么要变呢？小诊所有小诊所的活法，大医院有大医院的骄傲。无他，天下熙熙，皆为利来；天下攘攘，皆为利往。 设计原则 拆分足够微：划分比较细，但是也不能太细，增加管理问题 轻量级通信：rest，rpc等方式在网络上通信 单一职责原则：高内聚，低耦合，确定服务边界 如何设计微服务系统 服务拆分----》服务注册----》服务发现----》服务消费(调用另外一个服务)----》统一入口(服务很多的时候需要有一个统一的入口)----》配置管理(管理每个服务的配置信息)----》熔断机制(保护系统避免崩溃)----》自动扩展(根据负荷自动扩展集群) 微服务拆分的意义 易于实现 易于部署 易于维护 易于更新 本天气预报系统可以拆分为： 天气数据采集服务：数据采集和数据存储 天气预报服务：数据展示 天气数据API：数据查询 城市数据API：数据查询]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.天气预报系统-前端样式]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F4.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E5%89%8D%E7%AB%AF%E6%A0%B7%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[这是学习的第四篇文章，整一个前端的渲染引擎并且用bootstrap美化一下样式。 Thymeleaf 数据动态地渲染，这里采用Thymeleaf模板引擎。 首先是引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 后端的controller： 12345678910111213141516171819@RestController@RequestMapping("/report")public class WeatherReportController &#123; @Autowired private IWeatherReportService weatherReportService; @Autowired private ICityDataService cityDataService; @GetMapping("/cityId/&#123;cityId&#125;") public ModelAndView getReportByCityId(@PathVariable("cityId") String cityId, Model model) throws Exception &#123; Weather weather = weatherReportService.getDataByCityId(cityId); model.addAttribute("title","蜗牛天气预报"); model.addAttribute("cityId",cityId); model.addAttribute("cityList",cityDataService.listCity()); model.addAttribute("report",weather); return new ModelAndView("weather/report","reportModel",model); &#125;&#125; 前端的简单页面： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN" xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;蜗牛天气预报系统&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3 th:text="$&#123;reportModel.title&#125;"&gt;snail&lt;/h3&gt;&lt;!--下拉框来选择城市--&gt;&lt;select&gt; &lt;option th:each="city : $&#123;reportModel.cityList&#125;" th:value="$&#123;city.cityId&#125;" th:text="$&#123;city.cityName&#125;" th:selected="$&#123;city.cityId eq reportModel.cityId &#125;"&gt; &lt;/option&gt;&lt;/select&gt;&lt;!--显示一下选择后的城市的名称--&gt;&lt;h1 th:text="$&#123;reportModel.report.city&#125;"&gt;城市名称&lt;/h1&gt;&lt;!--显示这个城市的一些基本天气状况--&gt;&lt;p&gt; 空气质量指数：&lt;span th:text="$&#123;reportModel.report.aqi&#125;"&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; 当前温度：&lt;span th:text="$&#123;reportModel.report.wendu&#125;"&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt; 温馨提示：&lt;span th:text="$&#123;reportModel.report.ganmao&#125;"&gt;&lt;/span&gt;&lt;/p&gt;&lt;!--显示未来几天的天气状况--&gt;&lt;div th:each="forecast : $&#123;reportModel.report.forecast&#125;"&gt; &lt;div&gt; &lt;p th:text="$&#123;forecast.date&#125;"&gt;日期&lt;/p&gt; &lt;p th:text="$&#123;forecast.type&#125;"&gt;天气类型&lt;/p&gt; &lt;p th:text="$&#123;forecast.high&#125;"&gt;最高温度&lt;/p&gt; &lt;p th:text="$&#123;forecast.low&#125;"&gt;最低温度&lt;/p&gt; &lt;p th:text="$&#123;forecast.fengxiang&#125;"&gt;风向&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 注意 这里修正一下了一下之前存在的一个小错误，就是请求天气的接口应该是：http://wthrcdn.etouch.cn/weather_mini?citykey=xxx 而我之前程序中写的是http://wthrcdn.etouch.cn/weather_mini?cityKey=xxx 就是这个citykey中的k，应该是小写。导致请求不到数据，前端直接报错。 Bootstrap 稍微美化一下页面。引入bootstrap。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN" xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"&gt; &lt;link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous"&gt; &lt;title&gt;蜗牛天气预报系统&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="container controls-pane"&gt; &lt;div class="row"&gt; &lt;h3 th:text="$&#123;reportModel.title&#125;"&gt;snail&lt;/h3&gt; &lt;select class="custom-select" id="selectCityId"&gt; &lt;option th:each="city : $&#123;reportModel.cityList&#125;" th:value="$&#123;city.cityId&#125;" th:text="$&#123;city.cityName&#125;" th:selected="$&#123;city.cityId eq reportModel.cityId &#125;"&gt; &lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;h1 class="text-success" th:text="$&#123;reportModel.report.city&#125;"&gt;城市名称&lt;/h1&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;p&gt; 空气质量指数：&lt;span th:text="$&#123;reportModel.report.aqi&#125;"&gt;&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;p&gt; 当前温度：&lt;span th:text="$&#123;reportModel.report.wendu&#125;"&gt;&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;p&gt; 温馨提示：&lt;span th:text="$&#123;reportModel.report.ganmao&#125;"&gt;&lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;div class="card border-info" th:each="forecast : $&#123;reportModel.report.forecast&#125;"&gt; &lt;div class="card-body text-info"&gt; &lt;p class="card-text" th:text="$&#123;forecast.date&#125;"&gt;日期&lt;/p&gt; &lt;p class="card-text " th:text="$&#123;forecast.type&#125;"&gt;天气类型&lt;/p&gt; &lt;p class="card-text" th:text="$&#123;forecast.high&#125;"&gt;最高温度&lt;/p&gt; &lt;p class="card-text" th:text="$&#123;forecast.low&#125;"&gt;最低温度&lt;/p&gt; &lt;p class="card-text" th:text="$&#123;forecast.fengxiang&#125;"&gt;风向&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;!-- jQuery first, then Popper.js, then Bootstrap JS --&gt;&lt;script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"&gt;&lt;/script&gt;&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"&gt;&lt;/script&gt;&lt;script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"&gt;&lt;/script&gt;&lt;!-- Optional JavaScript --&gt;&lt;script type="text/javascript" th:src="@&#123;/js/weather/report.js&#125;"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 其中下拉框选中向后端请求每个城市数据的js： 1234567$(function()&#123; $("#selectCityId").change(function()&#123; var cityId = $("#selectCityId").val(); var url = '/report/cityId/'+ cityId; window.location.href = url; &#125;)&#125;); 最后的效果：]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.天气预报系统-天气数据同步]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F3.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E5%A4%A9%E6%B0%94%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[这是学习的第三篇文章，由于天气信息需要更新，所以我们需要一个定时器定时去获取一下最新的信息。由于本项目实现比较简单就可以用quartz来实现。 quartz如何整合 数据需要定时地刷新，不能等到用户来获取的时候才更新，这里用最常用的quartz定时器来实现。 首先时引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt;&lt;/dependency&gt; 下面定义一个执行的任务，先什么都不干，就打印一句话即可： 12345678@Slf4jpublic class WeatherDataSyncJob extends QuartzJobBean &#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; log.info("天气数据同步任务开始"); &#125;&#125; 下面要定义配置类： 首先是要定义这个任务的细节：JobDetail，就是说我们配置的这个quartz里面的任务是谁？给他起个名字啥的。 后面个是定义触发器，决定了刚才定义的这个JobDetail多长时间执行一次。这里是为了模拟想过，定义了两秒就执行一次。那么我们启动项目后，看到的效果应该是每两秒打印一次日志。 1234567891011121314151617181920212223@Configurationpublic class QuartzConfig &#123; //定义一个jobDetail,就是注册一个定时任务，具体如何执行时在WeatherDataSyncJob中定义 //具体何时执行，是下面的Trigger定义 @Bean public JobDetail weatherDataSyncDetail()&#123; return JobBuilder.newJob(WeatherDataSyncJob.class). withIdentity("WeatherDataSyncJob"). storeDurably().build(); &#125; //触发器 @Bean public Trigger weatherDataSyncTrigger()&#123; SimpleScheduleBuilder scheduleBuilder = SimpleScheduleBuilder .simpleSchedule() .withIntervalInSeconds(2)//两秒去自动执行一次 .repeatForever(); return TriggerBuilder.newTrigger().forJob(weatherDataSyncDetail()) .withIdentity("weatherDataSyncTrigger") .withSchedule(scheduleBuilder).build(); &#125;&#125; 拉取城市信息 网站： http://mobile.weather.com.cn/js/citylist.xml 比如我将江苏省的单独拿出来： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;xml&gt; &lt;c c1="0"&gt; &lt;d d1="101190101" d2="南京" d3="nanjing" d4="江苏"/&gt; &lt;d d1="101190102" d2="溧水" d3="lishui" d4="江苏"/&gt; &lt;d d1="101190103" d2="高淳" d3="gaochun" d4="江苏"/&gt; &lt;d d1="101190104" d2="江宁" d3="jiangning" d4="江苏"/&gt; &lt;d d1="101190105" d2="六合" d3="luhe" d4="江苏"/&gt; &lt;d d1="101190106" d2="江浦" d3="jiangpu" d4="江苏"/&gt; &lt;d d1="101190107" d2="浦口" d3="pukou" d4="江苏"/&gt; &lt;d d1="101190201" d2="无锡" d3="wuxi" d4="江苏"/&gt; &lt;d d1="101190202" d2="江阴" d3="jiangyin" d4="江苏"/&gt; &lt;d d1="101190203" d2="宜兴" d3="yixing" d4="江苏"/&gt; &lt;d d1="101190204" d2="锡山" d3="xishan" d4="江苏"/&gt; &lt;d d1="101190301" d2="镇江" d3="zhenjiang" d4="江苏"/&gt; &lt;d d1="101190302" d2="丹阳" d3="danyang" d4="江苏"/&gt; &lt;d d1="101190303" d2="扬中" d3="yangzhong" d4="江苏"/&gt; &lt;d d1="101190304" d2="句容" d3="jurong" d4="江苏"/&gt; &lt;d d1="101190305" d2="丹徒" d3="dantu" d4="江苏"/&gt; &lt;d d1="101190401" d2="苏州" d3="suzhou" d4="江苏"/&gt; &lt;d d1="101190402" d2="常熟" d3="changshu" d4="江苏"/&gt; &lt;d d1="101190403" d2="张家港" d3="zhangjiagang" d4="江苏"/&gt; &lt;d d1="101190404" d2="昆山" d3="kunshan" d4="江苏"/&gt; &lt;d d1="101190405" d2="吴中" d3="wuzhong" d4="江苏"/&gt; &lt;d d1="101190407" d2="吴江" d3="wujiang" d4="江苏"/&gt; &lt;d d1="101190408" d2="太仓" d3="taicang" d4="江苏"/&gt; &lt;d d1="101190501" d2="南通" d3="nantong" d4="江苏"/&gt; &lt;d d1="101190502" d2="海安" d3="haian" d4="江苏"/&gt; &lt;d d1="101190503" d2="如皋" d3="rugao" d4="江苏"/&gt; &lt;d d1="101190504" d2="如东" d3="rudong" d4="江苏"/&gt; &lt;d d1="101190507" d2="启东" d3="qidong" d4="江苏"/&gt; &lt;d d1="101190508" d2="海门" d3="haimen" d4="江苏"/&gt; &lt;d d1="101190509" d2="通州" d3="tongzhou" d4="江苏"/&gt; &lt;d d1="101190601" d2="扬州" d3="yangzhou" d4="江苏"/&gt; &lt;d d1="101190602" d2="宝应" d3="baoying" d4="江苏"/&gt; &lt;d d1="101190603" d2="仪征" d3="yizheng" d4="江苏"/&gt; &lt;d d1="101190604" d2="高邮" d3="gaoyou" d4="江苏"/&gt; &lt;d d1="101190605" d2="江都" d3="jiangdu" d4="江苏"/&gt; &lt;d d1="101190606" d2="邗江" d3="hanjiang" d4="江苏"/&gt; &lt;d d1="101190701" d2="盐城" d3="yancheng" d4="江苏"/&gt; &lt;d d1="101190702" d2="响水" d3="xiangshui" d4="江苏"/&gt; &lt;d d1="101190703" d2="滨海" d3="binhai" d4="江苏"/&gt; &lt;d d1="101190704" d2="阜宁" d3="funing" d4="江苏"/&gt; &lt;d d1="101190705" d2="射阳" d3="sheyang" d4="江苏"/&gt; &lt;d d1="101190706" d2="建湖" d3="jianhu" d4="江苏"/&gt; &lt;d d1="101190707" d2="东台" d3="dongtai" d4="江苏"/&gt; &lt;d d1="101190708" d2="大丰" d3="dafeng" d4="江苏"/&gt; &lt;d d1="101190709" d2="盐都" d3="yandu" d4="江苏"/&gt; &lt;d d1="101190801" d2="徐州" d3="xuzhou" d4="江苏"/&gt; &lt;d d1="101190802" d2="铜山" d3="tongshan" d4="江苏"/&gt; &lt;d d1="101190803" d2="丰县" d3="fengxian" d4="江苏"/&gt; &lt;d d1="101190804" d2="沛县" d3="peixian" d4="江苏"/&gt; &lt;d d1="101190805" d2="邳州" d3="pizhou" d4="江苏"/&gt; &lt;d d1="101190806" d2="睢宁" d3="suining" d4="江苏"/&gt; &lt;d d1="101190807" d2="新沂" d3="xinyi" d4="江苏"/&gt; &lt;d d1="101190901" d2="淮安" d3="huaian" d4="江苏"/&gt; &lt;d d1="101190902" d2="金湖" d3="jinhu" d4="江苏"/&gt; &lt;d d1="101190903" d2="盱眙" d3="xuyi" d4="江苏"/&gt; &lt;d d1="101190904" d2="洪泽" d3="hongze" d4="江苏"/&gt; &lt;d d1="101190905" d2="涟水" d3="lianshui" d4="江苏"/&gt; &lt;d d1="101190906" d2="淮阴区" d3="huaiyinqu" d4="江苏"/&gt; &lt;d d1="101190908" d2="淮安区" d3="huaianqu" d4="江苏"/&gt; &lt;d d1="101191001" d2="连云港" d3="lianyungang" d4="江苏"/&gt; &lt;d d1="101191002" d2="东海" d3="donghai" d4="江苏"/&gt; &lt;d d1="101191003" d2="赣榆" d3="ganyu" d4="江苏"/&gt; &lt;d d1="101191004" d2="灌云" d3="guanyun" d4="江苏"/&gt; &lt;d d1="101191005" d2="灌南" d3="guannan" d4="江苏"/&gt; &lt;d d1="101191101" d2="常州" d3="changzhou" d4="江苏"/&gt; &lt;d d1="101191102" d2="溧阳" d3="liyang" d4="江苏"/&gt; &lt;d d1="101191103" d2="金坛" d3="jintan" d4="江苏"/&gt; &lt;d d1="101191104" d2="武进" d3="wujin" d4="江苏"/&gt; &lt;d d1="101191201" d2="泰州" d3="taizhou" d4="江苏"/&gt; &lt;d d1="101191202" d2="兴化" d3="xinghua" d4="江苏"/&gt; &lt;d d1="101191203" d2="泰兴" d3="taixing" d4="江苏"/&gt; &lt;d d1="101191204" d2="姜堰" d3="jiangyan" d4="江苏"/&gt; &lt;d d1="101191205" d2="靖江" d3="jingjiang" d4="江苏"/&gt; &lt;d d1="101191301" d2="宿迁" d3="suqian" d4="江苏"/&gt; &lt;d d1="101191302" d2="沭阳" d3="shuyang" d4="江苏"/&gt; &lt;d d1="101191303" d2="泗阳" d3="siyang" d4="江苏"/&gt; &lt;d d1="101191304" d2="泗洪" d3="sihong" d4="江苏"/&gt; &lt;d d1="101191305" d2="宿豫" d3="suyu" d4="江苏"/&gt; &lt;/c&gt;&lt;/xml&gt; 其实思路很简单，就是从xml文件中获取所有的城市信息，转换为城市列表对象。然后遍历城市中的id，就可以根据id拼接url去直接去调用天气的接口去查询天气，然后重新覆盖redis中的天气数据即可。 12345678910111213141516171819202122232425262728@Slf4jpublic class WeatherDataSyncJob extends QuartzJobBean &#123; @Autowired private IWeatherDataService weatherDataService; @Autowired private ICityDataService cityDataService; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; log.info("天气数据同步任务开始"); //获取城市列表 List&lt;City&gt; cityList = null; try &#123; cityList = cityDataService.listCity(); &#125; catch (Exception e) &#123; log.error("获取城市列表失败！",e); &#125; //遍历城市id获取天气 for(City city:cityList)&#123; String cityId = city.getCityId(); log.info("定时器更新了&#123;&#125;这个城市的天气信息", city.getCityName()); weatherDataService.syncDataByCityId(cityId); &#125; log.info("天气数据同步任务结束"); &#125;&#125; 具体如何读取xml文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Service@Slf4jpublic class CityDataServiceImpl implements ICityDataService &#123; @Override public List&lt;City&gt; listCity() throws Exception &#123; //读取xml文件 Resource resource = new ClassPathResource("citylist.xml"); //读取文件的buffer流 BufferedReader bf = new BufferedReader(new InputStreamReader(resource.getInputStream(),"UTF-8")); StringBuffer buffer = new StringBuffer(); String line = ""; while((line = bf.readLine()) != null)&#123; buffer.append(line); &#125; //此时数据已经读到buffer里了 bf.close(); //xml转换为java对象 CityList cityList = (CityList) XmlBuilder.xmlStrToObj(CityList.class,buffer.toString()); return cityList.getCityList(); &#125;&#125;/** * @Author 【swg】. * @Date 2018/11/19 17:09 * @DESC xml转换为对象 * @CONTACT 317758022@qq.com */public class XmlBuilder &#123; public static Object xmlStrToObj(Class&lt;?&gt; clazz,String xmlStr) throws Exception&#123; Object xmlObject = null; Reader reader = null; JAXBContext context = JAXBContext.newInstance(clazz); //xml转为对象的接口 Unmarshaller unmarshaller = context.createUnmarshaller(); reader = new StringReader(xmlStr); xmlObject = unmarshaller.unmarshal(reader); if(null != reader)&#123; reader.close(); &#125; return xmlObject; &#125;&#125; 其中，解析xml我们用到了JAXB，他是什么呢？维基百科： JAXB（Java Architecture for XML Binding简称JAXB）允许Java开发人员将Java类映射为XML表示方式。JAXB提供两种主要特性：将一个Java对象序列化为XML，以及反向操作，将XML解析成Java对象。换句话说，JAXB允许以XML格式存储和读取数据，而不需要程序的类结构实现特定的读取XML和保存XML的代码。]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.天气预报系统-redis提升性能]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F2.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-redis%E6%8F%90%E5%8D%87%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[这是学习的第二篇文章，很显然，这个免费的接口能承受的并发是很低的，并且我们的服务器作为一个中转站去向这个接口请求数据也非常地耗时，于性能和稳定性都没有保障，所以我们需要redis作为缓存来提高性能。所以，我们需要用redis来重构一下。 先引入一下依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 思路很简单：缓存中有数据的时候，就直接从缓存中拿即可，若缓存中没有此数据，就去调用接口重新获取并且再存到缓存中。 123456789101112131415161718192021222324252627282930313233343536373839private static final long TIME_OUT = 30*60L;private WeatherResponse doGetWeather(String uri)&#123; //先去缓存中查询，有就直接拿缓存中的数据，否则调用接口 String key = uri; String strBody = null; WeatherResponse resp = null; ObjectMapper mapper = new ObjectMapper(); ValueOperations&lt;String,String&gt; ops = stringRedisTemplate.opsForValue(); if(stringRedisTemplate.hasKey(uri))&#123; //缓存有数据 log.info("Redis has data!"); strBody = ops.get(key); &#125;else&#123; //缓存没有数据 log.info("Redis don't thas data!"); ResponseEntity&lt;String&gt; resString = restTemplate.getForEntity(uri,String.class); if(resString.getStatusCodeValue() == 200) &#123; strBody = resString.getBody(); &#125; //数据写入缓存 ops.set(key,strBody,TIME_OUT, TimeUnit.SECONDS); &#125; try &#123; resp = mapper.readValue(strBody,WeatherResponse.class); &#125;catch (IOException e)&#123; log.error("Error!",e); &#125; return resp;&#125; 一开始可以将过期时间缩短一点，这里redis直接启动即可，默认端口是6379.]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.天气预报系统-简单接口调用]]></title>
    <url>%2F2019%2F02%2F21%2Fweather-for-spring-cloud%2F1.%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E7%B3%BB%E7%BB%9F-%E7%AE%80%E5%8D%95%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[慕课网的课程《从天气项目看 Spring Cloud 微服务治理》应该是入门spring cloud微服务最简单的实战视频了。这是学习的第一篇文章，因为业务很简单，就是获取天气并且展示出来，本节说明一下如何获取天气信息。代码详见尾部，本系列所有代码都分类保存再github上了。 一、获取天气信息 数据来源： http://wthrcdn.etouch.cn/weather_mini?citykey=xxx 或者 http://wthrcdn.etouch.cn/weather_mini?city=xxx 首先我们如何做一个查询天气的接口呢？其实特别简单，就是用HttpClient这个客户端来调用以上的接口，就可以拿到数据了。 我们所需要做的工作也非常少，就是封装一下数据，请求一下参数即可。 请求的数据是json，我们这里准备好数据bean即可： 123456789101112131415161718192021222324252627282930313233343536@Datapublic class WeatherResponse implements Serializable &#123; private Weather data; private Integer status; private String desc;&#125;@Datapublic class Weather implements Serializable &#123; private String city; private String aqi; private List&lt;Forecast&gt; forecast; private String ganmao; private String wendu; private Yesterday yesterday;&#125;@Datapublic class Forecast implements Serializable &#123; private String date; private String high; private String fengli; private String low; private String fengxiang; private String type;&#125;@Datapublic class Yesterday implements Serializable&#123; private String date; private String high; private String fx; private String low; private String fl; private String type;&#125; ok，数据载体已经好了，下面就是用RestTemplate调用url取获取天气信息，用上面的bean来封装： 1234567891011121314151617181920212223242526272829303132333435363738394041@Servicepublic class WeatherDataServiceImpl implements IWeatherDataService &#123; @Autowired private RestTemplate restTemplate; //统一接口前缀 private static final String WEATHER_URI = "http://wthrcdn.etouch.cn/weather_mini?"; @Override public WeatherResponse getDataByCityId(String cityId) &#123; String uri = WEATHER_URI + "citykey="+cityId; return doGetWeather(uri); &#125; @Override public WeatherResponse getDataByCityName(String cityName) &#123; String uri = WEATHER_URI + "city="+cityName; return doGetWeather(uri); &#125; //根据参数获取天气数据 private WeatherResponse doGetWeather(String uri)&#123; ResponseEntity&lt;String&gt; resString = restTemplate.getForEntity(uri,String.class); ObjectMapper mapper = new ObjectMapper(); WeatherResponse resp = null; String strBody = null; if(resString.getStatusCodeValue() == 200)&#123; strBody = resString.getBody(); &#125; try &#123; resp = mapper.readValue(strBody,WeatherResponse.class); &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; return resp; &#125;&#125; 最后再用一个controller来给一个接口即可。 二、注意 直接启动项目会报错： 123456789101112131415161718Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.2018-11-19 15:19:54.732 ERROR 13924 --- [ main] o.s.b.d.LoggingFailureAnalysisReporter : ***************************APPLICATION FAILED TO START***************************Description:Field restTemplate in com.swg.weatherbasic.service.impl.WeatherDataServiceImpl required a bean of type 'org.springframework.web.client.RestTemplate' that could not be found.The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true)Action:Consider defining a bean of type 'org.springframework.web.client.RestTemplate' in your configuration. 我们可以看到提示信息是：'org.springframework.web.client.RestTemplate' that could not be found.,错误就很明显了，这个玩意根本就没有在spring中注册，怎么可以注入呢? 所以，我们需要向spring注册一下这个bean： 12345678910@Configurationpublic class RestConfig &#123; @Autowired private RestTemplateBuilder builder; @Bean public RestTemplate restTemplate()&#123; return builder.build(); &#125;&#125; 三、彩蛋 将此小项目作为一个小版本，直接保存到码云上。如何做呢？ 其实很简单，先去码云上新建一个项目。然后在本地某一个文件夹下执行 1git clone xxx 然后将我们的项目直接拷贝到这个文件夹下。执行 123git add .git commit -am &apos;weather-basic&apos;git push origin master 这样就可以了。 代码地址：spring-cloud-weather-action----01 下面章节的代码依次类推，可能就不再赘述了。本项目的springboot版本是2.1.0.RELEASE，spring cloud用的是Finchley.RELEASE版本。]]></content>
      <tags>
        <tag>天气项目入门微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之ThreadLocal自问自答]]></title>
    <url>%2F2019%2F02%2F20%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8BThreadLocal%E8%87%AA%E9%97%AE%E8%87%AA%E7%AD%94%2F</url>
    <content type="text"><![CDATA[以问答的形式加深对threadlocal的理解，做到面试的镇定自若。 问题1：ThreadLocal了解吗？您能给我说说他的主要用途吗？ 官方定位：ThreadLocal类用来提供线程内部的局部变量。这种变量在多线程环境下访问（通过get和set方法访问）时能保证各个线程的变量相对独立于其他线程内的变量。 简单归纳就是： ThreadLocal的作用是提供线程内的局部变量 这种变量在线程的生命周期内起作用 不同的线程之间不会相互干扰 问题2：ThreadLocal实现原理是什么，它是怎么样做到局部变量不同的线程之间不会相互干扰的？ 通常，如果我不去看源代码的话，我猜ThreadLocal是这样子设计的：每个ThreadLocal类都创建一个Map，然后用线程的ID threadID作为Map的key，要存储的局部变量作为Map的value，这样就能达到各个线程的值隔离的效果。这是最简单的设计方法，JDK最早期的ThreadLocal就是这样设计的。 但是，JDK后面优化了设计方案，现时JDK8 ThreadLocal的设计是：每个Thread维护一个ThreadLocalMap哈希表，这个哈希表的key是ThreadLocal实例本身，value才是真正要存储的值Object。 那么为什么不用上面个设计呢？多简单啊！ 如果用Map来做的话，只能是用thread+threadlocal计算出来作为key，毕竟我存的不一定只有一个变量。那么不用他的时候，如何清理呢？只能是手动remove掉，但是一方面很麻烦，另一方面代码很丑陋，最后一方面是在remove的时候突然出现问题，那么就可能导致内存泄漏。 新的设计的好处： 当Thread销毁之后，对应的ThreadLocalMap也会随之销毁，能减少内存的使用。 假设当前thread一直活着（比如赖在线程池中），有些无用的threadlocal对象怎么清理呢？ key是一个软引用指向ThreadLocal实例，特性是下一次gc的时候就会被回收掉了，ThreadLocalMap中就会出现key为null的Entry. key回收掉了，value值还在啊，这个怎么回收！！！ ThreadLocal的get和set方法每次调用时，如果发现当前的entry的key为null（也就是被回收掉了），最终会调用expungeStaleEntry(int staleSlot)方法，该方法会把哈希表当前位置的无用数据清理掉（当然还有别的操作）。 但是最佳实践还是每次使用完ThreadLocal，都调用它的remove()方法，清除数据,确保不会出现内存泄漏问题。 问题3：您能说说ThreadLocal常用操作的底层实现原理吗？如存储set(T value)，获取get()，删除remove()等操作。 具体的代码就不贴了，核心代码都已经看过了。这里简单总结一下。 调用get() 获取当前线程Thread对象，进而获取此线程对象中维护的ThreadLocalMap对象。 判断当前的ThreadLocalMap是否存在,如果存在，则以当前的ThreadLocal 为 key，调用ThreadLocalMap中的getEntry方法获取对应的存储实体 e。找到对应的存储实体 e，获取存储实体 e 对应的 value值，即为我们想要的当前线程对应此ThreadLocal的值，返回结果值。 如果不存在，则证明此线程没有维护的ThreadLocalMap对象，调用setInitialValue方法进行初始化。返回setInitialValue初始化的值。 调用set(T value) 获取当前线程Thread对象，进而获取此线程对象中维护的ThreadLocalMap对象。 判断当前的ThreadLocalMap是否存在： 如果存在，则调用map.set设置此实体entry。 如果不存在，则调用createMap进行ThreadLocalMap对象的初始化，并将此实体entry作为第一个值存放至ThreadLocalMap中。 调用remove() 获取当前线程Thread对象，进而获取此线程对象中维护的ThreadLocalMap对象。 判断当前的ThreadLocalMap是否存在， 如果存在，则调用map.remove，以当前ThreadLocal为key删除对应的实体entry。 问题4：对ThreadLocal的常用操作实际是对线程Thread中的ThreadLocalMap进行操作，核心是ThreadLocalMap这个哈希表，你能谈谈ThreadLocalMap的内部底层实现吗? ThreadLocalMap的底层实现是一个定制的自定义HashMap哈希表，核心组成元素有： Entry[] table：底层哈希表 table,必要时需要进行扩容，底层哈希表 table.length 长度必须是2的n次方。 int size：实际存储键值对元素个数 entries int threshold：下一次扩容时的阈值，阈值 threshold = len(table) * 2 / 3。当 size &gt;= threshold 时，遍历table并删除key为null的元素，如果删除后size &gt;= threshold*3/4时，需要对table进行扩容 其中Entry[] table哈希表存储的核心元素是Entry，Entry包含： ThreadLocal&lt;?&gt; k：当前存储的ThreadLocal实例对象 Object value：当前 ThreadLocal 对应储存的值value 需要注意的是，此Entry继承了弱引用 WeakReference，所以在使用ThreadLocalMap时，发现key == null，则意味着此key ThreadLocal不在被引用，需要将其从ThreadLocalMap哈希表中移除。 问题5：ThreadLocalMap中的存储实体Entry使用ThreadLocal作为key，但这个Entry是继承弱引用WeakReference的，为什么要这样设计，使用了弱引用WeakReference会造成内存泄露问题吗？ 参考上一篇文章。 问题6：ThreadLocal和synchronized的区别? ThreadLocal和synchronized关键字都用于处理多线程并发访问变量的问题，只是二者处理问题的角度和思路不同。 ThreadLocal是一个Java类,通过对当前线程中的局部变量的操作来解决不同线程的变量访问的冲突问题。所以，ThreadLocal提供了线程安全的共享对象机制，每个线程都拥有其副本。 Java中的synchronized是一个保留字，它依靠JVM的锁机制来实现临界区的函数或者变量的访问中的原子性。在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。此时，被用作“锁机制”的变量时多个线程共享的。 同步机制(synchronized关键字)采用了以“时间换空间”的方式，提供一份变量，让不同的线程排队访问。而ThreadLocal采用了“以空间换时间”的方式，为每一个线程都提供一份变量的副本，从而实现同时访问而互不影响. 问题7：ThreadLocal在现时有什么应用场景？ 总的来说ThreadLocal主要是解决2种类型的问题： 解决并发问题：使用ThreadLocal代替synchronized来保证线程安全。同步机制采用了“以时间换空间”的方式，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 解决数据存储问题：ThreadLocal为变量在每个线程中都创建了一个副本，所以每个线程可以访问自己内部的副本变量，不同线程之间不会互相干扰。如一个Parameter对象的数据需要在多个模块中使用，如果采用参数传递的方式，显然会增加模块之间的耦合性。此时我们可以使用ThreadLocal解决。 一般的Web应用划分为展现层、服务层和持久层三个层次，在不同的层中编写对应的逻辑，下层通过接口向上层开放功能调用。在一般情况下，从接收请求到返回响应所经过的所有程序调用都同属于一个线程ThreadLocal是解决线程安全问题一个很好的思路，它通过为每个线程提供一个独立的变量副本解决了变量并发访问的冲突问题。在很多情况下，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。 总结 ThreadLocal提供线程内部的局部变量，在本线程内随时随地可取，隔离其他线程。 ThreadLocal的设计是：每个Thread维护一个ThreadLocalMap哈希表，这个哈希表的key是ThreadLocal实例本身，value才是真正要存储的值Object。 对ThreadLocal的常用操作实际是对线程Thread中的ThreadLocalMap进行操作。 ThreadLocalMap的底层实现是一个定制的自定义哈希表，ThreadLocalMap的阈值threshold = 底层哈希表table的长度 len * 2 / 3，当实际存储元素个数size 大于或等于 阈值threshold的 3/4 时size &gt;= threshold*3/4，则对底层哈希表数组table进行扩容操作。 ThreadLocalMap中的哈希表Entry[] table存储的核心元素是Entry，存储的key是ThreadLocal实例对象，value是ThreadLocal 对应储存的值value。需要注意的是，此Entry继承了弱引用 WeakReference，所以在使用ThreadLocalMap时，发现key == null，则意味着此key ThreadLocal不在被引用，需要将其从ThreadLocalMap哈希表中移除。 ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收。所以，在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。如果我们不主动调用上述操作，则会导致内存泄露。 为了安全地使用ThreadLocal，必须要像每次使用完锁就解锁一样，在每次使用完ThreadLocal后都要调用remove()来清理无用的Entry。这在操作在使用线程池时尤为重要。 ThreadLocal和synchronized的区别：同步机制(synchronized关键字)采用了以“时间换空间”的方式，提供一份变量，让不同的线程排队访问。而ThreadLocal采用了“以空间换时间”的方式，为每一个线程都提供一份变量的副本，从而实现同时访问而互不影响。 ThreadLocal主要是解决2种类型的问题：A. 解决并发问题：使用ThreadLocal代替同步机制解决并发问题。B. 解决数据存储问题：如一个Parameter对象的数据需要在多个模块中使用，如果采用参数传递的方式，显然会增加模块之间的耦合性。此时我们可以使用ThreadLocal解决。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之ThreadLocal内存泄漏问题]]></title>
    <url>%2F2019%2F02%2F20%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8BThreadLocal%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[ThreadLocal是面试重灾区，分为两篇来讲解其中的用法和原理。这是第二篇。 一、前言 ThreadLocal 的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。但是如果滥用 ThreadLocal，就可能会导致内存泄漏。下面，我们将围绕三个方面来分析 ThreadLocal 内存泄漏的问题: ThreadLocal 实现原理 ThreadLocal为什么会内存泄漏 ThreadLocal 最佳实践 二、ThreadLocal 实现原理 这部分内容上一篇已经详细讲解完毕，这里谨作为一个回顾或者总结吧！ ThreadLocal的实现是这样的：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 指向 ThreadLocal 实例本身，value 指向真正需要存储的 Object，这个值真实保存在线程实例上的。 也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 这里一定要注意，ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。我们从get和set代码中可以看到ThreadLocalMap的key是ThreadLocal 实例本身。 三、ThreadLocal为什么会内存泄漏 ThreadLocalMap使用ThreadLocal的弱引用作为key,如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 但是这些被动的预防措施并不能保证不会内存泄漏： 使用static的ThreadLocal，延长了ThreadLocal的生命周期，可能导致的内存泄漏 分配使用了ThreadLocal又不再调用get(),set(),remove()方法，那么就会导致内存泄漏。 四、为什么使用弱引用 从表面上看内存泄漏的根源在于使用了弱引用。网上的文章大多着重分析ThreadLocal使用了弱引用会导致内存泄漏，但是另一个问题也同样值得思考：为什么使用弱引用而不是强引用？ key 使用强引用：引用的ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。 key 使用弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal也会被回收。value在下一次ThreadLocalMap调用set,get，remove的时候会被清除。 比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用ThreadLocal不会内存泄漏，对应的value在下一次ThreadLocalMap调用set,get,remove的时候会被清除。 因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 五、ThreadLocal 最佳实践 综合上面的分析，我们可以理解ThreadLocal内存泄漏的前因后果，那么怎么避免内存泄漏呢？ 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。 在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 整理自： 深入分析 ThreadLocal 内存泄漏问题]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之ThreadLocal详解]]></title>
    <url>%2F2019%2F02%2F20%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8BThreadLocal%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[ThreadLocal是面试重灾区，但是好像我没遇到过有人问，尴尬脸，不过我们不能做砧板上的鱼肉静静等待宰割，分为两篇来讲解其中的用法和原理。这是第一篇。 一、ThreadLocal简介 ThreadLocal类用来提供线程内部的局部变量。这些变量在多线程环境下访问(通过get或set方法访问)时能保证各个线程里的变量相对独立于其他线程内的变量，ThreadLocal实例通常来说都是private static类型。 ThreadLocal类提供了四个对外开放的接口方法，这也是用户操作ThreadLocal类的基本方法： void set(Object value)设置当前线程的线程局部变量的值。 public Object get()该方法返回当前线程所对应的线程局部变量。 public void remove()将当前线程局部变量的值删除，目的是为了减少内存的占用，该方法是JDK 5.0新增的方法。需要指出的是，当线程结束后，对应该线程的局部变量将自动被垃圾回收，所以显式调用该方法清除线程的局部变量并不是必须的操作，但它可以加快内存回收的速度。 protected Object initialValue()返回该线程局部变量的初始值，该方法是一个protected的方法，显然是为了让子类覆盖而设计的。这个方法是一个延迟调用方法，在线程第1次调用get()或set(Object)时才执行，并且仅执行1次，ThreadLocal中的缺省实现直接返回一个null。 一个简单的小例子来感受ThreadLocal到底是什么以及怎么用： 运行结果： 1234567Thread-0张三李四王五Thread-1ChineseEnglish 分析 可以，看出虽然多个线程对同一个变量进行访问，但是由于threadLocal变量由ThreadLocal 修饰，则不同的线程访问的就是该线程设置的值，这里也就体现出来ThreadLocal的作用。 当使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。 二、扒开JDK threadlocal神秘面纱 threadlocal的原理图为： 那ThreadLocal内部是如何为每一个线程维护变量副本的呢？到底是什么原理呢？ 先来看一下ThreadLocal的set()方法的源码是如何实现的： 我们看到，首先通过getMap(Thread t)方法获取一个和当前线程相关的ThreadLocalMap，然后将变量的值设置到这个ThreadLocalMap对象中，当然如果获取到的ThreadLocalMap对象为空，就通过createMap方法创建。 我们再往下面去一点，比如map.set方法到底是怎么实现的？ 结合上面的图，其实我们可以发现，数据并不是放在所谓的Map集合中，而是放进了一个Entry数组中，这个entry索引是上面计算好的，entry的key是指向threadLocal的一个软引用，value是指向真实数据的一个强引用，以后再获取的时候，再以同样的方式计算得到索引下标即可。 上面代码出现的 ThreadLocalMap 是什么？ ThreadLocalMap是ThreadLocal类的一个静态内部类，它实现了键值对的设置和获取（对比Map对象来理解），每个线程中都有一个独立的ThreadLocalMap副本，它所存储的值，只能被当前线程读取和修改。 我们深入看一下getMap和createMap的实现 getMap: createMap: 代码非常直白，就是获取和设置Thread内的一个叫threadLocals的变量，而这个变量的类型就是ThreadLocalMap，这样进一步验证了上文中的观点：每个线程都有自己独立的ThreadLocalMap对象。 Thread源码中的threadLocals： 我们接着看ThreadLocal中的get方法如下 第一步 先获通过Thread.currentThread（）取当前线程 第二步 然后获取当前线程的threadLocals属性 第三步 在threadLocals属性里获取Entry实例 第四部 从Entry实例的value属性里获取到最后所要的Object对象 接下来讨论一下上面出现的ThreadLocalMap类以及Entry类，直接贴源码 Entry是ThreadLocalMap的内部类，而且ThreadLocalMap里拥有一个类型为Entry[]的table属性，而且每个线程实例有自己的ThreadLocalMap。到这里结论已经很明显了：负责保存ThreadLocal的key和value根本就不是一个Map类型，而是一个Entry数组! Entry继承WeakReference，因此继承拥有一个弱引用referent，而且自身也有一个value属性。Entry利用referent来保存threadLocal实例的弱引用，利用value保存Object的强引用。至于为什么一个是强引用，一个是弱引用，我们在下一篇中来探讨。 最后的问题是怎样在Entry数组里定位我们需要的Entry呢?其实上面在set的时候已经大概知道了，现在再来看看代码吧： 留意key.threadLocalHashCode这个属性，Entry在保存进Entry[]数组之前，会利用ThreadLocal的引用计算出一个hash值，然后利用这个hash值作为下标定位到Entry[]数组的某个位置； 原理总结：ThreadLocal类并没有一个Map来保存数据，数据都是保存在线程实例上的；客户端访问ThreadLocal实例的get方法，get方法通过Thread.getCurrentThread获得当前线程的实例，从而获得当前线程的ThreadLocalMap对象，而ThreadLocalMap里包含了一个Entry数组，里面的每个Entry保存了ThreadLocal引用以及Object引用，Entry的referent保存ThreadLocal的弱引用，Entry的value保存Object的强引用。 三、threadLoca应用 threadlocal实现的可复用的耗时统计工具Profiler 运行结果： 12Thread-0耗时： 1000Thread-1耗时： 1999 threadLocal实现数据库连接线程隔离 通过调用ConnectionManager.getConnection()方法，每个线程获取到的，都是和当前线程绑定的那个Connection对象，第一次获取时，是通过initialValue()方法的返回值来设置值的。通过ConnectionManager.setConnection(Connection conn)方法设置的Connection对象，也只会和当前线程绑定。这样就实现了Connection对象在多个线程中的完全隔离。 在Spring容器中管理多线程环境下的Connection对象时，采用的思路和以上代码非常相似。 四、threadLocal缺陷 ThreadLocal变量的这种隔离策略，也不是任何情况下都能使用的。 如果多个线程并发访问的对象实例只能创建那么一个，那就没有别的办法了，老老实实的使用同步机制吧。 下一篇探讨ThreadLocal 内存泄漏问题。 参考： 深入理解ThreadLocal]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之NIO]]></title>
    <url>%2F2019%2F02%2F20%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8BNIO%2F</url>
    <content type="text"><![CDATA[为了限制篇幅，关于IO这一块的内容，已经从本笔记中移除，具体还是另外看笔记，这里主要还是介绍NIO。 一、NIO 非阻塞的输入/输出 (NIO) 库是在 JDK 1.4 中引入的。NIO 弥补了原来的 I/O 的不足，提供了高速的、面向块的 I/O。 1.1 阻塞I/O通信模型 假如现在你对阻塞I/O已有了一定了解，我们知道阻塞I/O在调用InputStream.read()方法时是阻塞的，它会一直等到数据到来时（或超时）才会返回；同样，在调用ServerSocket.accept()方法时，也会一直阻塞到有客户端连接才会返回，每个客户端连接过来后，服务端都会启动一个线程去处理该客户端的请求。阻塞I/O的通信模型示意图如下： 缺点： 当客户端多时，会创建大量的处理线程。且每个线程都要占用栈空间和一些CPU时间 阻塞可能带来频繁的上下文切换，且大部分上下文切换可能是无意义的。 1.2 java NIO原理及通信模型 下面是java NIO的工作原理： 由一个专门的线程来处理所有的 IO 事件，并负责分发。 事件驱动机制：事件到的时候触发，而不是同步的去监视事件。 线程通讯：线程之间通过 wait,notify 等方式通讯。保证每次上下文切换都是有意义的。减少无谓的线程切换。 Java NIO的服务端只需启动一个专门的线程来处理所有的 IO 事件，这种通信模型是怎么实现的呢？java NIO采用了双向通道（channel）进行数据传输，而不是单向的流（stream），在通道上可以注册我们感兴趣的事件。一共有以下四种事件： 事件名 对应值 服务端接收客户端连接事件 SelectionKey.OP_ACCEPT(16) 客户端连接服务端事件 SelectionKey.OP_CONNECT(8) 读事件 SelectionKey.OP_READ(1) 写事件 SelectionKey.OP_WRITE(4) 服务端和客户端各自维护一个管理通道的对象，我们称之为selector，该对象能检测一个或多个通道 (channel) 上的事件。我们以服务端为例，如果服务端的selector上注册了读事件，某时刻客户端给服务端发送了一些数据，阻塞I/O这时会调用read()方法阻塞地读取数据，而NIO的服务端会在selector中添加一个读事件。服务端的处理线程会轮询地访问selector，如果访问selector时发现有感兴趣的事件到达，则处理这些事件，如果没有感兴趣的事件到达，则处理线程会一直阻塞直到感兴趣的事件到达为止。下面是我理解的java NIO的通信模型示意图： 二、关于阻塞与非阻塞，同步与非同步的理解 我们都知道常见的IO有四种方式，同步阻塞，同步非阻塞，异步阻塞，异步非阻塞。然而对于同步和阻塞的理解却一直很模糊。 2.1 同步与异步 所谓同步就是一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致。 而异步是不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列。 我们可以用打电话（同步）和发短信（异步）来很好的比喻同步与异步操作。 2.2 阻塞和非阻塞 阻塞就是 CPU 停下来等待一个慢的操作完成 CPU 才接着完成其它的事。 非阻塞就是在这个慢的操作在执行时 CPU 去干其它别的事，等这个慢的操作完成时，CPU 再接着完成后续的操作。 虽然表面上看非阻塞的方式可以明显的提高 CPU 的利用率，但是也带了另外一种后果就是系统的线程切换增加。 2.3 什么是阻塞IO？什么是非阻塞IO？ 在了解阻塞IO和非阻塞IO之前，先看下一个具体的IO操作过程是怎么进行的。 通常来说，IO操作包括：对硬盘的读写、对socket的读写以及外设的读写。 当用户线程发起一个IO请求操作（本文以读请求操作为例），内核会去查看要读取的数据是否就绪，对于阻塞IO来说，如果数据没有就绪，则会一直在那等待，直到数据就绪；对于非阻塞IO来说，如果数据没有就绪，则会返回一个标志信息告知用户线程当前要读的数据没有就绪。当数据就绪之后，便将数据拷贝到用户线程，这样才完成了一个完整的IO读请求操作，也就是说一个完整的IO读请求操作包括两个阶段： 查看数据是否就绪； 进行数据拷贝（内核将数据拷贝到用户线程）。 那么阻塞（blocking IO）和非阻塞（non-blocking IO）的区别就在于第一个阶段，如果数据没有就绪，在查看数据是否就绪的过程中是一直等待，还是直接返回一个标志信息。 Java中传统的IO都是阻塞IO，比如通过socket来读数据，调用read()方法之后，如果数据没有就绪，当前线程就会一直阻塞在read方法调用那里，直到有数据才返回；而如果是非阻塞IO的话，当数据没有就绪，read()方法应该返回一个标志信息，告知当前线程数据没有就绪，而不是一直在那里等待。 2.4 什么是同步IO？什么是异步IO？ 我们知道了，阻塞和非阻塞是判断数据是否就绪时如何处理，即IO操作的第一阶段。 那么什么是同步IO和异步IO呢？ 我们知道，同步是打电话，异步是发短信，打电话需要等到电话通了才能进行下一步，发短信就不用操心那么多了，我发出去就行了，至于什么时候发送、如何发送以及如何保证我这个短信一定能发出去，我是不管的。 同步IO即 如果一个线程请求进行IO操作，在IO操作完成之前，该线程会被阻塞；而异步IO为 如果一个线程请求进行IO操作，IO操作不会导致请求线程被阻塞。 描述的是用户线程与内核的交互方式： 同步是指用户线程发起 I/O 请求后需要等待或者轮询内核 I/O操作完成后才能继续执行； 异步是指用户线程发起I/O请求后仍继续执行，当内核I/O操作完成后会通知用户线程，或者调用用户线程注册的回调函数。 三、Channel（通道） 通道，顾名思义，就是通向什么的道路，为某个提供了渠道。在传统IO中，我们要读取一个文件中的内容，通常是像下面这样读取的： 这里的InputStream实际上就是为读取文件提供一个通道的。 因此可以将NIO 中的Channel同传统IO中的Stream来类比，但是要注意，传统IO中，Stream是单向的，比如InputStream只能进行读取操作，OutputStream只能进行写操作。而Channel是双向的，既可用来进行读操作，又可用来进行写操作。 通道包括以下类型： FileChannel：从文件中读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel 下面给出通过FileChannel来向文件中写入数据的一个例子： 四、Buffer（缓冲区） Buffer（缓冲区），是NIO中非常重要的一个东西，在NIO中所有数据的读和写都离不开Buffer。比如上面的一段代码中，读取的数据时放在byte数组当中，而在NIO中，读取的数据只能放在Buffer中。同样地，写入数据也是先写入到Buffer中。 上面的图描述了从一个客户端向服务端发送数据，然后服务端接收数据的过程。客户端发送数据时，必须先将数据存入Buffer中，然后将Buffer中的内容写入通道。服务端这边接收数据必须通过Channel将数据读入到Buffer中，然后再从Buffer中取出数据来处理。 缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 缓冲区包括以下类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 如果是对于文件读写，上面几种Buffer都可能会用到。但是对于网络读写来说，用的最多的是ByteBuffer。 五、缓冲区状态变量 capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数。 状态变量的改变过程举例： ① 新建一个大小为 8 个字节的缓冲区，此时 position 为 0，而 limit = capacity = 8。capacity 变量不会改变，下面的讨论会忽略它。 ② 从输入通道中读取 5 个字节数据写入缓冲区中，此时 position 移动设置为 5，limit 保持不变。 ③ 在将缓冲区的数据写到输出通道之前，需要先调用 flip() 方法，这个方法将 limit 设置为当前 position，并将 position 设置为 0。 buffer中的flip方法涉及到bufer中的Capacity,Position和Limit三个概念。其中Capacity在读写模式下都是固定的，就是我们分配的缓冲大小,Position类似于读写指针，表示当前读(写)到什么位置,Limit在写模式下表示最多能写入多少数据，此时和Capacity相同，在读模式下表示最多能读多少数据，此时和缓存中的实际数据大小相同。在写模式下调用flip方法，那么limit就设置为了position当前的值(即当前写了多少数据),postion会被置为0，以表示读操作从缓存的头开始读。也就是说调用flip之后，读写指针指到缓存头部，并且设置了最多只能读出之前写入的数据长度(而不是整个缓存的容量大小)。 ④ 从缓冲区中取 4 个字节到输出缓冲中，此时 position 设为 4。 ⑤ 最后需要调用 clear() 方法来清空缓冲区，此时 position 和 limit 都被设置为最初位置。 六、文件 NIO 实例 以下展示了使用 NIO 快速复制文件的实例： 七、Selector（选择器） 可以说它是NIO中最关键的一个部分，Selector的作用就是用来轮询每个注册的Channel，一旦发现Channel有注册的事件发生，便获取事件然后进行处理。 用单线程处理一个Selector，然后通过Selector.select()方法来获取到达事件，在获取了到达事件之后，就可以逐个地对这些事件进行响应处理。 因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件具有更好的性能。 下面从编程的角度具体来看看选择器是如何实现的。 7.1 创建选择器 1Selector selector = Selector.open(); 7.2 将通道注册到选择器上 123ServerSocketChannel ssChannel = ServerSocketChannel.open();ssChannel.configureBlocking(false);ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其它事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。 在将通道注册到选择器上时，还需要指定要注册的具体事件，主要有以下几类： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 它们在 SelectionKey 的定义如下： 1234public static final int OP_READ = 1 &lt;&lt; 0;public static final int OP_WRITE = 1 &lt;&lt; 2;public static final int OP_CONNECT = 1 &lt;&lt; 3;public static final int OP_ACCEPT = 1 &lt;&lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如： 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 7.3 监听事件 1int num = selector.select(); 使用 select() 来监听事件到达，它会一直阻塞直到有至少一个事件到达。 7.4 获取到达的事件 1234567891011Set&lt;SelectionKey&gt; keys = selector.selectedKeys();Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator();while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove();&#125; 7.5 事件循环 因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。 1234567891011121314while (true) &#123; int num = selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove(); &#125;&#125; 八、流与块 I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 一次处理一个字节数据，一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂处理机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。 面向块的 I/O 一次处理一个数据块，按块处理数据比按流处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 I/O 包和 NIO 已经很好地集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统中，处理速度也会更快。 九、一个完整 NIO 实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); ServerSocket serverSocket = ssChannel.socket(); InetSocketAddress address = new InetSocketAddress("127.0.0.1", 8888); serverSocket.bind(address); while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; ServerSocketChannel ssChannel1 = (ServerSocketChannel) key.channel(); // 服务器会为每个新连接创建一个 SocketChannel SocketChannel sChannel = ssChannel1.accept(); sChannel.configureBlocking(false); // 这个新连接主要用于从客户端读取数据 sChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel sChannel = (SocketChannel) key.channel(); System.out.println(readDataFromSocketChannel(sChannel)); sChannel.close(); &#125; keyIterator.remove(); &#125; &#125; &#125; private static String readDataFromSocketChannel(SocketChannel sChannel) throws IOException &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); StringBuffer data = new StringBuffer(); while (true) &#123; buffer.clear(); int n = sChannel.read(buffer); if (n == -1) break; buffer.flip(); int limit = buffer.limit(); char[] dst = new char[limit]; for (int i = 0; i &lt; limit; i++) dst[i] = (char) buffer.get(i); data.append(dst); buffer.clear(); &#125; return data.toString(); &#125;&#125; 12345678910public class NIOClient &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket("127.0.0.1", 8888); OutputStream out = socket.getOutputStream(); String s = "hello world"; out.write(s.getBytes()); out.close(); &#125;&#125; 十、NIO和IO的主要区别 IO NIO 面向流 面向缓冲 阻塞IO 非阻塞IO 无 选择器 面向流与面向缓冲 Java IO和NIO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 阻塞与非阻塞IO Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors） Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 十一、总结 NIO其实实现的是一个IO的多路复用，用select来同时监听多个channel，本质上还是同步阻塞的，需要select不断监听端口。但是对于IO各个通道来说就是可以看做是异步。 基本可以认为 “NIO = I/O多路复用 + 非阻塞式I/O”，大部分情况下是单线程，但也有超过一个线程实现NIO的情况 我们可以用打电话（同步）和发短信（异步）来很好的比喻同步与异步操作 阻塞就是 CPU 停下来等待一个慢的操作完成 CPU 才接着完成其它的事。 非阻塞就是在这个慢的操作在执行时 CPU 去干其它别的事，等这个慢的操作完成时，CPU 再接着完成后续的操作。两种方式各有优劣。 传统IO中，Stream是单向的，比如InputStream只能进行读取操作，OutputStream只能进行写操作。而Channel是双向的，既可用来进行读操作，又可用来进行写操作。 在NIO中，读取的数据只能放在Buffer中。同样地，写入数据也是先写入到Buffer中。缓冲区有三个状态变量：capacity：最大容量；position：当前已经读写的字节数；limit：还可以读写的字节数。 Selector的作用就是用来轮询每个注册的Channel，一旦发现Channel有注册的事件发生，便获取事件然后进行处理. NIO和IO的主要区别。 NIO适用场景 服务器需要支持超大量的长时间连接。比如10000个连接以上，并且每个客户端并不会频繁地发送太多数据。例如总公司的一个中心服务器需要收集全国便利店各个收银机的交易信息，只需要少量线程按需处理维护的大量长期连接。 BIO适用场景 适用于连接数目比较小，并且一次发送大量数据的场景，这种方式对服务器资源要求比较高，并发局限于应用中。 参考： IO - 同步，异步，阻塞，非阻塞 Java NIO：浅析I/O模型 NIO与传统IO的区别 Java NIO：NIO概述 Java I/O]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Socket基础]]></title>
    <url>%2F2019%2F02%2F20%2FSocket%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[学习I/O几种常见模型以及select、poll、epoll三种多路复用的具体实现。 一、I/O 模型 一个输入操作，即我们常说的读取一个文件，通常包括两个阶段： 等待数据准备好 从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。我们看到，这是一个比较麻烦的过程，可能是性能出现瓶颈的地方。 Unix 下有五种 I/O 模型： 阻塞式 I/O 非阻塞式 I/O I/O 复用（select 和 poll） 信号驱动式 I/O（SIGIO） 异步 I/O（AIO） 1.1 阻塞式 I/O 应用进程被阻塞，直到数据复制到应用进程缓冲区中才返回。 应该注意到，在阻塞的过程中，其它程序还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其他程序还可以执行，因此不消耗 CPU 时间，这种模型的执行效率会比较高。 下图中，recvfrom 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。 1ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); 1.2 非阻塞式 I/O 应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式成为轮询（polling）。 由于 CPU 要处理更多的系统调用，因此这种模型是比较低效的。 1.3 I/O 复用 使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读，这一过程会被阻塞，当某一个套接字可读时返回。之后再使用 recvfrom 把数据从内核复制到进程中。 它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。 如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。并且相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。 1.4 信号驱动 I/O 应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。 相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。 1.5 异步 I/O 进行 aio_read 系统调用会立即返回，应用进程继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。 异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。 1.6 同步 I/O 与异步 I/O 同步 I/O：应用进程在调用 recvfrom 操作时会阻塞。 异步 I/O：不会阻塞。 阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O 都是同步 I/O，虽然非阻塞式 I/O 和信号驱动 I/O 在等待数据阶段不会阻塞，但是在之后的将数据从内核复制到应用进程这个操作会阻塞。 1.7 五大 I/O 模型比较 前四种 I/O 模型的主要区别在于第一个阶段，而第二个阶段是一样的：将数据从内核复制到应用进程过程中，应用进程会被阻塞。 二、select/poll/epoll 这三个都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 2.1 select fd_set 表示描述符集合类型，有三个参数：readset、writeset 和 exceptset，分别对应读、写、异常条件的描述符集合。 timeout 参数告知内核等待所指定描述符中的任何一个就绪可花多少时间； 成功调用返回结果大于 0；出错返回结果为 -1；超时返回结果为 0。 每次调用 select 都需要将 fd_set \*readfds, fd_set \*writefds, fd_set \*exceptfds 链表内容全部从应用进程缓冲复制到内核缓冲。 返回结果中内核并没有声明 fd_set 中哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程需要遍历所有的 fd_set。 select 最多支持 1024 个描述符，其中 1024 由内核的 FD_SETSIZE 决定。如果需要打破该限制可以修改 FD_SETSIZE，然后重新编译内核。 2.2 poll 它和 select 功能基本相同。同样需要每次将描述符从应用进程复制到内核，poll 调用返回后同样需要进行轮询才能知道哪些描述符已经准备好。 poll 取消了 1024 个描述符数量上限，但是数量太大以后不能保证执行效率，因为复制大量内存到内核十分低效，所需时间与描述符数量成正比。 poll 在描述符的重复利用上比 select 的 fd_set 会更好。 如果在多线程下，如果一个线程对某个描述符调用了 poll 系统调用，但是另一个线程关闭了该描述符，会导致 poll 调用结果不确定，该问题同样出现在 select 中。 2.3 epoll epoll 仅仅适用于 Linux OS。 它是 select 和 poll 的增强版，更加灵活而且没有描述符数量限制。 它将用户关心的描述符放到内核的一个事件表中，从而只需要在用户空间和内核空间拷贝一次。 select 和 poll 方式中，进程只有在调用一定的方法后，内核才对所有监视的描述符进行扫描。而 epoll 事先通过 epoll_ctl() 来注册描述符，一旦基于某个描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个描述符，当进程调用 epoll_wait() 时便得到通知。 新版本的 epoll_create(int size) 参数 size 不起任何作用，在旧版本的 epoll 中如果描述符的数量大于 size，不保证服务质量。 epoll_ctl() 执行一次系统调用，用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理。 epoll_wait() 取出在内核中通过链表维护的 I/O 准备好的描述符，将他们从内核复制到应用进程中，不需要像 select/poll 对注册的所有描述符遍历一遍。 epoll 对多线程编程更有友好，同时多个线程对同一个描述符调用了 epoll_wait() 也不会产生像 select/poll 的不确定情况。或者一个线程调用了 epoll_wait 另一个线程关闭了同一个描述符也不会产生不确定情况。 三、select 和 poll 比较 3.1 功能 它们提供了几乎相同的功能，但是在一些细节上有所不同： select 会修改 fd_set 参数，而 poll 不会； select 默认只能监听 1024 个描述符，如果要监听更多的话，需要修改 FD_SETSIZE 之后重新编译； poll 提供了更多的事件类型。 3.2 速度 poll 和 select 在速度上都很慢。 它们都采取轮询的方式来找到 I/O 完成的描述符，如果描述符很多，那么速度就会很慢； select 只使用每个描述符的 3 位，而 poll 通常需要使用 64 位，因此 poll 需要复制更多的内核空间。 3.3 可移植性 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 四、eopll 工作模式 epoll_event 有两种触发模式：LT（level trigger）和 ET（edge trigger）。 4.1 LT 模式 当 epoll_wait() 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait() 时，会再次响应应用程序并通知此事件。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 4.2 ET 模式 当 epoll_wait() 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait() 时，不会再次响应应用程序并通知此事件。很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 五、select poll epoll 应用场景 很容易产生一种错觉认为只要用 epoll 就可以了，select poll 都是历史遗留问题，并没有什么应用场景，其实并不是这样的。 5.1 select 应用场景 select() poll() epoll_wait() 都有一个 timeout参数，在 select() 中 timeout 的精确度为 1ns，而 poll() 和 epoll_wait() 中则为 1ms。所以 select 更加适用于实时要求更高的场景，比如核反应堆的控制。 select 历史更加悠久，它的可移植性更好，几乎被所有主流平台所支持。 5.2 poll 应用场景 poll 没有最大描述符数量的限制，如果平台支持应该采用 poll 且对实时性要求并不是十分严格，而不是 select。 需要同时监控小于 1000 个描述符。那么也没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。epoll 的描述符存储在内核，不容易调试。 5.3 epoll 应用场景 程序只需要运行在 Linux 平台上，有非常大量的描述符需要同时轮询，而且这些连接最好是长连接。 六、对比 举例说明：老师收学生作业，相当于应用层调用I/O操作。 1、老师逐个收学生作业，学生没有做完，只能阻塞等待，收了之后，再去收下一个学生的作业。这显然存在性能问题。 2、怎么解决上面的问题？ 老师找个班长，班长负责收作业，班长的做法是：遍历问学生作业写好了吗，写好的，收起来交给老师。休息一会，再去遍历。。。 这个班长就是select。 存在问题 这个班长还有一个能力问题，最多只能管理1024个学生。 很多学生的作业没有写好，而且短时间写不好，班长还是不停地遍历去问，影响效率。 怎么解决问题1班长的能力问题？ 换一个能力更强的班长，可以管理更多的学生，这个班长就是poll。 怎么解决问题1、2，存在的能力问题和效率问题？ 换一个能力超级强的班长，可以管理无限多的学生，同时班长的做法是：遍历一次所有的学生，如果作业没有写完，告诉学生写好之后，放在一个固定的地方。这样的话，班长只需要定期到这个地方取作业就好了。这就是epoll。 参考： socket 理解 select poll epoll]]></content>
      <tags>
        <tag>Socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之泛型下]]></title>
    <url>%2F2019%2F02%2F19%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%B3%9B%E5%9E%8B%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[本节继续讨论泛型相关的知识。 6、泛型上下边界 我们再来讨论讨论通配符。 通配符有2种： 无边界通配符，用&lt;?&gt;表示。 有边界通配符，用&lt;? extends Object&gt;或者&lt;? super Object&gt;来表示。（Object仅仅是一个示例） 6.1 无边界 123List&lt;?&gt; list = new ArrayList&lt;String&gt;(); // 合法List&lt;?&gt; list = new ArrayList&lt;?&gt;(); // 不合法List&lt;String&gt; list = new ArrayList&lt;?&gt;(); // 不合法 对于带有通配符的引用变量，是不能调用具有与泛型参数有关的方法的。 1234List&lt;?&gt; list = new ArrayList&lt;String&gt;();list.add(1); // 编译不通过list.get(0); // 编译通过int size = list.size(); // 由于size()方法中不含泛型参数，所以可以在通配符变量中调用 总结起来，无边界通配符主要用做引用，可以调用与泛型参数无关的方法，不能调用参数中包含泛型参数的方法。 6.2 有边界 在使用泛型的时候，我们还可以为传入的泛型类型实参进行上下边界的限制，如：类型实参只准传入某种类型的父类或某种类型的子类。 上边界通配，用&lt;? extends 类型&gt;表示。其语法为： 1List&lt;? extends 类型1&gt; x = new ArrayList&lt;类型2&gt;(); 其中，类型2就只能是类型1或者是类型1的子类。下面代码验证合法性。 12List&lt;? extends Number&gt; x = new ArrayList&lt;Integer&gt;(); //由于Integer是Number的子类，这是合法的List&lt;? extends Number&gt; x = new ArrayList&lt;String&gt;(); //由于String不是Number的子类，这是不合法的 下边界通配，用&lt;? super 类型&gt;表示。其语法为： 1List&lt;? super 类型1&gt; x = new ArrayList&lt;类型2&gt;(); 其中，类型2就只能是类型1或者是类型1的超类。下面代码有验证合法性。 12List&lt;? super Integer&gt; x = new ArrayList&lt;Number&gt;(); //由于Number是Integer的超类，这是合法的List&lt;? super Integer&gt; x = new ArrayList&lt;String&gt;(); //由于String不是Integer的超类，这是不合法的 那么到底什么时候使用下边界通配，什么时候使用上边界通配呢？首先考虑一下怎样才能保证不会发生运行时异常，这是泛型要解决的首要问题，通过前面的内容可以看到，任何可能导致类型转换异常的操作都无法编译通过。 ⭐上边界通配：可以保证存放的实际对象至多是上边界指定的类型，那么在读取对象时，我们总是可以放心地将对象赋予上边界类型的引用。 1234List&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;();list1.add(1);List&lt;? extends Number&gt; list2 = list1;Number a = list2.get(0); // 编译通过 ⭐下边界通配：可以保证存放的实际对象至少是下边界指定的类型，那么在存入对象时，我们总是可以放心地将下边界类型的对象存入泛型对象中。 123List&lt;? super Integer&gt; list3 = new ArrayList&lt;Number&gt;();list3.add(1);list3.add(2); 总结： 如果你想从一个数据类型里获取数据，使用 ? extends 通配符。 如果你想把对象写入一个数据结构里，使用 ? super 通配符。 如果你既想存，又想取，那就别用通配符。 对于泛型方法添加上下边界： 1234567//在泛型方法中添加上下边界限制的时候，必须在权限声明与返回值之间的&lt;T&gt;上添加上下边界，即在泛型声明的时候添加//public &lt;T&gt; T showKeyName(Generic&lt;T extends Number&gt; container)，编译器会报错："Unexpected bound"public &lt;T extends Number&gt; T showKeyName(Generic&lt;T&gt; container)&#123; System.out.println("container key :" + container.getKey()); T test = container.getKey(); return test;&#125; 7、泛型的原理 7.1 类型擦除 Java中的泛型是通过类型擦除来实现的。所谓类型擦除，是指通过类型参数合并，将泛型类型实例关联到同一份字节码上。编译器只为泛型类型生成一份字节码，并将其实例关联到这份字节码上。类型擦除的关键在于从泛型类型中清除类型参数的相关信息，并且再必要的时候添加类型检查和类型转换的方法。 下面通过两个例子来证明在编译时确实发生了类型擦除。 例1分别创建实际类型为String和Integer的ArrayList对象，通过getClass()方法获取两个实例的类，最后判断这个实例的类是相等的，证明两个实例共享同一个类。 例2创建一个只能存储Integer的ArrayList对象，在add一个整型数值后，利用反射调用add(Object o) add一个asd字符串，此时运行代码不会报错，运行结果会打印出1和asd两个值。这时再里利用反射调用add(Integer o)方法，运行会抛出codeNoSuchMethodException异常。这充分证明了在编译后，擦除了Integer这个泛型信息，只保留了原始类型。 7.2 自动类型转换 上一节上说到了类型擦除，Java编译器会擦除掉泛型信息。那么调用ArrayList的get()最终返回的必然会是一个Object对象，但是我们在源代码并没有写过Object转成Integer的代码，为什么就能“直接”将取出来的对象赋予一个Integer类型的变量呢（如下面的代码第12行）？ 实际上，Java的泛型除了类型擦除之外，还会自动生成checkcast指令进行强制类型转换。上面的代码中的main方法编译后所对应的字节码如下。 看到第26行代码就是将Object类型的对象强制转换为Integer的指令。我们完全可以将上面的代码转换为下面的代码，它所实现的效果跟上面的泛型是一模一样的。既然泛型也需要进行强制转换，所以泛型并不会提供运行时效率，不过可以大大降低编程时的出错概率。 8、简单总结 8.1 类型擦除(Type Erasure) Java 的泛型是在编译器层次实现的。 在编译生成的字节码中不包含泛型中的类型参数，类型参数会在编译时去掉。 例如：List&lt;String&gt; 和 List&lt;Integer&gt; 在编译后都变成 List。 类型擦除的基本过程：将代码中的类型参数替换为具体的类，同时去掉 &lt;&gt; 的内容。 8.2 泛型的优势 编译时更强大的类型检测。 例如如下代码：方法传入一个String对象，传出一个String 对象，并强制转换为Integer对象。这段代码编译可以通过，因为都是Object的子类，但是运行时会产生ClassCastException。 而如果通过泛型来实现，则会在编译时进行类型的检测。例如如下代码：会产生编译错误。 提供自动和隐式的类型转换 8.3 &lt;T&gt; VS &lt;?&gt; 不同点： &lt;T&gt;用于泛型的定义，例如class MyGeneric&lt;T&gt; {...} &lt;?&gt;用于泛型的声明，即泛型的使用，例如MyGeneric&lt;?&gt; g = new MyGeneric&lt;&gt;(); 相同点：都可以指定上界和下界: 8.4 &lt;?&gt;不同于&lt;Object&gt; 指定未知类型，如List&lt;?&gt;。List&lt;?&gt;不等于List&lt;Object&gt; String是Object的子类，但是List&lt;String&gt;不是List&lt;Object&gt;的子类。 如果将List&lt;Object&gt;换成List&lt;?&gt;，则可以编译通过。 注意： 相同参数类型的泛型类的继承关系取决于泛型类自身的继承结构。 例如List&lt;String&gt;是Collection&lt;String&gt;的子类 当类型声明中使用通配符?时，其子类型可以在两个维度上扩展。 123例如 Collection&lt;? extends Number&gt;在维度1上扩展：List&lt;? extends Number&gt;在维度2上扩展：Collection&lt;Integer&gt; 9、Java泛型中List、List&lt;Object&gt;、List&lt;?&gt;的区别 List：原生态类型 List&lt;Object&gt;：参数化的类型，表明List中可以容纳任意类型的对象 List&lt;?&gt;：无限定通配符类型，表示只能包含某一种未知对象类型 我们创建了一个List&lt;String&gt;类型的对象strings，再把它赋给原生态类型List，这是可以的。但是第5行中尝试把它传递给List&lt;Object&gt;时，出现了一个类型不相容错误，注意，这是一个编译期错误。 这是因为泛型有子类型化的规则： List&lt;String&gt;是原生态类型List的一个子类型。虽然String是Object的子类型，但是由于泛型是不可协变的，List&lt;String&gt;并不是List&lt;Object&gt;的子类型，所以这里的传递无法通过编译。 List&lt;Object&gt;唯一特殊的地方只是Object是所有类型的超类，由于泛型的不可协变性，它只能表示List中可以容纳所有类型的对象，却不能表示任何参数类型的List&lt;E&gt;。 输出结果： 1211sss 总结： List&lt;Object&gt;:表示可用装载任意类型的对象，如上面最后一个例子，但是他不能接受List&lt;String&gt;的替换，因为不具有继承性，并且List&lt;Object&gt;如果可以被List&lt;String&gt;，就不符合原则了，因为List&lt;String&gt;只能接受String类型的对象。 List&lt;?&gt;:解决上面表面有继承关系的List的赋值问题，还有就是，他是用作声明能接收一种未知对象类型，而不是大杂烩啥都能接收。 List：原始类型，啥都没有限制。个人认为与List&lt;Object&gt;类似，但是又没有继承的限制。即啥类型都可以接收。 10、参考 http://hinylover.space/2016/06/25/relearn-java-generic-1/ java 泛型详解 https://www.cnblogs.com/rese-t/p/8158870.html]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之泛型上]]></title>
    <url>%2F2019%2F02%2F19%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%B3%9B%E5%9E%8B%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[本篇文章全面介绍Java泛型中的基础及原理。本节主要介绍什么是泛型、泛型的核心特性、泛型与继承注意点、泛型与多态的原理以及泛型的使用。 1、什么是泛型以及为什么用泛型 直接上例子进行说明： 毫无疑问，程序的运行结果会以崩溃结束： 1java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String 为什么会出现这种问题呢？ 集合本身无法对其存放的对象类型进行限定，可以涵盖Java中的所有类型。缺口太大，导致各种蛇、蚁、虫、鼠通通都可以进来。 由于我们要使用的实际存放类型的方法，所以不可避免地要进行类型转换。小对象转大对象很容易，大对象转小对象则有很大的风险，因为在编译时，我们无从得知对象真正的类型。 泛型就是为了解决这类问题而诞生的。 2、泛型的特性 2.1 泛型只在编译阶段有效 输出结果：类型相同 通过上面的例子可以证明，在编译之后程序会采取去泛型化的措施。也就是说Java中的泛型，只在编译阶段有效。在编译过程中，正确检验泛型结果后，会将泛型的相关信息擦除，并且在对象进入和离开方法的边界处添加类型检查和类型转换的方法。也就是说，泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 2.2 泛型的兼容性 Java编译器是向后兼容的，也就是低版本的源代码可以用高版本编译器进行编译。下面来看看那些兼容性代码。 引用和实例化都不包含泛型信息。 上面的这段代码是可以通过编译的，这是JDK1.4之前的写法，所以可以验证JDK1.5之后的编译器是可以兼容JDK1.4之前的源代码的。不过，笔者在JDK1.8.x版本的编译器进行编译时，会抛出如下所示的警告信息。很显然，如果类被定义成泛型类，但是在实际使用时不使用泛型特性，这是不推荐的做法！ 12注: Compatibility.java使用了未经检查或不安全的操作。注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。 引用使用泛型，实例化不使用泛型。 上面的代码编译不通过，由于对引用使用了泛型，其中的所能容纳的对象必须为String 类型。这种写法实际上跟完整写法的作用一致，不过Eclipse仍然会警告 1上面的代码编译不通过，由于对引用使用了泛型，其中的所能容纳的对象必须为String 类型。这种写法实际上跟完整写法的作用一致，不过Eclipse仍然会警告。 引用不使用泛型，实例化使用泛型。 上面的这段代码可以编译通过，其效果与1（不使用泛型）完全一致。结合2、3可以知道，编译时只能做引用的类型检查，而无法检查引用所指向对象的实际类型。 3、泛型与继承 在使用泛型时，引用的参数类型与实际对象的参数类型要保持一致（通配符除外），就算两个参数类型是继承关系也是不允许的。看看下面的2行代码，它们均不能通过编译。 12ArrayList&lt;String&gt; arrayList1 = new ArrayList&lt;Object&gt;(); //编译错误 ArrayList&lt;Object&gt; arrayList1 = new ArrayList&lt;String&gt;(); //编译错误 下面来探讨一下为什么不能这么做。 第1种情况，如果这种代码可以通过编译，那么调用get()方法返回的对象应该是String，但它实际上可以存放任意Object类型的对象，这样在调用类型转换指令时会抛出ClassCastException。 第2种情况。虽然String类型的对象转换为Object不会有任何问题，但是这有什么意义呢？我们原本想要用String对象的方法，但最终将其赋予了一个Object类型的引用。如果需要使用String中的某些方法，必须将Object强制转换为String。这样不会抛出异常，但是却违背了泛型设计的初衷。 4、泛型与多态 下面来考虑一下泛型中多态问题。普通类型的多态是通过继承并重写父类的方法来实现的，泛型也不例外，下面是一个泛型多态示例。 上面定义了一个泛型父类和一个实际参数为String类型的子类，并“重写”了set(T)和get()方法。Son类中的@Override注解也清楚地显示这是一个重写方法，最终执行的结果如下，与想象中的结果完全一致。 12I am father, t=hello worldI am son. 真的这么简单么？虽然表面上（源代码层面）来看，泛型多态与普通类的多态并无二样，但是其内部的实时原理却大相径庭。 泛型类Father在编译后会擦除泛型信息，所有的泛型参数都会用Object类替代。实际上，Father编译后的字节码与下面的代码完全一致。 Son类的与最终会变为： Father和Son类的set()方法的参数类型不一样，所以，这并不是方法重写，而是方法重载！但是，如果是重载，那么Son类就应该会继承Father类的set(Object)方法，也就是Son会同时包含set(String)和set(Object)，下面来测试一下。 123Son son = new Son();son.set("test");son.set(new Object()); // 编译错误 当set一个Object对象时，编译无法通过。这就很奇怪了，感觉跟之前学到的知识是相悖的。我们原本想通过重写方法来实现多态，但由于泛型的类型擦除，却最终变成了重载，所以类型擦除与多态有了矛盾。那么Java是怎么解决这个问题的呢？还是从字节码中找答案吧。Son类最终的编译结果如下： 1234567public void set(java.lang.String); // 我们重写的方法public java.lang.String get(); // 我们重写的方法public java.lang.Object get(); // 编译器生成的方法public void set(java.lang.Object); // 编译器生成的方法 ... 2: checkcast #39 // class java/lang/String ... ⭐这里面多了一个Object get()方法和set(Object)方法，这两个方法在Son类源代码里面并不存在，这是编译器为了解决泛型的多态问题而自动生成的方法，称为“桥方法”。这两个方法的签名与Father类中的两个方法的签名完全一致，这才是真正的方法重写。也就是说，子类真正重写的我们看不到的桥方法，啊，多么痛的领悟！！！@Override注解只是假象，让人误以为他们真的是重写方法。 再看看set(Object)桥方法的实现细节，先将Object对象强制转换为String对象，然后调用Son中的set(String)方法。饶了一个圈，最终才回到我们“重写”的方法。main方法中原本调用父类的set(Object)方法，由于子类通过桥方法重写了这个方法，所以最终的调用顺序是：set(Object) -&gt; set(String)。 与set(Object)桥方法的意义不同，Object get()并不仅仅解决泛型与重写的冲突，而更具有一般性。看看下面的代码，这是一个普通类的继承: 12345public class GeneralFather &#123; public Object get() &#123; return null; &#125;&#125; 123456public class GeneralSon extends GeneralFather &#123; @Override public String get() &#123; return ""; &#125;&#125; 子类的返回类型是父类的返回类型的子类，这是允许的，这种特性叫做Java返回值的协变性。而协变性的实现方法就是上面所述的桥方法。 这里还会有疑惑，set方法可以通过参数类型来确定调用的方法。但是，参数一样而返回值不一样是不能重载的。如果我们在源代码中通过编写String get()和Object get()方法是无法通过编译的。虽然，编译器无法通过编译，但是JVM是可以编写这两种方法的，它调用方法时，将返回值也作为方法签名的一部分。有种只许州官放火，不许百姓点灯的感觉。可以看到，JVM做了不少我们认为不合法的事情，所以如果不深入研究底层原理，有些问题根本解释不了。 5、泛型的使用 泛型有三种使用方式，分别为：泛型类、泛型接口、泛型方法. 5.1 泛型类 泛型类型用于类的定义中，被称为泛型类。通过泛型可以完成对一组类的操作对外开放相同的接口。最典型的就是各种容器类，如：List、Set、Map。 下面进行实例化： 结果为： 1212-27 09:20:04.432 13063-13063/? D/泛型测试: key is 12345612-27 09:20:04.432 13063-13063/? D/泛型测试: key is key_vlaue 定义的泛型类，就一定要传入泛型类型实参么？并不是这样，在使用泛型的时候如果传入泛型实参，则会根据传入的泛型实参做相应的限制，此时泛型才会起到本应起到的限制作用。如果不传入泛型类型实参的话，在泛型类中使用泛型的方法或成员变量定义的类型可以为任何的类型。 1234D/泛型测试: key is 111111D/泛型测试: key is 4444D/泛型测试: key is 55.55D/泛型测试: key is false 5.2 泛型接口 当实现泛型接口的类，未传入泛型实参时： 当实现泛型接口的类，传入泛型实参时： 5.3 泛型通配符 我们知道Ingeter是Number的一个子类，同时我们也验证过Generic&lt;Ingeter&gt;与Generic&lt;Number&gt;实际上是相同的一种基本类型。那么问题来了，在使用Generic&lt;Number&gt;作为形参的方法中，能否使用Generic&lt;Ingeter&gt;的实例传入呢？在逻辑上类似于Generic&lt;Number&gt;和Generic&lt;Ingeter&gt;是否可以看成具有父子关系的泛型类型呢？ 为了弄清楚这个问题，我们使用Generic&lt;T&gt;这个泛型类继续看下面的例子： 通过提示信息我们可以看到Generic&lt;Integer&gt;不能被看作为Generic&lt;Number&gt;的子类。由此可以看出:同一种泛型可以对应多个版本（因为参数类型是不确定的），不同版本的泛型类实例是不兼容的。 回到上面的例子，如何解决上面的问题？总不能为了定义一个新的方法来处理Generic&lt;Integer&gt;类型的类，这显然与java中的多态理念相违背。因此我们需要一个在逻辑上可以表示同时是Generic&lt;Integer&gt;和Generic&lt;Number&gt;父类的引用类型。由此类型通配符应运而生。 我们可以将上面的方法改一下： 类型通配符一般是使用'?'代替具体的类型实参，注意，此处’?'是类型实参，而不是类型形参 。重要说三遍！此处'?'是类型实参，而不是类型形参 ！ 此处'?'是类型实参，而不是类型形参 ！再直白点的意思就是，此处的'?'和Number、String、Integer一样都是一种实际的类型，可以把'?'看成所有类型的父类。是一种真实的类型。 可以解决当具体类型不确定的时候，这个通配符就是'?'；当操作类型时，不需要使用类型的具体功能时，只使用Object类中的功能。那么可以用'?'通配符来表示未知类型。 5.4 泛型方法 泛型类，是在实例化类的时候指明泛型的具体类型；泛型方法，是在调用方法的时候指明泛型的具体类型 。 1Object obj = genericMethod(Class.forName("com.test.test")); 再对泛型方法进行一个比较，加深理解： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public class GenericTest &#123; //这个类是个泛型类，在上面已经介绍过 public class Generic&lt;T&gt;&#123; private T key; public Generic(T key) &#123; this.key = key; &#125; //我想说的其实是这个，虽然在方法中使用了泛型，但是这并不是一个泛型方法。 //这只是类中一个普通的成员方法，只不过他的返回值是在声明泛型类已经声明过的泛型。 //所以在这个方法中才可以继续使用 T 这个泛型。 public T getKey()&#123; return key; &#125; /** * 这个方法显然是有问题的，在编译器会给我们提示这样的错误信息"cannot reslove symbol E" * 因为在类的声明中并未声明泛型E，所以在使用E做形参和返回值类型时，编译器会无法识别。 public E setKey(E key)&#123; this.key = key； &#125; */ //必须要声明E才行 public &lt;E&gt; E setKey(E key)&#123; this.key = (T)key; return key; &#125; &#125; /** * 这才是一个真正的泛型方法。 * 首先在public与返回值之间的&lt;T&gt;必不可少，这表明这是一个泛型方法，并且声明了一个泛型T * 这个T可以出现在这个泛型方法的任意位置. * 泛型的数量也可以为任意多个 * 如：public &lt;T,K&gt; K showKeyName(Generic&lt;T&gt; container)&#123; * ... * &#125; */ public &lt;T&gt; T showKeyName(Generic&lt;T&gt; container)&#123; System.out.println("container key :" + container.getKey()); //当然这个例子举的不太合适，只是为了说明泛型方法的特性。 T test = container.getKey(); return test; &#125; //这也不是一个泛型方法，这就是一个普通的方法，只是使用了Generic&lt;Number&gt;这个泛型类做形参而已。 public void showKeyValue1(Generic&lt;Number&gt; obj)&#123; Log.d("泛型测试","key value is " + obj.getKey()); &#125; //这也不是一个泛型方法，这也是一个普通的方法，只不过使用了泛型通配符? //同时这也印证了泛型通配符章节所描述的，?是一种类型实参，可以看做为Number等所有类的父类 public void showKeyValue2(Generic&lt;?&gt; obj)&#123; Log.d("泛型测试","key value is " + obj.getKey()); &#125; /** * 这个方法是有问题的，编译器会为我们提示错误信息："UnKnown class 'E' " * 虽然我们声明了&lt;T&gt;,也表明了这是一个可以处理泛型的类型的泛型方法。 * 但是只声明了泛型类型T，并未声明泛型类型E，因此编译器并不知道该如何处理E这个类型。 public &lt;T&gt; T showKeyName(Generic&lt;E&gt; container)&#123; ... &#125; */ /** * 这个方法也是有问题的，编译器会为我们提示错误信息："UnKnown class 'T' " * 对于编译器来说T这个类型并未项目中声明过，因此编译也不知道该如何编译这个类。 * 所以这也不是一个正确的泛型方法声明。 public void showkey(T genericObj)&#123; &#125; */ //在泛型类中声明了一个泛型方法，使用泛型E，这种泛型E可以为任意类型。可以类型与T相同，也可以不同。 //由于泛型方法在声明的时候会声明泛型&lt;E&gt;，因此即使在泛型类中并未声明泛型，编译器也能够正确识别泛型方法中识别的泛型。 public &lt;E&gt; void show_3(E t)&#123; System.out.println(t.toString()); &#125; //在泛型类中声明了一个泛型方法，使用泛型T，注意这个T是一种全新的类型，可以与泛型类中声明的T不是同一种类型。 public &lt;T&gt; void show_2(T t)&#123; System.out.println(t.toString()); &#125; public static void main(String[] args) &#123; &#125;&#125; 5.5 泛型方法与可变参数 1printMsg("111",222,"aaaa","2323.4",55.55); 5.6 静态方法与泛型 如果静态方法要使用泛型的话，必须将静态方法也定义成泛型方法 。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之克隆]]></title>
    <url>%2F2019%2F02%2F19%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%85%8B%E9%9A%86%2F</url>
    <content type="text"><![CDATA[面试的时候可能会问到克隆相关的深拷贝和浅拷贝，至少我是被问过的，所以对它们的了解是必要的，本篇文章探讨Java克隆方面的知识。 1. Java中对象创建的两种方式 clone顾名思义就是复制， 在Java语言中， clone方法被对象调用，所以会复制对象。所谓的复制对象，首先要分配一个和源对象同样大小的空间，在这个空间中创建一个新的对象。那么在java语言中，有几种方式可以创建对象呢？ 使用new操作符创建一个对象 使用clone方法复制一个对象 那么这两种方式有什么相同和不同呢？ new操作符的本意是分配内存。程序执行到new操作符时， 首先去看new操作符后面的类型，因为知道了类型，才能知道要分配多大的内存空间。分配完内存之后，再调用构造函数，填充对象的各个域，这一步叫做对象的初始化，构造方法返回后，一个对象创建完毕，可以把他的引用（地址）发布到外部，在外部就可以使用这个引用操纵这个对象。 而clone在第一步是和new相似的， 都是分配内存，调用clone方法时，分配的内存和源对象（即调用clone方法的对象）相同，然后再使用原对象中对应的各个域，填充新对象的域， 填充完成之后，clone方法返回，一个新的相同的对象被创建，同样可以把这个新对象的引用发布到外部。 2. 复制对象 or 复制引用 在Java中，以下类似的代码非常常见： 当Person p1 = p;执行之后， 是创建了一个新的对象吗？ 首先看打印结果： 12com.pansoft.zhangjg.testclone.Person@2f9ee1accom.pansoft.zhangjg.testclone.Person@2f9ee1ac 可已看出，打印的地址值是相同的，既然地址都是相同的，那么肯定是同一个对象。p和p1只是引用而已，他们都指向了一个相同的对象Person(23, &quot;zhang&quot;) 。 可以把这种现象叫做引用的复制。 而下面的代码是真真正正的克隆了一个对象。 从打印结果可以看出，两个对象的地址是不同的，也就是说创建了新的对象， 而不是把原对象的地址赋给了一个新的引用变量： 12com.pansoft.zhangjg.testclone.Person@2f9ee1accom.pansoft.zhangjg.testclone.Person@67f1fba0 以上代码执行完成后， 内存中的情景如下图所示： 3. 深拷贝 or 浅拷贝 age是基本数据类型，那么对它的拷贝没有什么疑议，直接将一个4字节的整数值拷贝过来就行。 name是String类型的， 它只是一个引用， 指向一个真正的String对象，那么对它的拷贝有两种方式： 直接将源对象中的name的引用值拷贝给新对象的name字段 或者是根据原Person对象中的name指向的字符串对象创建一个新的相同的字符串对象，将这个新字符串对象的引用赋给新拷贝的Person对象的name字段。 这两种拷贝方式分别叫做浅拷贝和深拷贝。深拷贝和浅拷贝的原理如下图所示： 下面通过代码进行验证。如果两个Person对象的name的地址值相同， 说明两个对象的name都指向同一个String对象， 也就是浅拷贝， 而如果两个对象的name的地址值不同， 那么就说明指向不同的String对象， 也就是在拷贝Person对象的时候， 同时拷贝了name引用的String对象， 也就是深拷贝。验证代码如下： 覆盖Object中的clone方法， 实现深拷贝.假设body类里面组合了head类。 Body中组合了Head，重写了Body的clone方法，那么显然第一个输出为false；但是没有对Head进行重写clone方法，那么他们指向的是同一个内存空间。即，没有重写clone的Head类只是浅拷贝。 12body == body1 : falsebody.head == body1.head : true 如果要使Body对象在clone时进行深拷贝， 那么就要在Body的clone方法中，将源对象引用的Head对象也clone一份。 打印结果： 12body == body1 : falsebody.head == body1.head : false 由此，我们得到一个结论：如果想要深拷贝一个对象， 这个对象必须要实现Cloneable接口，实现clone方法，并且在clone方法内部，把该对象引用的其他对象也要clone一份 ， 这就要求这个被引用的对象必须也要实现Cloneable接口并且实现clone方法。 那么，按照上面的结论， Body类组合了Head类， 而Head类组合了Face类，要想深拷贝Body类，必须在Body类的clone方法中将Head类也要拷贝一份，但是在拷贝Head类时，默认执行的是浅拷贝，也就是说Head中组合的Face对象并不会被拷贝。验证代码如下： 输出结果符合预期： 123body == body1 : falsebody.head == body1.head : falsebody.head.face == body1.head.face : true 内存结构图如下图所示： 那么此时Head中组合的Face又是一个浅拷贝。那么到底如何实现彻底的深拷贝呢？ 对于上面的例子来说，怎样才能保证两个Body对象完全独立呢？只要在拷贝Head对象的时候，也将Face对象拷贝一份就可以了。这需要让Face类也实现Cloneable接口，实现clone方法，并且在在Head对象的clone方法中，拷贝它所引用的Face对象。修改的部分代码如下： 再次运行上面的示例，得到的运行结果如下： 123body == body1 : falsebody.head == body1.head : falsebody.head.face == body1.head.face : false 这说名两个Body已经完全独立了，他们间接引用的face对象已经被拷贝，也就是引用了独立的Face对象。内存结构图如下： 显然，对于复杂的对象而言，用这种方式实现深拷贝是十分困难的。这时我们可以用序列化的方式来实现对象的深克隆。 4. 序列化解决多层克隆问题 首先由一个外部类Outer： 再来一个被序列化的类Inner: 再对克隆的对象进行测试： 运行结果： 1234falsefalseouter的name值为：outerInner的name值为：inner 参考： Java提高篇——对象克隆（复制） 详解Java中的clone方法 – 原型模式]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之异常]]></title>
    <url>%2F2019%2F02%2F19%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[在开发中，异常处理是一个不可绕开的话题，我们对于异常的处理已经非常熟练了，对于异常本身的概念、用法等不再赘述了，直接结合面试问题来加深对异常的理解吧。 Throwable 可以用来表示任何可以作为异常抛出的类，分为两种： Error 和 Exception。 1. 什么是Java异常 异常是发生在程序执行过程中阻碍程序正常执行的错误事件。比如：用户输入错误数据、硬件故障、网络阻塞等都会导致出现异常。 只要在Java语句执行中产生了异常，一个异常对象就会被创建，JRE就会试图寻找异常处理程序来处理异常。如果有合适的异常处理程序，异常对象就会被异常处理程序接管，否则，将引发运行环境异常，JRE终止程序执行。 Java异常处理框架只能处理运行时错误，编译错误不在其考虑范围之内。 2. Error和Exception的区别 Error：是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM出现的问题。 例如，Java虚拟机运行错误，当 JVM 不再有继续执行操作所需的内存资源时，将出现OutOfMemoryError。这些异常发生时，Java虚拟机一般会选择线程终止。 3. Java异常处理中有哪些关键字？ throw:有时我们需要显式地创建并抛出异常对象来终止程序的正常执行。throw关键字用来抛出并处理运行时异常。 throws:当我们抛出任何“被检查的异常(checked exception)”并不处理时，需要在方法签名中使用关键字throws来告知调用程序此方法可能会抛出的异常。调用方法可能会处理这些异常，或者同样用throws来将异常传给上一级调用方法。throws关键字后可接多个潜在异常，甚至是在main()中也可以使用throws。 try-catch:我们在代码中用try-catch块处理异常。当然，一个try块之后可以有多个catch子句，try-catch块也能嵌套。每个catch块必须接受一个（且仅有一个）代表异常类型的参数。 finally:finally块是可选的，并且只能配合try-catch一起使用。虽然异常终止了程序的执行，但是还有一些打开的资源没有被关闭，因此，我们能使用finally进行关闭。不管异常有没有出现，finally块总会被执行。 4. 描述一下异常的层级 Throwable是所有异常的父类，它有两个直接子对象Error,Exception，其中Exception又被继续划分为“被检查的异常(checked exception)”和”运行时的异常（runtime exception,即不受检查的异常）”。 Error表示编译时和系统错误，通常不能预期和恢复，比如硬件故障、JVM崩溃、内存不足等。 被检查的异常（Checked exception）在程序中能预期，并要尝试修复，如FileNotFoundException。我们必须捕获此类异常，并为用户提供有用信息和合适日志来进行调试。Exception是所有被检查的异常的父类。 运行时异常（Runtime Exception）又称为不受检查异常，源于糟糕的编程。比如我们检索数组元素之前必须确认数组的长度，否则就可能会抛出ArrayIndexOutOfBoundException运行时异常。RuntimeException是所有运行时异常的父类。 5. 描述Java 7 ARM(Automatic Resource Management，自动资源管理)特征和多个catch块的使用 如果一个try块中有多个异常要被捕获，catch块中的代码会变丑陋的同时还要用多余的代码来记录异常。有鉴于此，Java 7的一个新特征是：一个catch子句中可以捕获多个异常。示例代码如下： 1234catch(IOException | SQLException | Exception ex)&#123; logger.error(ex); throw new MyException(ex.getMessage());&#125; 大多数情况下，当忘记关闭资源或因资源耗尽出现运行时异常时，我们只是用finally子句来关闭资源。这些异常很难调试，我们需要深入到资源使用的每一步来确定是否已关闭。因此，Java 7用try-with-resources进行了改进：在try子句中能创建一个资源对象，当程序的执行完try-catch之后，运行环境自动关闭资源。 利用Try-Catch-Finally管理资源（旧的代码风格）： 1234567891011121314151617private static void printFile() throws IOException &#123; InputStream input = null; try &#123; input = new FileInputStream("file.txt");//可能发生异常1 int data = input.read();//可能发生异常2 while(data != -1)&#123; System.out.print((char) data); data = input.read(); &#125; &#125; finally &#123; if(input != null)&#123; input.close();//可能发生异常3 &#125; &#125;&#125; 假设try语句块抛出一个异常，然后finally语句块被执行。同样假设finally语句块也抛出了一个异常。那么哪个异常会根据调用栈往外传播？ 即使try语句块中抛出的异常与异常传播更相关，最终还是finally语句块中抛出的异常会根据调用栈向外传播。 在java7中，对于上面的例子可以用try-with-resource 结构这样写： 1234567891011private static void printFileJava7() throws IOException &#123; try(FileInputStream input = new FileInputStream("file.txt")) &#123; int data = input.read(); while(data != -1)&#123; System.out.print((char) data); data = input.read(); &#125; &#125;&#125; 当try语句块运行结束时，FileInputStream 会被自动关闭。这是因为FileInputStream 实现了java中的java.lang.AutoCloseable接口。所有实现了这个接口的类都可以在try-with-resources结构中使用。 当try-with-resources结构中抛出一个异常，同时FileInputStream被关闭时（调用了其close方法）也抛出一个异常，try-with-resources结构中抛出的异常会向外传播，而FileInputStream被关闭时抛出的异常被抑制了。 你可以在块中使用多个资源而且这些资源都能被自动地关闭。下面是例子： 12345678910111213private static void printFileJava7() throws IOException &#123; try( FileInputStream input = new FileInputStream("file.txt"); BufferedInputStream bufferedInput = new BufferedInputStream(input) ) &#123; int data = bufferedInput.read(); while(data != -1)&#123; System.out.print((char) data); data = bufferedInput.read(); &#125; &#125;&#125; 这些资源将按照他们被创建顺序的逆序来关闭。首先BufferedInputStream 会被关闭，然后FileInputStream会被关闭。 这个try-with-resources结构里不仅能够操作java内置的类。你也可以在自己的类中实现java.lang.AutoCloseable接口，然后在try-with-resources结构里使用这个类。 AutoClosable 接口仅仅有一个方法，接口定义如下： 1234public interface AutoClosable &#123; public void close() throws Exception;&#125; 任何实现了这个接口的方法都可以在try-with-resources结构中使用。下面是一个简单的例子： 1234567891011public class MyAutoClosable implements AutoCloseable &#123; public void doIt() &#123; System.out.println("MyAutoClosable doing it!"); &#125; @Override public void close() throws Exception &#123; System.out.println("MyAutoClosable closed!"); &#125;&#125; doIt()是方法不是AutoClosable 接口中的一部分，之所以实现这个方法是因为我们想要这个类除了关闭方法外还能做点其他事。 下面是MyAutoClosable 在try-with-resources结构中使用的例子： 123456private static void myAutoClosable() throws Exception &#123; try(MyAutoClosable myAutoClosable = new MyAutoClosable())&#123; myAutoClosable.doIt(); &#125;&#125; 运行结果： 12MyAutoClosable doing it!MyAutoClosable closed! 通过上面这些你可以看到，不论try-catch中使用的资源是自己创造的还是java内置的类型，try-with-resources都是一个能够确保资源能被正确地关闭的强大方法。 6. 在Java中throw与throws关键字之间的区别？ throws用于在方法签名中声明此方法可能抛出的异常，而throw关键字则是中断程序的执行并移交异常对象到运行时进行处理。 7. 在Java中怎么写自定义的异常？ 我们能继承Exception类或其任何子类来实现自己的自定义异常类。这自定义异常类可以有自己变量和方法来传递错误代码或其它异常相关信息来处理异常。 1234567891011121314@Datapublic class HappyBikeException extends RuntimeException&#123; private Integer code = ResponseEnum.ERROR.getCode(); public HappyBikeException(Integer code,String msg)&#123; super(msg); this.code = code; &#125; public HappyBikeException(String msg)&#123; super(msg); &#125;&#125; 8. Java中final,finally,finalize的区别？ 这是一个垃圾问题，很想删除掉，但是考虑到新手，还是保留一下吧，至少从单词上有那么一点点像。 final和finally在Java中是关键字，而finalize则是一个方法。 final关键字使得类变量不可变，避免类被其它类继承或方法被重写。finally跟try-catch块一起使用，即使是出现了异常，其子句总会被执行，通常，finally子句用来关闭相关资源。finally方法中的对象被销毁之前会被垃圾回收。 9. 在main方法抛出异常时发生了什么？ 答：当main方法抛出异常时，Java运行时间终止并在控制台打印异常信息和栈轨迹。 10. catch子句能为空吗？ catch后面括号里面不能为空。 答：可以有空的catch子句，但那是最糟糕的编程，因为那样的话，异常即使被捕获，我们也得不到任何的有用信息，对于调试来说会是个噩梦，因此，编程时永远不要有空的catch子句。Catch子句中至少要包含一个日志语句输出到控制台或保存到日志文件中。 11. 提供一些Java异常处理的最佳实践。 使用具体的异常方便调试 程序中早点抛出异常 捕获异常后让调用者处理异常 使用Java 7 ARM功能确保资源关闭或者用finally子句正确地关闭它们 为了调试需要总是记录异常信息 用多个catch子句实现更完全的关闭 你自己的应用API中用自定义的异常来抛出单种类型异常 遵循命名规定，以异常结束 在Javadoc中用@throws来标注方法抛出的异常 处理异常是有花销的，因此只有在必要时才抛出。否则，你会扑空或毫无收获。 12. try、catch、finally三个语句块应注意的问题 try、catch、finally三个语句块均不能单独使用，三者可以组成 try…catch…finally、try…catch、try…finally三种结构，catch语句可以有一个或多个，finally语句最多一个。 try、catch、finally三个代码块中变量的作用域为代码块内部，分别独立而不能相互访问。如果要在三个块中都可以访问，则需要将变量定义到这些块的外面。 多个catch块时候，只会匹配其中一个异常类并执行catch块代码，而不会再执行别的catch块，并且匹配catch语句的顺序是由上到下。 无论程序是否有异常，并且无论之间try-catch是否顺利执行完毕，都会执行finally语句。在以下特殊情况下，finally块不会执行：在finally语句块中发生异常；在前面代码中使用了System.exit()退出程序；程序所在线程死亡；关闭cpu。 ⭐当程序执行try块，catch块时遇到return语句或者throw语句，这两个语句都会导致该方法立即结束，所以系统并不会立即执行这两个语句，而是去寻找该异常处理流程中的finally块，如果没有finally块，程序立即执行return语句或者throw语句，方法终止。如果有finally块，系统立即开始执行finally块，只有当finally块执行完成后，系统才会再次跳回来执行try块、catch块里的return或throw语句，如果finally块里也使用了return或throw等导致方法终止的语句，则finally块已经终止了方法，不用再跳回去执行try块、catch块里的任何代码了。 13. 解释Java中的异常处理流程 异常处理完成以后，Exception对象会发生什么变化？ Exception对象会在下一个垃圾回收过程中被回收掉。 请写出 5 种常见到的runtime exception。 NullPointerException：当操作一个空引用时会出现此错误。 NumberFormatException：数据格式转换出现问题时出现此异常。 ClassCastException：强制类型转换类型不匹配时出现此异常。 ArrayIndexOutOfBoundsException：数组下标越界，当使用一个不存在的数组下标时出现此异常。 ArithmeticException：数学运行错误时出现此异常 参考： http://www.importnew.com/7383.html http://www.importnew.com/7541.html http://www.importnew.com/7820.html]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己实现一个简单的web服务器]]></title>
    <url>%2F2019%2F02%2F18%2Fthread%2F%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84web%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这里涉及网络编程的基本知识以及HTTP协议的基本认识，下面来一步一步实现一下最简单的一个web服务器。 一、用户请求域名到底发送了什么信息来 如果服务端都不知道客户端发来的是什么，那何谈对内容的解析呢？所以我们首先在服务端解析客户端的访问信息，比如我们比较关心的是请求的是什么路径： 此时我们访问地址： localhost:8888 打印出来的结果为： 其实对于我们这种比较简单的实现来说，红框的信息已经足够了。我们只要知道客户端要发来的资源名字是什么，我们根据这个名字取找响应的资源返回给客户端展示即可。由于我其实请求的是根路径，所以是/。如果我在这里请求 localhost:8888/index.html 那么就会显示 GET /index.html HTTP/1.1这样的信息。不再演示。 但是上面的写法是存在问题的，仅仅是演示而用，因为它会一直等待输入。 那么，不用while一直等待，而且只需要第一行即可，那么可以这样写： 好了，不能总之只接收吧，我们服务端要给客户端点什么。 二、服务端如何响应资源给客户端展示 首先我们得有资源才能展示，假设我们要展示index.html，我们将其暂时放在F:/webrooot下。 内容暂时为： 只是展示一下欢迎信息而已。服务端就需要读取这个文件，然后以流的形式发送给客户端的浏览器上，浏览器再解析展示。 此时再去访问页面，就会显示欢迎的信息啦！ 这里没有关闭流，也没有关闭socket，不过下面都会关闭掉。这个不重要，重要的是，这玩意都是在主线程中做的，显然不如多线程来的快，并且全写在主线程里，肯定是不够好的。下面就用多线程来实现。 三、普通的多线程实现方式 就是在这个线程中处理数据和返回数据。 其很简单，但是如果我想显示一张图片呢？ 比如我的根目录下有一张图片叫做：hh.jpg 很显然现在还是无法展示的，原因是我这里写死了是以text/html的形式响应，但是图片正确的响应是image/jpeg这种格式。 所以我们不能无脑写死，要进行适当的判断才行。下面贴出代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class ServerThread implements Runnable&#123; //根目录 private static final String webroot = "F:\\webroot\\"; private Socket client; InputStream ins; OutputStream out; //存放类型，比如jpg对应的是image/jpeg，这是http协议规定的每种类型的响应格式 private static Map&lt;String,String&gt; contentMap = new HashMap&lt;&gt;(); static &#123; contentMap.put("html","text/html;charset=utf-8"); contentMap.put("jpg","image/jpeg"); contentMap.put("png","image/jpeg"); &#125; public ServerThread(Socket client)&#123; this.client = client; init(); &#125; private void init() &#123; try &#123; ins = client.getInputStream(); out = client.getOutputStream(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; try &#123; go(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private void go() throws IOException &#123; //获取请求内容 BufferedReader reader = new BufferedReader(new InputStreamReader(ins)); //获取到请求的资源名称 String line = reader.readLine().split(" ")[1].replace("/","\\"); if(line.equals("\\"))&#123; line += "index.html"; &#125; System.out.println(line); //拼接起来就是资源的完整路径 File file = new File(webroot + line); //判断是否存在，存在则响应内容，不存在则告知不存在 if (file.exists()) &#123; //给用户响应 PrintWriter pw = new PrintWriter(out); InputStream i = new FileInputStream(webroot + line); //由于需要将图片也要传给前端，再用这个就不好办了，得用普通的文件输入流// BufferedReader fr = new BufferedReader(new InputStreamReader(i)); pw.println("HTTP/1.1 200 OK"); //返回的类型是动态判断的，图片用图片的类型，文本用文本的类型 String s = contentMap.get(line.substring(line.lastIndexOf(".")+1,line.length())); System.out.println("返回的类型为："+ s); pw.println("Content-Type: " + s); pw.println("Content-Length:" + i.available()); pw.println("Server: hello-server"); pw.println("Date:"+ new Date()); pw.println(""); pw.flush(); //写入输出流中通过PrintWriter发给浏览器 byte[] buff = new byte[1024]; int len = 0; while ( (len = i.read(buff)) != -1)&#123; out.write(buff,0,len); &#125; pw.flush(); pw.close(); i.close(); reader.close(); client.close(); &#125;else &#123; StringBuffer error = new StringBuffer(); error.append("HTTP /1.1 400 file not found /r/n"); error.append("Content-Type:text/html \r\n"); error.append("Content-Length:20 \r\n").append("\r\n"); error.append("&lt;h1 &gt;File Not Found..&lt;/h1&gt;"); try &#123; out.write(error.toString().getBytes()); out.flush(); out.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; ok,你会发现，我还添加了判断资源存在不存在的逻辑，这样显得更加健全一点。 当找得到资源得时候： 当找不到资源得时候： 四、线程池方式 其实很简单，线程池的优势以前也说过，不赘述，直接贴一下代码结束。 至此，我们完成了一个比较简单的web服务器的开发。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之JDK动态代理]]></title>
    <url>%2F2019%2F02%2F17%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8BJDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[代理模式可以说是经常面试被问的一个东西，因为spring aop的实现原理就是基于它，关于它，只要记住，它是运行时动态生成的一个代理类。在这个基础上，再去看看它底层源码，其实JDK已经帮我们最大程度上封装成简单的函数了，我们只需要传入几个参数就可以生成对应的代理对象。 代理模式是什么 定义：给某个对象提供一个代理对象，并由代理对象控制对于原对象的访问，即客户不直接操控原对象，而是通过代理对象间接地操控原对象。 RealSubject 是原对象（本文把原对象称为&quot;委托对象&quot;），Proxy 是代理对象。 Subject 是委托对象和代理对象都共同实现的接口。 Request() 是委托对象和代理对象共同拥有的方法。 结合生活理解代理模式 要理解代理模式很简单，其实生活当中就存在代理模式： 我们购买火车票可以去火车站买，但是也可以去火车票代售处买，此处的火车票代售处就是火车站购票的代理，即我们在代售点发出买票请求，代售点会把请求发给火车站，火车站把购买成功响应发给代售点，代售点再告诉你。 但是代售点只能买票，不能退票，而火车站能买票也能退票，因此代理对象支持的操作可能和委托对象的操作有所不同。 Java实现静态代理示例 代理的实现分为： 静态代理 代理类是在编译时就实现好的。也就是说 Java 编译完成后代理类是一个实际的 class 文件。 动态代理 代理类是在运行时生成的。也就是说 Java 编译完之后并没有实际的 class 文件，而是在运行时动态生成的类字节码，并加载到JVM中。 Java 实现动态代理 几个重要名词: 委托类和委托对象：委托类是一个类，委托对象是委托类的实例，即原类。 代理类和代理对象：代理类是一个类，代理对象是代理类的实例。 Java实现动态代理的大致步骤如下: 定义一个委托类和公共接口。 自己定义一个类（调用处理器类，即实现 InvocationHandler 接口），这个类的目的是指定运行时将生成的代理类需要完成的具体任务（包括Preprocess和Postprocess），即代理类调用任何方法都会经过这个调用处理器类（在本文最后一节对此进行解释）。 生成代理对象（当然也会生成代理类），需要为他指定(1)类加载器对象(2)实现的一系列接口(3)调用处理器类的实例。因此可以看出一个代理对象对应一个委托对象，对应一个调用处理器实例。 Java 实现动态代理主要涉及以下几个类: java.lang.reflect.Proxy: 这是生成代理类的主类，通过 Proxy 类生成的代理类都继承了 Proxy 类，即 DynamicProxyClass extends Proxy。 java.lang.reflect.InvocationHandler: 这里称他为&quot;调用处理器&quot;，他是一个接口，我们动态生成的代理类需要完成的具体内容需要自己定义一个类，而这个类必须实现 InvocationHandler 接口。 Proxy 类主要方法为： 这个静态函数的第一个参数是类加载器对象（即哪个类加载器来加载这个代理类到 JVM 的方法区） 第二个参数是接口（表明你这个代理类需要实现哪些接口） 第三个参数是调用处理器类实例（指定代理类中具体要干什么）。 这个函数是 JDK 为了程序员方便创建代理对象而封装的一个函数，因此你调用newProxyInstance()时直接创建了代理对象（略去了创建代理类的代码）。其实他主要完成了以下几个工作： Proxy 类还有一些静态方法，比如： InvocationHandler getInvocationHandler(Object proxy): 获得代理对象对应的调用处理器对象。 Class getProxyClass(ClassLoader loader, Class[] interfaces): 根据类加载器和实现的接口获得代理类。 Proxy 类中有一个映射表，映射关系为：(&lt;ClassLoader&gt;,(&lt;Interfaces&gt;,&lt;ProxyClass&gt;) )，可以看出一级key为类加载器，根据这个一级key获得二级映射表，二级key为接口数组，因此可以看出：一个类加载器对象和一个接口数组确定了一个代理类。 我们写一个简单的例子来阐述 Java 实现动态代理的整个过程： InvocationHandler 接口中有方法：invoke(Object proxy, Method method, Object[] args) 这个函数是在代理对象调用任何一个方法时都会调用的，方法不同会导致第二个参数method不同，第一个参数是代理对象（表示哪个代理对象调用了method方法，传递来的是），第二个参数是 Method 对象（表示哪个方法被调用了），第三个参数是指定调用方法的参数。 动态生成的代理类具有几个特点： 继承 Proxy 类，并实现了在Proxy.newProxyInstance()中提供的接口数组。 public final。 命名方式为 $ProxyN，其中N会慢慢增加，一开始是 $Proxy1，接下来是$Proxy2… 有一个参数为 InvocationHandler 的构造函数。这个从 Proxy.newProxyInstance() 函数内部的clazz.getConstructor(new Class[] { InvocationHandler.class }) 可以看出。 Java 实现动态代理的缺点：因为 Java 的单继承特性（每个代理类都继承了 Proxy 类），只能针对接口创建代理类，不能针对类创建代理类。 不难发现，代理类的实现是有很多共性的（重复代码），动态代理的好处在于避免了这些重复代码，只需要关注操作。 小栗子 假设模拟一个场景，买衣服，正常情况所有人买这件衣服要100块钱。 定义一个销售接口： 一个具体的实现类： 那么正常情况大家都要花100才能买这件衣服。但是现在对会员做活动，会员打5折。怎么做呢？正常思维是：增加一个接口，甚至更糟的想法是修改一下这个实现类，都是不好的，那么我们是否想过这样的方案：新建一个新的类，让这个代理类去做相应的逻辑呢？既不用修改原来的代码，而且还很简单就能实现。 现在写一个代理类： 那么调用的时候，一个是会员，一个是普通用户，根据身份调不同的方法即可： Java 动态代理的内部实现 现在我们就会有一个问题： Java 是怎么保证代理对象调用的任何方法都会调用 InvocationHandler 的 invoke() 方法的？ 这就涉及到动态代理的内部实现。假设有一个接口 Subject，且里面有 int request(int i) 方法，则生成的代理类大致如下： 通过上面的方法就成功调用了 invoke() 方法，所以这是代理类中已经注定要去执行 invoke() 方法了。 有一篇文章比较生动地阐述了动态代理的含义：Java帝国之动态代理]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础之注解]]></title>
    <url>%2F2019%2F02%2F17%2Fjava-basic%2Fjava%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解是一系列元数据，它提供数据用来解释程序代码，注释是给人看的，注解是给编译器看的，因此注解只在编译器有效。注解的实现原理涉及反射和动态代理，关于反射已经在前面说过，动态代理还没说，留在下一节。 注解语法 相信有不少的人员会认为注解的地位不高。其实同 classs 和 interface 一样，注解也属于一种类型。它是在 Java SE 5.0 版本中开始引入的概念。 注解的定义 注解通过 @interface 关键字进行定义。 1public @interface TestAnnotation &#123;&#125; 你可以简单理解为创建了一张名字为 TestAnnotation 的标签。 注解的使用 上面创建了一个注解，那么注解的的使用方法是什么呢。 12@TestAnnotationpublic class Test &#123;&#125; 不过，要想注解能够正常工作，还需要介绍一下一个新的概念那就是元注解。 什么是元注解 元注解是可以注解到注解上的注解，或者说元注解是一种基本注解，但是它能够应用到其它的注解上面。 元标签有 @Retention、@Documented、@Target、@Inherited、@Repeatable 5 种。 @Retention Retention 的英文意为保留期的意思。当 @Retention 应用到一个注解上的时候，它解释说明了这个注解的的存活时间。 RetentionPolicy.SOURCE 注解只在源码阶段保留，在编译器进行编译时它将被丢弃忽视。 RetentionPolicy.CLASS 注解只被保留到编译进行的时候，它并不会被加载到 JVM 中。 RetentionPolicy.RUNTIME 注解可以保留到程序运行的时候，它会被加载进入到 JVM 中，所以在程序运行时可以获取到它们。 12@Retention(RetentionPolicy.RUNTIME)public @interface TestAnnotation &#123;&#125; @Documented 顾名思义，这个元注解肯定是和文档有关。它的作用是能够将注解中的元素包含到 Javadoc 中去。 @Target Target 是目标的意思，@Target 指定了注解运用的地方。 ElementType.ANNOTATION_TYPE 可以给一个注解进行注解 ElementType.CONSTRUCTOR 可以给构造方法进行注解 ElementType.FIELD 可以给属性进行注解 ElementType.LOCAL_VARIABLE 可以给局部变量进行注解 ElementType.METHOD 可以给方法进行注解 ElementType.PACKAGE 可以给一个包进行注解 ElementType.PARAMETER 可以给一个方法内的参数进行注解 ElementType.TYPE 可以给一个类型进行注解，比如类、接口、枚举 @Inherited Inherited 是继承的意思，但是它并不是说注解本身可以继承，而是说如果一个超类被 @Inherited 注解过的注解进行注解的话，那么如果它的子类没有被任何注解应用的话，那么这个子类就继承了超类的注解。 1234567891011//定义一个被@Inherited注解的注解@Test@Inherited@Retention(RetentionPolicy.RUNTIME)@interface Test &#123;&#125;//父类被@Test注解，即上面说的被@Inherited 注解过的注解进行注解@Testpublic class A &#123;&#125;//那么class B也拥有@Test注解public class B extends A &#123;&#125; @Repeatable Repeatable 自然是可重复的意思。@Repeatable 是 Java 1.8 才加进来的，所以算是一个新的特性。 什么样的注解会多次应用呢？通常是注解的值可以同时取多个。 举个例子，一个人他既是程序员又是产品经理,同时他还是个画家。 12345678910111213141516171819202122232425262728//按照规定，它里面必须要有一个 value 的属性//属性类型是一个被 @Repeatable 注解过的注解数组，注意它是数组。@interface Persons &#123; Person[] value();&#125;//@Repeatable 后面括号中的类相当于一个容器注解//什么是容器注解呢？就是用来存放其它注解的地方。它本身也是一个注解。@Repeatable(Persons.class)@interface Person&#123; String role() default "";&#125;//有了上面两个注解，Persons相当于一个总标签//他里面可以放任意多个子标签，这些子标签类型是Person//并且是存放于这个总标签的Person类型的数组中。//那么既然有了总标签和放子标签的数组，那么，下面就可以定义子标签//子标签的类型自然就是Person，里面这里假设定义role属性//就是说这些子标签表示人的角色。//自然也就支持多种角色，那么定义多次即可。如下@Person(role="artist")@Person(role="coder")@Person(role="PM")public class SuperMan&#123;&#125; 注解的属性 123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface TestAnnotation &#123; public int id() default -1; public String msg() default "Hi";&#125; 使用： 12345//有默认值的时候，可以不执行属性@TestAnnotation(id=3,msg="hello annotation")public class Test &#123;&#125; 有一些规则： 修饰符只能是public 或默认(default) 参数成员只能用基本类型byte,short,int,long,float,double,boolean,char八种基本类型和String,Enum,Class,annotations及这些类型的数组 如果只有一个参数成员,最好将名称设为”value” 注解元素必须有确定的值,可以在注解中定义默认值,也可以使用注解时指定,非基本类型的值不可为null,常使用空字符串或0作默认值 在表现一个元素存在或缺失的状态时,定义一下特殊值来表示,如空字符串或负值 Java 预置的注解 @Deprecated 这个元素是用来标记过时的元素，想必大家在日常开发中经常碰到。编译器在编译阶段遇到这个注解时会发出提醒警告，告诉开发者正在调用一个过时的元素比如过时的方法、过时的类、过时的成员变量。 @Override 这个大家应该很熟悉了，提示子类要复写父类中被 @Override 修饰的方法 @SuppressWarnings 阻止警告的意思。之前说过调用被 @Deprecated 注解的方法后，编译器会警告提醒，而有时候开发者会忽略这种警告，他们可以在调用的地方通过 @SuppressWarnings 达到目的。 @SafeVarargs 参数安全类型注解。它的目的是提醒开发者不要用参数做一些不安全的操作,它的存在会阻止编译器产生 unchecked 这样的警告。 @FunctionalInterface 函数式接口注解，这个是 Java 1.8 版本引入的新特性。函数式编程很火，所以 Java 8 也及时添加了这个特性。 函数式接口 (Functional Interface) 就是一个具有一个方法的普通接口。 123456789101112131415@FunctionalInterfacepublic interface Runnable &#123; /** * When an object implementing interface &lt;code&gt;Runnable&lt;/code&gt; is used * to create a thread, starting the thread causes the object's * &lt;code&gt;run&lt;/code&gt; method to be called in that separately executing * thread. * &lt;p&gt; * The general contract of the method &lt;code&gt;run&lt;/code&gt; is that it may * take any action whatsoever. * * @see java.lang.Thread#run() */ public abstract void run();&#125; 我们进行线程开发中常用的 Runnable 就是一个典型的函数式接口，上面源码可以看到它就被 @FunctionalInterface 注解。 可能有人会疑惑，函数式接口标记有什么用，这个原因是函数式接口可以很容易转换为 Lambda 表达式。 注解与反射 注解通过反射获取。首先可以通过 Class 对象的 isAnnotationPresent() 方法判断它是否应用了某个注解 1public boolean isAnnotationPresent(Class&lt;? extends Annotation&gt; annotationClass) &#123;&#125; 然后通过 getAnnotation() 方法来获取 Annotation 对象。 1public &lt;A extends Annotation&gt; A getAnnotation(Class&lt;A&gt; annotationClass) &#123;&#125; 或者是 getAnnotations() 方法。 1public Annotation[] getAnnotations() &#123;&#125; 这里举个例子： 首先定义一个注解： 123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface TestAnnotation &#123; public int id() default -1; public String msg() default "Hi";&#125; 然后定义一个类，打上这个注解： 123@TestAnnotationpublic class Test &#123;&#125; 最后再main函数中拿到注解： 1234567891011121314public class Main4 &#123; public static void main(String[] args) &#123; //判断Test.class中是否存在TestAnnotation注解 boolean hasAnnotation = Test.class.isAnnotationPresent(TestAnnotation.class); if(hasAnnotation)&#123; System.out.println("注解存在..."); //从Test类中拿出TestAnnotation注解 TestAnnotation annotation = Test.class.getAnnotation(TestAnnotation.class); //拿到注解之后，可以拿出注解中的属性对应的默认值 System.out.println(annotation.id()); System.out.println(annotation.msg()); &#125; &#125;&#125; 上面演示的是从类上拿到注解，对于属性、方法同样都可以用反射拿到注解。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Test &#123; @Check(value="hi") int a; @Perform public void testMethod()&#123;&#125; public static void main(String[] args) &#123; try &#123;/*************拿到属性上的注解****************/ Field a = Test.class.getDeclaredField("a"); a.setAccessible(true); //获取一个成员变量上的注解 Check check = a.getAnnotation(Check.class); if ( check != null ) &#123; System.out.println("check value:"+check.value()); &#125;/*************拿到方法上的注解****************/ Method testMethod = Test.class.getDeclaredMethod("testMethod"); if ( testMethod != null ) &#123; // 获取方法中的注解 Annotation[] ans = testMethod.getAnnotations(); for( int i = 0;i &lt; ans.length;i++) &#123; System.out.println("method testMethod annotation:"+ans[i].annotationType().getSimpleName()); &#125; &#125; &#125; catch (NoSuchFieldException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.out.println(e.getMessage()); &#125; catch (SecurityException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.out.println(e.getMessage()); &#125; catch (NoSuchMethodException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); System.out.println(e.getMessage()); &#125; &#125;&#125; 注解实现原理 在上面获取注解时是这样写的： 1TestAnnotation annotation = Test.class.getAnnotation(TestAnnotation.class); 它是从class中获取出TestAnnotation注解的，所以肯定是在某个时候注解被加入到class结构中去了。 首先，我们知道从java源码到class字节码是由编译器完成的，编译器会对java源码进行解析并生成class文件。 而注解也是在编译时由编译器进行处理，编译器会对注解符号处理并附加到class结构中 根据jvm规范，class文件结构是严格有序的格式，唯一可以附加信息到class结构中的方式就是保存到class结构的attributes属性中 我们知道对于类、字段、方法，在class结构中都有自己特定的表结构，而且各自都有自己的属性，而对于注解，作用的范围也可以不同，可以作用在类上，也可以作用在字段或方法上，这时编译器会对应将注解信息存放到类、字段、方法自己的属性上。 在我们的Test类被编译后，在对应的Test.class文件中会包含一个RuntimeVisibleAnnotations属性，由于这个注解是作用在类上，所以此属性被添加到类的属性集上。即TestAnnotation注解的键值对value=test会被记录起来。 而当JVM加载Test.class文件字节码时，就会将RuntimeVisibleAnnotations属性值保存到Test的Class对象中，于是就可以通过Test.class.getAnnotation(TestAnnotation.class)获取到Test注解对象，进而再通过Test注解对象获取到Test里面的属性值。 Test注解对象是什么？其实注解被编译后的本质就是一个继承Annotation接口的接口。所以@TestAnnotation其实就是“public interface TestAnnotation extends Annotation” 当我们通过Test.class.getAnnotation(TestAnnotation.class)调用时，JDK会通过动态代理生成一个实现了TestAnnotation接口的对象，并把将RuntimeVisibleAnnotations属性值设置进此对象中，此对象即为TestAnnotation注解对象，通过它的value()方法就可以获取到注解值。 总结注解到底是什么以及注解到底有什么应用场景 注释是给人看的，注解是给编译器看的，以@Override注解为例，他的作用是告诉编译器他所注解的方法是重写父类中的方法，这样编译器就会去检查父类是否存在这个方法，以及这个方法的签名与父类是否相同。 注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用； 也就是说，注解只是描述java代码的代码，能被编译器解析，只有编译器或者虚拟机来主动解析他的时候，他才可能发挥作用。 注解分为三类，元注解，java自带的标准注解以及自定义注解。 注解的使用场景： 生成文档，通过代码里标识的元数据生成javadoc文档。 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理，例如使用反射注入实例。 我觉得这些说的太空洞了，注解在spring中就是非常常用的技术，比如，我指定这个类是@Controller或者@Service之类，那么我配置包扫描将其类路径全部扫描到后，启动容器的时候，这些类就会自动被spring所管理，即自动向spring注册，以后要注入这些组件的时候，就直接从spring的IOC容器中取出来即可。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础之冰川表面]]></title>
    <url>%2F2019%2F02%2F17%2Fjava-basic%2FJava%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%86%B0%E5%B7%9D%E8%A1%A8%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[本篇文章是罗列一些关于java基础的核心点，这些点是基础中的基础，也是重点中的重点。为什么本篇文章叫做冰川表面呢？我想表达的意思是，这些基础只是一个引子，背后牵扯出来的东西可能会很多，面试往往都是从基础的知识点慢慢深入挖掘的，所以千万不能忽视对于它们的复习。 一、关键字 final 1. 数据 声明数据为常量，可以是编译时常量，也可以是在运行时被初始化后不能被改变的常量。 对于基本类型，final 使数值不变； 对于引用类型，final 使引用不变，也就不能引用其它对象，但是被引用的对象本身是可以修改的。 1234final int x = 1;// x = 2; // cannot assign value to final variable 'x'final A y = new A();y.a = 1; 2. 方法 声明方法不能被子类覆盖。 private 方法隐式地被指定为 final，如果在子类中定义的方法和基类中的一个 private 方法签名相同，此时子类的方法不是覆盖基类方法，而是在子类中定义了一个新的方法。 3. 类 声明类不允许被继承。 static 1. 静态变量 静态变量在内存中只存在一份，只在类初始化时赋值一次。 静态变量：类所有的实例都共享静态变量，可以直接通过类名来访问它； 实例变量：每创建一个实例就会产生一个实例变量，它与该实例同生共死。 1234public class A &#123; private int x; // 实例变量 public static int y; // 静态变量&#125; 2. 静态方法 静态方法在类加载的时候就存在了，它不依赖于任何实例，所以静态方法必须有实现，也就是说它不能是抽象方法（abstract）。 3. 静态语句块 静态语句块在类初始化时运行一次。 4. 静态内部类 内部类的一种，静态内部类不依赖外部类，且不能访问外部类的非静态的变量和方法。 5. 静态导包 1import static com.xxx.ClassName.* 在使用静态变量和方法时不用再指明 ClassName，从而简化代码，但可读性大大降低。 6. 变量赋值顺序 静态变量的赋值和静态语句块的运行优先于实例变量的赋值和普通语句块的运行，静态变量的赋值和静态语句块的运行哪个先执行取决于它们在代码中的顺序。 存在继承的情况下，初始化顺序为： 父类（静态变量、静态语句块） 子类（静态变量、静态语句块） 父类（实例变量、普通语句块） 父类（构造函数） 子类（实例变量、普通语句块） 子类（构造函数） 二、Object 通用方法 概览 1234567891011public final native Class&lt;?&gt; getClass()public native int hashCode()public boolean equals(Object obj)protected native Object clone() throws CloneNotSupportedExceptionpublic String toString()public final native void notify()public final native void notifyAll()public final native void wait(long timeout) throws InterruptedExceptionpublic final void wait(long timeout, int nanos) throws InterruptedExceptionpublic final void wait() throws InterruptedExceptionprotected void finalize() throws Throwable &#123;&#125; equals() 1. equals() 与 == 的区别 对于基本类型，== 判断两个值是否相等，基本类型没有 equals() 方法。 对于引用类型，== 判断两个实例是否引用同一个对象，而 equals() 判断引用的对象是否等价。 1234Integer x = new Integer(1);Integer y = new Integer(1);System.out.println(x.equals(y)); // trueSystem.out.println(x == y); // false 默认情况下也就是从超类Object继承而来的equals方法与==是完全等价的，比较的都是对象的内存地址，但我们可以重写equals方法，使其按照我们的需求的方式进行比较，如String类重写了equals方法，使其比较的是字符的序列，而不再是内存地址。 2. 等价关系 （一）自反性 1x.equals(x); // true （二）对称性 1x.equals(y) == y.equals(x); // true （三）传递性 12if (x.equals(y) &amp;&amp; y.equals(z)) x.equals(z); // true; （四）一致性 多次调用 equals() 方法结果不变 1x.equals(y) == x.equals(y); // true （五）与 null 的比较 对任何不是 null 的对象 x 调用 x.equals(null) 结果都为 false 1x.euqals(null); // false; 3. 实现 检查是否为同一个对象的引用，如果是直接返回 true； 检查是否是同一个类型，如果不是，直接返回 false； 将 Object 实例进行转型； 判断每个关键域是否相等。 1234567891011121314151617181920212223public class EqualExample &#123; private int x; private int y; private int z; public EqualExample(int x, int y, int z) &#123; this.x = x; this.y = y; this.z = z; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; EqualExample that = (EqualExample) o; if (x != that.x) return false; if (y != that.y) return false; return z == that.z; &#125;&#125; hashCode() hasCode() 返回散列值，而 equals() 是用来判断两个实例是否等价。等价的两个实例散列值一定要相同，但是散列值相同的两个实例不一定等价。 在覆盖 equals() 方法时应当总是覆盖 hashCode() 方法，保证等价的两个实例散列值也相等。 下面的代码中，新建了两个等价的实例，并将它们添加到 HashSet 中。我们希望将这两个实例当成一样的，只在集合中添加一个实例，但是因为 EqualExample 没有实现 hasCode() 方法，因此这两个实例的散列值是不同的，最终导致集合添加了两个等价的实例。 1234567EqualExample e1 = new EqualExample(1, 1, 1);EqualExample e2 = new EqualExample(1, 1, 1);System.out.println(e1.equals(e2)); // trueHashSet&lt;EqualExample&gt; set = new HashSet&lt;&gt;();set.add(e1);set.add(e2);System.out.println(set.size()); // 2 理想的散列函数应当具有均匀性，即不相等的实例应当均匀分布到所有可能的散列值上。这就要求了散列函数要把所有域的值都考虑进来，可以将每个域都当成 R 进制的某一位，然后组成一个 R 进制的整数。R 一般取 31，因为它是一个奇素数，如果是偶数的话，当出现乘法溢出，信息就会丢失，因为与 2 相乘相当于向左移一位。 之所以选择31，是因为它是个奇素数，如果乘数是偶数，并且乘法溢出的话，信息就会丢失，因为与2相乘等价于移位运算。使用素数的好处并不是很明显，但是习惯上都使用素数来计算散列结果。31有个很好的特性，就是用移位和减法来代替乘法，可以得到更好的性能：31*i==(i&lt;&lt;5)-i。现在的VM可以自动完成这种优化。 12345678@Overridepublic int hashCode() &#123; int result = 17; result = 31 * result + x; result = 31 * result + y; result = 31 * result + z; return result;&#125; clone() 1. cloneable clone() 是 Object 的受保护方法，这意味着，如果一个类不显式去覆盖 clone() 就没有这个方法。 1234public class CloneExample &#123; private int a; private int b;&#125; 12CloneExample e1 = new CloneExample();// CloneExample e2 = e1.clone(); // 'clone()' has protected access in 'java.lang.Object' 接下来覆盖 Object 的 clone() 得到以下实现： 123456789public class CloneExample &#123; private int a; private int b; @Override protected CloneExample clone() throws CloneNotSupportedException &#123; return (CloneExample)super.clone(); &#125;&#125; 123456CloneExample e1 = new CloneExample();try &#123; CloneExample e2 = e1.clone();&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125; 1java.lang.CloneNotSupportedException: CloneTest 以上抛出了 CloneNotSupportedException，这是因为 CloneTest 没有实现 Cloneable 接口。 123456789public class CloneExample implements Cloneable &#123; private int a; private int b; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; 应该注意的是，clone() 方法并不是 Cloneable 接口的方法，而是 Object 的一个 protected 方法。Cloneable 接口只是规定，如果一个类没有实现 Cloneable 接口又调用了 clone() 方法，就会抛出 CloneNotSupportedException。 2. 深拷贝与浅拷贝 浅拷贝：拷贝实例和原始实例的引用类型引用同一个对象； 深拷贝：拷贝实例和原始实例的引用类型引用不同对象。 1234567891011121314151617181920212223public class ShallowCloneExample implements Cloneable &#123; private int[] arr; public ShallowCloneExample() &#123; arr = new int[10]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = i; &#125; &#125; public void set(int index, int value) &#123; arr[index] = value; &#125; public int get(int index) &#123; return arr[index]; &#125; @Override protected ShallowCloneExample clone() throws CloneNotSupportedException &#123; return (ShallowCloneExample) super.clone(); &#125;&#125; 123456789ShallowCloneExample e1 = new ShallowCloneExample();ShallowCloneExample e2 = null;try &#123; e2 = e1.clone();&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;e1.set(2, 222);System.out.println(e2.get(2)); // 222 12345678910111213141516171819202122232425262728public class DeepCloneExample implements Cloneable &#123; private int[] arr; public DeepCloneExample() &#123; arr = new int[10]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = i; &#125; &#125; public void set(int index, int value) &#123; arr[index] = value; &#125; public int get(int index) &#123; return arr[index]; &#125; @Override protected DeepCloneExample clone() throws CloneNotSupportedException &#123; DeepCloneExample result = (DeepCloneExample) super.clone(); result.arr = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) &#123; result.arr[i] = arr[i]; &#125; return result; &#125;&#125; 123456789DeepCloneExample e1 = new DeepCloneExample();DeepCloneExample e2 = null;try &#123; e2 = e1.clone();&#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace();&#125;e1.set(2, 222);System.out.println(e2.get(2)); // 2 使用 clone() 方法来拷贝一个对象即复杂又有风险，它会抛出异常，并且还需要类型转换。Effective Java 书上讲到，最好不要去使用 clone()，可以使用拷贝构造函数或者拷贝工厂来拷贝一个对象。 12345678910111213141516171819202122232425public class CloneConstructorExample &#123; private int[] arr; public CloneConstructorExample() &#123; arr = new int[10]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = i; &#125; &#125; public CloneConstructorExample(CloneConstructorExample original) &#123; arr = new int[original.arr.length]; for (int i = 0; i &lt; original.arr.length; i++) &#123; arr[i] = original.arr[i]; &#125; &#125; public void set(int index, int value) &#123; arr[index] = value; &#125; public int get(int index) &#123; return arr[index]; &#125;&#125; 1234CloneConstructorExample e1 = new CloneConstructorExample();CloneConstructorExample e2 = new CloneConstructorExample(e1);e1.set(2, 222);System.out.println(e2.get(2)); // 2 四、继承 访问权限 Java 中有三个访问权限修饰符：private、default、protected 以及public，如果不加访问修饰符，表示包级可见。 可以对类或类中的成员（字段以及方法）加上访问修饰符。 成员可见表示其它类可以用这个类的实例访问到该成员； 类可见表示其它类可以用这个类创建对象。 protected 用于修饰成员，表示在继承体系中成员对于子类可见，但是这个访问修饰符对于类没有意义。 如果子类的方法覆盖了父类的方法，那么子类中该方法的访问级别不允许低于父类的访问级别。这是为了确保可以使用父类实例的地方都可以使用子类实例，也就是确保满足里氏替换原则。 字段决不能是公有的，因为这么做的话就失去了对这个字段修改行为的控制，客户端可以对其随意修改。可以使用公有的 getter 和 setter 方法来替换公有字段。 抽象类与接口 1. 抽象类 抽象类和抽象方法都使用 abstract 进行声明。抽象类一般会包含抽象方法，抽象方法一定位于抽象类中。 抽象类和普通类最大的区别是，抽象类不能被实例化，需要继承抽象类才能实例化其子类。 1234567891011public abstract class AbstractClassExample &#123; protected int x; private int y; public abstract void func1(); public void func2() &#123; System.out.println("func2"); &#125;&#125; 123456public class AbstractExtendClassExample extends AbstractClassExample&#123; @Override public void func1() &#123; System.out.println("func1"); &#125;&#125; 123// AbstractClassExample ac1 = new AbstractClassExample(); // 'AbstractClassExample' is abstract; cannot be instantiatedAbstractClassExample ac2 = new AbstractExtendClassExample();ac2.func1(); 2. 接口 接口是抽象类的延伸，在 Java 8 之前，它可以看成是一个完全抽象的类，也就是说它不能有任何的方法实现。 从 Java 8 开始，接口也可以拥有默认的方法实现，这是因为不支持默认方法的接口的维护成本太高了。在 Java 8 之前，如果一个接口想要添加新的方法，那么要修改所有实现了该接口的类。 接口的成员（字段 + 方法）默认都是 public 的，并且不允许定义为 private 或者protected。 接口的字段默认都是 static 和 final 的。 1234567891011121314public interface InterfaceExample &#123; void func1(); default void func2()&#123; System.out.println("func2"); &#125; int x = 123; // int y; // Variable 'y' might not have been initialized public int z = 0; // Modifier 'public' is redundant for interface fields // private int k = 0; // Modifier 'private' not allowed here // protected int l = 0; // Modifier 'protected' not allowed here // private void fun3(); // Modifier 'private' not allowed here&#125; 123456public class InterfaceImplementExample implements InterfaceExample &#123; @Override public void func1() &#123; System.out.println("func1"); &#125;&#125; 1234// InterfaceExample ie1 = new InterfaceExample(); // 'InterfaceExample' is abstract; cannot be instantiatedInterfaceExample ie2 = new InterfaceImplementExample();ie2.func1();System.out.println(InterfaceExample.x); 3. 比较 从设计层面上看，抽象类提供了一种 IS-A 关系，那么就必须满足里式替换原则，即子类对象必须能够替换掉所有父类对象。而接口更像是一种 LIKE-A 关系，它只是提供一种方法实现契约，并不要求接口和实现接口的类具有 IS-A 关系。 从使用上来看，一个类可以实现多个接口，但是不能继承多个抽象类。 接口的字段只能是 static 和 final 类型的，而抽象类的字段没有这种限制。 接口的方法只能是 public 的，而抽象类的方法可以由多种访问权限。 4. 使用选择 使用抽象类： 需要在几个相关的类中共享代码。 需要能控制继承来的方法和域的访问权限，而不是都为 public。 需要继承非静态（non-static）和非常量（non-final）字段。 使用接口： 需要让不相关的类都实现一个方法，例如不相关的类都可以实现 Compareable 接口中的 compareTo() 方法； 需要使用多重继承。 在很多情况下，接口优先于抽象类，因为接口没有抽象类严格的类层次结构要求，可以灵活地为一个类添加行为。并且从 Java 8 开始，接口也可以有默认的方法实现，使得修改接口的成本也变的很低。 super 访问父类的构造函数：可以使用 super() 函数访问父类的构造函数，从而完成一些初始化的工作。 访问父类的成员：如果子类覆盖了父类的中某个方法的实现，可以通过使用 super 关键字来引用父类的方法实现。 12345678910111213public class SuperExample &#123; protected int x; protected int y; public SuperExample(int x, int y) &#123; this.x = x; this.y = y; &#125; public void func() &#123; System.out.println("SuperExample.func()"); &#125;&#125; 1234567891011121314public class SuperExtendExample extends SuperExample &#123; private int z; public SuperExtendExample(int x, int y, int z) &#123; super(x, y); this.z = z; &#125; @Override public void func() &#123; super.func(); System.out.println("SuperExtendExample.func()"); &#125;&#125; 12SuperExample e = new SuperExtendExample(1, 2, 3);e.func(); 12SuperExample.func()SuperExtendExample.func() 覆盖与重载 覆盖（Override）存在于继承体系中，指子类实现了一个与父类在方法声明上完全相同的一个方法； 重载（Overload）存在于同一个类中，指一个方法与已经存在的方法名称上相同，但是参数类型、个数、顺序至少有一个不同。应该注意的是，返回值不同，其它都相同不算是重载。 五、String String, StringBuffer and StringBuilder 1. 是否可变 String 不可变 StringBuffer 和 StringBuilder 可变 2. 是否线程安全 String 不可变，因此是线程安全的 StringBuilder 不是线程安全的 StringBuffer 是线程安全的，内部使用 synchronized 来同步 String 不可变的原因 1. 可以缓存 hash 值 因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。 2. String Pool 的需要 如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool。 3. 安全性 String 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 对象的那一方以为现在连接的是其它主机，而实际情况却不一定是。 4. 线程安全 String 不可变性天生具备线程安全，可以在多个线程中安全地使用。 String.intern() 使用 String.intern() 可以保证相同内容的字符串实例引用相同的内存对象。 下面示例中，s1 和 s2 采用 new String() 的方式新建了两个不同对象，而 s3 是通过 s1.intern() 方法取得一个对象引用，这个方法首先把 s1 引用的对象放到 String Poll（字符串常量池）中，然后返回这个对象引用。因此 s3 和 s1 引用的是同一个字符串常量池的对象。 12345String s1 = new String("aaa");String s2 = new String("aaa");System.out.println(s1 == s2); // falseString s3 = s1.intern();System.out.println(s1.intern() == s3); // true 如果是采用 “bbb” 这种使用双引号的形式创建字符串实例，会自动地将新建的对象放入 String Pool 中。 123String s4 = "bbb";String s5 = "bbb";System.out.println(s4 == s5); // true 在 Java 7 之前，字符串常量池被放在运行时常量池中，它属于永久代。而在 Java 7，字符串常量池被放在堆中。这是因为永久代的空间有限，在大量使用字符串的场景下会导致 OutOfMemoryError 错误。 六、基本类型与运算 包装类型 八个基本类型： boolean/1 byte/8 char/16 short/16 int/32 float/32 long/64 double/64 基本类型都有对应的包装类型，基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成。 12Integer x = 2; // 装箱int y = x; // 拆箱 new Integer(123) 与 Integer.valueOf(123) 的区别在于，new Integer(123) 每次都会新建一个对象，而 Integer.valueOf(123) 可能会使用缓存对象，因此多次使用 Integer.valueOf(123) 会取得同一个对象的引用。 123456Integer x = new Integer(123);Integer y = new Integer(123);System.out.println(x == y); // falseInteger z = Integer.valueOf(123);Integer k = Integer.valueOf(123);System.out.println(z == k); // true 编译器会在自动装箱过程调用 valueOf() 方法，因此多个 Integer 实例使用自动装箱来创建并且值相同，那么就会引用相同的对象。 123Integer m = 123;Integer n = 123;System.out.println(m == n); // true valueOf() 方法的实现比较简单，就是先判断值是否在缓存池中，如果在的话就直接使用缓存池的内容。 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 在 Java 8 中，Integer 缓存池的大小默认为 -128~127。 1234567891011121314151617181920212223242526272829static final int low = -128;static final int high;static final Integer cache[];static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127;&#125; Java 还将一些其它基本类型的值放在缓冲池中，包含以下这些： boolean values true and false all byte values short values between -128 and 127 int values between -128 and 127 char in the range \u0000 to \u007F 因此在使用这些基本类型对应的包装类型时，就可以直接使用缓冲池中的对象。 Differences between new Integer(123), Integer.valueOf(123) and just 123 switch 从 Java 7 开始，可以在 switch 条件判断语句中使用 String 对象。 123456789String s = "a";switch (s) &#123; case "a": System.out.println("aaa"); break; case "b": System.out.println("bbb"); break;&#125; switch 不支持 long，是因为 swicth 的设计初衷是为那些只需要对少数的几个值进行等值判断，如果值过于复杂，那么还是用 if 比较合适。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Condition详解]]></title>
    <url>%2F2019%2F02%2F15%2Fthread%2FCondition%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在线程间通信方式总结中有一个需求：轮流打印奇数和偶数，我们用wait和notify实现了一下，但是这种方式存在弊端，就是不能精确控制唤醒哪个线程，比如现在有一个需求是轮流打印ABC该怎么办呢？ 首先准备三个线程，分别执行打印方法，是一个死循环，每次休息一秒。 一、wait/notify实现轮流打印ABC三个字母 如果是不加任何控制策略的话，必然是无法保证按照A B C的顺序依次循环执行的，比如： 执行结果是： 1694620367 1234567ABCBAC... 那么如何保证按照我们这个顺序执行呢？如果还是沿用这个方法，只能这样写： 思想也很简单，就是搞一个变量，规定只有0的时候才打印A，只有1的时候才打印B，只有2的时候才打印C。那么，对于打印A的线程，只要不是0就wait()等待，一旦等于0就打印，并且加一；对于打印B的线程，只要不是1就wait()等待，一旦等于1就打印，并且加一。剩下同理。 这样，由于signal是一个成员变量，初始值为0.那么三个线程中PrintB和PrintC都等待，只有PrintA能执行打印，然后加为1，唤醒所有等待的线程来判断，此时打印A的线程和打印C的线程发现都不符合他们打印的条件，都进入了while中等待了，只有打印B的线程发现等于1，则不进入while循环，打印再加一。依次反复，可以得到顺序打印的A、B、C。 这种方式显然很不好，是靠notifyAll来唤醒所有线程来实现的，那么我们能不能唤醒指定的线程呢？这样处理起来更加优雅效率也会更高！ 二、Condition来实现 达到了上面一样的效果。此时，我们发现它的强大之处在于我们可以指定哪个线程唤醒了，这看起来是一点点进步，但是我们学习多线程那么长时间了，在我看来，是很大的一个进步，因为之前用notify是随机唤醒一个，notifyAll是唤醒全部，总是不能受我们的完全控制，虽然说线程的执行本身就是不确定的，因此不确定性是他们的天生属性，但是在某些场景下我们确实需要一个高效并且优雅的实现可控的方式，所以是很重要的。 它这种功能可以给我们带来什么呢？下面用它实现一个有界队列。（关于生产者消费者模式，当然也可以用了，写法非常简单，就是对照上面的例子改一下即可。） 三、Condition实现有界队列 我们已经接触了线程池，它里面涉及队列，有很多种队列，最常见的是ArrayBlockingQueue以及LinkedBlockingQueue，他们的源码中其实也是靠Condition来实现阻塞塞值和阻塞取值的。现在我们也来实现一个比较简单的ArrayBlockingQueue吧！ 首先明确一下队列是FIFO的，即先进先出，那么我们要保证先插入的要先弹出。其次要注意的是当没有元素的时候要阻塞，即等待有元素了才能获取；放入元素也是同理，要等有空位的才能重新放入。 如何实现以上这种数据结构呢？关于先进的先出来，我们可以用两个指针来实现，一个叫做addIndex，一个叫做removeIndex，初始都是指向第一个元素处。当塞进来一个元素，那么addIndex就自增，当自增到最后一个位置，这个时候数组不一定是满的，因为有可能前面的值已经被取出去了，所以还需要一个变量count来标志是否已经塞满，如果满了就阻塞，否则如果addIndex到最后一个位置，就重新置0. 对于removeIndex也是相同方向移除，假设最简单的情况，就是长度为5的数组，那么第一个元素放在0位置，第二个元素放在1位置，第三个元素放在2位置，此时要移除，那么第一个元素就是我们要的最先进来的元素，我们将其移除，并且removeIndex加一指向第二个元素。如此反复执行。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class MyQueue &#123; //指向的是刚入队的元素的下角标 private int addIndex; //指向的是刚出队的元素后面一个元素的下角标 private int removeIndex; //实际元素个数 private int count; private Lock lock = new ReentrantLock(); private Condition putCondition = lock.newCondition(); private Condition getCondition = lock.newCondition(); private Object[] myQueue; //初始化队列的长度 public MyQueue(int initSize)&#123; myQueue = new Object[initSize]; &#125; //向队列的尾部放入元素 public void put(Object object)&#123; lock.lock(); while (count == myQueue.length)&#123; //说明队列已经满了，需要等待一下，那么放元素的线程就要阻塞住，即等待 System.out.println(Thread.currentThread().getName()+"--队列已满，不能再塞值了，我要阻塞一会...."); try &#123; putCondition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //说明是可以放入元素的 myQueue[addIndex++] = object; if(addIndex == myQueue.length)&#123; addIndex = 0; &#125; //元素的数量要加一 count++; System.out.println(Thread.currentThread().getName()+"成功向队列放入一个元素，当前队列元素个数为---"+count); System.out.println(); getCondition.signal(); lock.unlock(); &#125; //从队列的头部获取元素 public Object get()&#123; lock.lock(); while (count == 0)&#123; //说明队列已经满了，需要等待一下，那么取元素的线程就要阻塞住，即等待 System.out.println(Thread.currentThread().getName()+"--队列已空，不能再取值了，我要阻塞一会...."); System.out.println(); try &#123; getCondition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; int removeValue = (int) myQueue[removeIndex]; myQueue[removeIndex++] = null; if (removeIndex == myQueue.length)&#123; removeIndex = 0; &#125; count--; System.out.println(Thread.currentThread().getName()+"成功从队列获取一个元素，当前队列的元素个数为---"+count); putCondition.signal(); lock.unlock(); return removeValue; &#125; public static void main(String[] args) &#123; MyQueue myQueue = new MyQueue(5); new Thread(new PutThread(myQueue)).start(); new Thread(new TakeThread(myQueue)).start(); new Thread(new TakeThread(myQueue)).start(); &#125;&#125;class PutThread implements Runnable&#123; private MyQueue myQueue; public PutThread(MyQueue myQueue)&#123; this.myQueue = myQueue; &#125; @Override public void run() &#123; for(int i=0;i&lt;10;i++)&#123; System.out.println(Thread.currentThread().getName()+"成功放入一个元素，元素为："+i); myQueue.put(i); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;class TakeThread implements Runnable&#123; private MyQueue myQueue; public TakeThread(MyQueue myQueue)&#123; this.myQueue = myQueue; &#125; @Override public void run() &#123; int res = (int) myQueue.get(); System.out.println(Thread.currentThread().getName()+"成功放入一个元素，元素为："+res); &#125;&#125; 四、Condition原理概述 我们在上面的学习中看到，对于一个线程，我们就要准备一个Condition对象，并且还要用可重入锁ReentrantLock来实现加锁： 123public Lock lock = new ReentrantLock();public Condition cp = lock.newCondition();public Condition cc = lock.newCondition(); 它的原理是什么呢？ 我们看到，创建一个condition对象是通过lock.newCondition(),而这个方法实际上是会new出一个ConditionObject对象，该类是AQS的一个内部类. 我们知道在锁机制的实现上，AQS内部维护了一个同步队列，如果是独占式锁的话，所有获取锁失败的线程的尾插入到同步队列，同样的，condition内部也是使用同样的方式，内部维护了一个 等待队列，所有调用condition.await方法的线程会加入到等待队列中，并且线程状态转换为等待状态。另外注意到ConditionObject中有两个成员变量： 1234/** First node of condition queue. */private transient Node firstWaiter;/** Last node of condition queue. */private transient Node lastWaiter; 这样我们就可以看出来ConditionObject通过持有等待队列的头尾指针来管理等待队列。主要注意的是Node类复用了在AQS中的Node类。所以理解了AQS就简单了。但是这个队列有一点不同，他是一个单向链表，而AQS中的同步队列式一个双向链表。 同时还有一点需要注意的是：我们可以多次调用lock.newCondition()方法创建多个condition对象，也就是一个lock可以持有多个等待队列。而在之前利用Object的方式实际上是指在对象Object对象监视器上只能拥有一个同步队列和一个等待队列，而并发包中的Lock拥有一个同步队列和多个等待队列。示意图如下： 如图所示，ConditionObject是AQS的内部类，因此每个ConditionObject能够访问到AQS提供的方法，相当于每个Condition都拥有所属同步器的引用。 好了，至此我们已经知道多次调用lock.newCondition()方法创建多个condition对象，并且实际上这个对象就是ConditionObject。AQS维护的同步队列是一个双向链表结构，而这个Condition对象维护的是一个单项链表结构。 五、await实现原理 当调用condition.await()方法后会使得当前获取lock的线程进入到等待队列，如果该线程能够从await()方法返回的话一定是该线程获取了与condition相关联的lock。await()方法源码为： 在当前线程调用condition.await()方法后，会使得当前线程释放lock然后加入到等待队列中，直至被signal/signalAll后会使得当前线程从等待队列中移至到同步队列中去，直到获得了lock后才会从await方法返回(跳出while循环那就不用继续等待了呗)，或者在等待时被中断会做中断处理。 所以对于await()方法来说，它实现的功能为：将要等待的线程封装成节点尾插入到等待队列中，然后跟wait一样释放这个等待线程的锁。这些做完了之后还需要while循环判断是否已经在同步队列中，这个isOnsyncQueue是由下面说到的signal方法触发的，由于此时还没有signal所以陷在死循环中出不来，就调用lockSupport.park方法使他进入等待状态；当有signal或者有中断发生的时候，就跳出循环，继续执行，此时如果是signal触发的，就会进入下一个if,那就调用acquireQueue方法，这个方法在我们之前说的AQS中是提及的，主要思想是如果这个节点的前驱节点是head那么就自旋获取锁，否则可能会阻塞。这里已经从大体上说明了这个方法的整体思路，下面继续详细分析分析。 在这段代码中，我们将知道： 是怎样将当前线程添加到等待队列中去的？ 释放锁的过程？ 怎样才能从await方法退出？ 第一个问题：是怎样将当前线程添加到等待队列中去的？ 这段代码就很容易理解了，将当前节点包装成Node，如果等待队列的firstWaiter为null的话（等待队列为空队列），则将firstWaiter指向当前的Node,否则，更新lastWaiter(尾节点)即可。就是通过尾插入的方式将当前线程封装的Node插入到等待队列中即可，同时可以看出等待队列是一个不带头结点的链式队列，之前我们学习AQS时知道同步队列是一个带头结点的链式队列，这是两者的一个区别。将当前节点插入到等待队列之后，会使当前线程释放lock，由fullyRelease方法实现，fullyRelease源码为： 调用AQS的模板方法release方法释放AQS的同步状态(这样也说明了退出await方法必须是已经获得了condition引用（关联）的lock)并且唤醒在同步队列中头结点的后继节点引用的线程，如果释放成功则正常返回，若失败的话就抛出异常。到目前为止，这两段代码已经解决了前面的两个问题的答案了，还剩下第三个问题，怎样从await方法退出？现在回过头再来看await方法有这样一段逻辑： 很显然，当线程第一次调用condition.await()方法时，会进入到这个while()循环中，因为判断的条件是这个线程是否在同步队列中，我们这个刚进等待队列，咋可能在同步队列。 然后通过LockSupport.park(this)方法使得当前线程进入等待状态，那么要想退出这个await方法第一个前提条件自然而然的是要先退出这个while循环，出口就只剩下两个地方： 逻辑走到break退出while循环； while循环中的逻辑判断为false。 再看代码出现第1种情况的条件是当前等待的线程被中断后代码会走到break退出，第二种情况是当前节点被移动到了同步队列中（即另外线程调用的condition的signal或者signalAll方法），while中逻辑判断为false后结束while循环。 总结下，就是当前线程被中断或者调用condition.signal/condition.signalAll方法当前节点移动到了同步队列后 ，这是当前线程退出await方法的前提条件。 当退出while循环后就会调用acquireQueued(node, savedState)，这个方法在介绍AQS的底层实现时说过了，该方法的作用是在自旋过程中线程不断尝试获取同步状态，直至成功（线程获取到lock）。 await方法示意图如下图： 如图，调用condition.await方法的线程必须是已经获得了lock，也就是当前线程是同步队列中的头结点。调用该方法后会使得当前线程所封装的Node尾插入到等待队列中。 此外，await也支持超时等待和不响应中断，这里不再赘述。 六、signal/signalAll实现原理 调用condition的signal或者signalAll方法可以将等待队列中等待时间最长的节点移动到同步队列中，使得该节点能够有机会获得lock。按照等待队列是先进先出（FIFO）的，所以等待队列的头节点必然会是等待时间最长的节点，也就是每次调用condition的signal方法是将头节点移动到同步队列中。 signal方法首先会检测当前线程是否已经获取lock，如果没有获取lock会直接抛出异常，如果获取的话再得到等待队列的头指针引用的节点，之后的操作的doSignal方法也是基于该节点。下面我们来看看doSignal方法做了些什么事情，doSignal方法源码为： 具体逻辑请看注释，真正对头节点做处理的逻辑在transferForSignal中，该方法源码为： 关键逻辑请看注释，这段代码主要做了两件事情1.将头结点的状态更改为CONDITION；2.调用enq方法，将该节点尾插入到同步队列中，关于enq方法请看AQS的底层实现这篇文章。现在我们可以得出结论：调用condition的signal的前提条件是当前线程已经获取了lock，该方法会使得等待队列中的头节点即等待时间最长的那个节点移入到同步队列，而移入到同步队列后才有机会使得等待线程被唤醒，即从await方法中的LockSupport.park(this)方法中返回，从而才有机会使得调用await方法的线程成功退出，此时就要回过头去再看看await方法的后续处理流程了。signal执行示意图如下图： sigllAll与sigal方法的区别体现在doSignalAll方法上，前面我们已经知道doSignal方法只会对等待队列的头节点进行操作，而doSignalAll只不过时间等待队列中的每一个节点都移入到同步队列中，即“通知”当前调用condition.await()方法的每一个线程。 整理自：详解Condition的await和signal等待/通知机制]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现生产者消费者模式]]></title>
    <url>%2F2019%2F02%2F13%2Fthread%2F%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[无论是面试还是工作中，生产者和消费者模式一直是一个比较常见的问题，今天，我们用各种方式来实现它。 前言 生产者和消费者问题是线程模型中的经典问题：生产者和消费者在同一时间段内共用同一个存储空间，生产者往存储空间中添加产品，消费者从存储空间中取走产品，当存储空间为空时，消费者阻塞，当存储空间满时，生产者阻塞。 一、wait/notify/notifyAll实现 有一个天猫小店专门负责生产商品，用户也可以去买商品： 一个生产者的线程： 同理，一个消费者的线程： 下面进行测试： 此时的结果为： 由于生产大于消费，造成产能过剩。 二、阻塞队列实现 这个的实现已经在JUC组件拓展-BlockingQueue中实现了。不再赘述。主要就是用到阻塞的put()和take()两个方法。 三、condition+Lock实现 这个也很简单，就是基于wait和notify的代码稍微改一下即可。 生产者改为： 消费者跟生产者一样的改法： 执行效果类似。关于Condition接口，在Condition详解这篇文章中进行了详细介绍。 其实还有一些其他的方式也可以实现生产者消费者模型，但是我觉得最核心的就是上面讲的三种，掌握这个就不慌了。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程间通信方式总结]]></title>
    <url>%2F2019%2F02%2F13%2Fthread%2F%E7%BA%BF%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[在前面的文章中已经介绍了wait和notify的基本知识，我们知道了他们都是Object这个基类中的方法，因此每个对象都天生拥有这两个方法，可见其重要性，在多线程的学习中，他们两兄弟可以实现线程之间的通信，当然了，还有许多其他的方式实现线程间通信，下面逐一击破。 一、前言 开发中不免会遇到需要所有子线程执行完毕通知主线程处理某些逻辑的场景。或者是线程 A 在执行到某个条件通知线程 B 执行某个操作。 二、等待通知机制 即用wait+notify来实现。 案例目标：两个线程交替打印奇偶数，一共100个。注意，这也是阿里的一道面试编程题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class TwoThreadWaitNotify &#123; private int count = 1; private boolean flag = false; public static void main(String[] args) &#123; TwoThreadWaitNotify twoThreadWaitNotify = new TwoThreadWaitNotify(); new Thread(new PrintOdd(twoThreadWaitNotify)).start(); new Thread(new PrintEven(twoThreadWaitNotify)).start(); &#125; //打印奇数 static class PrintOdd implements Runnable&#123; private TwoThreadWaitNotify twoThreadWaitNotify; public PrintOdd(TwoThreadWaitNotify twoThreadWaitNotify)&#123; this.twoThreadWaitNotify = twoThreadWaitNotify; &#125; @Override public void run() &#123; while(twoThreadWaitNotify.count &lt;= 100)&#123; synchronized (TwoThreadWaitNotify.class)&#123; if(!twoThreadWaitNotify.flag)&#123; System.out.println("奇数线程开始执行，打印："+twoThreadWaitNotify.count); twoThreadWaitNotify.count++; twoThreadWaitNotify.flag = true; TwoThreadWaitNotify.class.notify(); &#125;else &#123; try &#123; TwoThreadWaitNotify.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125; //打印偶数 static class PrintEven implements Runnable&#123; private TwoThreadWaitNotify twoThreadWaitNotify; public PrintEven(TwoThreadWaitNotify twoThreadWaitNotify)&#123; this.twoThreadWaitNotify = twoThreadWaitNotify; &#125; @Override public void run() &#123; while(twoThreadWaitNotify.count &lt;= 100)&#123; synchronized (TwoThreadWaitNotify.class)&#123; if(twoThreadWaitNotify.flag)&#123; System.out.println("偶数线程开始执行，打印："+twoThreadWaitNotify.count); twoThreadWaitNotify.count++; twoThreadWaitNotify.flag = false; TwoThreadWaitNotify.class.notify(); &#125;else &#123; try &#123; TwoThreadWaitNotify.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; &#125;&#125; 运行结果部分截图： 这里的线程 A 和线程 B 都对同一个对象 TwoThreadWaitNotify.class 获取锁，A 线程调用了同步对象的 wait() 方法释放了锁并进入 WAITING 状态。 B 线程调用了 notify() 方法，这样 A 线程收到通知之后就可以从 wait() 方法中返回。 这里利用了 TwoThreadWaitNotify.class 对象完成了通信。 有一些需要注意: wait() 、notify()、notifyAll() 调用的前提都是获得了对象的锁(也可称为对象监视器)。 调用 wait() 方法后线程会释放锁，进入WAITING 状态，该线程也会被移动到等待队列中。 调用 notify() 方法会将等待队列中的线程移动到同步队列中，线程状态也会更新为 BLOCKED 从 wait() 方法返回的前提是调用 notify() 方法的线程释放锁，wait() 方法的线程获得锁。 ⭐等待通知有着一个经典范式： 线程 A 作为消费者： 获取对象的锁。 2.进入 while(判断条件)，并调用 wait() 方法。 当条件满足跳出循环执行具体处理逻辑。 线程 B 作为生产者: 获取对象锁。 更改与线程 A 共用的判断条件。 调用 notify() 方法。 伪代码如下: 1234567891011121314//Thread Asynchronized(Object)&#123; while(条件)&#123; Object.wait(); &#125; //do something&#125;//Thread Bsynchronized(Object)&#123; 条件=false;//改变条件 Object.notify();&#125; 三、Join方式 这个之前也是提及过的，主要是可以打断主线程让子线程先执行完，但是缺点是粒度不够细腻，我不能控制子线程在某个点停一下让其他子线程执行。 核心逻辑: 123while (isAlive()) &#123; wait(0);&#125; 在 join 线程完成后会调用 notifyAll() 方法，是在 JVM 实现中调用，所以这里看不出来。 四、volatile 共享内存 因为 Java 是采用共享内存的方式进行线程通信的，所以可以采用以下方式用主线程关闭 A 线程: 输出结果： 12345thread A正在运行。。。thread A正在运行。。。thread A正在运行。。。thread A正在运行。。。thread A执行完毕 这里的 flag 存放于主内存中，所以主线程和线程 A 都可以看到。flag 采用 volatile 修饰主要是为了内存可见性。 五、CountDownLatch 并发工具 在前面的文章中我们基本知道它的使用，但是很遗憾，没有找一个比较实际的场景来描述它的功能，下面我将以一个实际场景来用CountDownLatch来解决这个问题。 假设小明和小红是一对夫妻，他们两准备烧一个菜，就叫做青椒炒肉丝。我们知道，要想炒出青椒炒肉丝，需要切好青椒，然后切好肉丝，如果想要肉的质感爽嫩，需要用淀粉揉一揉，加点醋，加点料酒去去腥，并且弄好了之后需要先炒一下肉。最后两样都准备好之后：即切好的青椒和预热好的肉丝，那么就可以合在一起炒一下出锅了。 假设切青椒需要3分钟，准备好肉需要5分钟，这两个同时准备好之后就可以进行烧菜了。如何最大程度上提高效率呢？ 显然，就是小明切青椒，小红搞肉丝，两个人并行。这个时候，我们可以用CountDownLatch来模拟这个场景。 切青椒线程： 准备肉丝的线程： 测试： 结果为： 12345小明开始切青椒...小红开始准备肉丝...小明切好青椒了...小红准备好肉丝了...over，食材全部准备好了，一起下锅 cost:5010 这里突出的就是，主线程等待两个线程都执行完了才能继续执行。 CountDownLatch 也是基于 AQS(AbstractQueuedSynchronizer) 实现的. 初始化一个 CountDownLatch 时告诉并发的线程，然后在每个线程处理完毕之后调用 countDown() 方法。 该方法会将 AQS 内置的一个 state 状态 -1 。 最终在主线程调用 await() 方法，它会阻塞直到 state == 0 的时候返回。 这个功能是不是很类似于上面的join，但是它比join灵活多了。 六、CyclicBarrier 并发工具 这个工具类从原理上来看与CountDownLatch非常类似，具体的使用可以看AQS实现的一些同步组件介绍。他们两是有区别的。该工具可以实现 CountDownLatch 同样的功能，但是要更加灵活。甚至可以调用 reset() 方法重置 CyclicBarrier (需要自行捕获 BrokenBarrierException 处理) 然后重新执行。就不再赘述了。 七、线程响应中断 这个我们之前也是提过的，就是interrupt来实现，线程方法里面用while不停地判断中断标志位从而达到自主中断的目的。 输出结果: 123thread A运行中。。thread A运行中。。thread A退出。。 可以采用中断线程的方式来通信，调用了 thread.interrupt() 方法其实就是将 thread 中的一个标志属性置为了 true。 并不是说调用了该方法就可以中断线程，如果不对这个标志进行响应其实是没有什么作用(这里对这个标志进行了判断)。 但是如果抛出了 InterruptedException 异常，该标志就会被 JVM 重置为 false。 八、线程池 awaitTermination() 方法 这个玩意与我们知道shutdown方法组合使用，我们知道，调用了 shutdown() 之后线程池会停止接受新任务，并且会平滑的关闭线程池中现有的任务。 关于awaitTermination()方法，接收timeout和TimeUnit两个参数，用于设定超时时间及单位。当等待超过设定时间时，会监测ExecutorService是否已经关闭，若关闭则返回true，否则返回false.因此，在shutdwon之后，我们可以用awaitTermination()不断地监测剩下的线程的执行状态，执行完毕就可以执行主线程了。 输出结果: 123452018-03-16 20:18:01.273 [pool-1-thread-2] INFO c.c.actual.ThreadCommunication - running22018-03-16 20:18:01.273 [pool-1-thread-1] INFO c.c.actual.ThreadCommunication - running2018-03-16 20:18:02.273 [main] INFO c.c.actual.ThreadCommunication - 线程还在执行。。。2018-03-16 20:18:03.278 [main] INFO c.c.actual.ThreadCommunication - 线程还在执行。。。2018-03-16 20:18:04.278 [main] INFO c.c.actual.ThreadCommunication - main over 最后再强调一下： 使用这个 awaitTermination() 方法的前提需要关闭线程池，如调用了 shutdown() 方法。 调用了 shutdown() 之后线程池会停止接受新任务，并且会平滑的关闭线程池中现有的任务。 九、管道通信 这个方式我见到的比较少，了解一下。 输出结果: 1234567891011122018-03-16 19:56:43.014 [Thread-0] INFO c.c.actual.ThreadCommunication - running2018-03-16 19:56:43.014 [Thread-1] INFO c.c.actual.ThreadCommunication - running22018-03-16 19:56:43.130 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=02018-03-16 19:56:43.132 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=12018-03-16 19:56:43.132 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=22018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=32018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=42018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=52018-03-16 19:56:43.133 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=62018-03-16 19:56:43.134 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=72018-03-16 19:56:43.134 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=82018-03-16 19:56:43.134 [Thread-1] INFO c.c.actual.ThreadCommunication - msg=9 Java 虽说是基于内存通信的，但也可以使用管道通信。 需要注意的是，输入流和输出流需要首先建立连接。这样线程 B 就可以收到线程 A 发出的消息了。 十、总结 实际开发中可以灵活根据需求选择最适合的线程通信方式。 整理自：深入理解线程通信]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC组件拓展-BlockingQueue]]></title>
    <url>%2F2019%2F02%2F12%2Fthread%2FJUC%E7%BB%84%E4%BB%B6%E6%8B%93%E5%B1%95-BlockingQueue%2F</url>
    <content type="text"><![CDATA[在之前的线程池原理介绍中，我们了解到在核心线程被全部占用并且没有空闲线程的时候，就会把后续的线程任务先放入一个队列结构中，然后按照队列的方式去消化任务。虽然队列有很多种，但是他们都有一个共同的名字叫做阻塞队列，本文来逐个击破揭开他们的面纱。 一、BlockingQueue 在Java中，BlockingQueue是一个接口，它的实现类有ArrayBlockingQueue、DelayQueue、 LinkedBlockingDeque、LinkedBlockingQueue、PriorityBlockingQueue、SynchronousQueue等，它们的区别主要体现在存储结构上或对元素操作上的不同，但是对于take与put操作的原理，却是类似的。 BlockingQueue 是一个先进先出的队列（Queue），为什么说是阻塞（Blocking）的呢？是因为 BlockingQueue 支持当获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回；也支持添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入。 add(anObject):把anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则抛出异常 offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false. ⭐put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续. ⭐与put相对应的是take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到Blocking有新的对象被加入为止 poll(time):取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null 注意：BlockingQueue 不接受 null 元素。试图 add 、 put 或 offer 一个 null 元素时，某些实现会抛出 NullPointerException 。 null 被用作指示 poll 操作失败的警戒值。 BlockingQueue 的各个实现都遵循了这些规则，当然我们也不用死记这个表格，知道有这么回事，然后写代码的时候根据自己的需要去看方法的注释来选取合适的方法即可。 一个 BlockingQueue 可能是有界的，如果在插入的时候，发现队列满了，那么 put 操作将会阻塞。通常，在这里我们说的无界队列也不是说真正的无界，而是它的容量是 Integer.MAX_VALUE（21亿多）。 BlockingQueue 实现主要用于生产者-消费者队列，但它另外还支持Collection 接口。因此，举例来说，使用remove(x) 从队列中移除任意一个元素是有可能的。然而，这种操作通常不 会有效执行，只能有计划地偶尔使用，比如在取消排队信息时。 BlockingQueue 的实现都是线程安全的，但是批量的集合操作如 addAll, containsAll, retainAll 和 removeAll 不一定是原子操作。如 addAll(c) 有可能在添加了一些元素后中途抛出异常，此时 BlockingQueue 中已经添加了部分元素，这个是允许的，取决于具体的实现。 下面来看看阻塞队列的各种具体的实现类。 二、ArrayBlockingQueue 构造函数必须带一个int参数来指明其大小 一个由数组结构组成的有界阻塞队列. 此队列按 FIFO（先进先出）原则对元素进行排序. ⭐其并发控制采用可重入锁来控制，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。 ⭐如果队列为空，这个时候读操作的线程进入到读线程队列排队，等待写线程写入新的元素，然后唤醒读线程队列的第一个等待线程。 ⭐如果队列已满，这个时候写操作的线程进入到写线程队列排队，等待读线程将队列元素移除腾出空间，然后唤醒写线程队列的第一个等待线程。 支持公平锁和非公平锁。公平的获取锁，也就是当前等待时间最长的线程先获取锁 三、LinkedBlockingQueue 大小不定的BlockingQueue 若其构造函数带一个规定大小的参数,生成的BlockingQueue有大小限制 若不带大小参数,所生成的BlockingQueue的大小由Integer.MAX_VALUE来决定 其所含的对象是以FIFO(先入先出)顺序排序的 ⭐链接队列的吞吐量通常要高于基于数组的队列，但是在大多数并发应用程序中，其可预知的性能要低 最新插入的数据在尾部，最新移除的对象在头部 四、PriorityBlockingQueue ⭐类似于LinkedBlockQueue,但其所含对象的排序不是FIFO,而是依据对象的自然排序顺序或者是构造函数的Comparator决定的顺序 一个无界的阻塞队列 五、SynchronousQueue ⭐它是一种阻塞队列，其中每个 put 必须等待一个 take，反之亦然。 ⭐同步队列没有任何内部容量，甚至连一个队列的容量都没有。 它是线程安全的，是阻塞的。 不允许使用 null 元素。 公平排序策略是指调用 put 的线程之间，或 take 的线程之间。 一个没有容量的并发队列有什么用了？或者说存在的意义是什么？ 尽管元素在SynchronousQueue 内部不会“停留”，但是并不意味着SynchronousQueue 内部没有队列。实际上SynchronousQueue 维护着线程队列，也就是插入线程或者移除线程在不同时存在的时候就会有线程队列。既然有队列，同样就有公平性和非公平性特性，公平性保证正在等待的插入线 程或者移除线程以FIFO的顺序传递资源。 它模拟的功能类似于生活中一手交钱一手交货这种情形，像那种货到付款或者先付款后发货模型不适合使用SynchronousQueue。首先要知道SynchronousQueue没有容纳元素的能力，即它的isEmpty()方法总是返回true，但是给人的感觉却像是只能容纳一个元素。 六、DelayQueue DelayQueue 对元素进行持有直到一个特定的延迟到期。注意其中的元素必须实现 java.util.concurrent.Delayed 接口。 七、生产者与消费者模式 阻塞队列的最常使用的例子就是生产者消费者模式,也是各种实现生产者消费者模式方式中首选的方式。使用者不用关心什么阻塞生产，什么时候阻塞消费，使用非常方便。 LinkedBlockingQueue来实现一个生产者与消费者模型： 运行效果： 12345678producer1 produce 95producer3 produce 36consumer0 consumer 95consumer2 consumer 36producer0 produce 27consumer4 consumer 27producer2 produce 75...... 八、ArrayBlockingQueue和LinkedBlockingQueue的区别 队列中锁的实现不同 ArrayBlockingQueue实现的队列中的锁是没有分离的，即生产和消费用的是同一个锁；另外，可以指定是否为公平锁，默认是非公平锁。 LinkedBlockingQueue实现的队列中的锁是分离的，在队头和队尾各持有一把锁，入队和出队之间不存在竞争。即生产用的是putLock，消费是takeLock，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 在生产或消费时操作不同 ArrayBlockingQueue实现的队列中在生产和消费的时候，是直接将枚举对象插入或移除的； LinkedBlockingQueue实现的队列中在生产和消费的时候，需要把枚举对象转换为Node&lt;E&gt;进行插入或移出(会生成一个额外的Node对象，这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。) 队列大小初始化方式不同 ArrayBlockingQueue实现的队列中必须指定队列的大小； LinkedBlockingQueue实现的队列中可以不指定队列的大小，但是默认是Integer.MAX_VALUE 作为开发者，我们需要注意的是，如果构造一个LinkedBlockingQueue对象，而没有指定其容量大小，LinkedBlockingQueue会默认一个类似无限大小的容量（Integer.MAX_VALUE），这样的话，如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已被消耗殆尽了。 在使用ArrayBlockingQueue和LinkedBlockingQueue分别对1000000个简单字符做入队操作时，LinkedBlockingQueue的消耗是ArrayBlockingQueue消耗的10倍左右，即LinkedBlockingQueue消耗在1500毫秒左右，而ArrayBlockingQueue只需150毫秒左右。 按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC组件拓展-ForkJoin简介]]></title>
    <url>%2F2019%2F02%2F12%2Fthread%2FJUC%E7%BB%84%E4%BB%B6%E6%8B%93%E5%B1%95-ForkJoin%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[JUC组件拓展-ForkJoin简介，本文只是初步认识认识一下ForkJoin是什么，不深究里面的原理。 ForkJoin 什么是Fork/Join框架 Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 我们再通过Fork和Join这两个单词来理解下Fork/Join框架，Fork就是把一个大任务切分为若干子任务并行的执行，Join就是合并这些子任务的执行结果，最后得到这个大任务的结果。比如计算1+2+。。＋10000，可以分割成10个子任务，每个子任务分别对1000个数进行求和，最终汇总这10个子任务的结果。 工作窃取算法 工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。工作窃取的运行流程图如下： 那么为什么需要使用工作窃取算法呢？ 假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如A线程负责处理A队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。 Fork/Join框架如何实现工作窃取的？ 这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。 Fork/Join框架有没有什么缺点？ 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 该如何设计一个Fork/Join框架? 第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 这里就先简单介绍一下，如果有必要，以后再细谈。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AQS实现的一些并发工具类]]></title>
    <url>%2F2019%2F02%2F12%2Fthread%2FAQS%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[在前面我们已经深入了解了AQS原理，本节介绍几个常用的基于AQS实现的并发工具类。 一、CountDownLatch 计数器减到0，处于等待的线程才会继续执行。只能用一次，不能重置。 比如有一个运算量很大的任务，我们可以将它拆分为多个子任务，等所有子任务全部完成之后，再执行最后的汇总工作。 下面用一个实例来看看它是如何使用的： 运行结果，截取了最后一点： 我们可以看到，主程序等待所有的子程序执行完毕，再执行，它是通过await()阻塞等待，直到计数器的值减到0为止。 那如果是这种场景呢：计算若干个子任务，给定一个时间，超过这个时间的话，就把这个任务放弃掉。 1countDownLatch.await(10, TimeUnit.MILLISECONDS); 二、Semaphore 能控制同一时间并发线程的数目 Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。很多年以来，我都觉得从字面上很难理解Semaphore所表达的含义，只能把它比作是控制流量的红绿灯，比如XX马路要限制流量，只允许同时有一百辆车在这条路上行使，其他的都必须在路口等待，所以前一百辆车会看到绿灯，可以开进这条马路，后面的车会看到红灯，不能驶入XX马路，但是如果前一百辆中有五辆车已经离开了XX马路，那么后面就允许有5辆车驶入马路，这个例子里说的车就是线程，驶入马路就表示线程在执行，离开马路就表示线程执行完成，看见红灯就表示线程被阻塞，不能执行。 Semaphore可以用于做流量控制，特别公用资源有限的应用场景，比如数据库连接。假如有一个需求，要读取几万个文件的数据，因为都是IO密集型任务，我们可以启动几十个线程并发的读取，但是如果读到内存后，还需要存储到数据库中，而数据库的连接数只有10个，这时我们必须控制只有十个线程同时获取数据库连接保存数据，否则会报错无法获取数据库连接。这个时候，我们就可以使用Semaphore来做流控，代码如下： 再来一个例子： 这里是一个线程获取一个许可，那么同一时间，可以有三个线程进来一起工作。那如果我改成一个线程获取三个许可呢？就像一个人同时占三个坑位，那么只有等这个人拉完了才能轮到下一个人了，那么此时就变成跟单线程一样了。 123semaphore.acquire(3);test(threadNum);semaphore.release(3); 考虑这个场景：并发太高了，就算是控制线程数量，也比较棘手；一个厕所三个坑位，外面人太多了，让三个人进来，其他的都给轰走。如何做到呢？ 1234if(semaphore.tryAcquire())&#123;//尝试获取一个许可 test(threadNum); semaphore.release();&#125; 输出结果：只有三条信息打印出来，其他的线程就都被丢弃了。 也可以给他一个超时时间，这里是5000毫秒。每个命令需要运行1000毫秒，那么程序等1000毫秒之后会打印三条；然后再等1000毫秒，又可以拿到新的三个许可，再打印三条；直到5000毫秒用完。可能会打印3*5条记录。剩下的5条记录由于已经超时，全部被放弃掉。 三、CyclicBarrier CyclicBarrier也是一个同步辅助类 , 它允许一组线程相互等待 , 直到到达某个公共的屏障点 , 通过它可以完成多个线程之间相互等待 ,只有当每个线程都准备好之后, 才能各自继续往下执行后续的操作, 和 CountDownLatch相似的地方就是, 它也是通过计数器来实现的. 当某个线程调用了 await()方法之后, 该线程就进入了等待状态 . 而且计数器就进行 -1 操作 , 当计数器的值达到了我们设置的初始值0的时候 , 之前调用了await() 方法而进入等待状态的线程会被唤醒继续执行后续的操作. 因为 CyclicBarrier释放线程之后可以重用, 所以又称之为循环屏障 . CyclicBarrier 使用场景和 CountDownLatch 很相似 , 可以用于多线程计算数据, 最后合并计算结果的应用场景 . 两者的区别： CountDownLatch的计数器只能使用一次 , 而 CyclicBarrier 的计数器可以使用 reset重置 循环使用 CountDownLatch 主要是 1 个 或者 n 个线程需要等待其它线程完成某项操作之后才能继续往下执行 , 其描述的是 1 个 或者 n 个线程与其它线程的关系 ; CyclicBarrier 主要是实现了 1 个或者多个线程之间相互等待,直到所有的线程都满足条件之后, 才执行后续的操作 , 其描述的是内部各个线程相互等待的关系 . CyclicBarrier 假如有 5 个线程都调用了 await() 方法 , 那这个 5 个线程就等着 , 当这 5 个线程都准备好之后, 它们有各自往下继续执行 , 如果这 5 个线程在后续有一个计算发生错误了 , 这里可以重置计数器 , 并让这 5 个线程再执行一遍 . 运行效果：先每隔一秒执行race方法打印出ready,等3个线程打印完毕，立即都将阻塞的log.info(&quot;continue...&quot;);全部打印出来。 也可以设定超时时间，超过时间了就不等了。 如果在大家已经都准备好了的时候，可以先做一件事情，即初始化执行一个线程，可以在声明CyclicBarrier后面增加一个线程来执行。 就像开会，人都到齐了之后，我们喊一声，人都到齐，我们现在开始开会了啊。下面就开始正式开会。 123private static CyclicBarrier cyclicBarrier = new CyclicBarrier(5,() -&gt; &#123; log.info("callback is running...");&#125;); 四、Exchanger Exchanger 类表示一种会合点，两个线程可以在这里交换对象。两个线程各自调用exchange 方法进行交换，当线程 A 调用 Exchange 对象的 exchange 方法后，它会陷入阻塞状态，直到线程 B 也调用了 exchange 方法，然后以线程安全的方式交换数据，之后线程 A 和 B 继续运行。 exchange 方法有两个重载实现，在交换数据的时候还可以设置超时时间。如果一个线程在超时时间内没有其他线程与之交换数据，就会抛出 TimeoutException 超时异常。如果没有设置超时时间，则会一直等待。 12345//交换数据，并设置超时时间public V exchange(V x, long timeout, TimeUnit unit)throws InterruptedException, TimeoutException//交换数据public V exchange(V x) throws InterruptedException 下面看一个小例子： 我们要注意，交换的时候两个线程要同时到达一个汇合点才会继续执行，即这里的a线程拿到b线程的值并且b拿到a的值，程序才会继续执行。 例子很简单，当两个线程都到达调用exchange方法的同步点的时候，两个线程就能交换彼此的数据。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池原理详解]]></title>
    <url>%2F2019%2F02%2F12%2Fthread%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[JAVA帮助开发者封装了一些现成的线程池调用，但是每种线程池都有自己的使用场景，如果不了解里面的原理，那么很容易掉进坑里，线程池原理也是面试的重灾区，因此本问将完整分析线程池的原理。 一、new thread弊端 从学习java多线程开始，我们就学习了用new thread来创建线程。但是他有一定的弊端： 每次new Thread新建对象，性能差 线程缺乏统一管理，可能无限制的新建线程，相互竞争，有可能占用过多系统资源导致死机或OOM 缺少更多功能，如更多执行、定期执行、线程中断 二、线程池好处 重用存在的线程，减少对象创建、消亡的开销，性能佳 可有效控制最大并发线程数，提高系统资源利用率，同时可以避免过多资源竞争，避免阻塞 提供定时执行、定期执行、单线程、并发数控制等功能 三、线程池相关参数 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize:核心线程数量 默认情况下，在创建了线程池后，线程池中的线程数为0， （除非调用prestartAllCoreThreads()和prestartCoreThread()方法，从方法名字可以看出，是预创建线程的意思，即在没有任务到来之前，就创建corePoolSize个线程或1个线程）当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； 当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。 maximumPoolSize:线程最大线程数 线程池中的最大线程数，表示线程池中最多能创建多少个线程。 超过就执行reject策略:如果队列满了,并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务 workQueue:阻塞队列，存储等待执行的任务，很重要，会对线程池运行过程产生重大影响，一般有以下几种选择： ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序； LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO （先进先出） 排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列； SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列； PriorityBlockingQueue：一个具有优先级的无限阻塞队列；底层用DelayedWorkQueue实现。 keepAliveTime：线程没有任务执行时最多保持多久时间终止 当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。（但是如果调用了allowCoreThreadTimeOut(boolean value)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0；） unit:keepAliveTime的时间单位 threadFactory：线程工厂，用来创建线程 threadFactory用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字 handler:饱和策略 当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。 这些参数全部传给ThreadPoolExecutor之后，ThreadPoolExecutor就可以为我们提供一个线程池，我们可以对这个线程池提交以及终止线程任务。 四、饱和策略 当线程池中已经到了完全没有办法再接收新的线程进来的时候，就会启动饱和策略。 1234java.util.concurrent.ThreadPoolExecutor.AbortPolicyjava.util.concurrent.ThreadPoolExecutor.CallerRunsPolicyjava.util.concurrent.ThreadPoolExecutor.DiscardOldestPolicyjava.util.concurrent.ThreadPoolExecutor.DiscardPolicy AbortPolicy：丢弃任务并抛出RejectedExecutionException异常（默认） CallerRunsPolicy：只用调用所在的线程运行任务 DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） DiscardPolicy：不处理，丢弃掉,不抛出异常。 五、线程池的源码解读 程序中要声明线程池，是这样写的： 12ExecutorService exec = Executors.newCachedThreadPool();exec.excute(Runnable command); 先来看看ExecutorService其中的奥秘。 5.1 ExecutorService和Executor的关系 Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的； 123public interface Executor &#123; void execute(Runnable command);&#125; ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等； 12345678910111213141516171819202122public interface ExecutorService extends Executor &#123; void shutdown(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); Future&lt;?&gt; submit(Runnable task); &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 可以看出，ExecutorService具备管理执行器和任务生命周期的方法，提交任务机制更加完善。Executor只是运行新任务的简单接口，目的是将任务提交和任务执行解耦。 5.2 ThreadPoolExecutor重要方法 我们知道，在执行Executors.newCachedThreadPool()的时候，内部是调用ThreadPoolExecutor的构造函数来生成Exceutors对象，即生成了线程池，因为继承关系是：ThreadPoolExecutor extends AbstractExecutorService implements ExecutorService extends Executor。构建好之后，就可以构建工作线程去执行任务。其中，流程是这样的： 所以，用于execute()或者submit()的线程任务都是被封装成worker去执行的。下面来看看execute()和submit()等核心方法。 在ThreadPoolExecutor类中有几个非常重要的方法： execute() execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit() submit()方法是在ExecutorService中声明的方法,这个方法也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果，去看submit()方法的实现，会发现它实际上还是调用的execute()方法，只不过它利用了Future来获取任务执行结果。 shutdown() 将线程池状态置为SHUTDOWN,并不会立即停止： 停止接收外部submit的任务内部正在跑的任务和队列里等待的任务，会执行完等到第二步完成后，才真正停止 shutdownNow() 将线程池状态置为STOP。企图立即停止，事实上不一定： 跟shutdown()一样，先停止接收外部提交的任务忽略队列里等待的任务尝试将正在跑的任务interrupt中断返回未执行的任务列表 它试图终止线程的方法是通过调用Thread.interrupt()方法来实现的，但是大家知道，这种方法的作用有限，如果线程中没有sleep 、wait、Condition、定时锁等应用, interrupt()方法是无法中断当前的线程的。所以，ShutdownNow()并不代表线程池就一定立即就能退出，它也可能必须要等待所有正在执行的任务都执行完成了才能退出。但是大多数时候是能立即退出的 awaitTermination(long timeOut, TimeUnit unit) 接收timeout和TimeUnit两个参数，用于设定超时时间及单位。当等待超过设定时间时，会监测ExecutorService是否已经关闭，若关闭则返回true，否则返回false。一般情况下会和shutdown方法组合使用。 5.3 Executors生成线程池 要配置一个线程池是比较复杂的，尤其是对于线程池的原理不是很清楚的情况下，很有可能配置的线程池不是较优的，因此在Executors类里面提供了一些静态工厂，生成一些常用的线程池。这个就涉及上面我们反复提及的核心类：ThreadPoolExecutor。 ⭐其实都是通过调用ThreadPoolExecutor来完成的，最后可以返回ExecutorService对象，其实说白了都是Excutor对象。 下面来分别看看比较常用的线程池。 newSingleThreadExecutor 创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 12345678910111213141516171819//创建一个核心线程个数和最大线程个数都为1的线程池//阻塞队列长度为Integer.MAX_VALUE//keeyAliveTime=0说明只要线程个数比核心线程个数多并且当前空闲则回收//线程由DefaultThreadFactory默认创建，有统一的命名规范，并且优先级是一样的public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;//使用自己的线程工厂来创建线程public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory));&#125; demo： 12345678910111213141516@Slf4jpublic class ThreadPoolTest3 &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newSingleThreadExecutor(); for(int i=0;i&lt;10;i++)&#123; final int index = i; exec.execute(() -&gt; &#123; log.info("task:&#123;&#125;,index:&#123;&#125;",Thread.currentThread().getId(),index); &#125;); &#125; exec.shutdown(); &#125;&#125; 运行结果： 12345678910task:10,index:0task:10,index:1task:10,index:2task:10,index:3task:10,index:4task:10,index:5task:10,index:6task:10,index:7task:10,index:8task:10,index:9 运行结果分析：单线程+有序。 newFixedThreadPool 创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 123456789101112131415//创建一个核心线程个数和最大线程个数都为nThreads的线程池//阻塞队列长度为Integer.MAX_VALUE//keeyAliveTime=0说明只要线程个数比核心线程个数多并且当前空闲则回收public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;//使用自己的线程工厂来创建线程public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);&#125; demo： 12345678910111213141516@Slf4jpublic class ThreadPoolTest2 &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newFixedThreadPool(3); for(int i=0;i&lt;10;i++)&#123; final int index = i; exec.execute(() -&gt; &#123; log.info("task:&#123;&#125;,index:&#123;&#125;",Thread.currentThread().getId(),index); &#125;); &#125; exec.shutdown(); &#125;&#125; 运行结果： 12345678910task:11,index:1task:11,index:3task:11,index:4task:11,index:5task:11,index:6task:11,index:7task:11,index:8task:11,index:9task:10,index:0task:12,index:2 结果分析：只创建了三个线程来执行。 newCachedThreadPool 创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 1234567891011121314151617//创建一个按需创建线程的线程池，初始线程个数为0，最多线程个数为Integer.MAX_VALUE//阻塞队列为同步队列//keeyAliveTime=60说明只要当前线程60s内空闲则回收//特殊在于加入到同步队列的任务会被马上被执行，同步队列里面最多只有一个任务，并且存在后马上会拿出执行public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;//使用自己的线程工厂来创建线程public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), threadFactory);&#125; demo： 12345678910111213141516@Slf4jpublic class ThreadPoolTest1 &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for(int i=0;i&lt;10;i++)&#123; final int index = i; exec.execute(() -&gt; &#123; log.info("task:&#123;&#125;,index:&#123;&#125;",Thread.currentThread().getId(),index); &#125;); &#125; exec.shutdown(); &#125;&#125; 运行结果： 12345678910task:10,index:0task:12,index:2task:14,index:4task:16,index:6task:18,index:8task:11,index:1task:13,index:3task:15,index:5task:17,index:7task:19,index:9 结果分析：按需创建线程，几乎一次循环就创建了一个新的线程来执行。 newScheduledThreadPool 创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 12345//创建一个最小线程个数corePoolSize，最大为Integer.MAX_VALUE//阻塞队列为DelayedWorkQueue的线程池public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; demo： 1234567891011121314151617181920212223242526272829303132//多长时间之后执行一次@Slf4jpublic class ThreadPoolTest4 &#123; public static void main(String[] args) &#123; ScheduledExecutorService exec = Executors.newScheduledThreadPool(3); exec.schedule(new Runnable() &#123; @Override public void run() &#123; log.info("schedule run"); &#125; &#125;,3, TimeUnit.SECONDS); exec.shutdown(); &#125;&#125;//定时执行，这里是每隔3秒执行一次@Slf4jpublic class ThreadPoolTest4 &#123; public static void main(String[] args) &#123; ScheduledExecutorService exec = Executors.newScheduledThreadPool(3); exec.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; log.info("schedule run"); &#125; &#125;,1,3,TimeUnit.SECONDS);//一开始延迟1秒执行任务，之后每隔3秒执行一次任务，不适合调用exec.shutdown();，因为会被关闭 &#125;&#125; newSingleThreadScheduledExecutor 创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。 123456//创建一个最小线程个数corePoolSize为1，最大为Integer.MAX_VALUE//阻塞队列为DelayedWorkQueue的线程池。public static ScheduledExecutorService newSingleThreadScheduledExecutor() &#123; return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1));&#125; 同上。demo不再赘述。 5.4 线程池实现原理–线程池状态 static final int RUNNING = 0; 当创建线程池后，初始时，线程池处于RUNNING状态； static final int SHUTDOWN = 1; 如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕； static final int STOP = 2; 如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务； static final int TERMINATED = 3; 当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态。 6.5 线程池实现原理–任务的执行 corePoolSize与maximumPoolSize的关系举个简单的例子形象理解就是： 假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 如果说新任务数目增长的速度远远大于工人做任务的速度，那么此时工厂主管可能会想补救措施，比如重新招4个临时工人进来； 然后就将任务也分配给这4个临时工人做； 如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 这个例子中的corePoolSize就是10，而maximumPoolSize就是14（10+4）。 maximumPoolSize可以看作是线程池的一种补救措施，即任务量突然过大时的一种补救措施。 在ThreadPoolExecutor类中，最核心的任务提交方法是execute()方法，虽然通过submit也可以提交任务，但是实际上submit方法里面最终调用的还是execute()方法，所以我们只需要研究execute()方法的实现原理即可： 注：execute()方法和submit()方法已经在前面讲过区别了。 123456789101112131415161718192021222324252627282930313233343536373839404142public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); // // Proceed in 3 steps: // // 1. // 判断当前的线程数是否小于corePoolSize,如果是，使用入参任务通过addWord方法创建一个新的线程， // 如果能完成新线程创建exexute方法结束，成功提交任务 // 2. // 在第一步没有完成任务提交；状态为运行并且能够成功加入任务到工作队列后，再进行一次check，如果状态 // 在任务加入队列后变为了非运行（有可能是在执行到这里线程池shutdown了），非运行状态下当然是需要 // reject；然后再判断当前线程数是否为0（有可能这个时候线程数变为了0），如是，新增一个线程； // 3. // 如果不能加入任务到工作队列，将尝试使用任务新增一个线程，如果失败，则是线程池已经shutdown或者线程池 // 已经达到饱和状态，所以reject这个任务 // int c = ctl.get(); // 工作线程数小于核心线程数 if (workerCountOf(c) &lt; corePoolSize) &#123; // 直接启动新线程，true表示会再次检查workerCount是否小于corePoolSize if (addWorker(command, true)) return; c = ctl.get(); &#125; // 如果工作线程数大于等于核心线程数 // 线程的的状态为RUNNING并且队列notfull if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 再次检查线程的运行状态，如果不是RUNNING直接从队列中移除 int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) // 移除成功，拒绝该非运行的任务 reject(command); else if (workerCountOf(recheck) == 0) // 防止了SHUTDOWN状态下没有活动线程了，但是队列里还有任务没执行这种特殊情况。 // 添加一个null任务是因为SHUTDOWN状态下，线程池不再接受新任务 addWorker(null, false); &#125; // 如果队列满了或者是非运行的任务都拒绝执行 else if (!addWorker(command, false)) reject(command);&#125; 对应的程序流程图为： 为了理解更加得透彻，用下图配合文字总结一下： 1.如果当前运行的线程少于corePoolSize，则创建新线程来执行任务（注意，执行这一步骤需要获取全局锁）。 2.如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue。 3.如果无法将任务加入BlockingQueue（队列已满），则在非corePool中创建新的线程来处理任务（注意，执行这一步骤需要获取全局锁）。 4.如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，并调用RejectedExecutionHandler.rejectedExecution()方法。 ThreadPoolExecutor采取上述步骤的总体设计思路，是为了在执行execute()方法时，尽可能地避免获取全局锁（那将会是一个严重的可伸缩瓶颈）。在ThreadPoolExecutor完成预热之后（当前运行的线程数大于等于corePoolSize），几乎所有的execute()方法调用都是执行步骤2，而步骤2不需要获取全局锁。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从CAS到Atomic包原理]]></title>
    <url>%2F2019%2F02%2F12%2Fthread%2F%E4%BB%8ECAS%E5%88%B0Atomic%E5%8C%85%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[我们知道，volatile保证了可见性，但是不能保证原子性，在面对线程安全问题时，就显地力不从心，那么除了synchronized关键字外，还有什么方式可以实现线程安全更新呢？本文首先介绍CAS是什么，引出JUC下一个重要的包：Atomic包。 一、CAS简介 CAS（Compare and Swap），即比较并替换，实现并发算法时常用到的一种技术，Doug lea大神在java同步器中大量使用了CAS技术，鬼斧神工的实现了多线程执行的安全性。 CAS的思想很简单：三个参数，一个当前内存值V、旧的预期值A、即将更新的值B，当且仅当预期值A和内存值V相同时，将内存值修改为B并返回true，否则什么都不做，并返回false。 二、n++问题 通过javap -verbose Case看看add方法的字节码指令： 我们可以看到，n++被拆分成了下面几个指令： 执行getfield拿到原始n； 执行iadd进行加1操作； 执行putfield写把累加后的值写回n； 通过volatile修饰的变量可以保证线程之间的可见性，但并不能保证这3个指令的原子执行，在多线程并发执行下，无法做到线程安全，得到正确的结果，那么应该如何解决呢？ 这里顺便提一下线程安全三个特性 原子性：提供了互斥访问，同一时刻只能有一个线程来对它进行操作。 可见性：一个线程对主内存的修改可以及时地被其他线程观察到。 有序性：一个线程观察其他线程中的指令的执行顺序，由于指令重排序的存在，该观察结果一般杂乱无序。 可以看到原子性是线程安全的一大特性。 三、解决方案一 在add方法加上synchronized修饰解决。 这个方案当然可行，但是性能上差了点，还有其它方案么？ 四、解决方案二 我们可不可以用一下乐观锁的思想呢？即不加锁，等真正要赋值的时候比较一下。 当然了，这段代码如果真的在并发下执行，肯定出问题，只有把这整个过程变成一个原子操作才行，即同一时刻只有一个线程才能修改变量a。 如何实现呢？ 我们注意到JUC下有个好东西，以Atomic打头的一些类。就可以很好地帮助我们实现对一个数加一减一的原子性操作。比如我们要安全地对n加一，可以这样做： 下面就以AtomicInteger的实现为例，分析一下CAS是如何实现的。 Unsafe，是CAS的核心类，由于Java方法无法直接访问底层系统，需要通过本地（native）方法来访问，Unsafe相当于一个后门，基于该类可以直接操作特定内存的数据。 变量valueOffset，表示该变量值在内存中的偏移地址，因为Unsafe就是根据内存偏移地址获取数据的。 变量value用volatile修饰，保证了多线程之间的内存可见性。 看看AtomicInteger如何实现并发下的累加操作： 假设线程A和线程B同时执行getAndIncrement操作（分别跑在不同CPU上）： 假设AtomicInteger里面的value原始值为0，即主内存中AtomicInteger的value为0，根据Java内存模型，线程A和线程B各自持有一份value的副本，值为0。 线程A通过getIntVolatile(var1, var2)拿到value值0，这时线程A被挂起。 线程B也通过getIntVolatile(var1, var2)方法获取到value值0，运气好，线程B没有被挂起，并执行compareAndSwapInt方法比较内存值也为0，成功修改内存值为1。 这时线程A恢复，执行compareAndSwapInt方法比较，发现自己手里的值(0)和内存的值(1)不一致，说明该值已经被其它线程提前修改过了，那只能重新来一遍了。 重新获取value值，因为变量value被volatile修饰，所以其它线程对它的修改，线程A总是能够看到，线程A继续执行compareAndSwapInt进行比较替换，直到成功。 整个过程中，利用CAS保证了对于value的修改的并发安全，继续深入看看Unsafe类中的compareAndSwapInt方法实现。 我们看到是一个本地方法，并且在每个操作系统的具体实现都是不大一样的，这里我们就不再深究了。只要知道它的比较和替换是一个原子操作即可。 五、其他重要的Atomic类 5.1 LongAdder 上面提到了AtomicInteger，那么必然也存在``AtomicLong`。用法和原理是一样的。 既然用LongAddr也可以，但是为什么不使用AtomicLong呢？换句话说，为什么AtomicLong可以实现，还要有LongAddr这个类呢？？？ LongAddr优点：我们从AtomicInteger这个类的实现看到，他是在一个死循环内不停地尝试修改目标值，直到修改成功。如果竞争不激烈的时候，修改成功的几率很高。否则修改失败的概率就会很高。在大量修改失败的时候，多次尝试，性能会受到一定的影响。 对于普通类型的Long和Double变量，JVM允许将64位的读操作和写操作拆成两个32位的操作。 我们知道JUC下面提供的原子类都是基于Unsafe类实现的，并由Unsafe来提供CAS的能力。CAS (compare-and-swap)本质上是由现代CPU在硬件级实现的原子指令，允许进行无阻塞，多线程的数据操作同时兼顾了安全性以及效率。getAndAddLong方法会以volatile的语义去读需要自增的域的最新值，然后通过CAS去尝试更新，正常情况下会直接成功后返回，但是在高并发下可能会同时有很多线程同时尝试这个过程，也就是说线程A读到的最新值可能实际已经过期了，因此需要在while循环中不断的重试，造成很多不必要的开销。 将AtomicLong核心数据value分离成一个数组，每个线程访问时，通过hash等算法，映射到其中一个数字进行计数。最终的计数结果则为这个数组的求和累加。其中热点数据value会被分离成多个单元的cell，每个cell独自维护内部的值，当前对象的实际值由cell累计合成。这样，热点就得到有效的分离并提高了并行度。 LongAddr在AtomicLong基础上将单点的更新压力分散到各个节点上。低并发时通过对base直接更新，得到与AtomicLong一样的性能。 缺陷：统计的时候，如果有并发更新，会有统计的误差，例如获取一个全局唯一的ID还是采用`AtomicLong`更好一点。 5.2 AtomicReference 这个其实很简单，用法如下： 其实这个方法实现的是对一个共享对象的原子性操作，保证对象更新的原子性。 5.3 AtomicIntegerFieldUpdater 假设现在有这样的一个场景： 一百个线程同时对一个int对象进行修改，要求只能有一个线程可以修改。 可能有的同学会这么写： 我们来分析一下，对于volatile变量，写的时候会将线程本地内存的数据刷新到主内存上，读的时候会将主内存的数据加载到本地内存里，所以可以保证可见行和单个读/写操作的原子性。 但是上例中先 先判断:!ischanged 再执行赋值操作：ischanged=true 该组合操作就不能保证原子性了，也就是说线程A A1-&gt;A2 , 线程B B1-&gt;B2 (第一个操作为volatile读或者第二个操作为volatile写的时候，编译器不会对两个语句重排序，所以最后的执行顺序满足顺序一致性模型的)，但是最后的执行结果可能是A1-&gt;B1-&gt;A2-&gt;B2。不满足需求. 这种情况下，AtomicIntegerFieldUpdater就可以派上用场了。 对于这个代码的理解可以用下面这个代码来： 运行结果： 12update success 1:200update fail 用AtomicIntegerFieldUpdater.newUpdater指定类里面的属性。这里我们要更新Test类里面的A字段（必须是volatile且不是static对象）。update.compareAndSet()方法使用cas机制，每次提交的时候都比较下test.a是不是100，如果是，则更新。 注意，不能使用final变量，因为语义冲突。对于AtomicIntegerFieldUpdater和AtomicLongFieldUpdater只能修改int/long类型的字段，不能修改其包装类型（Integer/Long）。如果要修改包装类型就需要使用AtomicReferenceFieldUpdater。 5.4 AtomicStampedReference 对于上面说的AtomicInteger等存在一个问题就是ABA问题。 ABA问题：其他线程将A改为B，又重新改为了A，本线程用期望值A与之进行比较，发现是相等的，则进行下面的操作。因为这个值已经被改变过，这就是ABA问题。 解决：用个版本号来控制，来防止ABA问题。 5.5 AtomicBoolean 场景：若干个线程进来，但是这个方法只能执行一次。 好了，其实Atomic包最核心的思想就是用无阻塞的CAS来代替锁实现高性能操作，是实现线程安全的一种可行方法，理解了CAS原理和他们的基本用法和场景使用，基本就可以了。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[volatile详解]]></title>
    <url>%2F2019%2F02%2F11%2Fthread%2Fvolatile%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[volatile是比较重要的关键字，它涉及JMM，我们需要对其进行深入了解。 一、java内存模型JMM JMM本身是一种抽象的概念，并不真实存在，它描述的是一组规则或规范，通过这组规范定义了程序中各个变量(包括实例字段，静态字段和构成数组对象的元素)的访问方式。 请务必区分HMM和JAVA内存区域，JMM描述的是一组规则，围绕原子性、有序性以及可见性展开。 大多数的变量是只能存储在主内存中的，线程也不能直接去主内存中读取数据，而是获取数据的副本，每个线程对这个副本进行修改后，会在某个时机刷新回主内存。每个线程之间的工作内存的值是互不透明的，因此不能互相访问，线程间的通信必须通过主内存来完成。 二、JMM主内存和工作内存都放些什么 主内存 存储JAVA实例对象 包括实例变量、类信息、常量、静态变量等 属于数据共享的区域，多线程并发操作时会引起线程安全问题 工作内存 存储当前方法的所有本地变量信息，本地变量对其他线程不可见(方法里的基本数据类型会直接被存储在工作内存的栈帧结构中) 字节码行号指示器、Native方法信息 如果是引用类型，引用存储在工作内存中，实例存储在主内存中 属于线程私有数据区域，不存在线程安全问题 三、指令重排序 为了提高执行性能，JVM会进行一定的指令重排序，禁止方式就是加入内存屏障指令，下面会说。 当然了，指令重排序需要满足一定的条件： 在单线程环境下不能改变程序运行的结果 存在数据依赖关系的不允许重排序 无法通过happend-before原则推导出来的，才能进行指令的重排序。 四、happend-before 多线程有两个基本的问题，就是原子性和可见性，而happens-before规则就是用来解决可见性的。 即：在时间上，动作A发生在动作B之前，能不能保证B可以看见A？如果可以保证的话，那么就可以说hb(A,B) 12345678910111213141516class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; //1 flag = true; //2 &#125; public void reader() &#123; if (flag) &#123; //3 int i = a; //4 …… &#125; &#125;&#125; 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens before规则，这个过程建立的happens before 关系可以分为两类： 根据程序次序规则，1 happens before 2; 3 happens before 4。 根据volatile规则，2 happens before 3。 根据happens before 的传递性规则，1 happens before 4。 上述happens before 关系的图形化表现形式如下： 在上图中，每一个箭头链接的两个节点，代表了一个happens before 关系。黑色箭头表示程序顺序规则；橙色箭头表示volatile规则；蓝色箭头表示组合这些规则后提供的happens before保证。 这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 说了那么多，java中是如何保证这种可见性的呢？Volatile闪亮登场。 五、什么是volatile volatile关键字的目的是保证被它修饰的共享变量对所有线程总是可见的。 六、为什么要用volatile Volatile变量修饰符如果使用恰当的话，它比synchronized的使用和执行成本会更低，因为它不会引起线程上下文的切换和调度。 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序。 七、volatile如何保证可见性 voliatile关键字保证了在进程中变量的变化的可见性。 在多线程的应用里，如果线程操作了一个没有被volatile关键字标记的变量，那么每个线程都会在使用到这个变量时从主存里拷贝这个变量到CPU的cache里面（为了性能！CPU缓存可比内存快多了）。如果你的电脑有多于一个CPU，那么每个线程都会在不同的CPU上面运行，这意味着每个线程都会把这个变量拷贝到不同的CPU cache里面，正如下图所示： 一个不带有volatile关键字的变量在JVM从主存里面读取数据到CPU cache或者从cache里面写入数据到主存时是没有保证的。 想象这样一个场景，当一到两个线程允许去共享一个包含了一个计数变量的对象，这个计数变量如下所定义 12345public class SharedObject &#123; public int counter = 0; //无关键字&#125; 然后，这线程一增加了counter变量的值，但是，但是同时线程一和线程二都有可能随时读取这个counter变量。 如果这个counter变量未曾使用volatile声明，那么我们就无法保证这个变量在两个线程中所位于的CPU的cache和主存中的值是否保持一致了。示意图如下： 那么部分的线程就不能看到这个变量最新的样子，因为这个变量还没有被线程写回到主存中，这就是可见性的问题，这个线程更新的变量对于其他线程是不可视的。 在声明了counter变量的volatile关键字后，所有写入到counter变量的值会被立即写回到主存中。同时，所有读取这个变量的线程会先把对应的工作内存置为无效，从主存里面读取这个变量，下面的代码就是声明带volatile关键字的变量的方法 12345public class SharedObject &#123; public volatile int counter = 0;&#125; 如此声明这个变量就保证了这个变量对于其他写这个变量的线程的可见性。 总结： 处理器为了提高处理速度，不直接和内存进行通讯，而是先将系统内存的数据读到内部缓存（L1,L2或其他）后再进行操作，但操作完之后不知道何时会写到内存，如果对声明了Volatile变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题，所以在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作的时候，会强制重新从系统内存里把数据读到处理器缓存里。 八、来详细说说volatile写-读的内存语义 volatile写的内存语义如下： 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 以上面示例程序VolatileExample为例，假设线程A首先执行writer()方法，随后线程B执行reader()方法，初始时两个线程的本地内存中的flag和a都是初始状态。下图是线程A执行volatile写后，共享变量的状态示意图： 如上图所示，线程A在写flag变量后，本地内存A中被线程A更新过的两个共享变量的值被刷新到主内存中。此时，本地内存A和主内存中的共享变量的值是一致的。 volatile读的内存语义如下： 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 如上图所示，在读flag变量后，本地内存B已经被置为无效。此时，线程B必须从主内存中读取共享变量。线程B的读取操作将导致本地内存B与主内存中的共享变量的值也变成一致的了。 如果我们把volatile写和volatile读这两个步骤综合起来看的话，在读线程B读一个volatile变量后，写线程A在写这个volatile变量之前所有可见的共享变量的值都将立即变得对读线程B可见。 下面对volatile写和volatile读的内存语义做个总结： 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了（其对共享变量所在修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（在写这个volatile变量之前对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息。 九、volatile如何禁止指令重排序 这就不得不提一个指令叫做：内存屏障了。 它可就厉害了， 保证特定操作的执行顺序 保证某些变量的内存可见性 通过插入内存屏障指令禁止在内存屏障前后的指令执行重排序优化。 这个指令对编译器和CPU的执行都是起作用的，可用强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本。 因此，从根本上来说，是内存屏障指令实现了volatile的可见性和禁止指令重排序的。 十、volatile的应用场景 volatile关键字只能对32位和64位的变量使用 synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件： 1）对变量的写操作不依赖于当前值 2）该变量没有包含在具有其他变量的不变式中 下面列举几个Java中使用volatile的几个场景。 ①.状态标记量 123456789volatile boolean flag = false; //线程1while(!flag)&#123; doSomething();&#125; //线程2public void setFlag() &#123; flag = true;&#125; ②.单例模式中的double check 1234567891011121314151617class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton();//非原子操作 &#125; &#125; return instance; &#125;&#125; instance = new Singleton();//非原子操作 执行这一句，JVM发生了如下事情： 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地出错了，不再是单例了。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读写锁ReentrantReadWriteLock]]></title>
    <url>%2F2019%2F02%2F11%2Fthread%2F%E8%AF%BB%E5%86%99%E9%94%81ReentrantReadWriteLock%2F</url>
    <content type="text"><![CDATA[读写锁的出现是为了提高性能，思想是：读读不互斥，读写互斥，写写互斥。本文来了解一下读写锁的使用和锁降级的概念。 1. 锁的分类 排他锁：在同一时刻只允许一个线程进行访问，其他线程等待； 读写锁：在同一时刻允许多个读线程访问，但是当写线程访问，所有的写线程和读线程均被阻塞。读写锁维护了一个读锁加一个写锁，通过读写锁分离的模式来保证线程安全，性能高于一般的排他锁。 2. 读写锁 我们对数据的操作无非两种：“读”和“写”，试想一个这样的情景，当十个线程同时读取某个数据时，这个操作应不应该加同步。答案是没必要的。只有以下两种情况需要加同步： 这十个线程对这个公共数据既有读又有写 这十个线程对公共数据进行写操作 以上两点归结起来就一点就是有对数据进行改变的操作就需要同步 所以 java5提供了读写锁这种锁支持多线程读操作不互斥，多线程读写互斥，多线程写互斥。读操作不互斥这样有助于性能的提高，这点在java5以前没有。 3. java并发包提供的读写锁 java并发包提供了读写锁的具体实现ReentrantReadWriteLock，它主要提供了一下特性： 公平性选择：支持公平和非公平（默认）两种获取锁的方式，非公平锁的吞吐量优于公平锁； 可重入：支持可重入，读线程在获取读锁之后能够再次获取读锁，写线程在获取了写锁之后能够再次获取写锁，同时也可以获取读锁； 锁降级：线程获取锁的顺序遵循获取写锁，获取读锁，释放写锁，写锁可以降级成为读锁。 4. 先看个小例子 读取数据和写入数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.HashMap;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;public class Demo &#123; //定义一个map用来读取和存放数据 private HashMap&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); //实例化ReentrantReadWriteLock private ReadWriteLock rwl = new ReentrantReadWriteLock(); //根据实例化对象分别获取读锁和写锁 private Lock r = rwl.readLock(); private Lock w = rwl.writeLock(); //读取数据 public void get(String key)&#123; //上读锁 r.lock(); System.out.println(Thread.currentThread().getName()+" 读操作开始执行"); try&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //读取数据 System.out.println(map.get(key)); &#125;finally &#123; //解读锁 r.unlock(); System.out.println(Thread.currentThread().getName()+" 读操作执行完毕"); &#125; &#125; //存入数据，即写数据 public void put(String key,String value)&#123; //上写锁 w.lock(); System.out.println(Thread.currentThread().getName()+" 写操作开始执行"); try&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //写数据 map.put(key, value); &#125;finally&#123; //解写锁 w.unlock(); System.out.println(Thread.currentThread().getName()+" 写操作执行完毕"); &#125; &#125; &#125; Main进行创建多线程测试：先来测试一下存在写的情况(只有写或者写读都有) 1234567891011121314151617181920212223242526272829303132333435363738public class Main &#123; public static void main(String[] args) &#123; Demo demo = new Demo(); //写 new Thread(new Runnable() &#123; @Override public void run() &#123; demo.put("key1", "value1"); &#125; &#125;).start(); //读 new Thread(new Runnable() &#123; @Override public void run() &#123; demo.get("key1"); &#125; &#125;).start(); //写 new Thread(new Runnable() &#123; @Override public void run() &#123; demo.put("key2", "value2"); &#125; &#125;).start(); //写 new Thread(new Runnable() &#123; @Override public void run() &#123; demo.put("key3", "value3"); &#125; &#125;).start(); &#125;&#125; 执行结果： 123456789Thread-0 写操作开始执行Thread-0 写操作执行完毕Thread-1 读操作开始执行value1Thread-1 读操作执行完毕Thread-2 写操作开始执行Thread-2 写操作执行完毕Thread-3 写操作开始执行Thread-3 写操作执行完毕 分析： 发现存在写的情况，那么就是一个同步等待的过程，即开始执行，然后等待3秒，执行完毕，符合第2个目录中提到的规则。 对只有读操作的情形进行测试 123456789101112131415161718192021222324252627282930public class Main &#123; public static void main(String[] args) &#123; Demo demo = new Demo(); demo.put("key1", "value1"); demo.put("key2", "value2"); demo.put("key3", "value3"); new Thread(new Runnable() &#123; @Override public void run() &#123; demo.get("key1"); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; demo.get("key2"); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; demo.get("key3"); &#125; &#125;).start(); &#125;&#125; 运行结果： 123456789Thread-0 读操作开始执行Thread-1 读操作开始执行Thread-2 读操作开始执行value1Thread-0 读操作执行完毕value2Thread-1 读操作执行完毕value3Thread-2 读操作执行完毕 分析 在主线程中先put进去几个数用于读的测试，下面开辟三个读线程，我们可以从执行结果中发现，其中一个线程进去之后，另外的线程能够立即再次进入，即这三把锁不是互斥的。 5. 锁降级 锁降级是指写锁将为读锁。 锁降级：从写锁变成读锁；锁升级：从读锁变成写锁。读锁是可以被多线程共享的，写锁是单线程独占的。也就是说写锁的并发限制比读锁高，这可能就是升级/降级名称的来源。 如下代码会产生死锁，因为同一个线程中，在没有释放读锁的情况下，就去申请写锁，这属于锁升级，ReentrantReadWriteLock是不支持的。 12345ReadWriteLock rtLock = new ReentrantReadWriteLock(); rtLock.readLock().lock(); //上读锁System.out.println("get readLock."); rtLock.writeLock().lock(); //读锁还没有释放，不允许上死锁System.out.println("blocking"); ReentrantReadWriteLock支持锁降级，如下代码不会产生死锁。 123456ReadWriteLock rtLock = new ReentrantReadWriteLock(); rtLock.writeLock().lock(); //上写锁System.out.println("writeLock"); rtLock.readLock().lock(); //可以在写锁没有释放的时候立即上读锁System.out.println("get read lock"); 利用这个机制：同一个线程中，在没有释放读锁的情况下，就去申请写锁，这属于锁升级，ReentrantReadWriteLock是不支持的。 在写锁没有释放的时候，先获取到读锁，然后再释放写锁，保证后面读到的数据的一致性。 123456789101112131415private volatile boolean isUpdate;public void readWrite()&#123; r.lock();//为了保证isUpdate能够拿到最新的值 if(isUpdate)&#123; r.unlock(); w.lock(); map.put("xxx","xxx"); r.lock();//写锁还没有释放，立即获取读锁，阻塞本线程，保证本线程下面读的一致性 w.unlock(); &#125; String value = map.get("xxx"); //读到的数据是本线程自己更新的数据，不会被其他线程打扰 System.out.println(value); r.unlock();&#125;]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从ReentrantLock引出AQS的原理]]></title>
    <url>%2F2019%2F02%2F11%2Fthread%2F%E4%BB%8EReentrantLock%E5%BC%95%E5%87%BAAQS%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[如果对并发编程稍微熟悉的话，就不会对ReentrantLock陌生，也可能对一些组件比如CountDownLatch,FutureTask以及Semaphore等同步组件耳闻过，他们都是JUC包下的类或者工具，他们都有一个共同的基础：AQS，即AbstractQueuedSynchronizer，从今天开始，让我们记住它，并且尝试去理解它。 一、ReentrantLock 首先我们先来看看ReentrantLock这个可重入锁的性质和使用，因为它往往会在面试中被面试官拿来同synchronized相比较。如果这种基本的比较都不知道的话，那就没有后续深入的探讨了，面试可能也会结束了。 它的用法极其简单，如下： 他们两兄弟的区别是： synchronized是关键字，ReentrantLock是一个类 ReentrantLock可以对获取锁的等待时间进行设置，避免死锁 ReentrantLock可以获取各种锁的信息 ReentrantLock可以灵活地实现多路通知 机制：synchronized操作MarkWord，lock调用Unsafe类的park()方法 ReentrantLock可以设置锁的公平性 ReentrantLock调用lock()之后必须调用unlock()释放锁 性能上ReentrantLock未必就比synchronized高，他们都是可重入的 可以看出，ReentrantLock更加灵活，可以更加细腻度操作锁，而synchronized看起来则相对比较笨拙，但是笨拙的是简单的，不存在忘记释放锁的问题。可谓存在即合理嘛！ 针对上文中提到的Unsafe类，其中最经典的一个方法是：compareAndSwapXXX这类CAS方法，它其实是JAVA留的一个后门，它可以直接操作内存，因此如果普通开发者拿来用的话，可能会出现各种问题，因此被成为不安全的类。 好了，关于区别已经说的差不多了，下面我们就要来真格的了，首先来翻翻源码。前方高能预警，请非战斗人员紧急撤离现场，老司机要开车了。 首先呢，我们来看看lock()方法的实现是： 123public void lock() &#123; sync.lock();&#125; 这里多了一个东西叫Sync，Sync为ReentrantLock里面的一个内部类，它继承AQS，它有两个子类：公平锁FairSync和非公平锁NonfairSync。 ReentrantLock里面大部分的功能都是委托给Sync来实现的，同时Sync内部定义了lock()抽象方法由其子类去实现，默认实现了nonfairTryAcquire(int acquires)方法，可以看出它是非公平锁的默认实现方式。 几乎每一个方法都是通过sync.xxx来实现的，而Sync这个内部类在AQS的基础上增加一些东西而已，所以本质上都是基于AQS来实现的。 不仅仅是这个，JUC包基本都是以AQS为基础构成，因此AQS可以理解为JUC的一个实现框架。既然AQS这么重要，下面有必要挖地三尺掘出它的原理。 二、AQS简介 java的内置锁一直都是备受争议的，在JDK 1.6之前，synchronized这个重量级锁性能一直都是较为低下，虽然在1.6后，进行大量的锁优化策略,但是与Lock相比synchronized还是存在一些缺陷的：虽然synchronized提供了便捷性的隐式获取锁释放锁机制（基于JVM机制），但是它却缺少了获取锁与释放锁的可操作性，可中断、超时获取锁，且它为独占式在高并发场景下性能大打折扣。 AQS：AbstractQueuedSynchronizer，即队列同步器。它是构建锁或者其他同步组件的基础框架（如ReentrantLock、ReentrantReadWriteLock、 Semaphore等），JUC并发包的作者（Doug Lea）期望它能够成为实现大部分同步需求的基础。它是JUC并发包中的核心基础组件。 AQS解决了在实现同步器时涉及当的大量细节问题，例如获取同步状态、FIFO同步队列。基于AQS来构建同步器可以带来很多好处。它不仅能够极大地减少实现工作，而且也不必处理在多个位置上发生的竞争问题。 AQS的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状态。 AQS使用一个int类型的成员变量state来表示同步状态，当state&gt;0时表示已经获取了锁，当state = 0时表示释放了锁。它提供了三个方法（getState()、setState(int newState)、compareAndSetState(int expect,int update)）来对同步状态state进行操作，当然AQS可以确保对state的操作是安全的。 AQS通过内置的FIFO同步队列来完成资源获取线程的排队工作，如果当前线程获取同步状态失败（锁）时，AQS则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。 三、CLH同步队列 CLH同步队列是一个FIFO双向队列，AQS依赖它来完成同步状态的管理，当前线程如果获取同步状态失败时，AQS则会将当前线程已经等待状态等信息构造成一个节点（Node）并将其加入到CLH同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点唤醒（公平锁），使其再次尝试获取同步状态。 在CLH同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、状态（waitStatus）、前驱节点（prev）、后继节点（next），CLH同步队列结构图如下： 举例理解：假设目前有三个线程Thread1、Thread2、Thread3同时去竞争锁，如果结果是Thread1获取了锁，Thread2和Thread3进入了等待队列，那么他们的样子如下： AQS的等待队列基于一个双向链表实现的，HEAD节点不关联线程，后面两个节点分别关联Thread2和Thread3，他们将会按照先后顺序被串联在这个队列上。这个时候如果后面再有线程进来的话将会被当做队列的TAIL。 四、入列 12345678910111213141516private Node addWaiter(Node mode) &#123; //新建Node Node node = new Node(Thread.currentThread(), mode); //快速尝试添加尾节点 Node pred = tail; if (pred != null) &#123; node.prev = pred; //CAS设置尾节点 if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; addWaiter(Node node)先通过快速尝试设置尾节点，如果失败，则调用enq(Node node)方法设置尾节点: 123456789101112131415161718private Node enq(final Node node) &#123; //多次尝试，直到成功为止 for (;;) &#123; Node t = tail; //tail不存在，设置为首节点 if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; //设置为尾节点 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 其实就很明了了，首先是尝试快速用CAS设置当前的节点为尾节点，但是可能存在并发问题设置不成功，下面用死循环的方式不断地尝试添加节点并且设置为尾节点，直到成功。 过程如下： 五、出列 CLH同步队列遵循FIFO，首节点的线程释放同步状态后，将会唤醒它的后继节点（next），而后继节点将会在获取同步状态成功时将自己设置为首节点，这个过程非常简单，head执行该节点并断开原首节点的next和当前节点的prev即可，注意在这个过程是不需要使用CAS来保证的，因为只有一个线程能够成功获取到同步状态。 其实这里按照源码的解释，是将第一个获取到同步状态的node作为新的head，然后将原来的head置空。 六、同步状态的获取与释放 在前面提到过，AQS是构建Java同步组件的基础，我们期待它能够成为实现大部分同步需求的基础。AQS的设计模式采用的模板方法模式，子类通过继承的方式，实现它的抽象方法来管理同步状态，对于子类而言它并没有太多的活要做，AQS提供了大量的模板方法来实现同步，主要是分为三类：独占式获取和释放同步状态、共享式获取和释放同步状态、查询同步队列中的等待线程情况。自定义子类使用AQS提供的模板方法就可以实现自己的同步语义。 下面具体来解释一下独占式和共享式的含义 在具体分析之前，我们先解释两种同步的方式，独占模式和共享模式： 独占模式：资源是独占的，一次只能一个线程获取。 共享模式：同时可以被多个线程获取，具体的资源的个数可以通过参数指定。 如果我们自己实现一个同步器的框架，我们怎么设计呢？下面可能是我们想到的比较通用的设计方案（独占模式）: 定义一个变量int state=0，使用这个变量表示被获取的资源的数量。 线程在获取资源前要先检查state的状态，如果为0，则修改为1，表示获取资源成功，否则表示资源已经被其他线程占用，此时线程要堵塞以等待其他线程释放资源。 为了能使得资源释放后找到那些为了等待资源而堵塞的线程，我们把这些线程保存在FIFO队列中。 当占有资源的线程释放掉资源后，可以从队列中唤醒一个堵塞的线程，由于此时资源已经释放，因此这个被唤醒的线程可以获取资源并且执行。 这个state变量到底是什么呢？ 当AQS的子类实现独占功能时，如ReentrantLock，资源是否可以被访问被定义为：只要AQS的state变量不为0，并且持有锁的线程不是当前线程，那么代表资源不可访问。此时，state是用来表示当前线程获取锁的可重入次数； 当AQS的子类实现共享功能时，如CountDownLatch，资源是否可以被访问被定义为：只要AQS的state变量不为0，那么代表资源不可以为访问。此时，state用来表示当前计数器的值。 七、独占式-独占式同步状态获取 独占式，同一时刻仅有一个线程持有同步状态。 独占式同步状态获取acquire(int arg)方法为AQS提供的模板方法，该方法为独占式获取同步状态，但是该方法对中断不敏感，也就是说由于线程获取同步状态失败加入到CLH同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移除。代码如下： 1234567public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) &#123; selfInterrupt(); &#125;&#125; tryAcquire：去尝试获取锁，获取成功则设置锁状态并返回true，否则返回false。该方法自定义同步组件自己实现(ReentrantLock中实现公平锁和非公平锁就是分别重写了这个方法实现的，下面看ReentrantLock的原理的时候就明白了)，该方法必须要保证线程安全的获取同步状态。 addWaiter：如果tryAcquire返回FALSE（获取同步状态失败），则调用该方法将当前线程加入到CLH同步队列尾部。 acquireQueued：当前线程会根据公平性原则来进行阻塞等待（自旋）,直到获取锁为止；并且返回当前线程在等待过程中有没有中断过。 selfInterrupt：产生一个中断。 对这里的acquireQueued有疑惑，下面来看看它做了什么。acquireQueued方法为一个自旋的过程，也就是说当前线程（Node）进入同步队列后，就会先进入一个自旋的过程，每个节点都会自省地观察，当条件满足，获取到同步状态后，就可以从这个自旋过程中退出。 1234567891011121314151617181920212223242526272829final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; //获取当前节点node的前驱结点p final Node p = node.predecessor(); //如果p确实是head，那说明当前节点node是可用的第一个线程 //即为当前队列的第一个线程，则最先处理它 //当前线程则尝试获取同步状态 if (p == head &amp;&amp; tryAcquire(arg)) &#123; //从这里可以看出，更新当前节点为头节点 //将原来头节点的next引用置空以供JVM回收 //具体见出列小标题下的示意图 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; //如果前驱节点不是头节点就继续阻塞继续等待呗 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 从上面代码中可以看到，当前线程会一直尝试获取同步状态，当然前提是只有其前驱节点为头结点才能够尝试获取同步状态，理由： 保持FIFO同步队列原则。 头节点释放同步状态后，将会唤醒其后继节点，后继节点被唤醒后需要检查自己是否为头节点。 对这个的理解简单来说就是： 在AQS中维护着一个FIFO的同步队列，当线程获取同步状态失败后，则会加入到这个CLH同步队列的队尾并一直保持着自旋。在CLH同步队列中的线程在自旋时会判断其前驱节点是否为首节点，如果当前节点的前驱节点就是头节点，则表明当前节点是当前队列中的第一个可用线程，则让其不断尝试获取同步状态，如果获取到，则退出CLH同步队列。当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点。 继续，我们看到，如果发现前驱节点并不是head，那么就说明是比较靠后的节点了，这个时候，很有可能需要一段时间之后才会用到它，所以根本不需要再参与自旋浪费CPU的性能了，即下面一个if: 123if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; 通过这段代码我们可以看到，在获取同步状态失败后，线程并不是立马进行阻塞，需要检查该线程的状态，检查状态的方法为 shouldParkAfterFailedAcquire(Node pred, Node node) 方法，该方法主要靠前驱节点判断当前线程是否应该被阻塞，代码如下： 12345678910111213141516171819private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; //前驱节点 int ws = pred.waitStatus; //状态为signal，表示当前线程处于等待状态，直接放回true if (ws == Node.SIGNAL) return true; //前驱节点状态 &gt; 0 ，则为Cancelled,表明该节点已经超时或者被中断了，需要从同步队列中取消 if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; //前驱节点状态为Condition、propagate else &#123; compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; 这段代码主要检查当前线程是否需要被阻塞，具体规则如下： 如果当前线程的前驱节点状态为SINNAL，则表明当前线程需要被阻塞，调用unpark()方法唤醒，直接返回true，当前线程阻塞 如果当前线程的前驱节点状态为CANCELLED（ws &gt; 0），则表明该线程的前驱节点已经等待超时或者被中断了，则需要从CLH队列中将该前驱节点删除掉，直到回溯到前驱节点状态 &lt;= 0 ，返回false 如果前驱节点非SINNAL，非CANCELLED，则通过CAS的方式将其前驱节点设置为SINNAL，返回false 针对pred.waitStatus的几种状态： 1234567891011/** waitStatus value to indicate thread has cancelled */static final int CANCELLED = 1;/** waitStatus value to indicate successor's thread needs unparking */static final int SIGNAL = -1;/** waitStatus value to indicate thread is waiting on condition */static final int CONDITION = -2;/** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */static final int PROPAGATE = -3; 如果 shouldParkAfterFailedAcquire(Node pred, Node node) 方法返回true，则调用parkAndCheckInterrupt()方法阻塞当前线程： 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; parkAndCheckInterrupt() 方法主要是把当前线程挂起，从而阻塞住线程的调用栈，同时返回当前线程的中断状态。其内部则是调用LockSupport工具类的park()方法来阻塞该方法。 那么，此时，当第一个线程已经执行完毕，释放锁了，就需要唤醒队列中后继节点： 12345678910public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) //唤醒后继节点 unparkSuccessor(h); return true; &#125; return false;&#125; 调用unparkSuccessor(Node node)唤醒后继节点： 123456789101112131415161718192021private void unparkSuccessor(Node node) &#123; //当前节点状态 int ws = node.waitStatus; //当前状态 &lt; 0 则设置为 0 if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); //当前节点的后继节点 Node s = node.next; //后继节点为null或者其状态 &gt; 0 (超时或者被中断了) if (s == null || s.waitStatus &gt; 0) &#123; s = null; //从tail节点来找可用节点 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; //唤醒后继节点 if (s != null) LockSupport.unpark(s.thread);&#125; 可能会存在当前线程的后继节点为null，超时、被中断的情况，如果遇到这种情况了，则需要跳过该节点，但是为何是从tail尾节点开始，而不是从node.next开始呢？原因在于node.next仍然可能会存在null或者取消了，所以采用tail回溯办法找第一个可用的线程。最后调用LockSupport的unpark(Thread thread)方法唤醒该线程。 从上面我可以看到，当需要阻塞或者唤醒一个线程的时候，AQS都是使用LockSupport这个工具类来完成的。 LockSupport定义了一系列以park开头的方法来阻塞当前线程，unpark(Thread thread)方法来唤醒一个被阻塞的线程。这些方法的实现都是通过Unsafe类调用native方法来实现的。 好了，至此就完完全全地搞明白了独占式同步状态获取acquire(int arg)方法的原理，特别是其中节点如何进出、队列第一个节点如何尝试获取同步状态、如何阻塞后继线程以及如何唤醒。 八、独占式获取响应中断 AQS提供了acquire(int arg)方法以供独占式获取同步状态，但是该方法对中断不响应，对线程进行中断操作后，该线程会依然位于CLH同步队列中等待着获取同步状态。为了响应中断，AQS提供了acquireInterruptibly(int arg)方法，该方法在等待获取同步状态时，如果当前线程被中断了，会立刻响应中断抛出异常InterruptedException。 具体原理就不深究了，其实源码跟上面个相差不大，只是不再是使用interrupted标志，而是直接抛出InterruptedException异常。再深究这博客没法继续写啦。 九、独占式超时获取 AQS除了提供上面两个方法外，还提供了一个增强版的方法：tryAcquireNanos(int arg,long nanos)。该方法为acquireInterruptibly方法的进一步增强，它除了响应中断外，还有超时控制。即如果当前线程没有在指定时间内获取同步状态，则会返回false，否则返回true。 针对超时控制，程序首先记录唤醒时间deadline :deadline = System.nanoTime() +nanosTimeout（时间间隔）。 如果获取同步状态失败，则需要计算出需要休眠的时间间隔nanosTimeout = deadline - System.nanoTime()，如果nanosTimeout &lt;= 0 表示已经超时了，返回false; 如果大于spinForTimeoutThreshold(1000L)则需要休眠nanosTimeout ; 如果nanosTimeout &lt;= spinForTimeoutThreshold ，就不需要休眠了，直接进入快速自旋的过程。原因在于 spinForTimeoutThreshold 已经非常小了，非常短的时间等待无法做到十分精确，如果这时再次进行超时等待，相反会让nanosTimeout 的超时从整体上面表现得不是那么精确，所以在超时非常短的场景中，AQS会进行无条件的快速自旋。 流程图如下： 十、共享式-共享式同步状态获取 共享式与独占式的最主要区别在于同一时刻独占式只能有一个线程获取同步状态，而共享式在同一时刻可以有多个线程获取同步状态。例如读操作可以有多个线程同时进行，而写操作同一时刻只能有一个线程进行写操作，其他操作都会被阻塞。 AQS提供acquireShared(int arg)方法共享式获取同步状态： 1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 从上面程序可以看出，方法首先是调用tryAcquireShared(int arg)方法尝试获取同步状态，如果获取失败则调用doAcquireShared(int arg)自旋方式获取同步状态，共享式获取同步状态的标志是返回 &gt;= 0 的值表示获取成功。自旋方式获取同步状态如下： 123456789101112131415161718192021222324252627private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 逻辑几乎和独占式锁的获取一模一样，这里的自旋过程中能够退出的条件是当前节点的前驱节点是头结点并且tryAcquireShared(arg)返回值大于等于0即能成功获得同步状态。 acquireShared(int arg)方法不响应中断，与独占式相似，AQS也提供了响应中断、超时的方法，分别是：acquireSharedInterruptibly(int arg)、tryAcquireSharedNanos(int arg,long nanos)，这里就不做解释了。 十一、共享式同步状态释放 获取同步状态后，需要调用release(int arg)方法释放同步状态，方法如下： 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 因为可能会存在多个线程同时进行释放同步状态资源，所以需要确保同步状态安全地成功释放，一般都是通过CAS和循环来完成的。 十二、再回过头来看看ReentrantLock的原理 在对AQS原理进行大概了梳理之后，再来理解ReentrantLock就比较容易了，因为大部分的事情都由AQS做完了，剩下的只要重写几个个性化的方法即可。 还是要看看最核心的方法：lock()方法 123public void lock() &#123; sync.lock();&#125; 下面来看看这个lock()，一点点进了抽象静态内部类Sync中去了： 1abstract void lock(); 上面说过，ReentrantLock里面大部分的功能都是委托给Sync来实现的，同时Sync内部定义了lock()抽象方法由其子类去实现的，所以这个lock方法的具体实现是在子类中完成的。Sync的子类有NonfairSync和FairSync这两个，一看就知道了，一个是非公平一个是公平。 十三、非公平锁 先来看看比较简单的非公平锁： 123456789101112131415161718static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() &#123; if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125; 我们看到，这个lock()方法里面首先用CAS尝试获取锁，获取不到则执行acquire()方法，这个方法就恰好是完全由AQS实现，那么就回到了上面介绍过的内容了。这里为了方便再贴一下源码： 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 首先就是调用tryAcquire()这个方法，即尝试获取锁，这个方法上面也提过，是留给具体的类自己去实现的，所以我们还要回到ReentrantLock中来看看，果然，在上面贴的NonfairSync这个类中对这个方法进行了重写。即： 123protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125; 调用的方法就是实现尝试获取锁的核心代码： 123456789101112131415161718192021222324final boolean nonfairTryAcquire(int acquires) &#123; //当前线程 final Thread current = Thread.currentThread(); //获取同步状态 int c = getState(); //state == 0,表示该锁未被任何线程占有，该锁能被当前线程获取 if (c == 0) &#123; //获取锁成功，设置为当前线程所有 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //线程重入 //判断锁持有的线程是否为当前线程 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 就很简单了，值得注意的是，为了支持重入性，在第二步增加了处理逻辑，如果该锁已经被线程所占有了，会继续检查占有线程是否为当前线程，如果是的话，同步状态加1返回true，表示可以再次获取成功。每次重新获取都会对同步状态进行加一的操作。 另外需要注意的是，这是非公平锁，就是说，一个线程进来，可能是比先进来的线程先获取锁，就像在开车的时候，总是会由一些车插到你的前面一样。但是如果它没有获取锁，则入队。 那么尝试获取锁的逻辑我们知道了，那么释放锁呢？ 123456789101112131415protected final boolean tryRelease(int releases) &#123; //1. 同步状态减1 int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; //2. 只有当同步状态为0时，锁成功被释放，返回true free = true; setExclusiveOwnerThread(null); &#125; // 3. 锁未被完全释放，返回false setState(c); return free;&#125; 需要注意的是，重入锁的释放必须得等到同步状态为0时锁才算成功释放，否则锁仍未释放。如果锁被获取n次，释放了n-1次，该锁未完全释放返回false，只有被释放n次才算成功释放，返回true。 十四、公平锁 何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO。ReentrantLock的构造方法无参时是构造非公平锁。 提供了有参构造函数，可传入一个boolean值，true时为公平锁，false时为非公平锁，源码为： 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 在上面非公平锁获取时（nonfairTryAcquire方法）只是简单的获取了一下当前状态做了一些逻辑处理，并没有考虑到当前同步队列中线程等待的情况。我们来看看公平锁的处理逻辑是怎样的，核心方法为： 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 这段代码的逻辑与nonfairTryAcquire基本上一致，唯一的不同在于增加了hasQueuedPredecessors的逻辑判断，方法名就可知道该方法用来判断当前节点在同步队列中是否有前驱节点的判断，如果有前驱节点说明有线程比当前线程更早的请求资源，根据公平性，当前线程请求资源失败。如果当前节点没有前驱节点的话，再才有做后面的逻辑判断的必要性。公平锁每次都是从同步队列中的第一个节点获取到锁，而非公平性锁则不一定，有可能刚释放锁的线程能再次获取到锁。 十五、公平锁 VS 非公平锁 公平锁每次获取到锁为同步队列中的第一个节点，保证请求资源时间上的绝对顺序，而非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，造成“饥饿”现象。 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。因此，ReentrantLock默认选择的是非公平锁，则是为了减少一部分上下文切换，保证了系统更大的吞吐量。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从底层理解synchronized]]></title>
    <url>%2F2019%2F02%2F11%2Fthread%2F%E4%BB%8E%E5%BA%95%E5%B1%82%E7%90%86%E8%A7%A3synchronized%2F</url>
    <content type="text"><![CDATA[上一章了解了synchronized的基本使用方式之后，接下来我们来深入了解了解其底层原理，并且说明对它的优化。 一、synchronized底层实现原理 首先给出一个不是结论的结论，synchronized的实现基础是：JAVA对象头和Monitor，理解了这两者的作用就理解了synchronized的实现原理。下面进行详细讲解。 ⭐然后在正式开始之前，先介绍一下锁的内存语义： 当线程释放锁时，JAVA内存模型会把该线程对应额本地内存中的共享变量刷新到主内存中 当线程获取锁时，JAVA内存模型会把该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。 在JAVA内存模型-线程共享这篇文章中介绍了对象头里面的基本构成。 我们着重看一下对象头，下面两个这里不需要关心。我们可以看到一个关键字：锁状态标志。因此Mark Word是实现锁的关键了。 我们也知道，Mark Word是一个可变的结构，可变的部分主要有如下： 其中，偏向所和轻量级锁是JDK1.6之后对synchronized优化所新加的，后文会探讨对synchronized的优化。 OK，到这里我们知道了每个对象区域的对象头这一块存储了关于锁的信息，即锁状态。仔细看表格，比如重量级锁，就是我们熟知的synchronized对象锁，它的说明是：指向重量级锁的指针。那这个锁是什么呢？指向的是什么位置呢？这个就不得不提及第二个关键字啦：Monitor Monitor:每个对象打娘胎生下来就自带了一把看不见的锁，成为内部锁或者Monitor锁，也称为管程或者监视器锁。我们可以理解为一种同步工具，也可以理解为同步对象。 那么回到上面的问题上来，这个指针指向的就是Monitor对象的起始地址，因此，每个对象都会存在一个Monitor与之关联，当这个Monitor被一个线程持有时，它就会处于锁定状态。 在Hotspot虚拟机中，这个Monitor是由ObjectMonitor实现的，位于虚拟机源码中，用C++实现。我们一起来看看吧！ 这个源码地址为：objectMonitor.hpp 我们看到了几个比较重要的关键字，首先，每个等待获取锁的线程都会被封装为ObjectWaiter对象。_WaitSet就是之前说的所有wait状态的线程都会被放在这里等待唤醒再去竞争锁；_EntryList就是所有等待获取锁的线程对象存放的地方。_owner指向的是当前获取到锁的线程对象。_count为计数，这个就跟可重入相关了，线程进来一次就加一次，为0的时候就说明释放锁了，那么此时处于_EntryList池中的线程都可以去竞争这把锁了。 将上面文字转换为图来理解就是： 以上就是Synchronized实现锁的原理。 二、synchronized在字节码层面的语义 我们拿下面这段程序作为示例： 我们对这两个方法进行javap的分析，针对第一个同步代码块： 我们可以看出来，synchronized同步代码块实现同步的关键指令是monitorenter和monitorexit。这恰好与上面说的monitor锁对应上，即多个线程在_EntryList中竞争，看谁能拿到monitor锁的指向全，拿到了就可以进来，拿不到就阻塞在monitorenter处继续等待。知道这个锁被释放了为止。 那么对于synchronized修饰的方法呢？ 如果是同步方法，在字节码层面的表示是略有不同的。我们注意到，是在某个标识位上给其打上ACC_SYNCHRONIZED标志，表示这是一个synchronized修饰的同步方法，那么下面对于锁竞争啥的都与上面一样，所以只是字节码层面的表示不同而已，原理都一样。 三、对synchronized的优化 对于synchronized的性能，在以前一直是嗤之以鼻的，这种观念从老一代的程序猿们口口相传到如今，可谓是根深蒂固，在以前的版本中，确实是很慢，原因如下： 早期版本中，synchronized属于重量级锁，依赖于Mutex Lock实现 线程之间的切换需要从用户态转换到核心态，开销较大 jdk1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 3.1 自旋锁 线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作，势必会给系统的并发性能带来很大的压力。同时我们发现在许多应用上面，对象锁的锁状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的。所以引入自旋锁。 何谓自旋锁？ 所谓自旋锁，就是让该线程等待一段时间，不会被立即挂起，看持有锁的线程是否会很快释放锁。怎么等待呢？执行一段无意义的循环即可（自旋）。 自旋等待不能替代阻塞，先不说对处理器数量的要求（多核，貌似现在没有单核的处理器了），虽然它可以避免线程切换带来的开销，但是它占用了处理器的时间。如果持有锁的线程很快就释放了锁，那么自旋的效率就非常好，反之，自旋的线程就会白白消耗掉处理的资源，它不会做任何有意义的工作，典型的占着茅坑不拉屎，这样反而会带来性能上的浪费。所以说，自旋等待的时间（自旋的次数）必须要有一个限度，如果自旋超过了定义的时间仍然没有获取到锁，则应该被挂起。 自旋锁在JDK 1.4.2中引入，默认关闭，但是可以使用-XX:+UseSpinning开开启，在JDK1.6中默认开启。同时自旋的默认次数为10次，可以通过参数-XX:PreBlockSpin来调整； 如果通过参数-XX:preBlockSpin来调整自旋锁的自旋次数，会带来诸多不便。假如我将参数调整为10，但是系统很多线程都是等你刚刚退出的时候就释放了锁（假如你多自旋一两次就可以获取锁），你是不是很尴尬。于是JDK1.6引入自适应的自旋锁，让虚拟机会变得越来越聪明。 3.2 适应自旋锁 JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。它怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。 有了自适应自旋锁，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测会越来越准确，虚拟机会变得越来越聪明。 3.3 锁消除 为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。 如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定，但是对于我们程序员来说这还不清楚么？我们会在明明知道不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？我们虽然没有显示使用锁，但是我们在使用一些JDK的内置API时，如StringBuffer、Vector、HashTable等，这个时候会存在隐形的加锁操作。比如StringBuffer的append()方法，Vector的add()方法： 12345678public void vectorTest()&#123; Vector&lt;String&gt; vector = new Vector&lt;String&gt;(); for(int i = 0 ; i &lt; 10 ; i++)&#123; vector.add(i + ""); &#125; System.out.println(vector);&#125; 在运行这段代码时，JVM可以明显检测到变量vector没有逃逸出方法vectorTest()之外，所以JVM可以大胆地将vector内部的加锁操作消除。 3.4 锁粗化 我们知道在使用同步锁的时候，需要让同步块的作用范围尽可能小—仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。 在大多数的情况下，上述观点是正确的，LZ也一直坚持着这个观点。但是如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗化的概念。 锁粗话概念比较好理解，就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。如上面实例：vector每次add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。 3.5 偏向锁 在大多数情况下，锁不存在多线程竞争，总是由同一个线程多次获得。 ⭐⭐⭐核心的思想是：如果一个线程获得了锁，那么锁就会进入偏向模式，此时Mark Word的结构也变为偏向锁结构，当该结构再次请求锁时，无需再做任何同步操作，即获取锁的过程只需要检查Mark Word的锁标记位位偏向锁以及当前线程ID等于Mark Word的ThreadId即可，这样省去了大量有关锁申请的操作。 引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的CAS原子指令的性能消耗）。上面说过，轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。 它的思想可以理解为CAS，因此这种锁不适合于锁竞争比较激烈的多线程场合。 偏向锁的获取和释放： 访问 Mark Word 中偏向锁的标识位是否为1，如果是1，则确定为偏向锁。 如果偏向锁的标识位为0，说明此时是处于无锁状态，则当前线程通过CAS操作尝试获取偏向锁，如果获取锁成功，则将Mark Word中的偏向线程ID设置为当前线程ID；并且将偏向标识位设为1。 如果偏向锁的标识位不为1，也不为0(此时偏向锁的标识位没有值)，说明发生了竞争，偏向锁已经膨胀为轻量级锁，这时使用CAS操作尝试获得锁。 如果是偏向锁，则判断 Mark Word 中的偏向线程ID是否指向当前线程，如果偏向线程ID指向当前线程，则表明当前线程已经获取到了锁； 如果偏向线程ID并未指向当前线程，则通过CAS操作尝试获取偏向锁，如果获取锁成功，则将 Mark Word 中的偏向线程ID设置为当前线程ID； 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点时(在这个时间点上没有正在执行的字节码)，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。 偏向锁的释放： 当其它的线程尝试获取偏向锁时，持有偏向锁的线程才会释放偏向锁。 释放偏向锁需要等待全局安全点(在这个时间点上没有正在执行的字节码)。 - 首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态， - 如果线程还活着，说明此时发生了竞争，则偏向锁升级为轻量级锁，然后刚刚被暂停的线程会继续往下执行同步代码。 3.6 轻量级锁 引入轻量级锁的主要目的是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。当关闭偏向锁功能或者多个线程竞争偏向锁导致偏向锁升级为轻量级锁，则会尝试获取轻量级锁。 ⭐轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。 轻量级锁的加锁过程： 1.当使用轻量级锁(锁标识位为00)时，线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中(注:锁记录中的标识字段称为Displaced Mark Word)。 2.将对象头中的MarkWord复制到栈桢中的锁记录中之后，虚拟机将尝试使用CAS将对象头中Mark Word替换为指向该线程虚拟机栈中锁记录的指针，此时如果没有线程占有锁或者没有线程竞争锁，则当前线程成功获取到锁，然后执行同步块中的代码。 3.如果在获取到锁的线程执行同步代码的过程中，另一个线程也完成了栈桢中锁记录的创建，并且已经将对象头中的MarkWord复制到了自己的锁记录中，然后尝试使用CAS将对象头中的MarkWord修改为指向自己的锁记录的指针，但是由于之前获取到锁的线程已经将对象头中的MarkWord修改过了(并且现在还在执行同步体中的代码,即仍然持有着锁)，所以此时对象头中的MarkWord与当前线程锁记录中MarkWord的值不同，导致CAS操作失败，然后该线程就会不停地循环使用CAS操作试图将对象头中的MarkWord替换为自己锁记录中MarkWord的值，(当循环次数或循环时间达到上限时停止循环)如果在循环结束之前CAS操作成功，那么该线程就可以成功获取到锁，如果循环结束之后依然获取不到锁，则锁获取失败，对象头中的MarkWord会被修改为指向重量级锁的指针，然后这个获取锁失败的线程就会被挂起，阻塞了。 4.当持有锁的那个线程执行完同步体之后，使用CAS操作将对象头中的MarkWord还原为最初的状态时(将对象头中指向锁记录的指针替换为Displaced Mark Word )，发现MarkWord已被修改为指向重量级锁的指针，因此CAS操作失败，该线程会释放锁并唤起阻塞等待的线程，开始新一轮夺锁之争，而此时，轻量级锁已经膨胀为重量级锁，所有竞争失败的线程都会阻塞，而不是自旋。 锁 优点 缺点 试用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距。 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 适用于只有一个线程访问同步块场景。 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度。 如果始终得不到锁竞争的线程使用自旋会消耗CPU。 追求响应时间。同步块执行速度非常快。 重量级锁 线程竞争不使用自旋，不会消耗CPU。 线程阻塞，响应时间缓慢。 追求吞吐量。同步块执行速度较长。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从卖票程序初步看synchronized的特性]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2F%E4%BB%8E%E5%8D%96%E7%A5%A8%E7%A8%8B%E5%BA%8F%E5%88%9D%E6%AD%A5%E7%9C%8Bsynchronized%E7%9A%84%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第五篇，在多线程学习和编程中，synchronized都是我们第一个要碰见的关键字，它很重要，因为它被认为还有优化的空间，并且它代表的是互斥锁的基本思想，JDK或者其他地方的源码随处可见，本文用一个卖票程序来切入synchronized的学习，从语法和使用上进行全面了解，并且对其引申出来的一些概念进行说明。 1. 线程安全问题产生原因 存在共享数据 存在多条线程共同操作这些共享数据 2. 线程安全问题解决方法 上面的问题归根结底是由于两个线程访问相同的资源造成的。对于并发编程，需要采取措施防止两个线程来访问相同的资源。 一种措施是当资源被一个线程访问时，为其加锁。第一个访问资源的线程必须锁定该资源，是其他任务在资源被解锁前不能访问该资源。 基本上所有的并发模式在解决线程安全问题时，都采用“序列化访问临界资源”的方案。即在同一时刻，只能有一个线程访问临界资源，也称作同步互斥访问。通常来说，是在访问临界资源的代码前面加上一个锁，当访问完临界资源后释放锁，让其他线程继续访问。 这里来好好谈谈Synchronized实现加锁的方式。 3. synchronized修饰符 synchronized：可以在任意对象及方法上加锁，而加锁的这段代码称为“互斥区”或“临界区”. synchronized满足了以下重要特性： 互斥性：即在同一时间只允许一个线程持有某个对象锁，通过这种特性来实现多线程的协调机制，这样在同一时间只有一个线程对需要同步的代码块进行访问，互斥性也称为操作的原子性。 可见性：必须确保在锁被释放之前，对共享变量所做的修改，对于随后获得该锁的另一个线程是可见的，否则另一个线程可能是在本地缓存的某个副本上继续操作，从而引起不一致 ⭐synchronized锁的不是代码，是对象！ 3.1 不使用synchronized会出现线程不安全问题 123456789101112131415161718public class SellTicket implements Runnable&#123; private int count = 5; @Override public void run() &#123; sellTicket(); &#125; private void sellTicket() &#123; if(count&gt;0)&#123; count--; System.out.println(Thread.currentThread().getName()+",还剩"+count); &#125;else &#123; System.out.println("票卖光了"); &#125; &#125; &#125; 1234567891011121314151617public class Main &#123; public static void main(String[] args) &#123; SellTicket sellTicket = new SellTicket(); //同时开启五个线程去卖票 Thread t1 = new Thread(sellTicket, "thread1"); Thread t2 = new Thread(sellTicket, "thread2"); Thread t3 = new Thread(sellTicket, "thread3"); Thread t4 = new Thread(sellTicket, "thread4"); Thread t5 = new Thread(sellTicket, "thread5"); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); &#125;&#125; 某一次运行的结果是: 12345thread2,还剩2thread1,还剩2thread3,还剩2thread4,还剩0thread5,还剩0 很显然，多个线程之间打架了，数据混乱了。这是因为，多个线程同时操作run（）方法，对count进行修改，进而造成错误。 3.2 使用synchronized来加锁 对卖票的核心方法上加上synchronized： 12345678private synchronized void sellTicket() &#123; if(count&gt;0)&#123; count--; System.out.println(Thread.currentThread().getName()+",还剩"+count); &#125;else &#123; System.out.println("票卖光了"); &#125;&#125; 或者写成同步代码块的形式： 12345678910private void sellTicket() &#123; synchronized (this) &#123; if(count&gt;0)&#123; count--; System.out.println(Thread.currentThread().getName()+",还剩"+count); &#125;else &#123; System.out.println("票卖光了"); &#125; &#125;&#125; 结果只有一个： 12345thread1 count:4thread4 count:3thread5 count:2thread3 count:1thread2 count:0 结果是正确的，可以看出代码A和代码B的区别就是在sellTicket()方法上加上了synchronized修饰。 说明：当多个线程访问MyThread 的run方法的时候，如果使用了synchronized修饰，那个多线程就会以排队的方式进行处理（这里排队是按照CPU分配的先后顺序而定的），一个线程想要执行synchronized修饰的方法里的代码，首先是尝试获得锁，如果拿到锁，执行synchronized代码体的内容，如果拿不到锁的话，这个线程就会不断的尝试获得这把锁，直到拿到为止，而且多个线程同时去竞争这把锁，也就是会出现锁竞争的问题。 3.3 一个对象有一把锁！不同对象不同锁！ 每次开启一个线程就new一个对象的话，即对每个不同的对象加锁，则互不干扰： 123456789101112131415public class Main &#123; public static void main(String[] args) &#123; Thread t1 = new Thread(new SellTicket(), "thread1"); Thread t2 = new Thread(new SellTicket(), "thread2"); Thread t3 = new Thread(new SellTicket(), "thread3"); Thread t4 = new Thread(new SellTicket(), "thread4"); Thread t5 = new Thread(new SellTicket(), "thread5"); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); &#125;&#125; 线程任务SellTicket()无论给不给sellTicket()加锁，结果都是一样的： 12345thread1,还剩4thread2,还剩4thread3,还剩4thread5,还剩4thread4,还剩4 这是因为我这里是五个不同的对象，每个对象各自获取自己的锁，互不影响，所以都是4. 关键字synchronized取得的锁都是对象锁，而不是把一段代码或方法当做锁，所以上述实例代码C中哪个线程先执行synchronized 关键字的方法，那个线程就持有该方法所属对象的锁，五个对象，线程获得的就是两个不同对象的不同的锁，他们互不影响的。 那么，我们在正常的场景的时候，肯定是有一种情况的就是，一个类new出来的所有对象会对一个变量count进行操作，那么如何实现哪？很简单就是加static，我们知道，用static修改的方法或者变量，在该类的所有对象是具有相同的引用的，这样的话，无论实例化多少对象，调用的都是一个方法。 Main函数不变： 123456789101112131415public class Main &#123; public static void main(String[] args) &#123; Thread t1 = new Thread(new SellTicket(), "thread1"); Thread t2 = new Thread(new SellTicket(), "thread2"); Thread t3 = new Thread(new SellTicket(), "thread3"); Thread t4 = new Thread(new SellTicket(), "thread4"); Thread t5 = new Thread(new SellTicket(), "thread5"); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); &#125;&#125; SellTicket则在卖票方法上增加static关键字： 1234567891011121314151617public class SellTicket implements Runnable&#123; private static int count = 5; @Override public void run() &#123; sellTicket(); &#125; private synchronized static void sellTicket() &#123; if(count&gt;0)&#123; count--; System.out.println(Thread.currentThread().getName()+",还剩"+count); &#125;else &#123; System.out.println("票卖光了"); &#125; &#125;&#125; 或者显示地锁住Class对象，即锁住类对象： 12345678910private static void sellTicket() &#123; synchronized (SellTicket.class) &#123; if(count&gt;0)&#123; count--; System.out.println(Thread.currentThread().getName()+",还剩"+count); &#125;else &#123; System.out.println("票卖光了"); &#125; &#125;&#125; 结果为: 12345thread1,还剩4thread2,还剩3thread4,还剩2thread3,还剩1thread5,还剩0 仔细看，我们给sellTicket设定为static静态方法，那么这个方法就从之前的对象方法上升到类级别方法，这个类所有的对象都调用的同一个方法。实现资源的共享和加锁。 上面讲的时对象锁和类锁，前者锁定的是某个实例对象，后者锁定的是Class对象。下面总结一下： 有线程访问对象的同步代码块时，另外的线程可以访问该对象的非同步代码块 若锁住的时同一个对象，一个线程在访问对象的同步代码块(同步方法)时，另一个访问对象的同步代码块(同步方法)的线程会被阻塞 同一个类的不同对象的对象锁互不干扰 类锁由于也是一种特殊的对象锁，因此表现与上述一致，只是由于一个类只有一把类锁，所以同一个类的不同对象使用类锁是同步的 类锁和对象锁互不干扰 4. Synchronized锁重入 4.1 什么是可重入锁 锁的概念就不用多解释了,当某个线程A已经持有了一个锁,当线程B尝试进入被这个锁保护的代码段的时候.就会被阻塞. ⭐而锁的操作粒度是&quot;线程”,而不是调用.同一个线程再次进入同步代码的时候.可以使用自己已经获取到的锁,这就是可重入锁。 4.2 可重入锁的小例子 1234567891011121314151617181920212223242526public class SyncDubbo &#123; public synchronized void method1()&#123; System.out.println("method1..."); method2(); &#125; public synchronized void method2()&#123; System.out.println("method2..."); method3(); &#125; public synchronized void method3()&#123; System.out.println("method3..."); &#125; public static void main(String[] args) &#123; SyncDubbo syncDubbo = new SyncDubbo(); new Thread(new Runnable() &#123; @Override public void run() &#123; syncDubbo.method1(); &#125; &#125;).start(); &#125;&#125; 示例代码向我们演示了，如何在一个已经被synchronized关键字修饰过的方法再去调用对象中其他被synchronized修饰的方法。 4.3 为什么要可重入 我们上一篇文章中介绍了“一个对象一把锁，多个对象多把锁”，可重入锁的概念就是：自己可以获取自己的内部锁。 假如有1个线程T获得了对象A的锁，那么该线程T如果在未释放前再次请求该对象的锁时，如果没有可重入锁的机制，是不会获取到锁的，这样的话就会出现死锁的情况。 就如代码A体现的那样，线程T在执行到method1（）内部的时候，由于该线程已经获取了该对象syncDubbo 的对象锁，当执行到调用method2（） 的时候，会再次请求该对象的对象锁，如果没有可重入锁机制的话，由于该线程T还未释放在刚进入method1（） 时获取的对象锁，当执行到调用method2（） 的时候，就会出现死锁。 4.4 可重入锁到底有什么用哪？ 正如上述代码A和（4.3）中解释那样，最大的作用是避免死锁。假如有一个场景：用户名和密码保存在本地txt文件中，则登录验证方法和更新密码方法都应该被加synchronized，那么当更新密码的时候需要验证密码的合法性，所以需要调用验证方法，此时是可以调用的。 4.5 什么是死锁？ 线程A当前持有互斥所锁lock1，线程B当前持有互斥锁lock2。接下来，当线程A仍然持有lock1时，它试图获取lock2，因为线程B正持有lock2，因此线程A会阻塞等待线程B对lock2的释放。如果此时线程B在持有lock2的时候，也在试图获取lock1，因为线程A正持有lock1，因此线程B会阻塞等待A对lock1的释放。二者都在等待对方所持有锁的释放，而二者却又都没释放自己所持有的锁，这时二者便会一直阻塞下去。这种情形称为死锁。 一个例子来说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class DeadLock &#123; private static Object obj1 = new Object(); private static Object obj2 = new Object(); public static void a()&#123; synchronized (obj1) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (obj2) &#123; System.out.println("a"); &#125; &#125; &#125; public static void b()&#123; synchronized (obj2) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (obj1) &#123; System.out.println("b"); &#125; &#125; &#125; public static void main(String[] args) &#123; DeadLock d = new DeadLock(); new Thread(new Runnable() &#123; @Override public void run() &#123; d.a(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; d.b(); &#125; &#125;).start(); &#125;&#125; 产生死锁的原因主要是： （1） 因为系统资源不足。 （2） 进程运行推进的顺序不合适。 （3） 资源分配不当等。 如何解决死锁： 尽量一个线程只获取一个锁。 一个线程只占用一个资源。 尝试使用定时锁，至少能保证锁最终会被释放。 4.6 可重入锁支持在父子类继承的环境中 1234567891011121314public class Father &#123; public synchronized void doSomething()&#123; ...... &#125; &#125; public class Child extends Father &#123; public synchronized void doSomething()&#123; ...... super.doSomething(); &#125; &#125; 执行子类的方法的时候,先获取了一次Widget的锁,然后在执行super的时候,就要获取一次,如果不可重入,那么就跪了. 在这里，可能会产生疑问： 重入”代表一个线程可以再次获得同一个对象的锁。可是你给出的代码示例中，我理解的是一个线程调用Child的doSomething方法前或得了Child对象的锁，super.doSomething方法调用时，次线程获得了Child对象父对象的锁。两个锁属于不同的对象，这还算是重入吗？ 解释：当Child实例对象调用doSomething方法时，此时持有的是Child实例对象的锁，之后调用super.doSomething();，这时仍然对于Child实例对象加锁，因为此时仍然使用的是Child实例对象内存空间的数据。 至于这句话的理解，就牵涉到继承的机制： 在一个子类被创建的时候，首先会在内存中创建一个父类对象，然后在父类对象外部放上子类独有的属性，两者合起来形成一个子类的对象。所以所谓的继承使子类拥有父类所有的属性和方法其实可以这样理解，子类对象确实拥有父类对象中所有的属性和方法，但是父类对象中的私有属性和方法，子类是无法访问到的，只是拥有，但不能使用。就像有些东西你可能拥有，但是你并不能使用。所以子类对象是绝对大于父类对象的，所谓的子类对象只能继承父类非私有的属性及方法的说法是错误的。可以继承，只是无法访问到而已。 之所以网上有很多说只继承protected或者private的，是因为从语言的角度出发的： 从内存的角度来看，的确是继承了的，可以写一个简单的继承类，debug看子类的属性是否存在父类的private属性，事实证明是有的。 针对这里有人说：不是创建一个父类对象，而只是创建一个父类空间并进行相应的初始化。对此，我一开始也是这么想的，不过当我看到这个答案的时候，又觉得很有道理： 会创建父类对象。《Java编程思想》（第四版）129页，当创建一个导出类对象时，该对象包含了一个基类的子对象，这子对象与你用基类直接创建的对象是一样的，二者区别在于后者来源于外部，而基类的子对象被包装在导出类对象内部。 5. 发生异常时会自动释放锁 1234567891011121314151617181920212223public class SyncException &#123; private int i = 0; public synchronized void operation() &#123; while (true) &#123; i++; System.out.println(Thread.currentThread().getName() + " , i= " + i); if (i == 10) &#123; Integer.parseInt("a"); &#125; &#125; &#125; public static void main(String[] args) &#123; final SyncException se = new SyncException(); new Thread(new Runnable() &#123; public void run() &#123; se.operation(); &#125; &#125;, "t1").start(); &#125;&#125; 执行结果如下： 123456789101112t1 , i= 2t1 , i= 3t1 , i= 4t1 , i= 5t1 , i= 6t1 , i= 7t1 , i= 8t1 , i= 9t1 , i= 10java.lang.NumberFormatException: For input string: &quot;a&quot; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) //其他输出信息 关于synchronized的优化放到后文去讲。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程重要的相关方法]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2F%E7%BA%BF%E7%A8%8B%E9%87%8D%E8%A6%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第四篇，本篇文章主要来看看线程相关的几个重要方法。 wait和sleep 他们最基本的差异是： wait是Object的一个方法，sleep是Thread类的方法 sleep可以在任何地方使用，但是wait方法只能在synchronied方法或synchronied块中使用 本质的区别是： Thread.sleep只会让出CPu，不会导致锁行为的改变 Object.wait不仅让出CPU，还会释放已经占用的同步资源锁 这个区别也就解释了为什么wait方法只能在synchronied方法或synchronied块中使用，因为没有获取过锁哪里来的释放锁呢？所以释放锁的前提是要获取锁。 下面来验证一下，眼见为实！ 对于sleep来说是没有锁的要求的，既不用获取锁也不用释放锁，关于这一点就不再验证了。 notify和notifyAll 先来了解一下锁池和等待池的概念。 锁池EntryLisy 假设线程A已经拥有了某个对象(不是类)的锁，而其他线程B、C想要调用这个对象的某个synchronized方法或块，由于B、C线程在进入对象的synchronized方法或块之前必须先获得该对象锁得拥有权，而恰巧该对象的锁正被线程A所占用，此时B、C线程就会被阻塞，进入一个地方去等待所得释放，这个地方就是该对象得锁池。 等待池WaitSet 假设线程A调用了某个对象的wait方法后，线程A就会释放该对象得锁，同时线程A句进入到该对象得等待池中，进入到等待池中得线程不会去竞争该对象的锁。 notify的作用就是随机唤醒一个线程进入等待池的线程，而notifyAll是唤醒所有处于等待池中线程，唤醒之后就可以再去竞争获得锁的机会了。 刚才的例子稍微改造一下，来了解一下notify的作用。还拿刚才那个例子： yield 当调用Thread.yield()函数时，会给线程调度器一个当前线程愿意让出CPU使用的暗示，但是线程调度器可能会忽略这个暗示。并且它也不会释放当前线程占用的锁。 yield()与无参的wait()的区别： 执行yield()后，当前线程由运行状态变为就绪状态。执行wait后，当前线程会失去对象的锁，状态变为WAITING状态。 执行yield()后，当前线程不会释放锁。执行wait后，当前线程会释放锁。 比较简单，就不举例了。 interrupt 它只是通知线程应该中断了。 如果线程处于被阻塞状态，那么线程将立即退出被阻塞状态，并抛出一个interruptedException异常 如果线程处于正常活动状态，那么会将该线程的中断标志设置为true。被设置中断标志的线程将继续正常运行，不受影响。 也就是说，中断一个线程是由被调用的线程状态和自己程序判断决定的。 阻塞状态下，线程会立即退出，并抛出异常 正常状态下，需要被调用的线程检查中断标志位，然后再根据中断标志位自行地停止线程 下面写一个demo来验证一下： 12345678910111213141516171819202122232425262728293031323334353637public class InterruptDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Runnable interruptTask = new Runnable() &#123; @Override public void run() &#123; int i = 0; try &#123; //在正常运行任务时，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程 while (!Thread.currentThread().isInterrupted()) &#123; Thread.sleep(100); // 休眠100ms i++; System.out.println(Thread.currentThread().getName() + " (" + Thread.currentThread().getState() + ") loop " + i); &#125; &#125; catch (InterruptedException e) &#123; //在调用阻塞方法时正确处理InterruptedException异常。（例如，catch异常后就结束线程。） System.out.println(Thread.currentThread().getName() + " (" + Thread.currentThread().getState() + ") catch InterruptedException."); &#125; &#125; &#125;; Thread t1 = new Thread(interruptTask, "t1"); System.out.println(t1.getName() +" ("+t1.getState()+") is new."); // 1.启动“线程t1” t1.start(); System.out.println(t1.getName() +" ("+t1.getState()+") is started."); // 2.主线程休眠300ms，然后主线程给t1发“中断”指令。 Thread.sleep(400); t1.interrupt(); System.out.println(t1.getName() +" ("+t1.getState()+") is interrupted."); // 3.主线程休眠300ms，然后查看t1的状态。 Thread.sleep(300); System.out.println(t1.getName() +" ("+t1.getState()+") is interrupted now."); &#125;&#125; 打印结果为： 12345678t1 (NEW) is new.t1 (RUNNABLE) is started.t1 (RUNNABLE) loop 1t1 (RUNNABLE) loop 2t1 (RUNNABLE) loop 3t1 (TIMED_WAITING) is interrupted.t1 (RUNNABLE) catch InterruptedException.t1 (TERMINATED) is interrupted now. 首先，是就绪状态，为new；接下来启动这个线程，状态变为started，由于此时一切安好，没有“打扰”这个线程的执行，所以每隔100毫秒打印一句(RUNNABLE) loop i出来；在400毫秒的安好之后，给他一个t1.interrupt();，此时线程可能恰好在执行sleep睡觉呢，这个interrupt一看你在阻塞(睡觉)，那还得了，立马停止这个线程并且抛出异常。 但是话说回头，本程序还用了if判断，只要标志位为false就不停循环，一旦标志位变为true则立马退出循环。所以即使你不睡觉，但是我还是能通过这个If来终止你的循环。 join join是加入的意思，非常形象生动。 12345678/** * Waits for this thread to die. * 调用方线程（调用join方法的线程）执行等待操作， * 直到被调用的线程（join方法所属的线程）结束，再被唤醒 */public final void join() throws InterruptedException &#123; join(0);&#125; 具体的实现如下： 我们知道wait是需要释放当前线程所占的对象锁的，而join基于wait实现，显然是可以的。 这里判断如果线程还在运行中的话，则继续等待，如果指定时间到了，或者线程运行完成了，则代码继续向下执行，调用线程就可以执行后面的逻辑了。 但是在这里没有看到哪里调用notify或者notifyAll方法，如果没有调用的话，那调用方线程会一直等待下去，那是哪里调用了唤醒它的方法呢？通过查证得知，原来在线程结束时，java虚拟机会执行该线程的本地exit方法，这个exit方法里面会调用notifyAll方法，唤醒所有等待的线程。 下面来两个例子来彻底理解它的用法。 例子一：有耐心的男孩： 男孩和女孩准备出去逛街 女孩开始化妆,男孩在等待。。。 女孩化妆完成！，耗时5000 男孩和女孩开始去逛街了 就是男孩和女孩准备去逛街，女孩要化妆先，等女孩化妆完成了，再一起去逛街。 例子二：没有耐心的男孩： 男孩和女孩准备出去逛街 女孩开始化妆,男孩在等待。。。 男孩等了2000, 不想再等了，去逛街了 女孩化妆完成！，耗时5000 男孩等了join(time)中的time时间，如果这个time时间到达之后，女孩所在的线程还没执行完，则不等待了，继续执行后面的逻辑，就是不等女孩了，自己去逛街。 总结 了解了这些核心方法之后，就可以对下面这幅图简单说一说啦： 首先是new Thread()只是新建状态，只有start之后才会进入runnable状态，注意这个状态里面可能有两种状态，一种是正在运行，即running，还有一种是就绪状态即ready，这两个状态归属于一类的原因是他们之间是在不断切换的，即CPU的时间片内临幸到这个进程，这个进程中有若干个线程的话，就会高速地切换各个线程逐个执行，达到宏观上是并行执行的效果。我们知道yield是给线程调度器一个暗示让出当前执行的线程的时间片，至于这个线程调度器听不听那就不知道了，所以存在一定的随机性。如果正常执行结束就进入最后的终止状态。往右边看，如果发生带时间的超时等待，如sleep(100)，本线程会阻塞，让出CPU执行权并且不改变锁状态，与之区别的是wait(100)这个方法不仅让出CPU执行权，还会释放锁，所以要调用wait方法必然要先获取锁，所以一般都是在synchronized中调用它。至于join(100)是指阻塞当前线程，让其他的线程先执行，底层是wait所以也会释放锁。超时等待只要等它时间过了就可以跳出阻塞状态了，或者用notify或者interrupt之类的来唤醒或者打断它。往左下角看，是锁获取的时候可能发生阻塞，这个时候只能等其他线程释放锁才行了。往左边看，是无限期等待的代表，唤醒手段与有限期等待是一样的。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程的状态]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2F%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第三篇，主要介绍一下线程的几个状态的含义。 Thread类源码中规定了几种线程的状态： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public enum State &#123; /** * Thread state for a thread which has not yet started. */ NEW, /** * Thread state for a runnable thread. A thread in the runnable * state is executing in the Java virtual machine but it may * be waiting for other resources from the operating system * such as processor. */ RUNNABLE, /** * Thread state for a thread blocked waiting for a monitor lock. * A thread in the blocked state is waiting for a monitor lock * to enter a synchronized block/method or * reenter a synchronized block/method after calling * &#123;@link Object#wait() Object.wait&#125;. */ BLOCKED, /** * Thread state for a waiting thread. * A thread is in the waiting state due to calling one of the * following methods: * &lt;ul&gt; * &lt;li&gt;&#123;@link Object#wait() Object.wait&#125; with no timeout&lt;/li&gt; * &lt;li&gt;&#123;@link #join() Thread.join&#125; with no timeout&lt;/li&gt; * &lt;li&gt;&#123;@link LockSupport#park() LockSupport.park&#125;&lt;/li&gt; * &lt;/ul&gt; * * &lt;p&gt;A thread in the waiting state is waiting for another thread to * perform a particular action. * * For example, a thread that has called &lt;tt&gt;Object.wait()&lt;/tt&gt; * on an object is waiting for another thread to call * &lt;tt&gt;Object.notify()&lt;/tt&gt; or &lt;tt&gt;Object.notifyAll()&lt;/tt&gt; on * that object. A thread that has called &lt;tt&gt;Thread.join()&lt;/tt&gt; * is waiting for a specified thread to terminate. */ WAITING, /** * Thread state for a waiting thread with a specified waiting time. * A thread is in the timed waiting state due to calling one of * the following methods with a specified positive waiting time: * &lt;ul&gt; * &lt;li&gt;&#123;@link #sleep Thread.sleep&#125;&lt;/li&gt; * &lt;li&gt;&#123;@link Object#wait(long) Object.wait&#125; with timeout&lt;/li&gt; * &lt;li&gt;&#123;@link #join(long) Thread.join&#125; with timeout&lt;/li&gt; * &lt;li&gt;&#123;@link LockSupport#parkNanos LockSupport.parkNanos&#125;&lt;/li&gt; * &lt;li&gt;&#123;@link LockSupport#parkUntil LockSupport.parkUntil&#125;&lt;/li&gt; * &lt;/ul&gt; */ TIMED_WAITING, /** * Thread state for a terminated thread. * The thread has completed execution. */ TERMINATED;&#125; 其实源码中已经详细说明了这几种状态的含义以及发生的时机。下面还是再看看： 新建(new)：创建后尚未启动的线程的状态 运行(Runnable)：包含Running和Ready两种状态 无限期等待(Waiting)：不会被分配CPU执行时间，需要显式被唤醒 无Timeout参数的Object.wait()方法 无Timeout参数的Thread.join()方法 LockSupport.park()方法 限期等待(Timed Waiting)：不会被分配CPU执行时间，在一定时间后会由系统自动唤醒 Thread.sleep()方法 设置了Timeout参数的Object.wait()方法 设置了Timeout参数的Thread.join()方法 LockSupport.parkNanos()方法 LockSupport.parkUntil()方法 阻塞(Blocked)：等待获取排他锁 结束(Terminated)：已终止线程的状态，线程已经结束执行 既然有这么多线程状态，那么必然会存在状态的转换，他们的状态是如何转换的呢？下面这张图就是比较全面的状态转换图： 初学者看到这张图一张是比较晕的，各种乱七八糟的东西堆在这里实在是难以下咽，不过一开始不能完全看懂是很正常的，需要进一步地学习一些细节，不断地来回顾，总有一天就会真正理解了。 下面就是要详细说说Thread类里面的一些方法啦，理解了这些方法，状态的切换图便一目了然。为了限制篇幅，本文先说到这里。]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java多线程之传参和返回值处理]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2Fjava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E4%BC%A0%E5%8F%82%E5%92%8C%E8%BF%94%E5%9B%9E%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第二篇，对于传参和返回值的问题，是面试中关于多线程这一块问得比较多的问题了，这里进行详细的说明。 一、如何给run()方法传参 我们知道多线程是通过star()方法让线程处于准备就绪状态，而实际运行的业务逻辑是放在run()方法体中的，但是run()方法是没有参数的方法，实际的业务场景中，我们可能需要向方法体中传递参数，实现的方式主要有三种： 构造函数传参，这个在上一篇文章中已经演示了。 成员变量传参，这个就是依靠set方法。 回调函数传参，这个稍微特殊一点。这里说明一下。 上面的两种向线程中传递数据的方法是最常用的。但这两种方法都是main方法中主动将数据传入线程类的。这对于线程来说，是被动接收这些数据的。 然而，在有些应用中需要在线程运行的过程中动态地获取数据，如在下面代码的run方法中产生了3个随机数，然后通过Work类的process方法求这三个随机数的和，并通过Data类的value将结果返回。从这个例子可以看出，在返回value之前，必须要得到三个随机数。也就是说，这个 value是无法事先就传入线程类的。 123456789101112131415161718192021222324252627282930313233343536373839class Data &#123; public int value = 0 ;&#125;class Work &#123; public void process(Data data, Integer[] numbers) &#123; for ( int n : numbers) &#123; data.value += n; &#125; &#125;&#125;public class MyThread3 extends Thread &#123; private Work work; public MyThread3(Work work) &#123; this .work = work; &#125; public void run() &#123; //1.随机生成3个数放进数组中 java.util.Random random = new java.util.Random(); Data data = new Data(); int n1 = random.nextInt( 1000 ); int n2 = random.nextInt( 2000 ); int n3 = random.nextInt( 3000 ); Integer[] numbers = new Integer[3]; numbers[0] = n1; numbers[1] = n2; numbers[2] = n3; //调用函数去计算这三个数之和，计算的结果存在Data实例中的value属性中 //这里process相当于回调函数，我调用这个函数，给我一个计算结果 work.process(data, numbers); System.out.println(String.valueOf(n1) + "+" + String.valueOf(n2) + "+" + String.valueOf(n3) + "=" + data.value); &#125; public static void main(String[] args) &#123; Thread t = new MyThread3( new Work()); t.start(); &#125;&#125; 其中一次的执行结果为： 1707+678+173=1558 在上面代码中的process方法被称为回调函数。从本质上说，回调函数就是事件函数。在Windows API中常使用回调函数和调用API的程序之间进行数据交互。因此，调用回调函数的过程就是最原始的引发事件的过程。在这个例子中调用了process方法来获得数据也就相当于在run方法中引发了一个事件。 二、如何处理线程返回值 由于线程相当于一个异步的处理函数，想要获取它的结果就不能像传统的获取它的return的值那么简单了，主要问题就在于它什么时候能处理好是不知道的，需要一定的机制去等待它处理好了再去获取它的处理结果。方式一般有三种。 2.1 主线程等待法 这个方法是最简单也是最容易想到的处理方式。下面搞个实例来看看大概是如何操作的。 首先写一个类，写这个的含义是，假如主线程不等待，将会一口气执行到最后一行，此时子线程可能还没执行完。就会出现打印空。 那么我们的主线程如何获取到子线程中赋予的值呢？一种方式就是死等，不停地轮询看你的值是否已经计算好了，一旦计算好就可以拿到这个值。类似于以下： 其实这就是自旋，即CPU停在这里等待，不能干其他事情，这必然会大大浪费CPU资源，所以虽然这种方式实现起来非常简单，但是不适合用。另外的缺点就是代码臃肿，比如我要等待的值不止一个，有多个，那是不是要写多个while循环来等待呢？此外，我们大多时候根本不知道这个子线程到底要执行多久，因为我们这里是每隔100毫秒轮询一次，那假如这个值在这100毫秒内值已经有了，那么是不能立即获取的。 针对以上不能精准控制的缺点，这里便有了第二种方法。 Join方法 Thread类中的join方法可以阻塞当前线程以等待子线程处理完毕。 在这里，由于是在主线程中调用的join，所以阻塞主线程，让子线程执行完毕再继续执行。 这种方法更简单，但是存在多个子线程的情况下，做到灵活以及精准控制是做不到的。 Callable接口实现 JAVA提供了有返回值的任务，即实现了Callable接口的任务，执行这个任务之后可以获取一个叫做Futrue的对象，通过get()就可以获取Callable任务返回的内容。 具体是如何获取返回的内容呢？有两种方式，一个是通过FutureTask这个类来获取，一个是通过线程池获取。 对于第一种方式，我们通过例子来理解。 先新建一个实现了Callable接口的任务： 把Callable任务放进FutureTask中，这个FutureTask再放进Thread中去执行： 发现我们的程序并没有显示地等待，FutureTask的get()方法完成了等待和获取返回值。下面来看看Future的继承关系： 我们发现，FutureTask实质上都是Runnable接口的实例，只是它还是Futrue接口的实例，所以不仅可以作为一个线程任务被执行，还可以接受一个Callable接口去接受它的返回值。因此是一个升级版的Runnable实例。 说完了FutureTask的实现方式，下面再来看看另一种方式，即线程池来实现。关于线程池，后文还会详细介绍，这里只是简单先运用一下。 达到了一样的效果。我们来分析分析。 我们发现，其实两种方式的根本就是Future这个接口，第一种是直接用了FutureTask这个类来手动实现，即不仅需要它接收一个Callable任务，还需要将其作为一个线程任务去手动执行。而第二种方式就比较简单了，有了线程池，我直接把Callable任务扔线程池去submit，就可以得到一个可以获取返回值的Future类型对象，就可以根据这个对象获取到值了。 所以两种方式本质上是一样一样的。 下面补充一下关于Runnable和Callable两者的区别： 表面区别： Runnable不返回任务执行结果，Callable可返回任务执行结果 Callable在任务无法计算结果时抛出异常，而Runnable不能]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程基本知识梳理]]></title>
    <url>%2F2019%2F02%2F10%2Fthread%2F%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文是关于JAVA多线程和并发的第一篇，主要说明基本概念，这是面试中最基本的要会的东西，如果这些都回答不了，基本上就没有机会了，本文从源码稍微深入一点去探讨常见的基本概念。本文并不会从最最最最基本的知识开始说起，将不费笔墨直击要害，所以需要一点多线程的基本知识才行，这也符合本博客的宗旨，即知识点再次提炼和升级。 一、进程和线程的区别 这一块详见 面试-进程与线程 里面的内容，相信已经够用了。 二、start()和run()方法的区别 以一个小例子入手，在主函数中尝试新建一个线程，并且以t.run()的形式去调用，从结果可以看出，java默认开启主线程来执行，当我们用t.run()去执行的时候，只是相当于简单的函数调用，因为从打印结果可以看出都是main进程，那么，实质上并没有新建一个子线程。 （注意，不是一调用就会去执行，而是说这个线程处于就绪状态，将有资格获得CPU的临幸，关于线程状态，后文会再次详细说明，关于start之后处于就绪状态这一点默认读者是清楚的，下面表述可能不会太顾及说明这一点）： 那么，从表象上我们已经知道，run只是简单的函数调用，start才会真正地开启一个新线程来执行，下面从源码层面来看看start()的基本实现方式。 说明一下，本源码是基于JDK1.8，我们看到它的核心实现是一个native方法，IDEA上已经看不了，只好去看看openJDK了。 直接打开网址： Thread.c 我们可以看到： 我们看到很多关于线程的方法，但是这里是看不到具体的实现的，我们看到上面引入了jvm.h的库，所以实现应该是在jvm相关的代码中，直接点开： jvm.cpp 可以看到如下： emmm，虽然不大看得懂，但是我们确实看到了start()会调用虚拟机去创建一个新的线程，最终再去调用run方法去执行。所以流程如下： 最终总结： 调用start()方法会创建一个新的子线程并启动 run()方法只是thread的一个普通方法的调用 三、Thread和Runnable是什么关系 还是老规矩，先来翻翻源码： 我们可以看到，Thread是一个class，而Runnable是一个interface，而Runnable中只有一个抽象方法就是run(). 那么，我们上面说到，新建一个线程是要靠start()来实现的，那么Runnable是如何来新建一个线程呢？它不是只有一个run()方法吗？ 此时再来看Thread类，它里面有大量的方法，就包含了run()和start()方法，它还有一个重要的构造函数为: 1public Thread(Runnable target) &#123;...&#125; 就是说，传入Runable接口实例，再调用Thread的start()方法创建子线程，再来调用重写的run()方法就可以了。下面举个例子。 先说说用Thread的方式来创建一个子线程类： 这也从侧面证明了，线程是交替执行的，但是因为属于同一个进程，共享同一个地址和资源，所以不需要进行切换，极大提高了CPU执行效率。 下面再来看看Runnable接口是怎么实现多线程的： 总结一下他们俩： Thread是一个类，Runnable是一个接口，前者实现后者 Thread有start方法，结合run()可以实现多线程，但是Runnable没有start()方法，所以要通过Thread()来实现，所以，两种方式最终都是通过Thread的start()来实现run()的多线程特性 由于JAVA是单一继承的，所以推荐多使用Runnable接口]]></content>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实例说明类加载过程]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E5%AE%9E%E4%BE%8B%E8%AF%B4%E6%98%8E%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十三篇文章，本文从一个简单程序入手，将前面所学串联起来，详细来看看类加载的过程到底是什么样子的。 零、Java虚拟机启动、加载类过程分析 下面我将定义一个非常简单的java程序并运行它，来逐步分析java虚拟机启动的过程。 12345678910111213package org.luanlouis.jvm.load; import sun.security.pkcs11.P11Util; public class Main&#123; public static void main(String[] args) &#123; System.out.println("Hello,World!"); ClassLoader loader = P11Util.class.getClassLoader(); System.out.println(loader); &#125; &#125; 在windows命令行下输入： java org.luanlouis.jvm.load.Main 当输入上述的命令时： windows开始运行{JRE_HOME}/bin/java.exe程序，java.exe 程序将完成以下步骤： 1.根据JVM内存配置要求，为JVM申请特定大小的内存空间； 2.创建一个引导类加载器实例，初步加载系统类到内存方法区区域中； 3.创建JVM 启动器实例 Launcher,并取得类加载器ClassLoader； 4.使用上述获取的ClassLoader实例加载我们定义的 org.luanlouis.jvm.load.Main类； 5.加载完成时候JVM会执行Main类的main方法入口，执行Main类的main方法； 6.结束，java程序运行结束，JVM销毁。 下面逐一分析一下这几个步骤。 一、根据JVM内存配置要求，为JVM申请特定大小的内存空间 JVM内存按照功能上的划分，可以粗略地划分为方法区(Method Area) 和堆(Heap),而所有的类的定义信息都会被加载到方法区中。 二、创建一个引导类加载器实例，初步加载系统类到内存方法区区域中 JVM申请好内存空间后，JVM会创建一个引导类加载器（Bootstrap Classloader）实例，引导类加载器是使用C++语言实现的，负责加载JVM虚拟机运行时所需的基本系统级别的类，如java.lang.String, java.lang.Object等等。 引导类加载器(Bootstrap Classloader)会读取 {JRE_HOME}/lib 下的jar包和配置，然后将这些系统类加载到方法区内。 本例中，引导类加载器是用 {JRE_HOME}/lib加载类的，不过，你也可以使用参数 -Xbootclasspath 或 系统变量sun.boot.class.path来指定的目录来加载类。 一般而言，{JRE_HOME}/lib下存放着JVM正常工作所需要的系统类，如下表所示： 文件名 描述 rt.jar 运行环境包，rt即runtime，J2SE 的类定义都在这个包内 charsets.jar 字符集支持包 jce.jar 是一组包，它们提供用于加密、密钥生成和协商以及 Message Authentication Code（MAC） jsse.jar 安全套接字拓展包Java™ Secure Socket Extension classlist 该文件内表示是引导类加载器应该加载的类的清单 net.properties JVM 网络配置信息 引导类加载器(Bootstrap ClassLoader） 加载系统类后，JVM内存会呈现如下格局： 引导类加载器将类信息加载到方法区中，以特定方式组织，对于某一个特定的类而言，在方法区中它应该有 运行时常量池、类型信息、字段信息、方法信息、类加载器的引用，对应class实例的引用等信息。 类加载器的引用,由于这些类是由引导类加载器(Bootstrap Classloader)进行加载的，而 引导类加载器是由C++语言实现的，所以是无法访问的，故而该引用为NULL 对应class实例的引用， 类加载器在加载类信息放到方法区中后，会创建一个对应的Class 类型的实例放到堆(Heap)中, 作为开发人员访问方法区中类定义的入口和切入点。 三、创建JVM 启动器实例 Launcher,并取得类加载器ClassLoader 上述步骤完成，JVM基本运行环境就准备就绪了。接着，我们要让JVM工作起来了：运行我们定义的程序 org.luanlouis,jvm.load.Main。 此时，JVM虚拟机调用已经加载在方法区的类sun.misc.Launcher 的静态方法getLauncher(), 获取sun.misc.Launcher 实例： 1234//获取Java启动器 sun.misc.Launcher launcher = sun.misc.Launcher.getLauncher(); //获取类加载器ClassLoader用来加载class到内存来 ClassLoader classLoader = launcher.getClassLoader(); sun.misc.Launcher 使用了单例模式设计，保证一个JVM虚拟机内只有一个sun.misc.Launcher实例。 在Launcher的内部，其定义了两个类加载器(ClassLoader),分别是sun.misc.Launcher.ExtClassLoader和sun.misc.Launcher.AppClassLoader，这两个类加载器分别被称为拓展类加载器(Extension ClassLoader) 和 应用类加载器(Application ClassLoader).如下图所示： 四、使用类加载器ClassLoader加载Main类 通过 launcher.getClassLoader()方法返回AppClassLoader实例，接着就是AppClassLoader加载 org.luanlouis.jvm.load.Main类的时候了。 12lassLoader classloader = launcher.getClassLoader();//取得AppClassLoader类 classLoader.loadClass("org.luanlouis.jvm.load.Main");//加载自定义类 上述定义的org.luanlouis.jvm.load.Main类被编译成org.luanlouis.jvm.load.Main class二进制文件，这个class文件中有一个叫常量池(Constant Pool)的结构体来存储该class的常量信息。常量池中有CONSTANT_CLASS_INFO类型的常量，表示该class中声明了要用到那些类： 当AppClassLoader要加载 org.luanlouis.jvm.load.Main类时，会去查看该类的定义，发现它内部声明使用了其它的类： sun.security.pkcs11.P11Util、java.lang.Object、java.lang.System、java.io.PrintStream、java.lang.Class；org.luanlouis.jvm.load.Main类要想正常工作，首先要能够保证这些其内部声明的类加载成功。所以AppClassLoader要先将这些类加载到内存中。（注：为了理解方便，这里没有考虑懒加载的情况，事实上的JVM加载类过程比这复杂的多） 加载顺序： 加载java.lang.Object、java.lang.System、java.io.PrintStream、java,lang.Class AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载； 而ExtClassLoader发现不是其加载范围，其返回null； AppClassLoader发现父类加载器ExtClassLoader无法加载， 则会查询这些类是否已经被BootstrapClassLoader加载过， 结果表明这些类已经被BootstrapClassLoader加载过， 则无需重复加载，直接返回对应的Class&lt;T&gt;实例； 加载sun.security.pkcs11.P11Util 此在{JRE_HOME}/lib/ext/sunpkcs11.jar包内，属于ExtClassLoader负责加载的范畴。 AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载； 而ExtClassLoader发现其正好属于加载范围，故ExtClassLoader负责将其加载到内存中。 ExtClassLoader在加载sun.security.pkcs11.P11Util时也分析这个类内都使用了哪些类， 并将这些类先加载内存后，才开始加载sun.security.pkcs11.P11Util， 加载成功后直接返回对应的Class&lt;sun.security.pkcs11.P11Util&gt;实例； 加载org.luanlouis.jvm.load.Main AppClassLoader尝试加载这些类的时候，会先委托ExtClassLoader进行加载； 而ExtClassLoader发现不是其加载范围，其返回null； AppClassLoader发现父类加载器ExtClassLoader无法加载， 则会查询这些类是否已经被BootstrapClassLoader加载过。 而结果表明BootstrapClassLoader 没有加载过它， 这时候AppClassLoader只能自己动手负责将其加载到内存中， 然后返回对应的Class&lt;org.luanlouis.jvm.load.Main&gt;实例引用； 以上三步骤都成功，才表示classLoader.loadClass(&quot;org.luanlouis.jvm.load.Main&quot;)完成，上述操作完成后，JVM内存方法区的格局会如下所示： 如上图所示： JVM方法区的类信息区是按照类加载器进行划分的，每个类加载器会维护自己加载类信息； 某个类加载器在加载相应的类时，会相应地在JVM内存堆（Heap）中创建一个对应的Class&lt;T&gt;，用来表示访问该类信息的入口 五、使用Main类的main方法作为程序入口运行程序 就是去执行指令，过程与Java如何执行一个最简单的程序类似。 六、方法执行完毕，JVM销毁，释放内存 对于本程序，主程序执行完毕，释放主函数所在的栈帧，释放堆中的内存。 七、再来回顾回顾java类加载器相关的概念吧 本处的内容为再次简单说明，具体见双亲委派模型。类加载器(Class Loader)：顾名思义，指的是可以加载类的工具。JVM自身定义了三个类加载器：引导类加载器(Bootstrap Class Loader)、拓展类加载器(Extension Class Loader )、应用加载器(Application Class Loader)。当然，我们有时候也会自己定义一些类加载器来满足自身的需要。 引导类加载器(Bootstrap Class Loader): 该类加载器使JVM使用C++/C底层代码实现的加载器，用以加载JVM运行时所需要的系统类，这些系统类在{JRE_HOME}/lib目录下。由于类加载器是使用平台相关的底层C++/C语言实现的， 所以该加载器不能被Java代码访问到。但是，我们可以查询某个类是否被引导类加载器加载过。我们经常使用的系统类如：java.lang.String,java.lang.Object,java.lang*… 这些都被放在 {JRE_HOME}/lib/rt.jar包内， 当JVM系统启动的时候，引导类加载器会将其加载到 JVM内存的方法区中。 拓展类加载器(Extension Class Loader): 该加载器是用于加载 java 的拓展类 ，拓展类一般会放在{JRE_HOME}/lib/ext/ 目录下，用来提供除了系统类之外的额外功能。拓展类加载器是是整个JVM加载器的Java代码可以访问到的类加载器的最顶端，即是超级父加载器，拓展类加载器是没有父类加载器的。（注意，其实引导类加载器不能算是扩展类加载器的父类，我们从源码中可以看出来的） 应用类加载器(Applocatoin Class Loader): 该类加载器是用于加载用户代码，是用户代码的入口。我经常执行指令 java xxx.x.xxx.x.x.XClass , 实际上，JVM就是使用的AppClassLoader加载 xxx.x.xxx.x.x.XClass 类的。 用户自定义类加载器（Customized Class Loader）：用户可以自己定义类加载器来加载类。所有的类加载器都要继承java.lang.ClassLoader类。 关于双亲委派模型，就不再赘述了。 八、线程上下文加载器 Java 任何一段代码的执行，都有对应的线程上下文。如果我们在代码中，想看当前是哪一个线程在执行当前代码，我们经常是使用如下方法： 1Thread thread = Thread.currentThread();//返回对当当前运行线程的引用 相应地，我们可以为当前的线程指定类加载器。在上述的例子中， 当执行 java org.luanlouis.jvm.load.Main 的时候，JVM会创建一个Main线程，而创建应用类加载器AppClassLoader的时候，会将AppClassLoader设置成Main线程的上下文类加载器： 123456789101112131415161718public Launcher() &#123; Launcher.ExtClassLoader var1; try &#123; var1 = Launcher.ExtClassLoader.getExtClassLoader(); &#125; catch (IOException var10) &#123; throw new InternalError("Could not create extension class loader", var10); &#125; try &#123; this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); &#125; catch (IOException var9) &#123; throw new InternalError("Could not create application class loader", var9); &#125; //将AppClassLoader设置成当前线程的上下文加载器 Thread.currentThread().setContextClassLoader(this.loader); //....... &#125; 线程上下文类加载器是从线程的角度来看待类的加载，为每一个线程绑定一个类加载器，可以将类的加载从单纯的 双亲加载模型解放出来，进而实现特定的加载需求。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态分派和动态分派]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E9%9D%99%E6%80%81%E5%88%86%E6%B4%BE%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E6%B4%BE%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十二篇文章，本章说明静态分派和动态分派的原理。 这里所谓的分派指的是在Java中对方法的调用。Java中有三大特性：封装、继承和多态。分派是多态性的体现，Java虚拟机底层提供了我们开发中“重写”和“重载”的底层实现。其中重载属于静态分派，而重写则是动态分派的过程。除了使用分派的方式对方法进行调用之外，还可以使用解析调用，解析调用是在编译期间就已经确定了，在类装载的解析阶段就会把符号引用转化为直接引用，不会延迟到运行期间再去完成。而分派调用则既可以是静态的也可以是动态（就是这里的静态分派和动态分派）的。 方法解析 对于方法的调用，虚拟机提供了四条方法调用的字节码指令，分别是： invokestatic: 调用静态方法 invokespecial: 调用构造方法，私有方法，父类方法 invokevirtual: 调用虚方法 invokeinterface: 调用接口方法 其中，1和2都可以在类加载阶段确定方法的唯一版本，因此，在类加载阶段就可以把符号引用解析为直接引用，在调用时刻直接找到方法代码块的内存地址进行执行（编译时已经找到了，并且存在方法调用的入口）；3和4则是在运行期间动态绑定方法的直接引用。 invokestatic指令和invokespecial指令调用的方法称为非虚方法，注意，final修饰的方法也属于虚方法。 静态分派 静态分派只会涉及重载，而重载是在编译期间确定的，那么静态分派自然是一个静态的过程（因为还没有涉及到Java虚拟机）。静态分派的最直接的解释是在重载的时候是通过参数的静态类型而不是实际类型作为判断依据的。比如创建一个类O，在O中创建了静态类内部类A，O中又有两个静态类内部类B、C继承了这个静态内部类A，那么实际上当编写如下的代码： 123456789101112131415161718192021public class O&#123; static class A&#123;&#125; static class B extends A&#123;&#125; static class C extends A&#123;&#125; public void a(A a)&#123; System.out.println("A method"); &#125; public void a(B b)&#123; System.out.println("B method"); &#125; public void a(C c)&#123; System.out.println("C method"); &#125; public static void main(String[] args)&#123; O o = new O(); A b = new B(); A c = new C(); o.a(b); o.a(c); &#125;&#125; 运行的结果是打印出连个“A method”。原因在于静态类型的变化仅仅在使用时发生，变量本身的类型不会发生变化。 比如我们这里中A b = new B();虽然在创建的时候是B的对象，但是当调用o.a(b)的时候才发现是A的对象，所以会输出“A method”。**也就是说在发生重载的时候，Java虚拟机是通过参数的静态类型而不是实际参数类型作为判断依据的。**因此，在编译阶段，Javac编译器选择了a(A a)这个重载方法。 虽然编译器能够在编译阶段确定方法的版本，但是很多情况下重载的版本不是唯一的，在这种模糊的情况下，编译器会选择一个更合适的版本。例如，重载的方法中，参数列表除了参数类型不一样，其他都一样，例接收的参数有char\int\long等，传入参数‘a’，则会调用需要char类型参数的方法，去掉需要char类型参数的方法，则会调用需要int类型参数的方法。这时发生了一次自动类型转换。同样，去掉需要int类型参数的方法，则会调用需要long类型参数的方法。这里再次发生类型转换，会按照char-&gt;int-&gt;long-&gt;float-&gt;double转换类型。 动态分派 动态分派与重写(Override)有着很密切的关联。如下代码： 12345678910111213141516171819202122232425262728package com.xtayfjpk.jvm.chapter8; public class DynamicDispatch &#123; static abstract class Human &#123; protected abstract void sayHello(); &#125; static class Man extends Human &#123; @Override protected void sayHello() &#123; System.out.println("man say hello"); &#125; &#125; static class Woman extends Human &#123; @Override protected void sayHello() &#123; System.out.println("woman say hello"); &#125; &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); man.sayHello(); woman.sayHello(); man = new Woman(); man.sayHello(); &#125; &#125; 这里显然不可能是根据静态类型来决定的，因为静态类型都是Human的两个变量man和woman在调用sayHello()方法时执行了不同的行为，并且变量man在两次调用中执行了不同的方法。 导致这个现象的原是是这两个变量的实际类型不同。那么Java虚拟机是如何根据实际类型来分派方法执行版本的呢，我们使用javap命令输出这段代码的字节码，结果如下： 1234567891011121314151617181920212223public static void main(java.lang.String[]); flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: new #16 // class com/xtayfjpk/jvm/chapter8/DynamicDispatch$Man 3: dup 4: invokespecial #18 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Man."&lt;init&gt;":()V 7: astore_1 8: new #19 // class com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman 11: dup 12: invokespecial #21 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman."&lt;init&gt;":()V 15: astore_2 16: aload_1 17: invokevirtual #22 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Human.sayHello:()V 20: aload_2 21: invokevirtual #22 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Human.sayHello:()V 24: new #19 // class com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman 27: dup 28: invokespecial #21 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Woman."&lt;init&gt;":()V 31: astore_1 32: aload_1 33: invokevirtual #22 // Method com/xtayfjpk/jvm/chapter8/DynamicDispatch$Human.sayHello:()V 36: return 0-15行的字节码是准备动作，作用是建立man和woman的内存空间，调用Man和Woman类的实例构造器，将这两个实例的引用存放在第1和第2个局部变量表Slot之中，这个动作对应了代码中这两句： 12Human man = new Man(); Human woman = new Woman(); 接下来的第16-21行是关键部分，第16和第20两行分别把刚刚创建的两个对象的引用压到栈顶，这两个对象是将执行的sayHello()方法的所有者，称为接收者(Receiver)。 第17和第21两行是方法调用指令，单从字节码的角度来看，这两条调用指令无论是指令(都是invokevirtual)还是参数(都是常量池中Human.sayHello()的符号引用)都完全一样，但是这两条指令最终执行的目标方法并不相同，其原因需要从invokevirutal指令的多态查找过程开始说起，invokevirtual指令的运行时解析过程大致分为以下步骤： 找到操作数栈顶的第一个元素所指向的对象实际类型，记作C。 如果在类型C中找到与常量中描述符和简单名称都相同的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，查找结束；不通过则返回java.lang.IllegalAccessError错误。 否则，按照继承关系从下往上依次对C的各个父类进行第2步的搜索与校验过程。 如果始终没有找到合适的方法，则抛出java.lang.AbstractMethodError错误。 由于invokevirtual指令执行的第一步就是在运行期确定接收者的实际类型，所以两次调用中的invokevirtual指令把常量池中的类方法符号引用解析到了不同的直接引用上，这个过程就是Java语言中方法重写的本质。我们把这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派。 单分派与多分派 方法的接收者与方法的参数统称为方法的宗量。根据分派基于多少种宗量，可以将分派划分为单分派与多分派两种。单分派是根据一个宗量来对目标方法进行选择，多分派则是根据多于一个宗量对目标方法进行选择。 在编译期的静态分派过程选择目标方法的依据有两点：一是静态类型；二是方法参数，所以Java语言的静态分派属于多分派类型。在运行阶段虚拟机的动态分派过程只能接收者的实际类型一个宗量作为目标方法选择依据，所以Java语言的动态分派属于单分派类型。所以Java语言是一门静态多分派，动态单分派语言。 JVM实现动态分派 动态分派在Java中被大量使用，使用频率及其高，如果在每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话就可能影响到执行效率，因此JVM在类的方法区中建立虚方法表（virtual method table）来提高性能。 ⭐⭐⭐每个类中都有一个虚方法表，表中存放着各个方法的实际入口。如果某个方法在子类中没有被重写，那子类的虚方法表中该方法的地址入口和父类该方法的地址入口一样，即子类的方法入口指向父类的方法入口。如果子类重写父类的方法，那么子类的虚方法表中该方法的实际入口将会被替换为指向子类实现版本的入口地址。 那么虚方法表什么时候被创建？虚方法表会在类加载的连接阶段被创建并开始初始化，类的变量初始值准备完成之后，JVM会把该类的方法表也初始化完毕。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类的初始化过程]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E7%B1%BB%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十一篇文章，其实在前面的文章中已经说到了类加载机制，但是为了本文的完整性，前面一部分还是重复地放在这里，后面会着重说明初始化过程。 1. 类加载过程 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)7个阶段。其中准备、验证、解析3个部分统称为连接（Linking）。如图所示： 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。以下陈述的内容都以HotSpot为基准。 2. 加载 在加载阶段（可以参考java.lang.ClassLoader的loadClass()方法），虚拟机需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流（并没有指明要从一个Class文件中获取，可以从其他渠道，譬如：网络、动态生成、数据库等）； 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构； 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口； 加载阶段和连接阶段（Linking）的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 3. 验证 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以魔术0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 4. 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值，假设一个类变量的定义为： 1public static int value=123; 那变量value在准备阶段过后的初始值为0而不是123.因为这时候尚未开始执行任何java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行。 至于“特殊情况”是指：public static final int value=123，即当类字段的字段属性是ConstantValue时，会在准备阶段初始化为指定的值，所以标注为final之后，value的值在准备阶段初始化为123而非0. 5. 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 6. 初始化 类初始化阶段是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的java程序代码。在准备阶段，变量已经赋过一次系统要求的初始值，而在初始化阶段，则根据程序猿通过程序制定的主观计划去初始化类变量和其他资源，或者说：初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程. &lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块static{}中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。如下： 123456789public class Test&#123; static &#123; i=0; System.out.println(i);//这句编译器会报错：Cannot reference a field before it is defined（非法向前应用） &#125; static int i=1;&#125; 那么去掉报错的那句，改成下面： 1234567891011121314public class Test&#123; static &#123; i=0;// System.out.println(i); &#125; static int i=1; public static void main(String args[]) &#123; System.out.println(i); &#125;&#125; 输出结果是什么呢？当然是1啦~在准备阶段我们知道i=0，然后类初始化阶段按照顺序执行，首先执行static块中的i=0,接着执行static赋值操作i=1,最后在main方法中获取i的值为1。 &lt;clinit&gt;()方法与实例构造器&lt;init&gt;()方法不同，它不需要显示地调用父类构造器，虚拟机会保证在子类&lt;cinit&gt;()方法执行之前，父类的&lt;clinit&gt;()方法已经执行完毕. ⭐由于父类的&lt;clinit&gt;()方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。 &lt;clinit&gt;()方法对于类或者接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生产&lt;clinit&gt;()方法。 虚拟机会保证一个类的&lt;clinit&gt;()方法在多线程环境中被正确的加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;()方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;()方法完毕。如果在一个类的&lt;clinit&gt;()方法中有耗时很长的操作，就可能造成多个线程阻塞，在实际应用中这种阻塞往往是隐藏的。 虚拟机规范严格规定了有且只有5中情况（jdk1.7）必须对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到 new , getstatic , putstatic , invokestatic 这些字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：①使用new关键字实例化对象的时候、②读取或设置一个类的静态字段（被final修饰、已在编译器把结果放入常量池的静态字段除外）的时候，以及③调用一个类的静态方法的时候。 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 下面说明三种被动引用(除了上面提到的五种情况外，所有引用类的方法都不会触发初始化，成为被动引用)。 第一种：通过子类引用父类的静态字段，不会导致子类初始化。 1234567public class SuperClass &#123; static &#123; System.out.println("superclass static init"); &#125; public static int value = 123;&#125; 12345public class SubClass extends SuperClass&#123; static&#123; System.out.println("SubClass static init"); &#125;&#125; 12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); &#125;&#125; 结果是： 12superclass static init123 说明：对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 第二种：通过数组定义来引用类，不会触发此类的初始化 12345678910package chapter12;//SuperClass复用上面个代码public class NotInitialization&#123; public static void main(String[] args) &#123; SuperClass[] sca = new SuperClass[10]; &#125;&#125; 运行结果：（无） 说明：从结果来看，显然没有触发类chapter12.SuperClass的初始化阶段，但是这段代码触发了另一个名叫 &quot;[Lchapter12.SuperClass&quot;的类的初始化阶段。这显然不是一个合法的类名称，他是由虚拟机自动生成的、直接继承于java.lang.Object的子类，创建动作由字节码制定newarray触发。 这个类代表了一个元素类型为chapter12.SuperClass的一维数组，数组中应有的属性和方法(用于可直接使用的只有被修饰为public的length属性和clone()方法)都实现在这个类里。Java语言中对数组的访问比C/C++相对安全是因为这个类封装了数组元素的访问方法，而C/C++ 直接翻译为对数组指针的移动。 第三种：常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化 12345678910111213141516public class ConstClass&#123; static &#123; System.out.println("ConstClass init!"); &#125; public static final String HELLOWORLD = "hello world";&#125;public class NotInitialization&#123; public static void main(String[] args) &#123; System.out.println(ConstClass.HELLOWORLD); &#125;&#125; 运行结果：hello world 说明：上述代码虽然在java源码中引用了ConstClass类中的常量hello world，但是其实在编译阶段通过常量传播优化，已经将此常量值hello world存储到了NotInitialization的常量池中，以后NotInitialization对常量ConstClass.HELLOWORLD的引用实际上都被转化为NotInitialization对自身常量池的引用了。 7. 接口的加载 接口的加载过程与类加载过程有一些不同，针对接口需要做一些特殊说明： 接口也有初始化过程，而接口中不能使用static{}语句块，但编译器仍然会为接口生成&quot;&lt;clinit()&gt;&quot;类构造器，用于初始化接口中所定义的成员变量。 接口与类真正所区别的是前面讲述的5种“有且仅有”情况的第三种：当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个借口在初始化时，并不要求其父接口全部都已经完成了初始化，只有在真正用到父接口时（如引用接口中定义的常量）才会初始化。 8. 例子巩固 1234567public class SSClass&#123; static &#123; System.out.println("SSClass"); &#125;&#125; 1234567891011121314public class SuperClass extends SSClass&#123; static &#123; System.out.println("SuperClass init!"); &#125; public static int value = 123; public SuperClass() &#123; System.out.println("init SuperClass"); &#125;&#125; 1234567891011121314public class SubClass extends SuperClass&#123; static &#123; System.out.println("SubClass init"); &#125; static int a; public SubClass() &#123; System.out.println("init SubClass"); &#125;&#125; 1234567public class NotInitialization&#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); &#125;&#125; 运行结果： 123SSClassSuperClass init!123 说明：对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 9.总结java执行顺序 举例立刻明白： 123456789101112131415public class Children extends Parent&#123; public Children() &#123; System.out.println("Children构造函数"); &#125; &#123; System.out.println("Children普通代码块"); &#125; static &#123; System.out.println("Children静态代码块"); &#125; public static void main(String[] args) &#123; Children children = new Children(); &#125;&#125; 1234567891011public class Parent &#123; public Parent() &#123; System.out.println("Parent构造函数"); &#125; &#123; System.out.println("Parent普通代码块"); &#125; static &#123; System.out.println("Parent静态代码块"); &#125;&#125; 执行结果： 123456Parent静态代码块Children静态代码块Parent普通代码块Parent构造函数Children普通代码块Children构造函数 总结： 123456父类静态块自身静态块父类块父类构造器自身块自身构造器 10. 总结java赋值顺序 举例立刻明白： 12345678910111213141516public class Parent &#123; public String flag = "父类成员变量赋值"; public Parent() &#123; System.out.println(); System.out.println("父类构造器---&gt;" + flag); flag = "父类构造器赋值"; System.out.println("父类构造器---&gt;" + flag); &#125; &#123; System.out.println("父类代码块---&gt;" + flag); flag = "父类代码块赋值"; System.out.println("父类代码块---&gt;" + flag); &#125;&#125; 123456789101112131415161718192021222324252627public class Children extends Parent&#123; public String flag = "成员变量赋值"; public Children() &#123; System.out.println(); System.out.println("子类构造器---&gt;" + flag); flag = "子类构造器赋值"; System.out.println("子类构造器---&gt;" + flag); &#125; &#123; System.out.println(); System.out.println("子类代码快---&gt;" + flag); flag = "子类代码块赋值"; System.out.println("子类代码块---&gt;" + flag); &#125; public void setFlag()&#123; System.out.println(); System.out.println("子类方法---&gt;" + flag); &#125; public static void main(String[] args) &#123; Children children = new Children(); children.setFlag(); &#125;&#125; 运行结果： 12345678910111213父类代码块---&gt;父类成员变量赋值父类代码块---&gt;父类代码块赋值父类构造器---&gt;父类代码块赋值父类构造器---&gt;父类构造器赋值子类代码快---&gt;成员变量赋值子类代码块---&gt;子类代码块赋值子类构造器---&gt;子类代码块赋值子类构造器---&gt;子类构造器赋值子类方法---&gt;子类构造器赋值 总结： 12345678910父类的静态变量赋值自身的静态变量赋值父类成员变量赋值父类块赋值父类构造器赋值自身成员变量赋值自身块赋值自身构造器赋值]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读5-Class文件中的方法表集合--method方法在class文件中是怎样组织的]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB5-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%E8%A1%A8%E9%9B%86%E5%90%88--method%E6%96%B9%E6%B3%95%E5%9C%A8class%E6%96%87%E4%BB%B6%E4%B8%AD%E6%98%AF%E6%80%8E%E6%A0%B7%E7%BB%84%E7%BB%87%E7%9A%84%2F</url>
    <content type="text"><![CDATA[继续讲class文件中的方法表集合。 1. 方法表集合概述 方法表集合是指由若干个方法表（method_info）组成的集合。对于在类中定义的若干个经过JVM编译成class文件后，会将相应的method方法信息组织到一个叫做方法表集合的结构中，字段表集合是一个类数组结构，如下图所示： 2. method方法的描述-方法表集合在class文件中的位置 method方法的描述-方法表集合紧跟在字段表集合的后面，如下图所示： 3. 一个类中的method方法应该包含哪些信息？----method_info结构体的定义 对于一个方法的表示，我们根据我们可以概括的信息如下所示： 实际上JVM还会对method方法的描述添加其他信息，我们将在后面详细讨论。如上图中的method_info结构体的定义，该结构体的定义跟描述field字段 的field_info结构体的结构几乎完全一致,如下图所示。 方法表的结构体由：访问标志(access_flags)、名称索引(name_index)、描述索引(descriptor_index)、属性表(attribute_info)集合组成。 访问标志(access_flags)： method_info结构体最前面的两个字节表示的访问标志（access_flags），记录这这个方法的作用域、静态or非静态、可变性、是否可同步、是否本地方法、是否抽象等信息，实际上不止这些信息，我们后面会详细介绍访问标志这两个字节的每一位具体表示什么意思。 名称索引(name_index)： 紧跟在访问标志（access_flags）后面的两个字节称为名称索引，这两个字节中的值指向了常量池中的某一个常量池项，这个方法的名称以UTF-8格式的字符串存储在这个常量池项中。如public void methodName(),很显然，“methodName”则表示着这个方法的名称，那么在常量池中会有一个CONSTANT_Utf8_info格式的常量池项，里面存储着“methodName”字符串，而mehodName()方法的方法表中的名称索引则指向了这个常量池项。 描述索引(descriptor_index)： 描述索引表示的是这个方法的特征或者说是签名，一个方法会有若干个参数和返回值，而若干个参数的数据类型和返回值的数据类型构成了这个方法的描述，其基本格式为： (参数数据类型描述列表)返回值数据类型 。我们将在后面继续讨论。 属性表(attribute_info)集合： 这个属性表集合非常重要，方法的实现被JVM编译成JVM的机器码指令，机器码指令就存放在一个Code类型的属性表中；如果方法声明要抛出异常，那么异常信息会在一个Exceptions类型的属性表中予以展现。Code类型的属性表可以说是非常复杂的内容，也是本文最难的地方。 4. 访问标志(access_flags)—记录着method方法的访问信息 访问标志（access_flags）共占有2 个字节，分为 16 位，这 16位 表示的含义如下所示： 举例：某个类中定义了如下方法： 12public static synchronized final void greeting()&#123; &#125; greeting()方法的修饰符有：public、static、synchronized、final 这几个修饰符修饰，那么相对应地，greeting()方法的访问标志中的ACC_PUBLIC、ACC_STATIC、ACC_SYNCHRONIZED、ACC_FINAL标志位都应该是1，即： 从上图中可以看出访问标志的值应该是二进制00000000 00111001,即十六进制0x0039。我们将在文章的最后一个例子中证实这点。 5. 名称索引和描述符索引----一个方法的签名 紧接着访问标志（access_flags）后面的两个字节，叫做名称索引(name_index)，这两个字节中的值是指向了常量池中某个常量池项的索引，该常量池项表示这这个方法名称的字符串。 方法描述符索引(descrptor_index)是紧跟在名称索引后面的两个字节，这两个字节中的值跟名称索引中的值性质一样，都是指向了常量池中的某个常量池项。这两个字节中的指向的常量池项，是表示了方法描述符的字符串。 所谓的方法描述符，实质上就是指用一个什么样的字符串来描述一个方法，方法描述符的组成如下图所示： 举例：对于如下定义的的greeting()方法，我们来看一下对应的method_info结构体中的名称索引和描述符索引信息是怎样组织的。 12public static synchronized final void greeting()&#123; &#125; 如下图所示,method_info结构体的名称索引中存储了一个索引值x，指向了常量池中的第x项，第 x项表示的是字符串&quot;greeting&quot;,即表示该方法名称是&quot;greeting&quot;；描述符索引中的y 值指向了常量池的第y项，该项表示字符串&quot;()V&quot;，即表示该方法没有参数，返回值是void类型。 6. 属性表集合–记录方法的机器指令和抛出异常等信息 属性表集合记录了某个方法的一些属性信息，这些信息包括： 这个方法的代码实现，即方法的可执行的机器指令 这个方法声明的要抛出的异常信息 这个方法是否被@deprecated注解表示 这个方法是否是编译器自动生成的 属性表（attribute_info）结构体的一般结构如下所示： 修正：属性长度为4个字节。 6.1 Code类型的属性表–method方法中的机器指令的信息 Code类型的属性表(attribute_info)可以说是class文件中最为重要的部分，因为它包含的是JVM可以运行的机器码指令，JVM能够运行这个类，就是从这个属性中取出机器码的。除了要执行的机器码，它还包含了一些其他信息，如下所示： Code属性表的组成部分： 机器指令----code： 目前的JVM使用一个字节表示机器操作码，即对JVM底层而言，它能表示的机器操作码不多于2的 8 次方，即 256个。class文件中的机器指令部分是class文件中最重要的部分，并且非常复杂，本文的重点不止介绍它 异常处理跳转信息—exception_table： 如果代码中出现了try{}catch{}块，那么try{}块内的机器指令的地址范围记录下来，并且记录对应的catch{}块中的起始机器指令地址，当运行时在try块中有异常抛出的话，JVM会将catch{}块对应懂得其实机器指令地址传递给PC寄存器，从而实现指令跳转； Java源码行号和机器指令的对应关系—LineNumberTable属性表： 编译器在将java源码编译成class文件时，会将源码中的语句行号跟编译好的机器指令关联起来，这样的class文件加载到内存中并运行时，如果抛出异常，JVM可以根据这个对应关系，抛出异常信息，告诉我们我们的源码的多少行有问题，方便我们定位问题。这个信息不是运行时必不可少的信息，但是默认情况下，编译器会生成这一项信息，如果你项取消这一信息，你可以使用-g:none 或-g:lines来取消或者要求设置这一项信息。如果使用了-g:none来生成class文件，class文件中将不会有LineNumberTable属性表，造成的影响就是 将来如果代码报错，将无法定位错误信息报错的行，并且如果项调试代码，将不能在此类中打断点（因为没有指定行号。） 局部变量表描述信息----LocalVariableTable属性表： 局部变量表信息会记录栈帧局部变量表中的变量和java源码中定义的变量之间的关系，这个信息不是运行时必须的属性，默认情况下不会生成到class文件中。你可以根据javac指令的-g:none或者-g:vars选项来取消或者设置这一项信息。 它有什么作用呢？ 当我们使用IDE进行开发时，最喜欢的莫过于它们的代码提示功能了。如果在项目中引用到了第三方的jar包，而第三方的包中的class文件中有无LocalVariableTable属性表的区别如下所示： Code属性表结构体的解释： attribute_name_index,属性名称索引，占有2个字节，其内的值指向了常量池中的某一项，该项表示字符串“Code”; attribute_length,属性长度，占有 4个字节，其内的值表示后面有多少个字节是属于此Code属性表的； max_stack,操作数栈深度的最大值，占有 2 个字节，在方法执行的任意时刻，操作数栈都不应该超过这个值，虚拟机的运行的时候，会根据这个值来设置该方法对应的栈帧(Stack Frame)中的操作数栈的深度； max_locals,最大局部变量数目，占有 2个字节，其内的值表示局部变量表所需要的存储空间大小； code_length,机器指令长度，占有 4 个字节，表示跟在其后的多少个字节表示的是机器指令； code,机器指令区域，该区域占有的字节数目由 code_length中的值决定。JVM最底层的要执行的机器指令就存储在这里； exception_table_length,显式异常表长度，占有2个字节，如果在方法代码中出现了try{} catch()形式的结构，该值不会为空，紧跟其后会跟着若干个exception_table结构体，以表示异常捕获情况； exception_table，显式异常表，占有8 个字节，start_pc,end_pc,handler_pc中的值都表示的是PC计数器中的指令地址。exception_table表示的意思是：如果字节码从第start_pc行到第end_pc行之间出现了catch_type所描述的异常类型，那么将跳转到handler_pc行继续处理。 attribute_count,属性计数器，占有 2 个字节，表示Code属性表的其他属性的数目 attribute_info,表示Code属性表具有的属性表，它主要分为两个类型的属性表：“LineNumberTable”类型和“LocalVariableTable”类型。 “LineNumberTable”类型的属性表记录着Java源码和机器指令之间的对应关系 “LocalVariableTable”类型的属性表记录着局部变量描述 举例： 如下定义Simple类，使用javac -g:none Simple.java 编译出Simple.class 文件，并使用javap -v Simple &gt; Simple.txt 查看反编译的信息，然后看Simple.class文件中的方法表集合是怎样组织的： 12345public class Simple &#123; public static synchronized final void greeting()&#123; int a = 10; &#125; &#125; 1. Simple.class文件组织信息如下所示： 如上所示，方法表集合使用了蓝色线段圈了起来。 请注意：方法表集合的头两个字节，即方法表计数器（method_count）的值是0x0002，它表示该类中有2 个方法。细心的读者会注意到，我们的Simple.java中就定义了一个greeting()方法，为什么class文件中会显示有两个方法呢？？ JVM为没有显式定义实例化构造方法的类，自动生成默认的实例化构造方法&quot;()&quot; 除了实例化构造方法，JVM还会在特殊的情况下生成一个叫类构造方法&quot;()&quot;。如果我们在类中使用到了static修饰的代码块，那么，JVM会在class文件中生成一个“()”构造方法。关于它们的具体细节，我将在后续的文章中详细讨论，在这里就不展开了。 Simple.class 中的() 方法: 解释： 方法访问标志(access_flags)： 占有 2个字节，值为0x0001,即标志位的第 16 位为 1，所以该()方法的修饰符是：ACC_PUBLIC; 名称索引(name_index)： 占有 2 个字节，值为 0x0004，指向常量池的第 4项，该项表示字符串“”，即该方法的名称是“”; 描述符索引(descriptor_index): 占有 2 个字节，值为0x0005,指向常量池的第 5 项，该项表示字符串“()V”，即表示该方法不带参数，并且无返回值（构造函数确实也没有返回值）； 属性计数器（attribute_count): 占有 2 个字节，值为0x0001,表示该方法表中含有一个属性表，后面会紧跟着一个属性表； 属性表的名称索引(attribute_name_index)：占有 2 个字节，值为0x0006,指向常量池中的第6 项，该项表示字符串“Code”，表示这个属性表是Code类型的属性表； 属性长度（attribute_length）：占有4个字节，值为0x0000 0011，即十进制的 17，表明后续的 17 个字节可以表示这个Code属性表的属性信息； 操作数栈的最大深度（max_stack）：占有2个字节，值为0x0001,表示栈帧中操作数栈的最大深度是1； 局部变量表的最大容量（max_variable）：占有2个字节，值为0x0001, JVM在调用该方法时，根据这个值设置栈帧中的局部变量表的大小； 机器指令数目(code_length)：占有4个字节，值为0x0000 0005,表示后续的5 个字节 0x2A 、0xB7、 0x00、0x01、0xB1表示机器指令; 机器指令集(code[code_length])：这里共有 5个字节，值为0x2A 、0xB7、 0x00、0x01、0xB1； 显式异常表集合（exception_table_count）： 占有2 个字节，值为0x0000,表示方法中没有需要处理的异常信息； Code属性表的属性表集合（attribute_count）： 占有2 个字节，值为0x0000，表示它没有其他的属性表集合，因为我们使用了-g:none 禁止编译器生成Code属性表的 LineNumberTable 和LocalVariableTable; B. Simple.class 中的greeting() 方法: 解释： 方法访问标志(access_flags)： 占有 2个字节，值为 0x0039 ,即二进制的00000000 00111001,即标志位的第11、12、13、16位为1，根据上面讲的方法标志位的表示，可以得到该greeting()方法的修饰符有：ACC_SYNCHRONIZED、ACC_FINAL、ACC_STATIC、ACC_PUBLIC; 名称索引(name_index)： 占有 2 个字节，值为 0x0007，指向常量池的第 7 项，该项表示字符串“greeting”，即该方法的名称是“greeting”; 描述符索引(descriptor_index): 占有 2 个字节，值为0x0005,指向常量池的第 5 项，该项表示字符串“()V”，即表示该方法不带参数，并且无返回值； 属性计数器（attribute_count): 占有 2 个字节，值为0x0001,表示该方法表中含有一个属性表，后面会紧跟着一个属性表； 属性表的名称索引(attribute_name_index)：占有 2 个字节，值为0x0006,指向常量池中的第6 项，该项表示字符串“Code”，表示这个属性表是Code类型的属性表； 属性长度（attribute_length）：占有4个字节，值为0x0000 0010，即十进制的16，表明后续的16个字节可以表示这个Code属性表的属性信息； 操作数栈的最大深度（max_stack）：占有2个字节，值为0x0001,表示栈帧中操作数栈的最大深度是1； 局部变量表的最大容量（max_variable）：占有2个字节，值为0x0001, JVM在调用该方法时，根据这个值设置栈帧中的局部变量表的大小； 机器指令数目(code_length)：占有4 个字节，值为0x0000 0004,表示后续的4个字节0x10、 0x0A、 0x3B、0xB1的是表示机器指令; 机器指令集(code[code_length])：这里共有4 个字节，值为0x10、 0x0A、 0x3B、0xB1 ； 显式异常表集合（exception_table_count）： 占有2 个字节，值为0x0000,表示方法中没有需要处理的异常信息； Code属性表的属性表集合（attribute_count）： 占有2 个字节，值为0x0000，表示它没有其他的属性表集合，因为我们使用了-g:none 禁止编译器生成Code属性表的 LineNumberTable 和LocalVariableTable; 6.2 Exceptions类型的属性表----method方法声明的要抛出的异常信息 有些方法在定义的时候，会声明该方法会抛出什么类型的异常，如下定义一个Interface接口，它声明了sayHello()方法，抛出Exception异常： 123public interface Interface &#123; public void sayHello() throws Exception; &#125; 现在让我们看一下Exceptions类型的属性表(attribute_info)结构体是怎样组织的： 如上图所示，Exceptions类型的属性表(attribute_info)结构体由一下元素组成： 属性名称索引(attribute_name_index)：占有 2个字节，其中的值指向了常量池中的表示&quot;Exceptions&quot;字符串的常量池项； 属性长度(attribute_length)：它比较特殊，占有4个字节，它的值表示跟在其后面多少个字节表示异常信息； 异常数量(number_of_exceptions)：占有2 个字节，它的值表示方法声明抛出了多少个异常，即表示跟在其后有多少个异常名称索引； 异常名称索引(exceptions_index_table)：占有2个字节，它的值指向了常量池中的某一项，该项是一个CONSTANT_Class_info类型的项，表示这个异常的完全限定名称； Exceptions类型的属性表的长度计算 如果某个方法定义中，没有声明抛出异常，那么，表示该方法的方法表(method_info)结构体中的属性表集合中不会有Exceptions类型的属性表；换句话说，如果方法声明了要抛出的异常，方法表(method_info)结构体中的属性表集合中必然会有Exceptions类型的属性表，并且该属性表中的异常数量不小于1。 我们假设异常数量中的值为 N，那么后面的异常名称索引的数量就为N，它们总共占有的字节数为N*2，而异常数量占有2个字节，那么将有下面的这个关系式： 属性长度(attribute_length)中的值= 2 + 2*异常数量(number_of_exceptions)中的值 Exceptions类型的属性表（attribute_info）的长度=2+4+属性长度(attribute_length)中的值 举例： 将上面定义的Interface接口类编译成class文件，然后我们查看Interface.class文件，找出方法表集合所在位置和相应的数据，并辅助javap -v Inerface 查看 由于sayHello()方法是在的Interface接口类中声明的，它没有被实现，所以它对应的方法表(method_info)结构体中的属性表集合中没有Code类型的属性表。 方法计数器（methods_count）中的值为0x0001，表明其后的方法表(method_info)就一个,即我们就定义了一个方法，其后会紧跟着一个方法表(method_info)结构体； 方法的访问标志（access_flags）的值是0x0401，二进制是00000100 00000001,第6位和第16位是1，对应上面的标志位信息，可以得出它的访问标志符有：ACC_ABSTRACT、ACC_PUBLIC。细心的读者可能会发现，在上面声明的sayHello()方法中并没有声明为abstract类型啊。确实如此，这是因为编译器对于接口内声明的方法自动加上ACC_ABSTRACT标志。 名称索引（name_index）中的值为0x0005，0x0005指向了常量池的第5项，第五项表示的字符串为“sayHello”，即表示的方法名称是sayHello 描述符索引(descriptor_index)中的值为0x0006,0x0006指向了常量池中的第6项，第6项表示的字符串为“()V” 表示这个方法的无入参，返回值为void类型 属性表计数器(attribute_count)中的值为0x0001,表示后面的属性表的个数就1个，后面紧跟着一个attribute_info结构体； 属性表（attribute_info）中的属性名称索引(attribute_name_index)中的值为0x0007，0x0007指向了常量池中的第7 项，第 7项指向字符串“Exceptions”，即表示该属性表表示的异常信息； 属性长度（attribute_length）中的值为：0x00000004,即后续的4个字节将会被解析成属性值； 异常数量（number_of_exceptions）中的值为0x0001,表示这个方法声明抛出的异常个数是1个； 异常名称索引(exception_index_table)中的值为0x0008,指向了常量池中的第8项，第8项表示的是CONSTANT_Class_info类型的常量池项，表示“java/lang/Exception”，即表示此方法抛出了java.lang.Exception异常。 7. IDE代码提示功能实现的基本原理 每个IDE都提供了代码提示功能，它们实现的基本原理其实就是IDE针对它们项目下的包中所有的class文件进行建模，解析出它们的方法信息，当我们一定的条件时，IDE会自动地将合适条件的方法列表展示给开发者，供开发者使用。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读4-Class文件中的字段表集合--field字段在class文件中是怎样组织的]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB4-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E5%AD%97%E6%AE%B5%E8%A1%A8%E9%9B%86%E5%90%88--field%E5%AD%97%E6%AE%B5%E5%9C%A8class%E6%96%87%E4%BB%B6%E4%B8%AD%E6%98%AF%E6%80%8E%E6%A0%B7%E7%BB%84%E7%BB%87%E7%9A%84%2F</url>
    <content type="text"><![CDATA[继续讲class文件中的字段表集合。 1. 字段表集合概述 字段表集合是指由若干个字段表（field_info）组成的集合。对于在类中定义的若干个字段，经过JVM编译成class文件后，会将相应的字段信息组织到一个叫做字段表集合的结构中，字段表集合是一个类数组结构，如下图所示： 注意：这里所讲的字段是指在类中定义的静态或者非静态的变量，而不是在类中的方法内定义的变量。请注意区别。 比如，如果某个类中定义了5个字段，那么，JVM在编译此类的时候，会生成5个字段表（field_info）信息,然后将字段表集合中的字段计数器的值设置成5，将5个字段表信息依次放置到字段计数器的后面。 2. 字段表集合在class文件中的位置 字段表集合紧跟在class文件的接口索引集合结构的后面，如下图所示： 3. Java中的一个Field字段应该包含那些信息？------字段表field_info结构体的定义 针对上述的字段表示，JVM虚拟机规范规定了field_info结构体来描述字段，其表示信息如下： 下面我将一一讲解FIeld_info的组成元素：访问标志（access_flags）、名称索引（name_index）、描述索引（descriptor_index）、属性表集合 4. field字段的访问标志 如上图所示定义的field_info结构体，field字段的访问标志(access_flags)占有两个字节，它能够表述的信息如下所示： 举例：如果我们在某个类中有定义field域：private static String str;，那么在访问标志上，第15位ACC_PRIVATE和第13位ACC_STATIC标志位都应该为1。field域str的访问标志信息应该是如下所示： 5. 字段的数据类型表示和字段名称表示 class文件对数据类型的表示如下图所示： field字段名称，我们定义了一个形如private static String str的field字段，其中&quot;str&quot;就是这个字段的名称。 class文件将字段名称和field字段的数据类型表示作为字符串存储在常量池中。在field_info结构体中，紧接着访问标志的，就是字段名称索引和字段描述符索引，它们分别占有两个字节，其内部存储的是指向了常量池中的某个常量池项的索引，对应的常量池项中存储的字符串，分别表示该字段的名称和字段描述符。 6. 属性表集合-----静态field字段的初始化 在定义field字段的过程中，我们有时候会很自然地对field字段直接赋值，如下所示： 12public static final int MAX=100; public int count=0; 对于虚拟机而言，上述的两个field字段赋值的时机是不同的： 对于非静态（即无static修饰）的field字段的赋值将会出现在实例构造方法()中 对于静态的field字段，有两个选择：1、在静态构造方法()中进行；2 、使用ConstantValue属性进行赋值 Sun javac编译器对于静态field字段的初始化赋值策略： 如果使用final和static同时修饰一个field字段，并且这个字段是基本类型或者String类型的，那么编译器在编译这个字段的时候，会在对应的field_info结构体中增加一个ConstantValue类型的结构体，在赋值的时候使用这个ConstantValue进行赋值； 如果该field字段并没有被final修饰，或者不是基本类型或者String类型，那么将在类构造方法()中赋值。 对于上述的public static final init MAX=100： javac编译器在编译此field字段构建field_info结构体时，除了访问标志、名称索引、描述符索引外，会增加一个ConstantValue类型的属性表。 7. 实例解析 定义如下一个简单的Simple类，然后通过查看Simple.class文件内容并结合javap -v Simple 生成的常量池内容，分析str field字段的结构： 1234public class Simple &#123; private transient static final String str ="This is a test"; &#125; 字段计数器中的值为0x0001,表示这个类就定义了一个field字段 字段的访问标志是0x009A,二进制是00000000 10011010，即第9、12、13、15位标志位为1，这个字段的标志符有：ACC_TRANSIENT、ACC_FINAL、ACC_STATIC、ACC_PRIVATE; 名称索引中的值为0x0005,指向了常量池中的第5项，为“str”,表明这个field字段的名称是str； 描述索引中的值为0x0006,指向了常量池中的第6项，为&quot;Ljava/lang/String;&quot;，表明这个field字段的数据类型是java.lang.String类型； 5.属性表计数器中的值为0x0001,表明field_info还有一个属性表； 6.属性表名称索引中的值为0x0007,指向常量池中的第7项，为“ConstantValue”,表明这个属性表的名称是ConstantValue，即属性表的类型是ConstantValue类型的； 7.属性长度中的值为0x0002，因为此属性表是ConstantValue类型，它的值固定为2； 8.常量值索引 中的值为0x0008,指向了常量池中的第8项，为CONSTANT_String_info类型的项，表示“This is a test” 的常量。在对此field赋值时，会使用此常量对field赋值。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读3-Class文件中的访问标志、类索引、父类索引、接口索引集合]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB3-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E8%AE%BF%E9%97%AE%E6%A0%87%E5%BF%97%E3%80%81%E7%B1%BB%E7%B4%A2%E5%BC%95%E3%80%81%E7%88%B6%E7%B1%BB%E7%B4%A2%E5%BC%95%E3%80%81%E6%8E%A5%E5%8F%A3%E7%B4%A2%E5%BC%95%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[讲完了class文件中的常量池，我们就相当于克服了class文件中最麻烦的模块了。现在，我们来看一下class文件中紧接着常量池后面的几个东西：访问标志、类索引、父类索引、接口索引集合。 1. 访问标志、类索引、父类索引、接口索引集合 在class文件中的位置 2. 访问标志(access_flags)能够表示什么？ 访问标志（access_flags）紧接着常量池后，占有两个字节，总共16位，如下图所示： 当JVM在编译某个类或者接口的源代码时，JVM会解析出这个类或者接口的访问标志信息，然后，将这些标志设置到访问标志（access_flags）这16个位上。JVM会考虑如下设置如下访问表示信息： a. 类或接口 我们知道，每个定义的类或者接口都会生成class文件（这里也包括内部类，在某个类中定义的静态内部类也会单独生成一个class文件）。 对于定义的类，JVM在将其编译成class文件时，会将class文件的访问标志的第11位设置为1 。第11位叫做ACC_SUPER标志位； 对于定义的接口，JVM在将其编译成class文件时，会将class文件的访问标志的第8位 设置为 1 。第8位叫做ACC_INTERFACE标志位； b. 访问权限：public类型和包package类型。 如果类或者接口被声明为public类型的，那么，JVM将其编译成class文件时，会将class文件的访问标志的第16位设置为1 。第16位叫做ACC_PUBLIC标志符； c. 类是否为抽象类型的，即我们定义的类有没有被abstract关键字修饰，即我们定义的类是否为抽象类。 1public abstract class MyClass&#123;......&#125; 定义某个类时，JVM将它编译成class文件的时候，会将class文件的访问标志的第7位设置为1 。第7位叫做ACC_ABSTRACT标志位。 另外值得注意的是，对于定义的接口，JVM在编译接口的时候也会对class文件的访问标志上的ACC_ABSTRACT标志位设置为 1； d. 该类是否被声明了final类型,即表示该类不能被继承。 此时JVM会在编译class文件的过程中，会将class文件的访问标志的第12位设置为 1 。第12位叫做ACC_FINAL标志位； e.是否是JVM通过java源代码文件编译而成的 如果我们这个class文件不是JVM通过java源代码文件编译而成的，而是用户自己通过class文件的组织规则生成的，那么，一般会对class文件的访问标志第4位设置为 1 。通过JVM编译源代码产生的class文件此标志位为 0，第4位叫做ACC_SYNTHETIC标志位； f. 枚举类 对于定义的枚举类如：public enum EnumTest{…}，JVM也会对此枚举类编译成class文件，这时，对于这样的class文件，JVM会对访问标志第2位设置为 1 ，以表示它是枚举类。第2位叫做ACC_ENUM标志位； g. 注解类 对于定义的注解类如：public @interface{…},JVM会对此注解类编译成class文件，对于这样的class文件，JVM会将访问标志第3位设置为1，以表示这是个注解类，第3位叫做ACC_ANNOTATION标志位。 当JVM确定了上述标志位的值后，就可以确定访问标志（access_flags）的值了。实际上JVM上述标志会根据上述确定的标志位的值，对这些标志位的值取或，便得到了访问标志（access_flags）。如下图所示: 举例 定义一个最简单的类Simple.java，使用编译器编译成class文件，然后观察class文件中的访问标志的值，以及使用javap -v Simple 查看访问标志。 123public class Simple &#123; &#125; 使用UltraEdit查看编译成的class文件，如下图所示： 上述的图中黄色部分表示的是常量池部分,常量池后面紧跟着就是访问标志，它的十六进制值为0x0021,二进制的值为：00000000 00100001，由二进制的1的位数可以得出第11、16位为1，分别对应ACC_SUPER标志位和ACC_PUBLIC标志位。验证一下: 3. 类索引(this_class)是什么？ 我们知道一般情况下一个Java类源文件经过JVM编译会生成一个class文件，也有可能一个Java类源文件中定义了其他类或者内部类，这样编译出来的class文件就不止一个，但每一个class文件表示某一个类，至于这个class表示哪一个类，便可以通过 类索引 这个数据项来确定。JVM通过类的完全限定名确定是某一个类。 类索引的作用，就是为了指出class文件所描述的这个类叫什么名字。 类索引紧接着访问标志的后面，占有两个字节，在这两个字节中存储的值是一个指向常量池的一个索引，该索引指向的是CONSTANT_Class_info常量池项. 以上面定义的Simple.class 为例，如下图所示，查看他的类索引在什么位置和取什么值。 由上可知，它的类索引值为0x0001,那么，它指向了常量池中的第一个常量池项，那我们再看一下常量池中的信息。使用javap -v Simple,常量池中有以下信息： 可以看到常量池中的第一项是CONSTANT_Class_info项，它表示一个&quot;com/louis/jvm/Simple&quot;的类名。即类索引是告诉我们这个class文件所表示的是哪一个类。 4. 父类索引(super_class)是什么？ Java支持单继承模式，除了java.lang.Object 类除外，每一个类都会有且只有一个父类。class文件中紧接着类索引(this_class)之后的两个字节区域表示父类索引，跟类索引一样，父类索引这两个字节中的值指向了常量池中的某个常量池项CONSTANT_Class_info，表示该class表示的类是继承自哪一个类。 5. 接口索引集合(interfaces)是什么？ 一个类可以不实现任何接口，也可以实现很多个接口，为了表示当前类实现的接口信息，class文件使用了如下结构体描述某个类的接口实现信息: 由于类实现的接口数目不确定，所以接口索引集合的描述的前部分叫做接口计数器（interfaces_count），接口计数器占用两个字节，其中的值表示着这个类实现了多少个接口，紧跟着接口计数器的部分就是接口索引部分了，每一个接口索引占有两个字节，接口计数器的值代表着后面跟着的接口索引的个数。接口索引和类索引和父类索引一样，其内的值存储的是指向了常量池中的常量池项的索引，表示着这个接口的完全限定名。 举例： 定义一个Worker接口，然后类Programmer实现这个Worker接口，然后我们观察Programmer的接口索引集合是怎样表示的。 12345public interface Worker&#123; public void work(); &#125; 1234567public class Programmer implements Worker &#123; @Override public void work() &#123; System.out.println("I'm Programmer,Just coding...."); &#125; &#125;]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读2-Class文件中的常量池]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB2-Class%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84%E5%B8%B8%E9%87%8F%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[上一节Class类文件结构大致地介绍了class文件的组织结构，接下来，我们将深入每一个结构，来详细了解它们。这一章节呢，我们就来扒一扒class文件中非常重要 的一个数据区域------常量池。它在JVM虚拟机中扮演了非常重要的地位。 本篇内容来自于java虚拟机原理图解，自己一边理解一边进行复制整理得此文章，也是看了很多遍，逐渐地好像懂了常量池怎么玩的，所以一定要坚持，读不懂多读几遍一定可以读懂的。 本篇文章内容过多，这里将目录列举在此。 常量池是什么 常量池在class文件的什么位置？ 常量池里面是怎么组织的？ 常量池项 (cp_info) 的结构是什么？ 常量池能够表示哪些信息？ int和float数据类型的常量在常量池中是怎样表示和存储的？ long和 double数据类型的常量在常量池中是怎样表示和存储的？ String类型的字符串常量在常量池中是怎样表示和存储的？ 类文件中定义的类名和类中使用到的类在常量池中是怎样被组织和存储的？ 类中引用到的field字段在常量池中是怎样描述的？ 类中引用到的method方法在常量池中是怎样描述的？ 类中引用到某个接口中定义的method方法在常量池中是怎样描述的？ 更好地支持动态语言所增加的三项 1. 常量池是什么 可以理解为class文件之中的资源仓库，它是class文件结构中与其他项目关联最多的数据类型，也是占用class文件空间最大的数据项目之一，同时它还是class文件中第一个出现表类型的数据项目． 由于常量池的数量是不固定的，所以在常量池入口需要放置一项u2（即２个字节）类型的数据，代表常量池容量计数值（constant-pool-count）(从１开始，将０表示不引用任何常量). 常量池中主要存放两大类常量：字面量（Literal）和符号引用(Synbolic Reference)． 字面量：比较接近于Java语言层面的常量概念，如文本字符串，声明为final的常量值. 符号引用：包括如下三类常量： 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 2. 常量池在class文件的什么位置？ 3. 常量池的里面是怎么组织的？ 常量池的组织很简单，前端的两个字节占有的位置叫做常量池计数器(constant_pool_count)，它记录着常量池的组成元素 常量池项(cp_info) 的个数。紧接着会排列着constant_pool_count-1个常量池项(cp_info)。如下图所示： 4. 常量池项 (cp_info) 的结构是什么？ 每个常量池项(cp_info) 都会对应记录着class文件中的某种类型的字面量。让我们先来了解一下常量池项(cp_info)的结构吧： JVM虚拟机规定了不同的tag值和不同类型的字面量对应关系如下： 所以根据cp_info中的tag 不同的值，可以将cp_info 更细化为以下结构体： 现在让我们看一下细化了的常量池的结构会是类似下图所示的样子： 5. 常量池能够表示那些信息？ 6. int和float数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Integer_info, CONSTANT_Float_info) Java语言规范规定了 int类型和Float 类型的数据类型占用 4 个字节的空间。那么存在于class字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将 int和Float类型的常量分别使用CONSTANT_Integer_info和 Constant_float_info表示，他们的结构如下所示： 举例：建下面的类 IntAndFloatTest.java，在这个类中，我们声明了五个变量，但是取值就两种int类型的10 和Float类型的11f. 123456789public class IntAndFloatTest &#123; private final int a = 10; private final int b = 10; private float c = 11f; private float d = 11f; private float e = 11f; &#125; 然后用编译器编译成IntAndFloatTest.class字节码文件，我们通过javap -v IntAndFloatTest 指令来看一下其常量池中的信息，可以看到虽然我们在代码中写了两次10 和三次11f，但是常量池中，就只有一个常量10 和一个常量11f,如下图所示: 从结果上可以看到常量池第#8 个常量池项(cp_info) 就是CONSTANT_Integer_info,值为10；第#23个常量池项(cp_info) 就是CONSTANT_Float_info,值为11f。 代码中所有用到 int 类型 10 的地方，会使用指向常量池的指针值#8 定位到第#8 个常量池项(cp_info)，即值为 10的结构体 CONSTANT_Integer_info，而用到float类型的11f时，也会指向常量池的指针值#23来定位到第#23个常量池项(cp_info) 即值为11f的结构体CONSTANT_Float_info。如下图所示： 7. long和 double数据类型的常量在常量池中是怎样表示和存储的？(CONSTANT_Long_info、CONSTANT_Double_info ) Java语言规范规定了 long 类型和 double类型的数据类型占用8 个字节的空间。那么存在于class 字节码文件中的该类型的常量是如何存储的呢？相应地，在常量池中，将long和double类型的常量分别使用CONSTANT_Long_info和Constant_Double_info表示，他们的结构如下所示： 代码中所有用到 long 类型-6076574518398440533L 的地方，会使用指向常量池的指针值#18 定位到第 #18 个常量池项(cp_info)，即值为-6076574518398440533L 的结构体CONSTANT_Long_info，而用到double类型的10.1234567890D时，也会指向常量池的指针值#26 来定位到第 #26 个常量池项(cp_info) 即值为10.1234567890D的结构体CONSTANT_Double_info。如下图所示： 8. String类型的字符串常量在常量池中是怎样表示和存储的？（CONSTANT_String_info、CONSTANT_Utf8_info） 对于字符串而言，JVM会将字符串类型的字面量以UTF-8 编码格式存储到在class字节码文件中。这么说可能有点摸不着北，我们先从直观的Java源码中中出现的用双引号&quot;&quot; 括起来的字符串来看，在编译器编译的时候，都会将这些字符串转换成CONSTANT_String_info结构体，然后放置于常量池中。其结构如下所示： 如上图所示的结构体，CONSTANT_String_info结构体中的string_index的值指向了CONSTANT_Utf8_info结构体，而字符串的utf-8编码数据就在这个结构体之中。如下图所示： 请看一例，定义一个简单的StringTest.java类，然后在这个类里加一个&quot;JVM原理&quot; 字符串，然后，我们来看看它在class文件中是怎样组织的。 123456public class StringTest &#123; private String s1 = "JVM原理"; private String s2 = "JVM原理"; private String s3 = "JVM原理"; private String s4 = "JVM原理"; &#125; 在上面的图中，我们可以看到CONSTANT_String_info结构体位于常量池的第#15个索引位置。而存放&quot;Java虚拟机原理&quot; 字符串的 UTF-8编码格式的字节数组被放到CONSTANT_Utf8_info结构体中，该结构体位于常量池的第#16个索引位置。上面的图只是看了个轮廓，让我们再深入地看一下它们的组织吧。请看下图： 9. 类文件中定义的类名和类中使用到的类在常量池中是怎样被组织和存储的？(CONSTANT_Class_info) JVM会将某个Java 类中所有使用到了的类的完全限定名 以二进制形式的完全限定名 封装成CONSTANT_Class_info结构体中，然后将其放置到常量池里。CONSTANT_Class_info 的tag值为 7 。其结构如下： 类的完全限定名和二进制形式的完全限定名 在某个Java源码中，我们会使用很多个类，比如我们定义了一个 ClassTest的类，并把它放到com.louis.jvm 包下，则 ClassTest类的完全限定名为com.louis.jvm.ClassTest，将JVM编译器将类编译成class文件后，此完全限定名在class文件中，是以二进制形式的完全限定名存储的，即它会把完全限定符的&quot;.“换成”/&quot; ，即在class文件中存储的 ClassTest类的完全限定名称是&quot;com/louis/jvm/ClassTest&quot;。因为这种形式的完全限定名是放在了class二进制形式的字节码文件中，所以就称之为 二进制形式的完全限定名。 举例，我们定义一个很简单的ClassTest类，来看一下常量池是怎么对类的完全限定名进行存储的。 123public class ClassTest &#123; private Date date =new Date(); &#125; 如上图所示，在ClassTest.class文件的常量池中，共有 3 个CONSTANT_Class_info结构体，分别表示ClassTest 中用到的Class信息。 我们就看其中一个表示com/jvm.ClassTest的CONSTANT_Class_info 结构体。它在常量池中的位置是#1，它的name_index值为#2，它指向了常量池的第2 个常量池项，如下所示: 注意： 对于某个类而言，其class文件中至少要有两个CONSTANT_Class_info常量池项，用来表示自己的类信息和其父类信息。(除了java.lang.Object类除外，其他的任何类都会默认继承自java.lang.Object）如果类声明实现了某些接口，那么接口的信息也会生成对应的CONSTANT_Class_info常量池项。 除此之外，如果在类中使用到了其他的类，只有真正使用到了相应的类，JDK编译器才会将类的信息组成CONSTANT_Class_info常量池项放置到常量池中。如下： 12345678910import java.util.Date; public class Other&#123; private Date date; public Other() &#123; Date da; &#125; &#125; 上述的Other的类，在JDK将其编译成class文件时，常量池中并没有java.util.Date对应的CONSTANT_Class_info常量池项，为什么呢? 在Other类中虽然定义了Date类型的两个变量date、da，但是JDK编译的时候，认为你只是声明了“Ljava/util/Date”类型的变量，并没有实际使用到Ljava/util/Date类。将类信息放置到常量池中的目的，是为了在后续的代码中有可能会反复用到它。很显然，JDK在编译Other类的时候，会解析到Date类有没有用到，发现该类在代码中就没有用到过，所以就认为没有必要将它的信息放置到常量池中了。 将上述的Other类改写一下，仅使用new Date()，如下所示： 12345678import java.util.Date; public class Other&#123; public Other() &#123; new Date(); &#125; &#125; 总结： 对于某个类或接口而言，其自身、父类和继承或实现的接口的信息会被直接组装成CONSTANT_Class_info常量池项放置到常量池中； 类中或接口中使用到了其他的类，只有在类中实际使用到了该类时，该类的信息才会在常量池中有对应的CONSTANT_Class_info常量池项； 类中或接口中仅仅定义某种类型的变量，JDK只会将变量的类型描述信息以UTF-8字符串组成CONSTANT_Utf8_info常量池项放置到常量池中，上面在类中的private Date date;JDK编译器只会将表示date的数据类型的“Ljava/util/Date”字符串放置到常量池中。 10. 类中引用到的field字段在常量池中是怎样描述的？(CONSTANT_Fieldref_info, CONSTANT_Name_Type_info) 一般而言，我们在定义类的过程中会定义一些 field 字段，然后会在这个类的其他地方（如方法中）使用到它。有可能我们在类的方法中只使用field字段一次，也有可能我们会在类定义的方法中使用它很多很多次。 举一个简单的例子，我们定一个叫Person的简单java bean，它有name和age两个field字段，如下所示： 1234567891011121314151617181920public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 在上面定义的类中，我们在Person类中的一系列方法里，多次引用到namefield字段 和agefield字段，对于JVM编译器而言，name和age只是一个符号而已，并且它在由于它可能会在此类中重复出现多次，所以JVM把它当作常量来看待，将name和age以field字段常量的形式保存到常量池中。 将它name和age封装成 CONSTANT_Fieldref_info 常量池项，放到常量池中，在类中引用到它的地方，直接放置一个指向field字段所在常量池的索引。 上面的Person类，使用javap -v Person指令，查看class文件的信息，你会看到，在Person类中引用到age和namefield字段的地方，都是指向了常量池中age和namefield字段对应的常量池项中。表示field字段的常量池项叫做CONSTANT_Fieldref_info。 怎样描述某一个field字段的引用？ 实例解析： 现在，让我们来看一下Person类中定义的namefield字段在常量池中的表示。通过使用javap -v Person会查看到如下的常量池信息： 请读者看上图中namefield字段的数据类型，它在#6个常量池项，以UTF-8编码格式的字符串“Ljava/lang/String;” 表示，这表示着这个field 字段是java.lang.String 类型的。关于field字段的数据类型，class文件中存储的方式和我们在源码中声明的有些不一样。请看下图的对应关系： 注意： 如果我们在类中定义了field 字段，但是没有在类中的其他地方用到这些字段，它是不会被编译器放到常量池中的。 只有在类中的其他地方引用到了，才会将他放到常量池中。 11. 类中引用到的method方法在常量池中是怎样描述的？(CONSTANT_Methodref_info, CONSTANT_Name_Type_info) 1. 举例 还是以Person类为例。在Person类中，我们定义了setName(String name)、getName()、setAge(int age)、getAge()这些方法： 123456789101112131415161718192021public class Person &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125; 虽然我们定义了方法，但是这些方法没有在类总的其他地方被用到（即没有在类中其他的方法中引用到），所以它们的方法引用信息并不会放到常量中。 现在我们在类中加一个方法 getInfo()，调用了getName()和getAge() 方法： 1234public String getInfo() &#123; return getName()+"\t"+getAge(); &#125; 这时候JVM编译器会将getName()和getAge()方法的引用信息包装成CONSTANT_Methodref_info结构体放入到常量池之中。 这里的方法调用的方式牵涉到Java非常重要的一个术语和机制，叫动态绑定。这个动态绑定问题以后在单独谈谈。 2. 表示一个方法引用 3. 方法描述符的组成 4. getName() 方法引用在常量池中的表示 12. 类中引用到某个接口中定义的method方法在常量池中是怎样描述的？(CONSTANT_InterfaceMethodref_info, CONSTANT_Name_Type_info) 当我们在某个类中使用到了某个接口中的方法，JVM会将用到的接口中的方法信息方知道这个类的常量池中。 比如我们定义了一个Worker接口，和一个Boss类，在Boss类中调用了Worker接口中的方法，这时候在Boss类的常量池中会有Worker接口的方法的引用表示。 12345public interface Worker&#123; public void work(); &#125; 12345678public class Boss &#123; public void makeMoney(Worker worker) &#123; worker.work(); &#125; &#125; 如上图所示，在Boss类的makeMoney()方法中调用了Worker接口的work()方法，机器指令是通过invokeinterface指令完成的，invokeinterface指令后面的操作数，是指向了Boss常量池中Worker接口的work()方法描述，表示的意思就是：“我要调用Worker接口的work()方法”。 Worker接口的work()方法引用信息，JVM会使用CONSTANT_InterfaceMethodref_info结构体来描述，CONSTANT_InterfaceMethodref_info定义如下： CONSTANT_InterfaceMethodref_info结构体和上面介绍的CONSTANT_Methodref_info 结构体很基本上相同，它们的不同点只有： CONSTANT_InterfaceMethodref_info 的tag 值为11，而CONSTANT_Methodref_info的tag值为10； CONSTANT_InterfaceMethodref_info 描述的是接口中定义的方法，而CONSTANT_Methodref_info描述的是实例类中的方法； 其他的基本与上面一个一毛一样。参照上面个理解即可。 13. CONSTANT_MethodType_info，CONSTANT_MethodHandle_info，CONSTANT_InvokeDynamic_info 这三项主要是为了让Java语言支持动态语言特性而在Java 7 版本中新增的三个常量池项，只会在极其特别的情况能用到它，在class文件中几乎不会生成这三个常量池项。 其实我花了一些时间来研究这三项，并且想通过各种方式生成这三项，不过没有成功，最后搞的还是迷迷糊糊的。从我了解到的信息来看，Java 7对动态语言的支持很笨拙，并且当前没有什么应用价值，然后就对着三项的研究先放一放了。）]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补充阅读1-Class类文件结构]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E8%A1%A5%E5%85%85%E9%98%85%E8%AF%BB-Class%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[总体概览一下Class文件是什么以及有什么。 整体感知 class文件是一种8位字节的二进制流文件， 各个数据项按顺序紧密的从前向后排列， 相邻的项之间没有间隙， 这样可以使得class文件非常紧凑， 体积轻巧， 可以被JVM快速的加载至内存， 并且占据较少的内存空间。 我们的Java源文件， 在被编译之后， 每个类（或者接口）都单独占据一个class文件， 并且类中的所有信息都会在class文件中有相应的描述， 由于class文件很灵活， 它甚至比Java源文件有着更强的描述能力。 Class文件格式 换成表格的形式： 类型 名称 数量 u4 magic 1 u2 minor_version 1 u2 major_version 1 u2 constant_pool_count 1 cp_info constant_pool constant_pool_count - 1 u2 access_flags 1 u2 this_class 1 u2 super_class 1 u2 interfaces_count 1 u2 interfaces interfaces_count u2 fields_count 1 field_info fields fields_count u2 methods_count 1 method_info methods methods_count u2 attribute_count 1 attribute_info attributes attributes_count NO1. 魔数(magic) 所有的由Java编译器编译而成的class文件的前4个字节都是“0xCAFEBABE” 它的作用在于： 当JVM在尝试加载某个文件到内存中来的时候，会首先判断此class文件有没有JVM认为可以接受的“签名”，即JVM会首先读取文件的前4个字节，判断该4个字节是否是“0xCAFEBABE”，如果是，则JVM会认为可以将此文件当作class文件来加载并使用。 NO2.版本号(minor_version,major_version) 主版本号和次版本号在class文件中各占两个字节，副版本号占用第5、6两个字节，而主版本号则占用第7，8两个字节。JDK1.0的主版本号为45，以后的每个新主版本都会在原先版本的基础上加1。若现在使用的是JDK1.7编译出来的class文件，则相应的主版本号应该是51,对应的7，8个字节的十六进制的值应该是 0x33。 JVM在加载class文件的时候，会读取出主版本号，然后比较这个class文件的主版本号和JVM本身的版本号，如果JVM本身的版本号小于class文件的版本号，JVM会认为加载不了这个class文件，会抛出我们经常见到的&quot;java.lang.UnsupportedClassVersionError: Bad version number in .class file &quot; Error错误；反之，JVM会认为可以加载此class文件，继续加载此class文件。 NO3.常量池计数器(constant_pool_count) 常量池是class文件中非常重要的结构，它描述着整个class文件的字面量信息。 常量池是由一组constant_pool结构体数组组成的，而数组的大小则由常量池计数器指定。常量池计数器constant_pool_count 的值等于constant_pool表中的成员数+ 1。constant_pool表的索引值只有在大于 0 且小于constant_pool_count时(即1~(constant_pool_count-1))才会被认为是有效的。 这个容量计数是从1而不是从0开始的，如果常量池容量为十六进制数0x0016，即十进制22，这就代表着常量池中有21个常量，索引值范围为1-21。在Class文件格式规范制定时，设计者将第0项常量空出来是有特殊考虑的，用于在特定情况下表达“不引用任何一个常量池项目”。 NO4.常量池数据区(constant_pool[contstant_pool_count-1]) 常量池，constant_pool是一种表结构,它包含 Class 文件结构及其子结构中引用的所有字符串常量、 类或接口名、字段名和其它常量。 常量池中的每一项都具备相同的格式特征——第一个字节作为类型标记用于识别该项是哪种类型的常量，称为 “tag byte” 。常量池的索引范围是 1 至constant_pool_count−1。常量池的具体细节我们会稍后讨论。 NO6.访问标志(access_flags) 访问标志，access_flags 是一种掩码标志，用于表示某个类或者接口的访问权限及基础属性。 NO7.类索引(this_class) 类索引，this_class的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类或接口。 NO8.父类索引(super_class) 父类索引，对于类来说，super_class 的值必须为 0 或者是对constant_pool 表中项目的一个有效索引值。如果它的值不为 0，那 constant_pool 表在这个索引处的项必须为CONSTANT_Class_info 类型常量，表示这个 Class 文件所定义的类的直接父类。当前类的直接父类，以及它所有间接父类的access_flag 中都不能带有ACC_FINAL 标记。对于接口来说，它的Class文件的super_class项的值必须是对constant_pool表中项目的一个有效索引值。constant_pool表在这个索引处的项必须为代表 java.lang.Object 的 CONSTANT_Class_info 类型常量 。如果 Class 文件的 super_class的值为 0，那这个Class文件只可能是定义的是java.lang.Object类，只有它是唯一没有父类的类。 NO9.接口计数器(interfaces_count) 接口计数器，interfaces_count的值表示当前类或接口的直接父接口数量。 NO10.接口信息数据区(interfaces[interfaces_count]) 接口表，interfaces[]数组中的每个成员的值必须是一个对constant_pool表中项目的一个有效索引值， 它的长度为 interfaces_count。每个成员 interfaces[i] 必须为 CONSTANT_Class_info类型常量，其中 0 ≤ i &lt;interfaces_count。在interfaces[]数组中，成员所表示的接口顺序和对应的源代码中给定的接口顺序（从左至右）一样，即interfaces[0]对应的是源代码中最左边的接口。 NO11.字段计数器(fields_count) 字段计数器，fields_count的值表示当前 Class 文件 fields[]数组的成员个数。 fields[]数组中每一项都是一个field_info结构的数据项，它用于表示该类或接口声明的类字段或者实例字段。 NO12.字段信息数据区(fields[fields_count]) 字段表，fields[]数组中的每个成员都必须是一个fields_info结构的数据项，用于表示当前类或接口中某个字段的完整描述。 fields[]数组描述当前类或接口声明的所有字段，但不包括从父类或父接口继承的部分。 NO13.方法计数器(methods_count) 方法计数器， methods_count的值表示当前Class 文件 methods[]数组的成员个数。Methods[]数组中每一项都是一个 method_info 结构的数据项。 NO14.方法信息数据区(methods[methods_count]) 方法表，methods[] 数组中的每个成员都必须是一个 method_info 结构的数据项，用于表示当前类或接口中某个方法的完整描述。如果某个method_info 结构的access_flags 项既没有设置 ACC_NATIVE 标志也没有设置ACC_ABSTRACT 标志，那么它所对应的方法体就应当可以被 Java 虚拟机直接从当前类加载，而不需要引用其它类。 method_info结构可以表示类和接口中定义的所有方法，包括实例方法、类方法、实例初始化方法方法和类或接口初始化方法方法 。methods[]数组只描述当前类或接口中声明的方法，不包括从父类或父接口继承的方法。 NO15.属性计数器(attributes_count) 属性计数器，attributes_count的值表示当前 Class 文件attributes表的成员个数。attributes表中每一项都是一个attribute_info 结构的数据项。 NO16.属性信息数据区(attributes[attributes_count]) 属性表，attributes 表的每个项的值必须是attribute_info结构。 在Java 7 规范里，Class文件结构中的attributes表的项包括下列定义的属性： InnerClasses 、 EnclosingMethod 、 Synthetic 、Signature、SourceFile，SourceDebugExtension 、Deprecated、RuntimeVisibleAnnotations 、RuntimeInvisibleAnnotations以及BootstrapMethods属性。 对于支持 Class 文件格式版本号为 49.0 或更高的 Java 虚拟机实现，必须正确识别并读取attributes表中的Signature、RuntimeVisibleAnnotations和RuntimeInvisibleAnnotations属性。对于支持Class文件格式版本号为 51.0 或更高的 Java 虚拟机实现，必须正确识别并读取 attributes表中的BootstrapMethods属性。Java 7 规范 要求任一 Java 虚拟机实现可以自动忽略 Class 文件的 attributes表中的若干 （甚至全部） 它不可识别的属性项。任何本规范未定义的属性不能影响Class文件的语义，只能提供附加的描述信息 。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存分配和回收策略]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第十篇文章，本章对内存分配和垃圾回收的细节再次详细说明一下，并且说明一下逃逸分析/栈上分配以及TLAB两种方式的概念和原理。 1. 对象优先在Eden分配 前面文章曾介绍HotSpot虚拟机新生代内存布局及算法: （1）、将新生代内存分为一块较大的Eden空间和两块较小的Survivor空间； （2）、每次使用Eden和其中一块Survivor； （3）、当回收时，将Eden和使用中的Survivor中还存活的对象一次性复制到另外一块Survivor； （4）、而后清理掉Eden和使用过的Survivor空间； （5）、后面就使用Eden和复制到的那一块Survivor空间，重复步骤3； 默认Eden：Survivor=8:1，即每次可以使用90%的空间，只有一块Survivor的空间被浪费； 大多数情况下，对象在新生代Eden区中分配； 当Eden区没有足够空间进行分配时，JVM将发起一次Minor GC（新生代GC）； Minor GC时，如果发现存活的对象无法全部放入Survivor空间，只好通过分配担保机制提前转移到老年代。 2. 大对象直接进入老年代 大对象指需要大量连续内存空间的Java对象，如，很长的字符串、数组； 经常出现大对象容易导致内存还有不少空间就提前触发GC,以获取足够的连续空间来存放它们，所以应该尽量避免使用创建大对象； -XX:PretenureSizeThreshold： 可以设置这个阈值，大于这个参数值的对象直接在老年代分配； 默认为0（无效），且只对Serail和ParNew两款收集器有效； 如果需要使用该参数，可考虑ParNew+CMS组合。 3. 长期存活的对象将进入老年代 JVM给每个对象定义一个对象年龄计数器，其计算流程如下： 在Eden中分配的对象，经Minor GC后还存活，就复制移动到Survivor区，年龄为1； 而后每经一次Minor GC后还存活，在Survivor区复制移动一次，年龄就增加1岁； 如果年龄达到一定程度，就晋升到老年代中； -XX:MaxTenuringThreshold： 设置新生代对象晋升老年代的年龄阈值，默认为15； 4. 动态对象年龄判定 JVM为更好适应不同程序，不是永远要求等到MaxTenuringThreshold中设置的年龄； 如果在Survivor空间中相同年龄的所有对象大小总和大于Survivor空间的一半，大于或等于该年龄的对象就可以直接进入老年代 5. 空间分配担保 在前面曾简单介绍过分配担保： 当Survivor空间不够用时，需要依赖其他内存（老年代）进行分配担保（Handle Promotion）； 分配担保的流程如下： 在发生Minor GC前，JVM先检查老年代最大可用的连续空间是否大于新生所有对象空间； 如果大于，那可以确保Minor GC是安全的； 如果不大于，则JVM查看HandlePromotionFailure值是否允许担保失败； 如果允许，就继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小； 如果大于，将尝试进行一次Minor GC，但这是有风险的； 如果小于或HandlePromotionFailure值不允许冒险，那这些也要改为进行一次Full GC； 尝试Minor GC的风险–担保失败： 因为尝试Minor GC前面，无法知道存活的对象大小，所以使用历次晋升到老年代对象的平均大小作为经验值； 假如尝试的Minor GC最终存活的对象远远高于经验值的话，会导致担保失败（Handle Promotion Failure）； 失败后只有重新发起一次Full GC，这绕了一个大圈，代价较高； 但一般还是要开启HandlePromotionFailure，避免Full GC过于频繁，而且担保失败概率还是比较低的； JDK6-u24后，JVM代码中已经不再使用HandlePromotionFailure参数了； 规则变为： ⭐⭐⭐只要老年代最大可用的连续空间大于新生所有对象空间或历次晋升到老年代对象的平均大小，就会进行Minor GC；否则进行Full GC； ⭐⭐⭐即老年代最大可用的连续空间小于新生所有对象空间时，不再检查HandelPromotionFailure，而直接检查历次晋升到老年代对象的平均大小； 6. 逃逸分析 般认为new出来的对象都是被分配在堆上，但是这个结论不是那么的绝对，通过对Java对象分配的过程分析，可以知道有两个地方会导致Java中new出来的对象并不一定分配在所认为的堆上。这两个点分别是Java中的逃逸分析和TLAB（Thread Local Allocation Buffer）。 6.1 什么是栈上分配？ 栈上分配主要是指在Java程序的执行过程中，在方法体中声明的变量以及创建的对象，将直接从该线程所使用的栈中分配空间。 一般而言，创建对象都是从堆中来分配的，这里是指在栈上来分配空间给新创建的对象。 6.2 什么是逃逸？ 逃逸是指在某个方法之内创建的对象，除了在方法体之内被引用之外，还在方法体之外被其它变量引用到； 这样带来的后果是在该方法执行完毕之后，该方法中创建的对象将无法被GC回收，由于其被其它变量引用。 正常的方法调用中，方法体中创建的对象将在执行完毕之后，将回收其中创建的对象；而此时由于无法回收，即成为逃逸。 123456789101112static V global_v; public void a_method()&#123; V v=b_method(); c_method(); &#125; public V b_method()&#123; V v=new V(); return v; &#125; public void c_method()&#123; global_v=new V(); &#125; 其中b_method方法内部生成的V对象的引用被返回给a_method方法内的变量v，c_method方法内生成的V对象被赋给了全局变量global_v。这两种场景都发生了（引用）逃逸。 6.3 逃逸分析 在JDK 6之后支持对象的栈上分析和逃逸分析，在JDK7中完全支持栈上分配对象。其是否打开逃逸分析依赖于以下JVM的设置： -XX:+DoEscapeAnalysis 6.4 栈上分配与逃逸分析的关系 进行逃逸分析之后，产生的后果是所有的对象都将由栈上分配，而非从JVM内存模型中的堆来分配。 6.5 逃逸分析／栈上分配的优劣分析 JVM在Server模式下的逃逸分析可以分析出某个对象是否永远只在某个方法、线程的范围内，并没有“逃逸”出这个范围，逃逸分析的一个结果就是对于某些未逃逸对象可以直接在栈上分配，由于该对象一定是局部的，所以栈上分配不会有问题。 消除同步。 线程同步的代价是相当高的，同步的后果是降低并发性和性能。逃逸分析可以判断出某个对象是否始终只被一个线程访问，如果只被一个线程访问，那么对该对象的同步操作就可以转化成没有同步保护的操作，这样就能大大提高并发程度和性能。 矢量替代。 逃逸分析方法如果发现对象的内存存储结构不需要连续进行的话，就可以将对象的部分甚至全部都保存在CPU寄存器内，这样能大大提高访问速度。 劣势： 栈上分配受限于栈的空间大小，一般自我迭代类的需求以及大的对象空间需求操作，将导致栈的内存溢出；故只适用于一定范围之内的内存范围请求。 6.6 测试 123456789101112131415161718192021222324class EscapeAnalysis &#123; private static class Foo &#123; private int x; private static int counter; //会发生逃逸 public Foo() &#123; x = (++counter); &#125; &#125; public static void main(String[] args) &#123; //开始时间 long start = System.nanoTime(); for (int i = 0; i &lt; 1000 * 1000 * 10; ++i) &#123; Foo foo = new Foo(); &#125; //结束时间 long end = System.nanoTime(); System.out.println("Time cost is " + (end - start)); &#125;&#125; 未开启逃逸分析设置为： -server -verbose:gc 在未开启逃逸分析的状况下运行情况如下： 12345678910[GC 5376K-&gt;427K(63872K), 0.0006051 secs] [GC 5803K-&gt;427K(63872K), 0.0003928 secs] [GC 5803K-&gt;427K(63872K), 0.0003639 secs] [GC 5803K-&gt;427K(69248K), 0.0003770 secs] [GC 11179K-&gt;427K(69248K), 0.0003987 secs] [GC 11179K-&gt;427K(79552K), 0.0003817 secs] [GC 21931K-&gt;399K(79552K), 0.0004342 secs] [GC 21903K-&gt;399K(101120K), 0.0002175 secs] [GC 43343K-&gt;399K(101184K), 0.0001421 secs] Time cost is 58514571 开启逃逸分析设置为： -server -verbose:gc -XX:+DoEscapeAnalysis 开启逃逸分析的状况下，运行情况如下： Time cost is 10031306 未开启逃逸分析时，运行上述代码，JVM执行了GC操作，而在开启逃逸分析情况下，JVM并没有执行GC操作。同时，操作时间上，开启逃逸分析的程序运行时间是未开启逃逸分析时间的1/5。 7. 再来聊聊TLAB JVM在内存新生代Eden Space中开辟了一小块线程私有的区域，称作TLAB（Thread-local allocation buffer）。默认设定为占用Eden Space的1%。在Java程序中很多对象都是小对象且用过即丢，它们不存在线程共享也适合被快速GC，所以对于小对象通常JVM会优先分配在TLAB上，并且TLAB上的分配由于是线程私有所以没有锁开销。因此在实践中分配多个小对象的效率通常比分配一个大对象的效率要高。 也就是说，Java中每个线程都会有自己的缓冲区称作TLAB（Thread-local allocation buffer），每个TLAB都只有一个线程可以操作，TLAB结合bump-the-pointer技术可以实现快速的对象分配，而不需要任何的锁进行同步，也就是说，在对象分配的时候不用锁住整个堆，而只需要在自己的缓冲区分配即可。 8. 对象内存分配过程再升级 编译器通过逃逸分析，确定对象是在栈上分配还是在堆上分配。如果是在堆上分配，则进入选项2. 如果tlab_top + size &lt;= tlab_end，则在在TLAB上直接分配对象并增加tlab_top 的值，如果现有的TLAB不足以存放当前对象则3. 重新申请一个TLAB，并再次尝试存放当前对象。如果放不下，则4. 在Eden区加锁（这个区是多线程共享的），如果eden_top + size &lt;= eden_end则将对象存放在Eden区，增加eden_top 的值，如果Eden区不足以存放，则5. 执行一次Young GC（minor collection）。 经过Young GC之后，如果Eden区任然不足以存放当前对象，则直接分配到老年代。 老年代还是不足，则触发Full GC，再不足就OOM错误]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集器介绍]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第九篇文章，主要介绍七种比较经典的垃圾收集器的实现原理。 垃圾收集器 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 1. Serial 收集器 Serial 翻译为串行，垃圾收集和用户程序不能同时执行，这意味着在执行垃圾收集的时候需要停顿用户程序。除了 CMS 和 G1 之外，其它收集器都是以串行的方式执行。CMS 和 G1 可以使得垃圾收集和用户程序同时执行，被称为并发执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 模式下的默认新生代收集器，因为在用户的桌面应用场景下，分配给虚拟机管理的内存一般来说不会很大。Serial 收集器收集几十兆甚至一两百兆的新生代停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿是可以接受的。 2. ParNew 收集器 它是 Serial 收集器的多线程版本。 是 Server模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作。 默认开始的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数。 3. Parallel Scavenge 收集器 与 ParNew 一样是并行的多线程收集器。 其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 -XX:MaxGCPauseMillis 参数以及直接设置吞吐量大小的 -XX:GCTimeRatio 参数（值为大于 0 且小于 100 的整数）。缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 还提供了一个参数 -XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为 GC 自适应的调节策略（GC Ergonomics）。 4. Serial Old 收集器 是 Serial 收集器的老年代版本，也是给 Client 模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器 CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。 特点：并发收集、低停顿。并发指的是用户线程和 GC 线程同时运行。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。可以使用 -XX:CMSInitiatingOccupancyFraction 来改变触发 CMS 收集器工作的内存占用百分，如果这个值设置的太大，导致预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器 G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 Java 堆被分为新生代、老年代和永久代，其它收集器进行收集的范围都是整个新生代或者老生代，而 G1 可以直接对新生代和永久代一起回收。 G1 把新生代和老年代划分成多个大小相等的独立区域（Region），新生代和永久代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 8. 比较 收集器 串行/并行/并发 新生代/老年代 收集算法 目标 适用场景 Serial 串行 新生代 复制 响应速度优先 单 CPU 环境下的 Client 模式 Serial Old 串行 老年代 标记-整理 响应速度优先 单 CPU 环境下的 Client 模式、CMS 的后备预案 ParNew 串行 + 并行 新生代 复制算法 响应速度优先 多 CPU 环境时在 Server 模式下与 CMS 配合 Parallel Scavenge 串行 + 并行 新生代 复制算法 吞吐量优先 在后台运算而不需要太多交互的任务 Parallel Old 串行 + 并行 老年代 标记-整理 吞吐量优先 在后台运算而不需要太多交互的任务 CMS 并行 + 并发 老年代 标记-清除 响应速度优先 集中在互联网站或 B/S 系统服务端上的 Java 应用 G1 并行 + 并发 新生代 + 老年代 标记-整理 + 复制算法 响应速度优先 面向服务端应用，将来替换 CMS]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC相关]]></title>
    <url>%2F2019%2F02%2F09%2FJVM%2FGC%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第八篇文章，我们知道，JVM为我们管理垃圾对象实现自动回收，让我们不需要太关心内存释放问题，一定程度上减少了内存溢出的错误。这一切的背后是如何实现的呢？ 一、垃圾标记算法 1.1 引用计数法 算法思想 给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值加一；当引用失效时，计数器☞减一；任何时候计数器为0的对象是不可能再被使用的。 主要缺陷 无法解决对象间相互循环引用的问题。 举个例子 12345678910111213141516171819202122232425public class Test &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; private byte[] bigSize = new byte[2 * _1MB]; public static void testGC() &#123; Test objA = new Test();//count=1 Test objB = new Test();//count=1 objA.instance = objB;//count=2 objB.instance = objA;//count=2 objA = null;//count=1 objB = null;//count=1 System.gc(); &#125; public static void main(String[] args) &#123; testGC(); &#125;&#125; 输入参数 -verbose:gc -XX:+PrintGCDetails 结果 1234567891011[GC (System.gc()) [PSYoungGen: 6063K-&gt;600K(37888K)] 6063K-&gt;608K(123904K), 0.0037131 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 600K-&gt;0K(37888K)] [ParOldGen: 8K-&gt;529K(86016K)] 608K-&gt;529K(123904K), [Metaspace: 2595K-&gt;2595K(1056768K)], 0.0062705 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] Heap PSYoungGen total 37888K, used 328K [0x00000000d6100000, 0x00000000d8b00000, 0x0000000100000000) eden space 32768K, 1% used [0x00000000d6100000,0x00000000d6152030,0x00000000d8100000) from space 5120K, 0% used [0x00000000d8100000,0x00000000d8100000,0x00000000d8600000) to space 5120K, 0% used [0x00000000d8600000,0x00000000d8600000,0x00000000d8b00000) ParOldGen total 86016K, used 529K [0x0000000082200000, 0x0000000087600000, 0x00000000d6100000) object space 86016K, 0% used [0x0000000082200000,0x0000000082284778,0x0000000087600000) Metaspace used 2601K, capacity 4486K, committed 4864K, reserved 1056768K class space used 288K, capacity 386K, committed 512K, reserved 1048576K 分析 日志中6063K-&gt;600K(37888K)，从原来的6M内存变成了600k，表明对象已被回收，从而表明JVM没有使用引用计数算法。Java中使用了可达性分析算法来来判定对象是否存活。 1.2 可达性分析算法 这个算法的基本思路就是通过一系列的称谓GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所有走过的路径为引用链，当一个对象到GC Roots没有任何引用链时，则证明此对象时不可用的，下面看一下例子： 上面的这张图，对象object5、object6、object7虽然互相没有关联，但是它们到GC Roots是不可达的，所以它们将会被判定为是可回收的对象 注：Java语言中，可作为GC Roots的对象包括下面几种： 虚拟机栈(栈帧中的本地变量表)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI(即一般说的Native方法)引用的对象 活跃线程引用的对象 二、Java中的引用类型 从JDK1.2之后，Java对引用的概念进行了扩充，将引用分为强引用，软引用，弱引用，虚引用，这四种引用的强度一次逐渐减弱 强引用就是指在程序代码之中普遍存在的，类似 Object obj = new Object() 这类的引用，只要强引用还存在，垃圾回收器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非需要的对象，对于软引用关联着的对象，在系统将要发生内存异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存异常 弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前，当垃圾收集器工作时，无论当前内存释放足够，都会回收掉只被弱引用关联的对象 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例，对一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 三、两次标记 《深入理解java虚拟机》原文： 在java根搜索算法中判断对象的可达性，对于不可达的对象，也并不一定是必须清理。这个时候有一个缓刑期，真正的判断一个对象死亡，至少要经过俩次标记过程： 如果对象在进行根搜索后发现没有与GC roots相关联的引用链，那他将会第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法，当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这俩种情况都视为“没有必要执行”。 即当一个对象重写了finalize()方法的时候，这个对象被判定为有必要执行finalize()方法，那么这个对象被放置在F-Queue队列之中，并在稍后由一条由虚拟机自动建立的、低优先级的Finalizer线程去执行。这里所谓的执行是指虚拟机会出发这个方法，但不承诺会等待它运行结束。这样做的原因：如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（极端的情况下），将可能会导致F-Queue队列中的其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己----只要重新与引用链上的任何建立关联即可，那么在第二次标记时它将会被移出“即将回收”的集合；如果对象这时候没有逃脱，就会被回收。 3.1 finalize的工作原理 一旦垃圾收集器准备好释放对象占用的存储空间，它首先调用finalize()，而且只有在下一次垃圾收集过程中，才会真正回收对象的内存.所以如果使用finalize()，就可以在垃圾收集期间进行一些重要的清除或清扫工作. 3.2 finalize()在什么时候被调用? 所有对象被Garbage Collection时自动调用,比如运行System.gc()的时候. 程序退出时为每个对象调用一次finalize方法。 显式的调用finalize方法 这个方法的用途就是：在该对象被回收之前，该对象的finalize()方法会被调用。这里的回收之前指的就是被标记之后，问题就出在这里，有没有一种情况就是原本一个对象开始不再上一章所讲的“关系网”（引用链）中，但是当开发者重写了finalize()后，并且将该对象重新加入到了“关系网”中，也就是说该对象对我们还有用，不应该被回收，但是已经被标记啦，怎么办呢？ 针对这个问题，虚拟机的做法是进行两次标记，即第一次标记不在“关系网”中的对象，并且要判断该对象有没有实现finalize()方法了，如果没有实现就直接判断该对象可回收。如果实现了就会先放在一个队列中，并由虚拟机建立的一个低优先级的线程去执行它。 随后就会进行第二次的小规模标记，如果对象还没有逃脱，在这次被标记的对象就会真正的被回收了。 四、垃圾收集算法 4.1 标记-清除算法 最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经基本介绍过了。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记-清除算法的执行过程如图： 4.2 复制算法 为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，未免太高了一点。 复制算法的执行过程如图： 4.3 标记-整理算法 复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，“标记-整理”算法的示意图如图 4.4 分代收集算法 当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收 五、新生代和老年代 5.1 新生代 新生代分为三个区域，一个Eden区和两个Survivor区，它们之间的比例为（8：1：1），这个比例也是可以修改的。通常情况下，对象主要分配在新生代的Eden区上，少数情况下也可能会直接分配在老年代中。 Java虚拟机每次使用新生代中的Eden和其中一块Survivor（From），在经过一次MinorGC后，将Eden和Survivor中还存活的对象一次性地复制到另一块Survivor空间上（这里使用的复制算法进行GC），最后清理掉Eden和刚才用过的Survivor（From）空间。将此时在Survivor空间存活下来的对象的年龄设置为1，以后这些对象每在Survivor区熬过一次GC，它们的年龄就加1，当对象年龄达到某个年龄（默认值为15）时，就会把它们移到老年代中。 在新生代中进行GC时，有可能遇到另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象，这些对象将直接通过分配担保机制进入老年代。 总结： 1、Minor GC是发生在新生代中的垃圾收集，采用的复制算法； 2、新生代中每次使用的空间不超过90%，主要用来存放新生的对象； 3、Minor GC每次收集后Eden区和一块Survivor区都被清空； 5.1 老年代 老年代里面存放都是生命周期长的对象，对于一些较大的对象（即需要分配一块较大的连续内存空间），是直接存入老年代的，还有很多从新生代的Survivor区域中熬过来的对象。 老年代中使用的是Full GC，Full GC所采用的是标记-清除或者标记-整理算法。老年代中的Full GC不像Minor GC操作那么频繁，并且进行一次Full GC所需要的时间要比Minor GC的时间长。 5.2 触发Full GC的条件 老年代空间不足 JDK8以前的永久代空间不足，现在永久代已经被元数据区代替 CMS GC时出现promotion failed，concurrent mode failure(下面文章讲到CMS垃圾收集器的时候会说明) minor GC晋升到老年代的平均大小大于老年代的剩余空间 调用System.gc()提醒JVM回收一下，只是提醒 5.3 对象如何晋升到老年代 一般有如下情况会晋升： 经历一定minor次数依然存活的对象 survivor区中存放不下的对象 新生成的大对象 5.4 常用的调优参数 5.5 内存申请过程 A. JVM会试图为相关Java对象在Eden中初始化一块内存区域 B. 当Eden空间足够时，内存申请结束。否则到下一步 C. JVM试图释放在Eden中所有不活跃的对象（Minor GC）, 释放后若Eden空间仍然不足以放入新对象，则试图将部分Eden中活跃对象放入Survivor区 D. 当Survivor区空间不够时或者某些对象熬的时间比较长，则Survivor区这些对象会被移到Old区 E. 当Old区空间不够时，JVM会在Old区进行完全的垃圾收集（Full GC） F. 完全垃圾收集后，若Survivor及Old区仍然无法存放从Eden复制过来的部分对象，导致JVM无法在Eden区为新对象创建内存区域，则出现out of memory错误.]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA内存模型常问面试题]]></title>
    <url>%2F2019%2F02%2F08%2FJVM%2FJAVA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%B8%B8%E9%97%AE%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第七篇文章，介绍一些面试比较常问的问题。 一、JVM三大性能调优参数-Xms -Xmx -Xss的含义 一般我们可以传入以下参数去调整堆和内存所占的大小： 1java -Xms128m -Xmx128m -Xss256k -jar xxx.jar -Xms ：堆的初始值 -Xmx ：堆能达到的最大值 -Xss ：规定了每个线程虚拟机栈的大小 二、JAVA内存模型中堆和栈的区别 首先来了解一下几种不同的内存分配策略： 静态存储：编译时确定每个数据目标在运行时的存储空间需求，比如static声明的静态变量，这里的数据一般都放在方法区，java8中这个区域叫做元数据区，用的时物理内存，并且之前合在一起的字符串常量池也被移到了堆区，详情见上一篇文章。 栈式存储：数据去需求在编译时未知，运行时模块入口前确定，比如基本数据类型，都是在运行的时候，才知道数据(字面量)到底是什么，对于JVM，一个方法内的执行，局部变量表和操作数栈的大小时确定的，即引用变量和栈空间大小是编译器确定的，至于字面量等运行时才能确定。 堆式存储：编译时或运行时模块入口都无法确定，动态分配，比如可变长度串、对象实例 下面来看看栈和堆的联系： 引用对象或者数组时，栈里定义变量保存堆中目标的首地址。 下面来看看栈和堆的区别： 管理方式：栈自动释放，堆需要GC 空间大小：栈比堆小 碎片相关：栈产生的碎片远小于堆 分配方式：栈支持静态和动态分配，而堆仅支持动态分配 效率：栈的效率比堆高 简单总结：栈比较小，随着方法执行完毕自动释放，栈数据结构简单，所以操作也简单高效。堆放各种对象实例和数组，必定要比较大的空间，那么需要GC来回收不需要的数据，效率低并且碎片也比较多，由于堆的操作比较复杂，所以数据结构也复杂，效率低。 三、元空间、堆、线程独占部分间的联系 先来看一个最简单的程序： 我们分别从元空间、堆、以及线程独占的部分来看看分别存储了啥： 学到这里，对于这些东西已经不需要解释了。针对JVM内存模型的知识在这里就串联起来了。了解到这里，对内存模型这一块基本的知识已经差不多了。 四、再来说说字符串 之前在java字符串核心一网打尽文章中，其实是对于JDK8这个版本的字符串特性进行详细的解读，其中也介绍了intern这个方法的含义和用法，由于JDK6和JDK6+关于intern是不一样的，这里对比一下。 对于JDK8： 12345678910111213public static void main(String[] args) throws ClassNotFoundException &#123; //第一种情况 String str1 = new String("a"); str1.intern(); String str2 = "a"; System.out.println(str1 == str2); //第二种情况 String str3 = new String("a") + new String("a"); str3.intern(); String str4 = "aa"; System.out.println(str3 == str4);&#125; 输出结果为： 12falsetrue 但是在JDK6中执行结果为： 12falsefalse 这个问题困扰了我很久，由于之前基础不是太扎实，所以直接就跳过了这个问题，在面试的时候几乎也不会太深究，但是一直成为我心里的坎。今天要把他解决掉。在说明这个问题之前，需要说明一下JVM有三种常量池： 4.1 三种常量池 Class文件中的常量池 这里面主要存放两大类常量：字面量和符号引用，符号引用包含三类常量： 类和接口的全限定名(Full Qualified Name) 字段的名称和描述符(Descriptor) 方法的名称和描述符 这个用javap看一下就能明白，这里只涉及字符串就不谈其他的了。简单地说，用双引号引起来的字符串字面量都会进这里面。 1String str2 = "a"; 这里的str2就是符号引用，a就是字面量。 运行时常量池 方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池(Constant Pool Table)，存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池。 全局字符串 HotSpot VM里，记录interned string的一个全局表叫做StringTable，它本质上就是个HashSet&lt;String&gt;。这是个纯运行时的结构，而且是惰性（lazy）维护的。 注意，它里面存放的是引用。 所以，一般我们说一个字符串进入了全局的字符串常量池其实是说在这个StringTable中保存了对它的引用，反之，如果说没有在其中就是说StringTable中没有对它的引用。 4.2 字面量进入字符串常量池的时机 先给出一个结论：就HotSpot VM的实现来说，加载类的时候，那些字符串字面量会进入到当前类的运行时常量池，不会进入全局的字符串常量池（即在StringTable中并没有相应的引用，在堆中也没有对应的对象产生） 那么加载类的过程发生的是什么呢？ R大的一篇文章： 在类加载阶段， JVM会在堆中创建 对应这些 class文件常量池中的 字符串对象实例 并在字符串常量池中驻留其引用。具体在resolve阶段执行。这些常量全局共享。 这里说的比较笼统，没错，是resolve阶段，但是并不是大家想的那样，立即就创建对象并且在字符串常量池中驻留了引用。 JVM规范里明确指定resolve阶段可以是lazy的。 所以，类加载的时候，必定要做的东西是，将class文件中字面量和符号引用放入运行时常量池中，而JVM规范里Class文件的常量池项的类型，有两种东西：CONSTANT_Utf8和CONSTANT_String。后者是String常量的类型，但它并不直接持有String常量的内容，而是只持有一个index，这个index所指定的另一个常量池项必须是一个CONSTANT_Utf8类型的常量，这里才真正持有字符串的内容。 CONSTANT_Utf8会在类加载的过程中就全部创建出来，而CONSTANT_String则是lazy resolve的，例如说在第一次引用该项的ldc指令被第一次执行到的时候才会resolve。 4.3 ldc指令是什么东西？ 简单地说，它用于将int、float或String型常量值从常量池中推送至栈顶 以下面代码为例： 12345public class Abc &#123; public static void main(String[] args) &#123; String a = "AA"; &#125; &#125; 查看其编译后的Class文件如下： 根据上面说的，在类加载阶段，这个 resolve 阶段（ constant pool resolution）是lazy的。换句话说并没有真正的对象，字符串常量池里自然也没有。执行ldc指令就是触发这个lazy resolution动作的条件。 ldc字节码在这里的执行语义是：到当前类的运行时常量池去查找该index对应的项,即上面说的CONSTANT_String指向的index，如果该项尚未resolve则resolve之，并返回resolve后的内容。 在遇到String类型常量时，resolve的过程如果发现StringTable已经有了内容匹配的java.lang.String的引用，则直接返回这个引用，反之，如果StringTable里尚未有内容匹配的String实例的引用，则会在Java堆里创建一个对应内容的String对象，然后在StringTable记录下这个引用，并返回这个引用出去。 这里很重要，昭示了一个重要问题：String a = &quot;AA&quot;;这一句执行完，要看字符串常量池中是否已经存在，不存在的话是要在堆中先创建对象的，然后把堆地址给全局的字符串常量池。 理解到这，有些问题就可以解决了，这里先不回答最上面的问题，先来看看下面的例子。注意运行环境是JDK8： 123456789class NewTest0 &#123; public static String s1="static"; // 第一句 public static void main(String[] args) &#123; String s2 = new String("he")+new String("llo"); //第二句 s2.intern(); // 第三句 String s3="hello"; //第四句 System.out.println(s2 == s3);//第五句，输出是true。 &#125;&#125; &quot;static&quot; &quot;he&quot; &quot;llo&quot; &quot;hello&quot;都会进入Class的常量池， 按照上面说的，类加载阶段由于resolve 阶段是lazy的，所以是不会创建实例，更不会驻留字符串常量池了。 但是要注意这个“static”和其他三个不一样，它是静态的，在类加载阶段中的初始化阶段，会为静态变量指定初始值，也就是要把“static”赋值给s1，这个赋值操作要怎么搞啊，先ldc指令把它放到栈顶，然后用putstatic指令完成赋值。注意，ldc指令，根据上面说的，会创建&quot;static&quot;字符串对象，并且会保存一个指向它的引用到字符串常量池。 运行main方法后，首先是第二句，一样的，要先用ldc把&quot;he&quot;和&quot;llo&quot;送到栈顶，换句话说，会创建他俩的对象（注意，在堆中开辟本体所占的空间，还没到new的那一步），并且会保存引用到字符串常量池中（把本地在堆中空间地址传给字符串常量池）；然后有个＋号对吧，内部是创建了一个StringBuilder对象，一路append，最后调用StringBuilder对象的toString方法得到一个String对象（内容是hello，注意这个toString方法会new一个String对象），并把它赋值给s2（s2指向的是new出来的新对象，是新的一块内存空间）。 注意，此时还没有把hello的引用放入字符串常量池。然后是第三句，intern方法一看，字符串常量池里面没有，它会把上面的这个hello对象的引用保存到字符串常量池，然后返回这个引用，但是这个返回值我们并没有使用变量去接收，所以没用。 第四句，字符串常量池里面已经有了，直接用嘛。所以s2和s3都是s2的指向的地址。 再来看个例子： 123456789101112public static void main(String[] args) &#123; // ① String s1=new String("he")+new String("llo"); String s2=new String("h")+new String("ello"); // ② String s3=s1.intern(); // ③ String s4=s2.intern(); // ④ System.out.println(s1==s3); System.out.println(s1==s4);&#125; 首先是将一些符号引用和字面量从class文件的常量池中撞到运行时常量池。然后运行main方法，先看第一句，会创建&quot;he&quot;和&quot;llo&quot;对象，并放入字符串常量池，然后会创建一个&quot;hello&quot;对象，没有放入字符串常量池，s1指向这个&quot;hello&quot;对象。 第二句，创建&quot;h&quot;和&quot;ello&quot;对象，并放入字符串常量池，然后会创建一个&quot;hello&quot;对象，没有放入字符串常量池，s2指向这个&quot;hello&quot;对象。 第三句，字符串常量池里面还没有，于是会把s1指向的String对象的引用放入字符串常量池（换句话说，放入池中的引用和s1指向了同一个对象），然后会把这个引用返回给了s3，所以s3==s1是true。 第四句，字符串常量池里面已经有了，直接将它返回给了s4，所以s4==s1是true。 此时，回到一开始： 123456789101112131415161718192021public static void main(String[] args) throws ClassNotFoundException &#123; //第一种情况 //1 String str1 = new String("a"); //2 str1.intern(); //3 String str2 = "a"; //4 System.out.println(str1 == str2); //第二种情况 //5 String str3 = new String("a") + new String("a"); //6 str3.intern(); //7 String str4 = "aa"; //8 System.out.println(str3 == str4);&#125; 在jdk1.6及以前，调用intern() 如果常量池中不存在值相等的字符串时，jvm会复制一个字符串到创量池中，并返回常量池中的字符串。 而在jdk1.7及以后，调用intern() 如果常量池中不存在值相等的字符串时，jvm只是在常量池记录当前字符串的引用，并返回当前字符串的引用。 所以在JDK6情况下，都是返回false，原因是：第一种情况下，执行第一句，看到有个字符串&quot;a&quot;，那么首先是创建&quot;a&quot;本体对象，并且把副本放入字符串常量池中。执行第二句，发现字符串常量池中已经存在，则不放了。执行第三句， s2指向的是字符串常量池中的&quot;a&quot;，这个字符串常量池&quot;a&quot;所在的地址，肯定与堆中的新new出来的不一样。所以返回false。 第二种情况，第一句相当于： 其实相当于: 123String s1 = new String("a");String s2 = new String("a");String str3 = (new StringBuilder()).apend(s1).apend(s2).toString(); 会先在堆中创建两个对象&quot;a&quot;，拷贝一个副本到字符串常量池中，此时&quot;a&quot;已经存在于字符串常量池中了。然后拼接生成一个新的对象&quot;aa&quot;在堆中，这种拼接出来的&quot;aa&quot;此时是不会把副本放进字符串常量池的，因为字符串常量池只保存已确定的字面量，这种拼接的属于运行完成才能确定，所以字符串常量池中没有，直到执行第6句，才会尝试把&quot;aa&quot;副本放入字符串常量池，但是还是跟上面一样，一个指向堆，一个指向字符串常量池，肯定不相等。 在JDK6+情况下，第一个返回false，第二个返回true。原因是：第一种情况下，执行第一句，首先是创建&quot;a&quot;本体对象，并且把引用放进字符串常量池中，然后new，开辟新的地址空间，此时str1指向的是new出来的空间的引用。执行第二句，尝试将str1的引用放入字符串常量池，但是池中已经存在了，所以不能放，所以一个指向堆，一个是本体对象的引用，不一样，所以为false。第二种情况，&quot;a&quot;跟上面一样，在堆中开辟，然后引用放入字符串常量池中，后面拼接成&quot;aa&quot;，此时只是在堆中开辟空间，下面执行intern尝试把它的引用传给字符串常量池，由于字符串常量池中没有，所以就放进去了。此时字符串常量池中的引用与&quot;aa&quot;对象实际的堆地址是一样的，所以为true. 这边有一个事实：在执行String s1 = new String(&quot;a&quot;)的new之前，JVM先看到有一个字符串&quot;a&quot;，则会先看看字符串常量池中是否有这个&quot;a&quot;，有则直接返回字符串常量池引用，没有则给它开辟空间，并且把这个空间的引用传给字符串常量池。 整理自：木女孩的回答]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内连接和外连接]]></title>
    <url>%2F2019%2F02%2F05%2Fmysql%2F%E5%86%85%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%A4%96%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[在电信实习的时候，天天有人来面试，问的问题我都听的蛮清楚的，比如内连接和外连接的区别，好像很简单，但还是说的不好，这里总结一下。 12345 A表 B表id name id name 1 a 1 b 2 b 3 c4 c 内连接 内连接就是左表和右表相同的数据: 1select * from A inner join B on A.id=B.id 结果： 12id name id name 1 a 1 b 左外连接 左外连接就是以左表为准，去匹配右表，左表有多少条数据，结果就是多少条数据 1select * from A left join B on A.id=B.id 1234id name id name 1 a 1 b 2 b null null4 c null null 右外连接 右外连接就是与左外连接反之，以右表为准，去匹配左表，右表有多少条数据，结果就是多少条数据 1select * from A right join B on A.id=B.id 123id name id name 1 a 1 b null null 3 c 交叉连接 交叉连接不带 WHERE 子句，它返回被连接的两个表所有数据行的笛卡尔积，返回到 结果集合中的数据行数等于第一个表中符合查询条件的数据行数乘以第二个表中符合查 询条件的数据行数。 1select * from A join B 1234567id name id name1 a 1 b1 a 3 c2 b 1 b2 b 3 c4 c 1 b4 c 3 c 内连接和外连接的区别 内连接只列出两张表共同匹配的数据行，而外连接的结果集中不仅包含符合连接条件的数据行，还包括左表(左外连接或左连接)或右表(右外连接或右连接)中的所有数据行。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA内存模型-线程共享]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2FJAVA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-%E7%BA%BF%E7%A8%8B%E5%85%B1%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第六篇文章，介绍线程共享区域。 一、内存模型–JAVA堆 java堆一般是java虚拟机所管理的内存中最大的一块。 java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。 堆上存放对象实例和数组。 java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 二、内存模型–方法区 方法区和堆一样，是各个线程共享的内存区域。 它用于存储已被虚拟机加载的类信息、常量、静态变量、及时编译器编译后的代码等数据。 其中，类信息包含类的版本、字段、接口、方法 八、PermGen与Metaspace 其实，方法区可以理解为一个规范，jdk6的具体实现是PermGen,而后来的版本具体实现是Metaspace。它们有一定的区别。 在 HotSpot JVM 中，永久代中用于存放类和方法的元数据以及常量池，比如Class和Method。每当一个类初次被加载的时候，它的元数据都会放到永久代中。 永久代是有大小限制的，它用的是JVM内存，即与堆内存等价的no heap区域，因此如果加载的类太多，很有可能导致永久代内存溢出，即万恶的 java.lang.OutOfMemoryError: PermGen ，为此我们不得不对虚拟机做调优。 由于 PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。 根据上面的各种原因，PermGen 最终被移除，方法区移至 Metaspace，字符串常量移至 Java Heap。Metaspace并不在虚拟机中，而是使用本地内存,十分方便管理，不会出现永久带内存溢出问题，垃圾回收的时候这个单独区域方便处理。 三、运行时常量池 是方法区的一部分。 类文件中除了类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法的运行时常量池中存放。 这里尤其值得注意的是字符串的创建，会被扔到字符串常量池中。如果是new，那么还是在堆重创建的。当然，运行时也可以产生新的常量放入池中，比如讲new出来的字符串用intern()方法便可以在运行时将其放到常量池中。 举例 123456789101112public static void main(String[] args) &#123; String str1 = "hello"; String str2 = "hello"; System.out.println(str1 == str2); //true String str3 = new String("hello"); System.out.println(str1 == str3); //false System.out.println(str1 == str3.intern()); //true &#125; 说明 对于直接声明的内容相同的字符串，对于str2来说是不需要重新分配地址的，因为str1的hello这个常量已经存在于常量池中了。所以他们两个其实是一个东西。 对于new出来的str3，是不会直接扔到常量池中的，他是在堆中分配，地址不一样，所以显然是false。 String类的intern()方法，使得运行时将堆中产生的对象放入常量池中，所以是true。 这里我在java字符串核心一网打尽中已经详细说明了，不再赘述。 四、对象探秘 4.1 对象的创建过程 类加载检查：检查该对象的类是否已经被加载、解析、初始化过，如果没有则先进行类加载操作。 分配内存：如果内存规整使用“指针碰撞”分配，否则一般使用“空闲列表”分配，具体看垃圾回收器是否带有整理（Compact）空闲内存功能。 初始化：将内存区初始化置零，不包含对象头，这一步保证了对象的实例字段在java代码中可以不赋初值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 对象头设置：这个对象是哪个类的实例、如何找到类的元数据信息、哈希码、GC分代年龄信息等即为对象头 对象的方法：即按照程序员的意愿进行初始化 4.2 对象的内存布局 对象头 一部分称为Mark Word，存储对象自身运行时的数据，包含哈希码、GC分代年龄、锁状态标志等等。 采用压缩存储，压缩到虚拟机位数（32位/64位）。由于对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word被设计为一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。 另一部分为类型指针，指向它的类元数据，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息不一定要经过对象本身。 如果对象是一个java数组，那么在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通java对象的元数据信息确定java对象的大小，但是从数组的元数据中却无法确定数组的大小。 实例数据 实例数据部分是对象真正存储的有效信息，也是在程序中定义的各种类型的字段内容。 无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。 从分配策略中可以看出，相同宽度的字段总是分配在一起，在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前。 对齐填充 非必需，只有前两者加起来非8的倍数时才会有。 因为HotSpot VM 的自动内存管理系统要求对象起始地址必须是8字节的整数倍，也就是说，对象的大小必须是8字节的整数倍。不对齐的时候，需要通过它来填充对齐。 九、对象的访问定位 通过句柄访问 通过句柄访问对象：当java虚拟机GC移动堆对象时，并不需要修改reference，只需修改句柄对象的实例数据指针。 通过直接指针访问 通过直接指针访问对象：加快了对象访问速度，比间接访问少一次对象实例数据的访问，HotSpot则采用的这种访问方式。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA内存模型-线程私有]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2FJAVA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-%E7%BA%BF%E7%A8%8B%E7%A7%81%E6%9C%89%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第五篇文章，JVM的内存模型一般是面试必问的点，因为对JVM内存模型有所了解，才会有可能知道调优手段。本篇文章首先介绍线程私有的一些区域。 一、从整体看JVM运行时内存模型 下面详细说说各个部分的作用。 二、内存模型–程序计数器 占用内存小：是一块较小的内存空间，当前线程所执行的字节码的行号指示器。 PC作用：字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 线程独立：为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们成这类内存区域为“线程私有”的内存。 native方法：如果线程正在执行的是一个java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空(undefined). 无内存溢出异常：此内存区域是唯一一个在java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 三、内存模型–JAVA虚拟机栈 线程私有，生命周期与线程相同。 虚拟机栈描述的是Java方法的内存模型：每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程。 四、栈帧 我们口中常常提到的栈与堆，其中栈就是现在讲的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 局部变量表存放了编译期可知的各种基本数据类型(boolean,byte,char,short,int,float,long,double),对象引用(它不等同于对象本身，可能是一个指向对象地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)、returnAddress类型(指向了一条字节码指令的地址) 其中64位长度的long和double类型的数据会占用2个局部变量空间，其余的数据类型只占用1个。 局部变量表所需的内存空间在编译期完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 如果线程请求的栈深度大于虚拟机所允许的深度，比如递归层数过多，将抛出StackOverflowError异常；如果虚拟机可以动态扩展，即虚拟机栈申请过多，扩展时却无法申请到足够的内存，就会抛出OutOfMemoryError异常。 五、内存模型–本地方法栈 本地方法栈与虚拟机栈所发挥的作用是非常相似的，他们之间的区别不过是虚拟机栈尾虚拟机执行java方法(也就是字节码)服务，而本地方法栈则为虚拟机用到的Native方法服务。 Sun HotSpot虚拟机直接将本地方法栈和虚拟机栈合二为一。 与虚拟机栈一样会抛出StackOverflowError异常或者OutOfMemoryError异常。 什么是native方法？ 简单地讲，一个Native Method就是一个java调用非java代码的接口。一个Native Method是这样一个java的方法：该方法的实现由非java语言实现，比如C。这个特征并非java所特有，很多其它的编程语言都有这一机制，比如在C＋＋中，你可以用extern “C”告知C＋＋编译器去调用一个C的函数。 下一篇来看看线程共享的区域。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细谈loadClass]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2F%E7%BB%86%E8%B0%88loadClass%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第四篇文章，对于获取Class对象，其实我们不知不觉中已经接触过两种了，一种就是loadClass，一种就是反射中的forName，它们到底有什么区别呢？其实涉及了类加载过程的区别。下面好好来探讨一下。 一、问题的提出 对于之前的 测试代码： 12345678public class Test &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; MyClassLoader myClassLoader = new MyClassLoader("C:\\Users\\swg\\Desktop\\","myClassLoader"); Class c = myClassLoader.loadClass("Robot"); System.out.println(c.getClassLoader()); c.newInstance(); &#125;&#125; 不知道大家有没有疑惑，我们这里是用了loadClass(name)来加载对应的Class对象的，最后还需要进行newInstance()。那么为什么要调用newInstance()才行呢？ 1.1 new的方式构建对象实例 下面要进行相应的测试。对于Robot.java: 首先用new的方式： 显示结果为： 1hello , i am a robot! 1.2 loadClass来获取Class对象 如果仅仅这样写，显示结果仅仅为： 1sun.misc.Launcher$AppClassLoader@18b4aac2 也就是说，并不会触发static静态块的执行，也就是说这个类根本就没有初始化。 1.3 forName来获取Class对象 显示结果为： 1hello , i am a robot! 触发了静态块的执行。 二、类加载过程 要想说明上面区别产生的原因，这里必须要介绍一个从未使用过的类加载的过程。 类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)7个阶段。其中准备、验证、解析3个部分统称为连接（Linking）。如图所示： 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。 2.1 加载 在加载阶段（可以参考java.lang.ClassLoader的loadClass()方法），虚拟机需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流（并没有指明要从一个Class文件中获取，可以从其他渠道，譬如：网络、动态生成、数据库等）； 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构； 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口； 加载阶段和连接阶段（Linking）的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 2.2 验证 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以魔术0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 2.3 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值，假设一个类变量的定义为： 1public static int value=123; 那变量value在准备阶段过后的初始值为0而不是123.因为这时候尚未开始执行任何java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行。 至于“特殊情况”是指：public static final int value=123，即当类字段的字段属性是ConstantValue时，会在准备阶段初始化为指定的值，所以标注为final之后，value的值在准备阶段初始化为123而非0. 2.4 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 对于这里说的：将符号引用替换为直接引用。很多人包括我第一次看到的时候感觉莫名其妙，教材上也是直接用这些专用名词，给我们的学习带来了极大的困扰。这里还是要解释一下。 比如以下代码： 123public static void main(String[] args) &#123; String s = "abc";&#125; s是符号引用，而abc是字面量。 此时，知道了什么是符号引用就好办了，因为符号引用一般都是放在栈中的，这个玩意肯定是依赖于实际的东西，相当于一个指针，多以我们程序需要将其解析成这个实际东西所在的真正的地址。所以，一旦解析了，那么内存中必然实际存在了这个对象，即拥有实际的物理地址了。 2.5 初始化 类初始化阶段是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的java程序代码。在准备阶段，变量已经赋过一次系统要求的初始值，而在初始化阶段，则根据程序猿通过程序制定的主观计划去初始化类变量和其他资源，或者说：初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程. 三、new、loadClass、forName 正常情况下，我们一般构建对象实例是通过new的方式，new是隐式构建对象实例，不需要newInstance()，并且可以用带参数的构造器来生成对象实例； 对于new，我们有点基础的，是知道，已经一直来到了最后初始化完成的这一步，生成了可以直接使用的对象实例。由于篇幅不宜太长，不想展开讲new的过程发生了什么，这里先贴个我觉得讲的不错的链接：https://www.jianshu.com/p/ebaa1a03c594 然而loadClass(name)这种显示调用的方式，我们可以看到，只有加载的功能，而没有后续连接以及初始化的过程。 所以loadClass(name)需要进行newInstance()才能生成对应的对象实例，并且这个newInstance()方法不支持参数调用，要想实现输入参数生成实例对象，需要通过反射获取构造器对象传入参数再生成对象实例。 这里也就解释了为什么要newInstance()，因为不这样，loadClass(name)只是加载，并没有后续过程，也就是说这个类根本就没有动它，仅仅是加载进来而已。从代码层面调用loadClass()的时候，我们可以看到一个之前故意忽视的东西： 这个resolve默认是传入false的，那么进来看看这个resolveClass()方法： 再下去是native方法，不必关心，我们只看方法的注释即可，写的是链接指定的类，就是上面的连接过程。我们由上面知道，如果这个方法能执行，那么就会触发验证、准备、解析这三个过程，而准备阶段是会去执行静态方法或静态块，类变量会被进行初始化，即分配内存，但是仅仅赋初值即可。 所以，loadClass(name)有一种懒加载的思想在里面，要用了再去进行初始化，而不是一开始就初始化好。 既然已经知道了new和loadClass的区别了，下面再来看看Class.forName(),聪明的读者估计已经可以猜到了，没错，根据实验的结果来看，它至少要进行到连接完，实质它也完成了初始化，即已经到达第三步： 总结一下：loadClass仅仅是第一步的加载，而forName和new都是已经初始化好了。 存在的原因 所谓存在即合理，forName的用法，最常见的莫过于用于加载数据库驱动这，我们这里实验一下，首先引入相关的依赖： 经典写法来啦： 点进去看看： 我们这个时候发现，里面是一个static方法，也就是说，我们要立即创建驱动。所以这个时候必须用forname方法啦！ 那么对于loadClass，其实上面已经提及了，就是懒加载，这个思想再spring中是到处可见的，bean只是加载，但是步进行初始化，等用的时候再去初始化，提高性能。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双亲委派模型]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第三篇文章，谈到JVM类加载机制，双亲委派模型是绕不开的话题，名字看好像是个高大上、深不可测的玩意，其实逐步揭开面纱之后很简单。下面我们就来揭揭看。 回顾类加载器 上一节简单说明了类加载器的作用，只说到一个核心功能是加载class文件。但是，绝对没有这么简单，神书《深入理解Java虚拟机》第二版对类加载器的说明： 代码编译的结果从本地机器码转变成字节码，是存储格式的一小步，却是编程语言发展的一大步。 Java虚拟机把描述类的数据从Class文件加载进内存，并对数据进行校验，转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 虚拟机设计团队把类加载阶段中的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这动作的代码模块成为“类加载器”。 类加载器虽然只用于实现类的加载动作，但它在Java程序中起到的作用却远远不限于类加载阶段。对于任意一个类，都需要由加载他的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类命名空间。这句话可以表达的更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来自同一个Class文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这个两个类就必定不相等。 对于上面进行一些说明： 注意，加载之后要将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构（方法区就是用来存放已被加载的类信息，常量，静态变量，编译后的代码的运行时内存区域） 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。这个Class对象并没有规定是在Java堆内存中，它比较特殊，虽为对象，但存放在方法区中。 这样，就可以使用这个类了。 还有，关于相等，只有在满足如下三个类“相等”判定条件，才能判定两个类相等。 两个类来自同一个Class文件 两个类是由同一个虚拟机加载 两个类是由同一个类加载器加载 什么是双亲委派模型 我们上一节已经知道了有四种类加载器，它们的实际关系为： 从这个图来看，是一个继承的关系，是这样吗？我们用代码来看看是不是真的是这样。 代码还是用上一篇文章自定义类加载器来测试： 结果是： 从这个结果就很容易看出，层级关系是与上图所述的一样。那么，这个层级关系其实就是我们下面要说的双亲委派模型的结构。 这里还想补充一点：就是为什么最后一个是null，即bootstrap为什么显示null，其实是因为它是用C++实现的，不是java语言实现的，所以与其他几个都有区别，这里根据就调用不到，所以显示null。如果非要看bootstrap里面大概如何实现的，需要去看看opjdk的代码。 结合代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; //1.加锁 synchronized (getClassLoadingLock(name)) &#123; //2.首先看看当前类加载器是否已经加载过，没有则委派给父亲查询 Class&lt;?&gt; c = findLoadedClass(name); //3.如果当前类加载器没有加载过，进来 if (c == null) &#123; long t0 = System.nanoTime(); try &#123; //4.看是否有父类加载器，有则进来 if (parent != null) &#123; //5.父类加载器看看是否已经加载过 //注意，这里是各递归函数，如果由下至上查询都没有加载过，则从上至下尝试去加载 c = parent.loadClass(name, false); &#125; else &#123; //进到这个，是来看看bootstrap类加载器是否加载过，没有加载过则加载 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; //6.如果所有类加载器都没有加载过，则开始尝试从上而下逐级去加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //去加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; //一开始是false if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 其实很简单，就是先一级一级往上查询是否已经加载过，加载过直接返回即可；一直查询到bootstrap类加载器，都没有加载过，那么就从bootstrap类加载器开始一级一级向下到他们的扫描范围内尝试加载这个class文件，知道自定义类加载(如果有的话)，没有则返回找不到。 说一下代码的实现思路。代码使用递归实现的，先一级一级找父亲，即一级一级向上入栈，某一个查到了就返回，每一层递归停留在c = parent.loadClass(name, false);；都查不到，再一级一级出栈去执行，那么就从c = findBootstrapClassOrNull(name);后面的代码继续执行，那么显然就是执行if (c == null) {...}尝试去加载。 为什么要用双亲委派模型 为什么需要双亲委派模型呢？假设没有双亲委派模型，试想一个场景： 黑客自定义一个java.lang.String类，该String类具有系统的String类一样的功能，只 是在某个函数稍作修改。比如equals函数，这个函数经常使用，如果在这这个函数中， 黑客加入一些“病毒代码”。并且通过自定义类加载器加入到JVM中。此时，如果没有双亲 委派模型，那么JVM就可能误以为黑客自定义的java.lang.String类是系统的String类， 导致“病毒代码”被执行。 而有了双亲委派模型，黑客自定义的java.lang.String类永远都不会被加载进内存。因为首先是最顶端的类加载器加载系统的java.lang.String类，最终自定义的类加载器无法加载java.lang.String类。 或许你会想，我在自定义的类加载器里面强制加载自定义的java.lang.String类，不去通过调用父加载器不就好了吗?确实，这样是可行。但是，在JVM中，判断一个对象是否是某个类型时，如果该对象的实际类型与待比较的类型的类加载器不同，那么会返回false。 举个简单例子： ClassLoader1、ClassLoader2都加载java.lang.String类，对应Class1、Class2对象。 那么Class1对象不属于ClassLoad2对象加载的java.lang.String类型。 委托机制的意义：防止内存中出现多份同样的字节码 比如两个类A和类B都要加载System类： 如果不用委托而是自己加载自己的，那么类A就会加载一份System字节码，然后类B又会加载一份System字节码，这样内存中就出现了两份System字节码。 如果使用委托机制，会递归的向父类查找，也就是首选用Bootstrap尝试加载，如果找不到再向下。这里的System就能在Bootstrap中找到然后加载，如果此时类B也要加载System，也从Bootstrap开始，此时Bootstrap发现已经加载过了System那么直接返回内存中的System即可而不需要重新加载，这样内存中就只有一份System的字节码了。 一个面试题 能不能自己写个类叫java.lang.System？ 显然是不可以的，可能方案是自己搞一个这个类放在特殊目录，用自定义类加载器去加载，然而系统自身的类加载器会先去加载使用，下次再用的时候，是先逐级向上查询是否已经加载过，根本没有机会让自定义类加载器去加载。 所以，如果非要用，那么必定是要破坏双亲委派模型了，那么又回到为什么要用双亲委派模型的问题上了，所以，为了自己写一个java.lang.System而破坏双亲委派模型，我只能说，脑子秀逗了。所以不要搞这些东西，包名或类名写的不一样即可。 一个问题 那么为什么不能用一个加载器去一个目录加载所有呢？还要分这么多的类加载器，不是麻烦么？ 其实，这个问题也是比较可笑的，毕竟每个层级的功能是不一样的，比如bootstrap是加载最核心的文件，没有它，都玩不起来。而自定义的呢？是比较特殊的需求，需要的时候才用到。对于这种有个性化的要求，一套代码来实现，显然是不合理的。 比如这个回答是根据加载的方式来思考的： 每一个类加载器都是为了去在不同的情景下去加载类。比如，你可以从联网服务器上加载一个class文件，也可以从远程web服务器下载二进制类。这么设计是因为我们需要类加载器提供一致的接口，这样客户端就可以加载类但是却不用管类加载器到底是怎么实现的。启动类加载器能够加载JVM_HOME/lib 下的类，但如果我们需要在其他的情况下加载类呢？简单来说，加载类的方法有无数种，我们需要一个灵活的加载器系统去在特定的情况下按照我们的想法来加载类。 还有一个回答是说更方便地对特定类进行优化： 虽然 对java 虚拟机没有研究过，java 为什么不能 一个加载器 加载全部的类 很明显， 实现起来也可以 但是需要 的 代码 更多，也更难 为各种类进行 优化，为了更简单的抽象 我在明确知道 该类是启动类的情况下，我就会 为该类 进行优化。 如果是自定义类，可能就 不会进行 此类优化。 在明确 目的的情况下， 专用代码 比 通用代码 更简单，也更有效。 总之，就是为了清晰和方便，这也是我们在进行软件设计的时候最基本的要求，即不能写死代码，影响扩展性；层次结构也不能写的太乱，影响后续的优化。 至此，双亲委派模型就讲完了。我们也清晰地知道了其设计思想和好处。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈ClassLoader]]></title>
    <url>%2F2019%2F02%2F05%2FJVM%2F%E6%B5%85%E8%B0%88ClassLoader%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第二篇文章，上一篇文章初步提到了class文件，以及一个最简单程序执行的指令含义，我们提到，是由JAVA虚拟机先加载这些编译好的class文件，然后再去根据解析出来的指令去转换为具体平台上的机器指令执行，但是加载这个class文件时如何加载的呢？其实就涉及比较重要的东西：ClassLoader 有一个基本认识，从编译到实例化对象的过程可以概括为以下三个阶段： 编译器将xxx.java源文件编译为xxx.class字节码文件 ClassLoader将字节码转换为JVM种的Class&lt;xxx&gt;对象 JVM利用Class&lt;xxx&gt;对象实例化为xxx对象 一、JVM系统结构 ClassLoader：依据特定格式，加载class文件到内存 Execution Engine：对命令进行解析 Native Interface：融合不同开发语言的原生库为Java所用 Runtime Data Area：JVM内存空间结构模型 首先通过ClassLoader加载符合条件的字节码文件到内存中，然后通过Execution Engine解析字节码指令，交由操作系统去执行。 二、什么是ClassLoader ClassLoader在java中有着非常重要的作用，它主要工作在Class装载的加载阶段，其主要作用是从系统外部获得Class二进制数据流。他是JAVA的核心组件，所有的Class都是由ClassLoader进行加载的，ClassLoader负责通过将Class文件里的二进制数据流装载进系统，然后交给JAVA虚拟机进行连接、初始化等操作。 简而言之，就是加载字节码文件。 我们翻开ClassLoader源码看看： 1public abstract class ClassLoader &#123;...&#125; 它是一个抽象类，下面我们再来说具体的实现类。 里面比较重要的是loadClass()方法： 123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125; 就是根据name来加载字节码文件，返回Class实例，加载不到则抛出ClassNotFoundException异常。 三、ClassLoader的种类 启动类加载器（Bootstrap ClassLoader）：由C++语言实现（针对HotSpot）,加载核心库java.*。 扩展类加载器（Extension ClassLoader）：Java编写，加载扩展库javax.* 它扫描的是哪个路径呢？ 我们看到，它负责将 &lt;JAVA_HOME &gt;/lib/ext或者由系统变量-Djava.ext.dir指定位置中的类库 加载到内存中。 应用程序类加载器（Application ClassLoader）：Java编写，加载程序所在目录 它负责将 用户类路径(java -classpath或-Djava.class.path变量所指的目录，即当前类所在路径及其引用的第三方类库的路径，看截图的最后一行，显示的是当前项目路径。 自定义ClassLoader：自定义 四、如何自定义ClassLoader 要自己实现一个ClassLoader，其核心涉及两个方法： 123456789protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name);&#125;protected final Class&lt;?&gt; defineClass(byte[] b, int off, int len) throws ClassFormatError&#123; return defineClass(null, b, off, len, null);&#125; 首先想一下为什么是这两个类？ 其实答案在loadClass()这个方法里面。如果已经熟悉双亲委派模型的同学，都会知道加载Class对象是先委派给父亲，看父亲是否已经加载，如果没有加载过，则从最顶层父亲开始逐层往下进行加载，这一块详细在下一篇文章中解释，我们先走马观花看看这个的核心方法长啥样： 123456789101112131415161718192021222324252627282930313233343536373839protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; //首先看看当前类加载器是否已经加载过，没有则委派给父亲查询 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //注意，这里是各递归函数，如果由下至上查询都没有加载过，则从上至下尝试去加载 c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; //如果所有类加载器都没有加载过，则开始尝试从上而下逐级去加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //去加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 如果我们不去重写findClass(name)方法，默认是直接抛出找不到的异常，所以我们要对这个方法进行重写。 由于字节码文件是一堆二进制流，所以需要一个方法来根据这个二进制流来定义成一个类，即defineClass()这个方法来实现这个功能。 说的比较抽象，下面来真正实践一把！ 五、实践自定义ClassLoader 首先写一个类：Robot.java 12345public class Robot &#123; static &#123; System.out.println("hello , i am a robot!"); &#125;&#125; 在对Robot.java用javac编译之后形成Robot.class文件，就要删除本项目下的这个Robot.java文件，要不然就会被AppClassLoader类加载先加载了，而无法再被我们的自定义类加载器再去加载。这个Robot.class文件我就直接放到桌面去了。路径为C:/Users/swg/Desktop/. 然后定义一个自定义的ClassLoader，按照上面的理论，只要重写findClass就可以指定到某个地方获取class字节码文件，此时获取的是二进制流文件，转换为字节数组，最后借用defineClass获取真正的Class对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MyClassLoader extends ClassLoader&#123; //执行加载的class文件的路径 private String path; //自定义类加载器的名字 private String classLoaderName; MyClassLoader(String path,String classLoaderName)&#123; this.path = path; this.classLoaderName = classLoaderName; &#125; //用于寻找类文件 @Override protected Class findClass(String name)&#123; byte[] b = loadClassData(name); return defineClass(name,b,0,b.length); &#125; //用于加载类文件 private byte[] loadClassData(String name) &#123; name = path + name + ".class"; InputStream in = null; ByteArrayOutputStream out = null; try&#123; in = new FileInputStream(new File(name)); out = new ByteArrayOutputStream(); int i=0; while ((i = in.read()) != -1)&#123; out.write(i); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; try &#123; in.close(); out.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; return out.toByteArray(); &#125;&#125; 最后测试一下能不能用自定义类加载器去加载到Robot对应的Class对象： 12345678public class Test &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; MyClassLoader myClassLoader = new MyClassLoader("C:\\Users\\swg\\Desktop\\","myClassLoader"); Class c = myClassLoader.loadClass("Robot"); System.out.println(c.getClassLoader()); c.newInstance(); &#125;&#125; 打印结果： 12MyClassLoader@677327b6hello , i am a robot! 好了，学习了关于ClassLoader的分类以及如何自定义ClassLoader，我们知道了类加载器的基本实现，上面谈到了一个重要方法是loadClass，这就涉及了类加载器的双亲委派模型。下一节从代码层面好好来说说这个，其实很简单。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年展望]]></title>
    <url>%2F2019%2F02%2F05%2Fsuibi%2F2019%E5%B9%B4%E5%B1%95%E6%9C%9B%2F</url>
    <content type="text"><![CDATA[今天是大年初一，算是真正步入2019年了。保存几张老家门口拍的照片，以作纪念。 老家越来越冷清，越来越萧条。 今年四月底顺利毕业的话，就真的工作了，在南京，本科+研究生读了七年书，加上小时候每年暑假都来南京玩，对南京的熟悉的程度远远大于家乡盐城。 所以希望可以努力，在南京能扎下根，然后一家人全搬过去。 在找工作方面，不想过多地提及了，我相信只要一直努力，就一定会有好的结果。 在2018年，算是学习java的进阶之年，自己学习了很多新的技术，也好好地夯实了基础，把以前很多模糊的问题搞清楚了，确实，基础真的太重要的，光学时髦的框架，可以写写简单的CRUD应用，是远远不够的，我觉得程序员的目标是可以造出大家都认可并且乐于使用的开源作品，没有好的基础，便是天方夜谭，也只能永远做一个普通的码农。 所以，我给自己定一个三年的目标，三年以后，无论是搞java还是搞大数据还是其他，我希望能达到中高级水平，在这个行业方向上有较好的基础和较深的认识。为后续更高的发展打下坚实的基础。 我希望，github真正能成为我出发的地方，并且能够走很远。犹记得，14年在大神室友的推荐下，注册了github，但是真正使用还是从去年开始吧。所以甚是惭愧，如果早一点上路，虽然追赶不上大神的脚步（已经进了google），但是进个二线比如京东、美团等都是轻而易举吧，但是谈这些确实是废话，没有人有假如。 除了对未来三年的一些初步想法之外，我还是希望我与家人都有一个健健康康的身体，所以工作以后身体的锻炼是必不可少的，八块腹肌是不指望了，至少爬几层楼不用喘吧。 一切还是视实际情况而定，但是终身学习的信念要埋藏在心里，这个时代以及未来的时代，选择了这一行，掉头发是注定的了，但是如果仅仅以掉头发的代价，可以让家人舒舒服服，健健康康，开开心心的话，那给我剃个光头也无妨了。 废话不多说了，我要继续完成本笔记的JVM部分了，不忘初心，套用喜剧之王的台词：努力，奋斗！]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解java反射机制]]></title>
    <url>%2F2019%2F02%2F04%2Fjava-basic%2F%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3java%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[反射机制这一块也是面试经常会被问到的，我从反射的基本概念到反射的一些面试题出发，好好理一理反射的知识。 1. 什么是反射 标准定义：JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用方法的功能成为反射机制。 注意几个关键字：运行状态中，动态获取。 2. Class对象和实例对象 想要理解反射首先需要知道Class这个类，它的全称是java.lang.Class类。java是面向对象的语言，讲究万物皆对象，即使强大到一个类，它依然是另一个类（Class类）的对象，换句话说，普通类是Class类的对象，即Class是所有类的类（There is a class named Class）。 我们知道java世界是运行在JVM之上的，我们编写的类代码，在经过编译器编译之后，会为每个类生成对应的.class文件，这个就是JVM可以加载执行的字节码。 运行时期间，当我们需要实例化任何一个类时，JVM会首先尝试看看在内存中是否有这个类，如果有，那么会直接创建类实例；如果没有，那么就会根据类名去加载这个类，当加载一个类，或者当加载器(class loader)的defineClass()被JVM调用，便会为这个类产生一个Class对象（一个Class类的实例），用来表达这个类，该类的所有实例都共同拥有着这个Class对象，而且是唯一的。 也就是说，加载.class文件之后会生成一个对应的Class对象。下面说说如何获取这个Class对象。 3. 取得Class对象的三种方式 我们假设有这么一个类叫MyClass： 1public class MyClass &#123; &#125; 第一种方式：通过“类名.class”的方式取得 1Class classInstance= MyClass.class; 例如： 123Class clazz = Car.class;Class cls1 = int.class;Class cls2 = String.class; 第二种方式：通过类创建的实例对象的getClass方法取得 12MyClass myClass = new MyClass();Class classInstance = myClass.getClass(); 第三种方式：通过Class类的静态方法forName方法取得（参数是带包名的完整的类名） 12345try &#123; Class classInstance = Class.forName("mypackage.MyClass");&#125; catch (ClassNotFoundException e) &#123; e.printStackTrace();&#125; 上面三种方法取得的对象都是相同的，所以效果上等价。 classInstance是类类型，通过类类型可以得到一个类的属性和方法等参数，这是反射的基础。 4. 利用反射API全面分析类的信息——方法，成员变量，构造器 反射的一大作用是用于分析类的结构，或者说用于分析和这个类有关的所有信息。而这些信息就是类的基本的组成： 方法，成员变量和构造器。 在java种万物皆对象，一个类中的方法，成员变量和构造器也分别对应着一个对象 每个方法都对应有一个保存和该方法有关信息的Method对象， 这个对象所属的类是java.lang.reflect.Method; 每个成员变量都对应有一个保存和该变量有关信息的Field对象，这个对象所属的类是 java.lang.reflect.Field 每个构造器都对应有一个保存和该构造器有关信息的Constructor对象，这个对象所属的类是java.lang.reflect.Constructor 假设c是一个类的Class对象： 通过 c.getDeclaredMethods()可取得这个类中所有声明方法对应的Method对象组成的数组 通过 c.getDeclaredFields()可取得这个类中所有声明的成员变量对应的Field对象组成的数组 通过 c.getConstructors(); 可取得这个类中所有构造函数所对应的Constructor对象所组成的数组 1234567Method [] methods = c.getDeclaredMethods(); // 获取方法对象列表 Field [] fields = c.getDeclaredFields(); // 获取成员变量对象列表Constructor [] constructors = c.getConstructors(); // 获取构造函数对象列表xxx.getName()就可以打印出对应的名字了。 5. 更多的反射api getMethods和getDeclaredMethods方法 getMethods取得的method对应的方法包括从父类中继承的那一部分，而 getDeclaredMethods取得的method对应的方法不包括从父类中继承的那一部分 一个普通的类，他们的基类都是Object，那么如果用getMethods，遍历得到的结果，会发现Object中的基础方法名都会被打印出来。 诸如wait(),equals(),toString(),getClass(), notify(),notifyAll(),hashCode()等等。 通过method.getReturnType()获取方法返回值对应的Class对象 12Class returnClass = method.getReturnType(); // 获取方法返回值对应的Class对象String returnName = returnClass.getName(); //获取返回值所属类的类名——也即返回值类型 通过method.getParameterTypes()获取方法各参数的Class对象组成的数组 12345Class [] paramsClasses = method.getParameterTypes();for (Class pc: paramsClasses) &#123; String paramStr = pc.getName(); // 获取当前参数类型 paramsStr+=paramStr + " ";&#125; 获取成员变量类型对应的的Class对象 123Field field = c.getDeclaredField("name"); // 取得名称为name的field对象field.setAccessible(true); // 这一步很重要！！！设置为true才能访问私有成员变量name的值！String nameValue = (String) field.get(obj); // 获取obj中name成员变量的值 通过getType方法读取成员变量类型的Class对象 12Field field = class1.getDeclaredField(number");System.out.print(field.getType().getName()); 因为java权限的原因，直接读取私有成员变量的值是非法的（加了field.setAccessible(true)后就可以了），但仍可以直接读取私有成员变量的类型 利用反射API分析类中构造器信息 123public class MyClass &#123; public MyClass(int a, String str)&#123;&#125;&#125; 12345678910111213public static void printContructorsMessage (Object obj) &#123;Class c = obj.getClass(); // 取得obj所属类对应的Class对象Constructor [] constructors = c.getDeclaredConstructors();for (Constructor constructor : constructors) &#123; Class [] paramsClasses = constructor.getParameterTypes(); String paramsStr = ""; for (Class pc : paramsClasses) &#123; String paramStr = pc.getName(); paramsStr+=paramStr + " "; &#125; System.out.println("构造函数的所有参数的类型列表：" + paramsStr);&#125;&#125; 运行结果： 1构造函数的所有参数的类型列表：int java.lang.String 6. 利用反射动态加载类，并用该类创建实例对象 我们用普通的方式使用一个类的时候，类是静态加载的 ，而使用Class.forName(“XXX”)这种方式，则属于动态加载一个类 静态加载的类在编译的时候就能确定该类是否存在，但动态加载一个类的时候却无法在编译阶段确定是否存在该类，而是在运行时候才能够确定是否有这个类，所以要捕捉可能发生的异常. Class对象有一个newInstance方法，我们可以用它来创建实例对象 12Class classInstance = Class.forName("mypackage.MyClass");MyClass myClass = (MyClass) classInstance.newInstance(); 7. 总结 反射为我们提供了全面的分析类信息的能力，例如类的方法，成员变量和构造器等的相关信息，反射能够让我们很方便的获取这些信息， 而实现这个获取过程的关键是取得类的Class对象，然后根据Class对象取得相应的Method对象，Field对象和Constructor对象，再分别根据各自的API取得信息。 反射还为我们提供动态加载类的能力 API中getDeclaredXXX和getXXX的区别在于前者只获取本类声明的XXX（如成员变量或方法），而不获取超类中继承的XXX， 后者都可以获取 API中， getXXXs（注意后面的s）返回的是一个数组， 而对应的 getXXX（“键”）按键获取一个值（这个时候因为可能报已检查异常所以要用try*catch语句包裹） 私有成员变量是不能直接获取到值的！因为java本身的保护机制，允许你取得私有成员变量的类型，但是不允许直接获取值，所以要对对应的field对象调用field.setAccessible(true) 放开权限 8. 面试 什么是反射 反射是一种能够在程序运行时动态访问、修改某个类中任意属性（状态）和方法（行为）的机制 反射到底有什么具体的用处 操作因访问权限限制的属性和方法； 实现自定义注解； 动态加载第三方jar包，解决android开发中方法数不能超过65536个的问题； 按需加载类，节省编译和初始化APK的时间； 反射的原理是什么 当我们编写完一个Java项目之后，每个java文件都会被编译成一个.class文件，这些Class对象承载了这个类的所有信息，包括父类、接口、构造函数、方法、属性等，这些class文件在程序运行时会被ClassLoader加载到虚拟机中。当一个类被加载以后，Java虚拟机就会在内存中自动产生一个Class对象。我们通过new的形式创建对象实际上就是通过这些Class来创建，只是这个过程对于我们是透明的而已。 反射的工作原理就是借助Class.java、Constructor.java、 Method.java、Field.java这四个类在程序运行时动态访问和修改任何类的行为和状态。 如何获取Class对象 Class的forName()方法的返回值就是Class类型，也就是动态导入类的Class对象的引用 1public static Class&lt;?&gt; forName(String className) throws ClassNotFoundException 每个类都会有一个名称为Class的静态属性，通过它也是可以获取到Class对象 1Class&lt;Student&gt; clazz = Student.class; Object类中有一个名为getClass的成员方法，它返回的是对象的运行时类的Class对象。因为Object类是所有类的父类，所以，所有的对象都可以使用该方法得到它运行时类的Class对象 12Student stu = new Student();Class&lt;Student&gt; clazz = stu.getClass(); 反射的特点 优点 灵活、自由度高：不受类的访问权限限制，想对类做啥就做啥 缺点 性能问题 通过反射访问、修改类的属性和方法时会远慢于直接操作，但性能问题的严重程度取决于在程序中是如何使用反射的。如果使用得很少，不是很频繁，性能将不会是什么问题； 安全性问题 反射可以随意访问和修改类的所有状态和行为，破坏了类的封装性，如果不熟悉被反射类的实现原理，随意修改可能导致潜在的逻辑问题； 如何提高反射性能 java应用反射的时候，性能往往是java程序员担心的地方，那么在大量运用反射的时候，性能的微弱提升，对这个系统而言都是如旱地逢甘霖。 setAccessible(true),可以防止安全性检查（做这个很费时） 做缓存，把要经常访问的元数据信息放入内存中，class.forName 太耗时 getMethods() 等方法尽量少用，尽量调用getMethod(name)指定方法的名称，减少遍历次数 java面试中面试官让你讲讲反射，应该从何讲起？ 先讲反射机制，反射就是程序运行期间JVM会对任意一个类洞悉它的属性和方法，对任意一个对象都能够访问它的属性和方法。依靠此机制，可以动态的创建一个类的对象和调用对象的方法。 其次就是反射相关的API，只讲一些常用的，比如获取一个Class对象。Class.forName(完整类名)。通过Class对象获取类的构造方法，class.getConstructor。根据Class对象获取类的方法，getMethod和getMethods。使用Class对象创建一个对象，class.newInstance等。 最后可以说一下反射的优点和缺点，优点就是增加灵活性，可以在运行时动态获取对象实例。缺点是反射的效率很低，而且会破坏封装，通过反射可以访问类的私有方法，不安全。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java如何执行一个最简单的程序]]></title>
    <url>%2F2019%2F02%2F03%2FJVM%2FJava%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E4%B8%80%E4%B8%AA%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本篇为学习JAVA虚拟机的第一篇文章，需要之前对JVM有一定了解的基础。我们都知道，JAVA号称：一次编译多处运行。这就离不开字节码文件和虚拟机啦！那么，虚拟机到底是如何去执行一个简单的程序的呢？理解了这个，我们就可以理解java时如何做到平台无关的了。下面我们来分析分析。 首先，写一个最简单的程序： 123456789public class Main &#123; public static void main(String[] args) &#123; int i=1,j=5; i++; ++j; System.out.println(i); System.out.println(j); &#125;&#125; 运行之后的结果想必就一目了然，我们就通过这个程序来分析分析到底是怎么执行这个程序额的。 首先呢，java程序的执行经历编译，编译成系统能识别的文件，这里的系统对应java语言就是JVM，即JAVA虚拟机。JVM在识别之后，再去与我们真正的操作系统进行交互和处理。 所以，我们要执行一个.java程序，必须要先进行编译。初学者都会学习一个指令叫做javac： 我们会发现路径下面就会多一个.class文件，这就是编译之后的文件。直接点开： 123456789101112131415161718//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//public class Main &#123; public Main() &#123; &#125; public static void main(String[] var0) &#123; byte var1 = 1; byte var2 = 5; int var3 = var1 + 1; int var4 = var2 + 1; System.out.println(var3); System.out.println(var4); &#125;&#125; 我们看第一行注释，说的是编译后的文件已经自动被IDEA反编译了，所以我们还能看得懂。真正的文件是： 12345漱壕 4          &lt;init&gt; ()V Code LineNumberTable main ([Ljava/lang/String;)V SourceFile Main.java         Main java/lang/Object java/lang/System out Ljava/io/PrintStream; java/io/PrintStream println (I)V !            *? ?       E   &lt;=??? ? ? ? ? 我们可以看到，其实是一堆乱码，根本看不懂。而在执行的时候，class文件是一种8位字节的二进制流文件。放在sublime中可以看到二进制文件（以16进制显示，在JAVA虚拟机中将来了解这各文件的含义，我们可以看到第一个单词是cafe babe，表明这是一个class字节码文件）： 那么我们想看看.class中的信息，还是需要反编译，这个时候可以用javap指令来做。如果我们对其不熟悉，可以先执行javap -help来了解了解。 12345678910111213141516171819用法: javap &lt;options&gt; &lt;classes&gt;其中, 可能的选项包括: -help --help -? 输出此用法消息 -version 版本信息 -v -verbose 输出附加信息 -l 输出行号和本地变量表 -public 仅显示公共类和成员 -protected 显示受保护的/公共类和成员 -package 显示程序包/受保护的/公共类 和成员 (默认) -p -private 显示所有类和成员 -c 对代码进行反汇编 -s 输出内部类型签名 -sysinfo 显示正在处理的类的 系统信息 (路径, 大小, 日期, MD5 散列) -constants 显示最终常量 -classpath &lt;path&gt; 指定查找用户类文件的位置 -cp &lt;path&gt; 指定查找用户类文件的位置 -bootclasspath &lt;path&gt; 覆盖引导类文件的位置 我们注意到，有一个-c是进行反汇编，那么就用它试试: 123456789101112131415161718192021222324252627E:\JavaBasic\src&gt;javap -c Main.classCompiled from &quot;Main.java&quot;public class Main &#123; public Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: iconst_1 1: istore_1 2: iconst_5 3: istore_2 4: iinc 1, 1 7: iinc 2, 1 10: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 13: iload_1 14: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 17: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 20: iload_2 21: invokevirtual #3 // Method java/io/PrintStream.println:(I)V 24: return&#125; 那么这反汇编出来的东西是什么呢？这是一连串的指令，其实这些是加载class文件时真正执行的java虚拟机指令。 我们来看看它的含义吧！ 仔细看看，其实发现并不神秘，一个函数的执行是一个入栈出栈的过程。ok，大体了解了字节码文件是什么以及里面的指令含义之后，我们对java如何执行它已经大体清楚了。下面执行一下： 那么如何运行呢？ 其实这是废话，初学java其实是java Main运行的： 1234E:\JavaBasic\src&gt;java Main26 这个时候，class文件可以移植到任何平台上去，比如直接上传到linux上，只要JDK或者JRE环境类似即可，就可以直接运行了，不需要编译，也不需要关心是什么系统。这就做到了一次编译到处运行。 下面总结一下： Java源码首先被编译成字节码，再由不同平台的JVM进行解析，JAVA语言在不同平台上运行时不需要进行重新编译，JAVA虚拟机在执行字节码的时候，把字节码转换为具体平台上的机器指令，然后各种操作系统就可以正确识别了。这就是JAVA如何执行代码和平台无关性的原因。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Redis一些重要的面试点]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2F%E5%85%B3%E4%BA%8ERedis%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9A%84%E9%9D%A2%E8%AF%95%E7%82%B9%2F</url>
    <content type="text"><![CDATA[这里整理一些面试题目，很多已经在前面的文章中详细说明了，这里算是总结一下。也补充了一点新的知识。 Redis有哪些数据结构？ 字符串 String、字典 Hash 、列表 List 、集合 Set 、有序集合 SortedSet。 如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。 String类型的底层数据结构 Redis 是一个键值对数据库, 数据库的值可以是字符串、集合、列表等多种类型的对象， 而数据库的键则总是字符串对象。 对于那些包含字符串值的字符串对象来说， 每个字符串对象都包含一个 sds 值。 “包含字符串值的字符串对象”，这种说法初听上去可能会有点奇怪， 但是在 Redis 中， 一个字符串对象除了可以保存字符串值之外， 还可以保存 long 类型的值， 所以为了严谨起见， 这里需要强调一下： 当字符串对象保存的是字符串时， 它包含的才是 sds 值， 否则的话， 它就是一个 long 类型的值。 举个例子， 以下命令创建了一个新的数据库键值对， 这个键值对的键和值都是字符串对象， 它们都包含一个 sds 值： 12345redis&gt; SET book &quot;Mastering C++ in 21 days&quot;OKredis&gt; GET book&quot;Mastering C++ in 21 days&quot; 目前来说， 只要记住这个事实即可： 在 Redis 中， 客户端传入服务器的协议内容、 aof 缓存、 返回给客户端的回复， 等等， 这些重要的内容都是由 sds 类型来保存的。 在 C 语言中，字符串可以用一个 \0 结尾的 char 数组来表示。 比如说， hello world 在 C 语言中就可以表示为 &quot;hello world\0&quot; 。 这种简单的字符串表示，在大多数情况下都能满足要求，但是，它并不能高效地支持长度计算和追加（append）这两种操作： 每次计算字符串长度（strlen(s)）的复杂度为 θ(N) 。 对字符串进行 N 次追加，必定需要对字符串进行 N 次内存重分配（realloc）。 在 Redis 内部， 字符串的追加和长度计算很常见， 而 APPEND 和 STRLEN 更是这两种操作，在 Redis 命令中的直接映射， 这两个简单的操作不应该成为性能的瓶颈。 另外， Redis 除了处理字符串之外， 还需要处理单纯的字节数组， 以及服务器协议等内容， 所以为了方便起见， Redis 的字符串表示还应该是二进制安全的： 程序不应对字符串里面保存的数据做任何假设， 数据可以是以 \0 结尾的 C 字符串， 也可以是单纯的字节数组， 或者其他格式的数据。 考虑到这两个原因， Redis 使用 sds 类型替换了 C 语言的默认字符串表示： sds 既可高效地实现追加和长度计算， 同时是二进制安全的。 在前面的内容中， 我们一直将 sds 作为一种抽象数据结构来说明， 实际上， 它的实现由以下两部分组成： 1234567891011121314typedef char *sds;struct sdshdr &#123; // buf 已占用长度 int len; // buf 剩余可用长度 int free; // 实际保存字符串数据的地方 char buf[];&#125;; 其中，类型 sds 是 char * 的别名（alias），而结构 sdshdr 则保存了 len 、 free 和 buf 三个属性。 作为例子，以下是新创建的，同样保存 hello world 字符串的 sdshdr 结构： 12345struct sdshdr &#123; len = 11; free = 0; buf = &quot;hello world\0&quot;; // buf 的实际长度为 len + 1&#125;; 通过 len 属性， sdshdr 可以实现复杂度为 θ(1) 的长度计算操作。 另一方面， 通过对 buf 分配一些额外的空间， 并使用 free 记录未使用空间的大小， sdshdr 可以让执行追加操作所需的内存重分配次数大大减少。 为了易于理解，我们用一个 Redis 执行实例作为例子，解释一下，当执行以下代码时， Redis 内部发生了什么： 12345678redis&gt; SET msg "hello world"OKredis&gt; APPEND msg " again!"(integer) 18redis&gt; GET msg"hello world again!" 首先， SET 命令创建并保存 hello world 到一个 sdshdr 中，这个 sdshdr 的值如下： 12345struct sdshdr &#123; len = 11; free = 0; buf = "hello world\0";&#125; 当执行 APPEND 命令时，相应的 sdshdr 被更新，字符串 &quot; again!&quot; 会被追加到原来的 “hello world” 之后： 12345struct sdshdr &#123; len = 18; free = 18; buf = "hello world again!\0 "; // 空白的地方为预分配空间，共 18 + 18 + 1 个字节&#125; 在这个例子中， 保存 “hello world again!” 共需要 18 + 1 个字节， 但程序却为我们分配了 18 + 18 + 1 = 37 个字节 —— 这样一来， 如果将来再次对同一个 sdshdr 进行追加操作， 只要追加内容的长度不超过 free 属性的值， 那么就不需要对 buf 进行内存重分配。 这种分配策略会浪费内存吗？ 执行过 APPEND 命令的字符串会带有额外的预分配空间， 这些预分配空间不会被释放， 除非该字符串所对应的键被删除， 或者等到关闭 Redis 之后， 再次启动时重新载入的字符串对象将不会有预分配空间。 因为执行 APPEND 命令的字符串键数量通常并不多， 占用内存的体积通常也不大， 所以这一般并不算什么问题。 另一方面， 如果执行 APPEND 操作的键很多， 而字符串的体积又很大的话， 那可能就需要修改 Redis 服务器， 让它定时释放一些字符串键的预分配空间， 从而更有效地使用内存。 当然， sds 也对操作的正确实现提出了要求 —— 所有处理 sdshdr 的函数，都必须正确地更新 len 和 free 属性，否则就会造成 bug 。 更多参见：简单动态字符串 从海量数据中查询某一固定前缀的key 使用keys指令可以扫出指定模式的key列表。 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 基于游标的迭代器，需要使用上一次游标延续之前的迭代过程。游标为0的时候代表开始或结束。 1234#模式scan cursor match pattern count#示例scan 0 match k* count 10 Redis做异步队列 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。 如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 如果有大量的key需要设置同一时间过期，一般需要注意什么？ 如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。 Redis如何做持久化的？ bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要aof来配合使用。在redis实例重启时，会使用bgsave持久化文件重新构建内存，再使用aof重放近期的操作指令来实现完整恢复重启之前的状态。 对方追问那如果突然机器掉电会怎样？取决于aof日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。 对方追问bgsave的原理是什么？你给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行bgsave操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 Pipeline有什么好处，为什么要用pipeline？ 可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 Redis的同步机制了解么？ Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 是否使用过Redis集群，集群的原理是什么？ Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 整理自： 天下无难试之Redis面试题刁难大全]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种主流缓存框架介绍]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2F%E5%87%A0%E7%A7%8D%E4%B8%BB%E6%B5%81%E7%BC%93%E5%AD%98%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第十二篇文章。本文对Guava Cache,Memcache以及redis进行简单介绍和对比。 缓存特征 缓存都会涉及：命中率、最大元素、清空策略(FIFO,LFU,LRU,过期时间，随机) 影响缓存命中率因素 业务场景和业务需求：适合读多写少的场景 缓存的设计(粒度和策略)：缓存粒度越小，命中率越高 缓存容量(经常用LRU)和基础设施(是否可扩展，避免缓存失效-一致性hash算法和几点冗余) 缓存分类 本地缓存：编程实现（成员变量、局部变量、静态变量）、Guava Cache 分布式缓存：Memcache，Redis 本地缓存：各应用之间无法很好地共享，与应用本身耦合过紧；而分布式缓存，本身就是独立的应用，各独立应用之间共享缓存。 Guava Cache 设计思想类似于jdk1.7中的ConcurrentHashMap，也是用多个segments的细粒度锁，在保证线程安全的同时，支持高并发场景的需求。 下面数据存储就是以键值对的形式存储，另外，需要处理缓存过期、动态加载等算法逻辑，所以需要一些额外的信息来实现这些操作。 主要实现的功能有：自动将节点加入到缓存结构中，当缓存的数据超过设置的最大值时，用LRU算法来移除。他具备根据节点上次被访问或者写入的时间来计算他的过期机制。 memcache memcache简单认识 memcache是一个高性能的分布式的内存对象缓存系统，它在内存里维护一个统一的巨大的hash表。能用来缓存各种格式的数据，包括图像、视频、文件以及数据库检索等结果. memcache是以守护程序方式运行于一个或多个服务器中，随时会接收客户的连接和操作。 存在memcache中的对象实际放置在内存中，这也是memcache如此高效的原因。 本身是不提供分布式的解决方案的。分布式是在客户端实现的，通过客户端的路由来处理达到分布式的目的。 应用服务器每次在存储某个key和value的时候，通过某种算法把key映射到某台服务器上。 一致性hash算法 客户端实现分布式：一致性hash算法，这个算法已经详细介绍过了。 memcache一些特性 Memcached单进程在32位系统中最大使用内存为2G，若在64位系统则没有限制,这是由于32位系统限制单进程最多可使用2G内存,要使用更多内存，可以分多个端口开启多个Memcached进程。 32 位寻址空间只有 4GB 大小，于是 32 位应用程序进程最大只能用到 4GB 的内存。然而，除了应用程序本身要用内存，操作系统内核也需要使用。应用程序使用的内存空间分为用户空间和内核空间，每个 32 位程序的用户空间可独享前 2GB 空间（指针值为正数），而内核空间为所有进程共享 2GB 空间（指针值为负数）。所以，32 位应用程序实际能够访问的内存地址空间最多只有 2GB。 最大30天的数据过期时间，设置为永久也会在这个时间过期。最长键长为250字节，大于该长度无法存储。最大同时连接数是200; memcache是一种无阻塞的socket通信方式服务，基于libevent库，犹豫无阻塞通信，对内存读写速度非常快。 不适用memcached的业务场景？ 缓存对象的大小大于1MB 虚拟主机不让运行memcached服务 key的长度大于250字符 需要持久化 不能够遍历memcached中所有的item？ 这个操作的速度相对缓慢且阻塞其他的操作 memcache如何分配内存？ 这张图片里面涉及了slab_class、slab、page、chunk四个概念，它们之间的关系是： MemCache将内存空间分为一组slab 每个slab下又有若干个page，每个page默认是1M，如果一个slab占用100M内存的话，那么这个slab下应该有100个page 每个page里面包含一组chunk，chunk是真正存放数据的地方，同一个slab里面的chunk的大小是固定的 有相同大小chunk的slab被组织在一起，称为slab_class 那么是具体如何分配的呢？ MemCache中的value过来存放的地方是由value的大小决定的，value总是会被存放到与chunk大小最接近的一个slab中，比如slab[1]的chunk大小为80字节、slab[2]的chunk大小为100字节、slab[3]的chunk大小为128字节（相邻slab内的chunk基本以1.25为比例进行增长，MemCache启动时可以用-f指定这个比例），那么过来一个88字节的value，这个value将被放到2号slab中。 放slab的时候，首先slab要申请内存，申请内存是以page为单位的，所以在放入第一个数据的时候，无论大小为多少，都会有1M大小的page被分配给该slab。申请到page后，slab会将这个page的内存按chunk的大小进行切分，这样就变成了一个chunk数组，最后从这个chunk数组中选择一个用于存储数据。 如果这个slab中没有chunk可以分配了怎么办，如果MemCache启动没有追加-M（禁止LRU，这种情况下内存不够会报Out Of Memory错误），那么MemCache会把这个slab中最近最少使用的chunk中的数据清理掉，然后放上最新的数据。 MemCache的内存分配chunk里面会有内存浪费，88字节的value分配在128字节（紧接着大的用）的chunk中，就损失了30字节，但是这也避免了管理内存碎片的问题 MemCache的LRU算法不是针对全局的，是针对slab的 该可以理解为什么MemCache存放的value大小是限制的，因为一个新数据过来，slab会先以page为单位申请一块内存，申请的内存最多就只有1M，所以value大小自然不能大于1M了 最后再总结一下memcache MemCache中可以保存的item数据量是没有限制的，只要内存足够 MemCache单进程在32位机中最大使用内存为2G，64位机则没有限制 Key最大为250个字节，超过该长度无法存储 单个item最大数据是1MB，超过1MB的数据不予存储 MemCache服务端是不安全的，比如已知某个MemCache节点，可以直接telnet过去，并通过flush_all让已经存在的键值对立即失效 不能够遍历MemCache中所有的item，因为这个操作的速度相对缓慢且会阻塞其他的操作 MemCache的高性能源自于两阶段哈希结构：第一阶段在客户端，通过Hash算法根据Key值算出一个节点；第二阶段在服务端，通过一个内部的Hash算法，查找真正的item并返回给客户端。从实现的角度看，MemCache是一个非阻塞的、基于事件的服务器程序 MemCache设置添加某一个Key值的时候，传入expire为0表示这个Key值永久有效，这个Key值也会在30天之后失效 redis redis特点 支持数据持久化，可以将内存中的数据保存到磁盘。 支持更多的数据结构 支持数据备份 性能极高，读可以达到11万次每秒；写达到8万1千次每秒 redis所有操作都是原子性，并且支持几个操作一起的原子性 支持发布-订阅功能 redis适用场景 取最新n个数据、排行榜 精准过期时间 计数器 唯一性检查 实时系统、垃圾系统、缓存等 redis VS memcache 当提到redis就问memcache，当提到memcache就提到redis，说明这两者用的都十分广泛，redis号称“强化版memcached”，他们之间的区别到底是啥呢？ 基本命令 memcache支持的命令很少，因为他只支持String的操作，通讯协议包括文本格式和二进制格式，用于满足简单网络客户端工具（如telnet）和对性能要求更高的客户端的不同需求；redis操作类似，只是数据结构更复杂以支持更多的特性，如发布订阅、消息队列等。redis的客户端-服务器通讯协议完全采用文本格式(Redis Cluster服务端节点之间通讯采用二进制格式)。 事务 redis通过multi / watch / exec等命令可以支持事务的概念，原子性的执行一批命令; memcache:即使在多线程模式，所有的命令都是原子的；命令序列不是原子的。在并发的情况下，您也可能覆写了一个被其他进程set的item。memcached 1.2.5以及更高版本，提供了gets和cas命令，它们可以解决上面的问题。如果您使用gets命令查询某个key的item，memcached会给您返回该item当前值的唯一标识。如果您覆写了这个item并想把它写回到memcached中，您可以通过cas命令把那个唯一标识一起发送给 memcached。如果该item存放在memcached中的唯一标识与您提供的一致，您的写操作将会成功。如果另一个进程在这期间也修改了这个 item，那么该item存放在memcached中的唯一标识将会改变，您的写操作就会失败。 数据备份，有效性，持久化等 memcached不保证存储的数据的有效性，slab内部基于LRU也会自动淘汰旧数据;memcached也不做数据的持久化工作; redis可以以master-slave的方式配置服务器，slave节点对数据进行replica备份，slave节点也可以充当read only的节点分担数据读取的工作;redis内建支持两种持久化方案，snapshot快照和AOF增量Log方式。 性能 memcached自身并不主动定期检查和标记哪些数据需要被淘汰，只有当再次读取相关数据时才检查时间戳，或者当内存不够使用需要主动淘汰数据时进一步检查LRU数据。 redis为了减少大量小数据CMD操作的网络通讯时间开销 RTT (Round Trip Time)，支持pipeline和script技术。 集群 memcached的服务器端互相完全独立，客户端通常通过对键值应用hash算法决定数据的分区，为了减少服务器的增减对hash结果的影响，导致大面积的缓存失效，多数客户端实现了一致性hash算法。 redis3.0已经支持服务端集群了。 性能对比 由于redis只使用单核，而memcached可以使用多核，所以平均每一个核上redis在存储小数据时比memcached性能更高。而在100k以上的数据中，memcached性能要高于redis，虽然redis最近也在存储大数据的性能上进行优化，但是比起memcached，还是稍有逊色 内存使用效率 使用简单的key-value存储的话，memcached的内存利用率更高，而如果redis采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于memcached。另外，memcached使用预分配的内存池的方式，带来一定程度的空间浪费 并且在内存仍然有很大空间时，新的数据也可能会被剔除，而redis使用现场申请内存的方式来存储数据，不会剔除任何非临时数据 redis更适合作为存储而不是cache。 redis支持服务器端的数据操作 redis相比memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，通常在memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。在redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果需要缓存能够支持更复杂的结构和操作，那么redis会是不错的选择 何时应该使用memcache: 首先就是对小型静态数据进行缓存处理，最具代表性的例子就是HTML代码片段。这是因为memcached在处理元数据时所消耗的内存资源相对更少. 在以前，redis3.0版本之前，memcached在横向扩展方面也比redis更具优势。由于其在设计上的思路倾向以及相对更为简单的功能设置，memcached在实现扩展时的难度比redis低得多。 何时应该使用redis： 其他场景都可以用redis来替换。 相比于武断的LRU(即最低近期使用量)算法，redis允许用户更为精准地进行细化控制，利用六种不同回收策略确切提高缓存资源的实际利用率。redis还采用更为复杂的内存管理与回收对象备选方案。 memcached将键名限制在250字节，值也被限制在不超过1MB，且只适用于普通字符串。redis则将键名与值的最大上限各自设定为512MB，且支持二进制格式。 它所保存的数据具备透明化特性，也就是说服务器能够直接对这些数据进行操作. redis还提供可选而且能够具体调整的数据持久性方案 redis能够提供复制功能。复制功能旨在帮助缓存体系实现高可用性配置方案，从而在遭遇故障的情况下继续为应用程序提供不间断的缓存服务。 使用redis的正确姿势： 要进行master-slave配置，出现服务故障时可以支持切换。 在master侧禁用数据持久化，只需在slave上配置数据持久化。 物理内存+虚拟内存不足，这个时候dump一直死着，时间久了机器挂掉。这个情况就是灾难。 当redis物理内存使用超过内存总容量的3/5时就会开始比较危险了，就开始做swap,内存碎片大。 当达到最大内存时，会清空带有过期时间的key，即使key未到过期时间。 redis与DB同步写的问题，先写DB，后写redis，因为写内存基本上没有问题。]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis事务]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2FRedis%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第十一篇文章。详细探讨redis事务的用法和原理。 redis 事务是一组命令的集合，至少是两个或两个以上的命令，redis 事务保证这些命令被执行时中间不会被任何其他操作打断。 事务基本认识 当客户端处于非事务状态下时， 所有发送给服务器端的命令都会立即被服务器执行。 但是， 当客户端进入事务状态之后， 服务器在收到来自客户端的命令时， 不会立即执行命令， 而是将这些命令全部放进一个事务队列里， 然后返回 QUEUED ， 表示命令已入队。 事务执行 前面说到， 当客户端进入事务状态之后， 客户端发送的命令就会被放进事务队列里。 但其实并不是所有的命令都会被放进事务队列， 其中的例外就是 EXEC 、 DISCARD 、 MULTI 和 WATCH 这四个命令 —— 当这四个命令从客户端发送到服务器时， 它们会像客户端处于非事务状态一样， 直接被服务器执行： 如果客户端正处于事务状态， 那么当 EXEC 命令执行时， 服务器根据客户端所保存的事务队列， 以先进先出（FIFO）的方式执行事务队列中的命令： 最先入队的命令最先执行， 而最后入队的命令最后执行。 事务基本命令介绍 除了 EXEC 之外， 服务器在客户端处于事务状态时， 不加入到事务队列而直接执行的另外三个命令是 DISCARD 、 MULTI 和 WATCH 。 DISCARD 命令用于取消一个事务， 它清空客户端的整个事务队列， 然后将客户端从事务状态调整回非事务状态， 最后返回字符串 OK 给客户端， 说明事务已被取消。 Redis 的事务是不可嵌套的， 当客户端已经处于事务状态， 而客户端又再向服务器发送 MULTI 时， 服务器只是简单地向客户端发送一个错误， 然后继续等待其他命令的入队。 MULTI 命令的发送不会造成整个事务失败， 也不会修改事务队列中已有的数据。 WATCH 只能在客户端进入事务状态之前执行， 在事务状态下发送 WATCH 命令会引发一个错误， 但它不会造成整个事务失败， 也不会修改事务队列中已有的数据（和前面处理 MULTI 的情况一样）。 正常情况 123multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//命令入队exec//提交事务（执行命令） 异常情况 1234multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//正常命令入队set key//错误命令，直接报错exec//事务被丢弃，提交失败 例外情况 1234multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//正常命令入队incr key//虽然字符串不能增一，但是不报错，入队exec//自增会失败，但是key被设置成功了，整个事务没有回滚 放弃事务 123multi//开启事务，下面的命令先不执行，先暂时保存起来set key val//正常命令入队discard 乐观锁 乐观锁：每次拿数据的时候都认为别人不会修改该数据，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这条数据，一般使用版本号进行判断，乐观锁使用于读多写少的应用类型，这样可以提高吞吐量。 乐观锁大多情况是根据数据版本号(version)的机制实现的，何为数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库添加一个version字段来实现读取出数据时，将此版本号一起读出，之后更新时，对此版本号加1，此时将提交数据的版本号与数据库表对应记录的当前版本号进行比对，如果提交的数据版本号大于数据库表的当前版本，则予以更新，否则认为是过期数据，不予更新。 A B 读出版本号为1，操作 A操作时，读出版本号也为1，进行某个操作(修改) 执行修改，version+1=2，因为2&gt;1，所以更新 … … 执行修改，version+1=2，发现数据库记录的版本也为2，2=2,更新失败 watch机制 WATCH 命令用于在事务开始之前监视任意数量的键： 当调用 EXEC 命令执行事务时， 如果任意一个被监视的键已经被其他客户端修改了， 那么整个事务不再执行， 直接返回失败。 123456set k1 1 //设置k1值为1watch k1 //监视k1(其他客户端不能修改k1值)set k1 2 //设置k1值为2multi //开始事务set k1 3 //修改k1值为3exex //提交事务，k1值仍为2，因为事务开始之前k1值被修改了 watch机制举例 大家可能知道redis提供了基于incr命令来操作一个整数型数值的原子递增，那么我们假设如果redis没有这个incr命令，我们该怎么实现这个incr的操作呢？ 正常情况下我们想要对一个整形数值做修改是这么做的(伪代码实现)： 123val = GET mykeyval = val + 1SET mykey $val 但是上述的代码会出现一个问题,因为上面吧正常的一个incr(原子递增操作)分为了两部分,那么在多线程(分布式)环境中，这个操作就有可能不再具有原子性了。 研究过java的juc包的人应该都知道cas，那么redis也提供了这样的一个机制，就是利用watch命令来实现的。 具体做法如下: 123456WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。 由于WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以在一般的情况下我们需要在EXEC执行失败后重新执行整个函数。 执行EXEC命令后会取消对所有键的监控，如果不想执行事务中的命令也可以使用UNWATCH命令来取消监控。 watch机制原理 WATCH 命令的实现 在每个代表数据库的 redis.h/redisDb 结构类型中， 都保存了一个 watched_keys 字典， 字典的键是这个数据库被监视的键， 而字典的值则是一个链表， 链表中保存了所有监视这个键的客户端。 比如说，以下字典就展示了一个 watched_keys 字典的例子： 其中， 键 key1 正在被 client2 、 client5 和 client1 三个客户端监视， 其他一些键也分别被其他别的客户端监视着。 WATCH 命令的作用， 就是将当前客户端和要监视的键在 watched_keys 中进行关联。 举个例子， 如果当前客户端为 client10086 ， 那么当客户端执行 WATCH key1 key2 时， 前面展示的 watched_keys 将被修改成这个样子： 通过 watched_keys 字典， 如果程序想检查某个键是否被监视， 那么它只要检查字典中是否存在这个键即可； 如果程序要获取监视某个键的所有客户端， 那么只要取出键的值（一个链表）， 然后对链表进行遍历即可。 WATCH 的触发 在任何对数据库键空间（key space）进行修改的命令成功执行之后 （比如 FLUSHDB 、 SET 、 DEL 、 LPUSH 、 SADD 、 ZREM ，诸如此类）， multi.c/touchWatchedKey 函数都会被调用 —— 它检查数据库的 watched_keys 字典， 看是否有客户端在监视已经被命令修改的键， 如果有的话， 程序将所有监视这个/这些被修改键的客户端的 REDIS_DIRTY_CAS 选项打开： 当客户端发送 EXEC 命令、触发事务执行时， 服务器会对客户端的状态进行检查： 如果客户端的 REDIS_DIRTY_CAS 选项已经被打开，那么说明被客户端监视的键至少有一个已经被修改了，事务的安全性已经被破坏。服务器会放弃执行这个事务，直接向客户端返回空回复，表示事务执行失败。 如果 REDIS_DIRTY_CAS 选项没有被打开，那么说明所有监视键都安全，服务器正式执行事务。 举个例子，假设数据库的 watched_keys 字典如下图所示： 如果某个客户端对 key1 进行了修改（比如执行 DEL key1 ）， 那么所有监视 key1 的客户端， 包括 client2 、 client5 和 client1 的 REDIS_DIRTY_CAS 选项都会被打开， 当客户端 client2 、 client5 和 client1 执行 EXEC 的时候， 它们的事务都会以失败告终。 最后，当一个客户端结束它的事务时，无论事务是成功执行，还是失败， watched_keys 字典中和这个客户端相关的资料都会被清除。 事务的 ACID 性质 Redis 事务保证了其中的一致性（偶尔也有可能不一致）和隔离性，但并不保证原子性和持久性。 原子性（Atomicity） 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 如果一个事务队列中的所有命令都被成功地执行，那么称这个事务执行成功。 另一方面，如果 Redis 服务器进程在执行事务的过程中被停止 —— 比如接到 KILL 信号、宿主机器停机，等等，那么事务执行失败。 当事务失败时，Redis 也不会进行任何的重试或者回滚动作。 一致性（Consistency） Redis 的一致性问题可以分为三部分来讨论：入队错误、执行错误、Redis 进程被终结。 前面两者上面已经讨论过了，这里再重复一下. 入队错误 入队错误一般是错误的命令(不考虑能不能执行，命令本身就是错误的)，带有不正确入队命令的事务不会被执行，也不会影响数据库的一致性； 执行错误 如果命令在事务执行的过程中发生错误，比如说，对一个不同类型的 key 执行了错误的操作， 那么 Redis 只会将错误包含在事务的结果中， 这不会引起事务中断或整个失败，不会影响已执行事务命令的结果，也不会影响后面要执行的事务命令， 所以它对事务的一致性也没有影响。 Redis 进程被终结 如果 Redis 服务器进程在执行事务的过程中被其他进程终结，或者被管理员强制杀死，那么根据 Redis 所使用的持久化模式，可能有以下情况出现： 内存模式：如果 Redis 没有采取任何持久化机制，那么重启之后的数据库总是空白的，所以数据总是一致的。 RDB 模式：在执行事务时，Redis 不会中断事务去执行保存 RDB 的工作，只有在事务执行之后，保存 RDB 的工作才有可能开始。所以当 RDB 模式下的 Redis 服务器进程在事务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到 RDB 文件里。所以显然会造成不一致 AOF 模式：因为保存 AOF 文件的工作在后台线程进行，所以即使是在事务执行的中途，保存 AOF 文件的工作也可以继续进行,如果事务语句未写入到 AOF 文件，那么显然是一致的，因为事务里的操作全部失败；如果事务的部分语句被写入到 AOF 文件，并且 AOF 文件被成功保存，那么不完整的事务执行信息就会遗留在 AOF 文件里，当重启 Redis 时，程序会检测到 AOF 文件并不完整，Redis 会退出，并报告错误。需要使用 redis-check-aof 工具将部分成功的事务命令移除之后，才能再次启动服务器。还原之后的数据总是一致的，而且数据也是最新的（直到事务执行之前为止）。 隔离性（Isolation） Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。 持久性（Durability） 在单纯的内存模式下，事务肯定是不持久的。 在 RDB 模式下，服务器可能在事务执行之后、RDB 文件更新之前的这段时间宕机，所以 RDB 模式下的 Redis 事务也是不持久的。 在 AOF 的“总是 SYNC ”模式下，事务的每条命令在执行成功之后，都会立即调用 fsync 或 fdatasync 将事务数据写入到 AOF 文件。但是，这种保存是由后台线程进行的，主线程不会阻塞直到保存成功，所以从命令执行成功到数据保存到硬盘之间，还是有一段非常小的间隔，服务器也有可能出现问题，所以这种模式下的事务也是不持久的。 都是不持久的。 总结 MULTI 命令的执行标记着事务的开始 当客户端进入事务状态之后， 服务器在收到来自客户端的命令时， 不会立即执行命令， 而是将这些命令全部放进一个事务队列里， 然后返回 QUEUED ， 表示命令已入队 Redis 的事务保证了 ACID 中的一致性（C）（偶尔也有可能不一致）和隔离性（I），但并不保证原子性（A）和持久性（D）。 不加入到事务队列而直接执行的四个命令为：EXEC 、 DISCARD 、 MULTI 和 WATCH DISCARD 命令用于取消一个事务 Redis 的事务是不可嵌套的 WATCH 只能在客户端进入事务状态之前执行 WATCH机制的原理 参考： 事务 redis的事务和watch]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存更新]]></title>
    <url>%2F2019%2F02%2F02%2Fredis%2FRedis%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第十篇文章。redis缓存更新策略学习。 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching，我们下面一一来看一下这四种Pattern。这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 先来看看缓存可能存在的一些问题，目的是突出缓存使用策略选择的重要性。 1.缓存穿透 缓存穿透是说访问一个缓存中没有的数据，但是这个数据数据库中也不存在。 解决方案是： 缓存空对象。如果缓存未命中，而数据库中也没有这个对象，则可以缓存一个空对象到缓存。如果使用Redis，这种key需设置一个较短的时间，以防内存浪费。 缓存预测。预测key是否存在。如果缓存的量不大可以使用hash来判断，如果量大可以使用布隆过滤器来做判断。采用布隆，将所有可能存在的数据哈希到一个足够大的BitSet中，不存在的数据将会被拦截掉，从而避免了对存储系统的查询压力。 2.缓存并发 多个客户端同时访问一个没有在cache中的数据，这时每个客户端都会执行从DB加载数据set到缓存，就会造成缓存并发。 缓存预热。提前把所有预期的热数据加到缓存。定位热数据还是比较复杂的事情，需要根据自己的服务访问情况去评估。这个方案只能减轻缓存并发的发生次数不能全部抵制。 缓存加锁。 如果多个客户端访问不存在的缓存时，在执行加载数据并set缓存这个逻辑之前先加锁，只能让一个客户端执行这段逻辑。 3.缓存雪崩 缓存雪崩是缓存服务暂时不能提供服务，导致所有的请求都直接访问DB。 解决方案： 构建高可用的缓存系统。目前常用的缓存系统Redis和Memcache都支持高可用的部署方式，所以部署的时候不防先考虑是否要以高可用的集群方式部署。 限流。Netflix的Hystrix是非常不错的工具，在用缓存时不妨搭配它来使用。 4.Cache Aside Pattern 一种错误的做法是：先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，直到这个缓存失效为止。 Cache Aside Pattern是最常用最常用的pattern了。其具体逻辑如下： 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？ 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 但还是存在问题的。比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。不过，实际上出现的概率可能非常低. 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 5.Read/Write Through Pattern Read Through：读取数据的时候如果当前缓存中没有数据，惯常的操作都是应用程序去DB加载数据，然后加入到缓存中。Read Through与之不同的是我们不需要在应用程序自己加载数据了，缓存层会帮忙做件事。 Write Through：更新数据的时候，如果命中缓存，则先更新缓存然后缓存在负责把数据更新到数据库；如果没有命中缓存则直接更新数据库。 这种方式缓存层直接屏蔽了DB，应用程序只需要更缓存打交道。优点是应用逻辑简单了，而且更高效了；缺点是缓存层的实现相对复杂一些。 6.Write Back Pattern Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道Unix/Linux非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。 另外，Write Back实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的write back会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。 7.实际使用的一些策略 业务方（调用者）更新 传统上，更新缓存都是由业务方来做，也就是由调用者负责更新DB和缓存。 DB中间件监听DB变化，更新缓存 现在有种新的办法就是利用DB中间件监听DB变化（比如阿里的Canal中间件，点评的Puma），从而对缓存进行更新。 这种办法的一个好处就是：把缓存的更新逻辑，和业务逻辑解藕。业务只更新DB，缓存的更新被放在另外一个专门的系统里面。 8.总结 一句话，无论谁先谁后，只要更新缓存和更新DB不是原子的，就可能导致不一致。 总之，只是从实际业务来讲，一般缓存也都是保持“最终一致性“，而不是和DB的强一致性。 并且一般建议先更新DB，再更新缓存，优先保证DB数据正确。 9.一致性问题 上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新Cache成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback.后续再探讨。 参考1：https://coolshell.cn/articles/17416.html 参考2：https://www.jianshu.com/p/3c111e4719b8 参考3：缓存更新策略/缓存穿透/缓存雪崩]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存设计与优化]]></title>
    <url>%2F2019%2F02%2F01%2Fredis%2F%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第九篇文章。介绍redis缓存中 一些重要的问题。 1. 缓存收益和成本 1.1 收益 加速读写 降低后端负载(降低mysql负载) 1.2 成本 数据不一致：缓存层和数据层有时间窗口不一致，和更新策略有关 代码维护成本：多了一层缓存逻辑 运维成本：例如redis cluster 1.3 使用场景 降低后端负载：对于高消耗的SQL：join结果集、分组统计结果；对这些结果进行缓存。 加速请求响应 大量写合并为批量写：如计数器先redis累加再批量写入DB 2. 缓存的更新策略 LRU/LFU/FIFO算法剔除：例如maxmemory-policy FIFO(first in first out) 先进先出策略，最先进入缓存的数据在缓存空间不够的情况下（超出最大元素限制）会被优先被清除掉，以腾出新的空间接受新的数据。策略算法主要比较缓存元素的创建时间。在数据实效性要求场景下可选择该类策略，优先保障最新数据可用。 LFU(less frequently used) 最少使用策略，无论是否过期，根据元素的被使用次数判断，清除使用次数较少的元素释放空间。策略算法主要比较元素的hitCount（命中次数）。在保证高频数据有效性场景下，可选择这类策略。 LRU(least recently used) 最近最少使用策略，无论是否过期，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素释放空间。策略算法主要比较元素最近一次被get使用时间。在热点数据场景下较适用，优先保证热点数据的有效性。 超时剔除：例如expire 主动更新：开发控制生命周期（最终一致性，时间间隔比较短） 低一致性：最大内存和淘汰策略 高一致性：超时剔除和主动更新结合，最大内存和淘汰策略兜底。 3. 缓存粒度控制 3.1 缓存粒度控制三个角度 通用性：全量属性更好(添加删除属性不需要改东西) 占用空间：部分属性更好 代码维护：表面上全量属性更好(添加删除属性不需要改东西) 4. 缓存穿透优化 4.1 定义 大量请求不命中,缓存已经没有存在的意义了： 4.2 产生原因 业务代码自身问题 恶意攻击、爬虫等 4.3 如何发现 业务响应时间 业务本身问题 相关指标：总调用数、缓存层命中数、存储层命中数 4.4 解决方案 方案一：缓存空对象 存在的问题 需要更多的键:恶意攻击、爬虫会有很多乱七八糟的键，当量很大时，会有风险，所以会对这种空对象设置缓存时间控制风险 缓存层和存储层数据“短期”不一致：缓存了空对象，但是当业务恢复了，真实数据又存在于DB中了，那么在这个空对象过期时间内，取到的仍然是空对象，造成短期内数据不一致的问题。解决：可以订阅消息，当恢复正常后接受到消息，然后刷新缓存。 方案二：布隆过滤器拦截 什么是Bloom Filter？ 布隆过滤器（Bloom Filter）是1970年由布隆提出的, “a space-efficient probabilistic data structure”。它实际上是一个很长的二进制矢量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢，上述三种结构的检索时间复杂度分别为O(n),O(log n),O(n/k)。 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。 优点：相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数（O（k））。另外, 散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。布隆过滤器可以表示全集，其它任何数据结构都不能；k和m相同，使用同一组散列函数的两个布隆过滤器的交并差运算可以使用位操作进行。 缺点：但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。另外，一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加1,这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。在降低误算率方面，有不少工作，使得出现了很多布隆过滤器的变种。 检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。 Bloom Filter应用场景？ 用Redis的Bitmap作为位数组构建起来的可扩展的布隆过滤器。 Redis实现的布隆过滤器如何快速有效删除数据？：EXPIRE “bitmap的key值” 0 4.5 解决方案对比 5. 无底洞问题优化 5.1 问题描述 2010年，facebook有了3000个Memcache节点 发现问题：&quot;加&quot;机器性能没能提升，反而下降 5.2 问题原因 当存在的节点异常多的时候，IO的代价已经超过数据传输，上文提到的facebook的节点已经超过3000个，在这种情况下再增加节点已经没法再提高效率了。 5.3 问题解决—优化IO 命令本身的效率：例如sql优化，命令优化 网络次数：减少通信次数 降低接入成本:长连/连接池,NIO等。 IO访问合并:O(n)到O(1)过程:批量接口(mget)，就是上一篇文章中介绍的对于mget的四个方案。 6. 缓存雪崩优化 6.1 什么是缓存雪崩？ 从下图可以很清晰出什么是缓存雪崩：由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。 缓存雪崩的英文原意是 stampeding herd（奔逃的野牛），指的是缓存层宕掉后，流量会像奔逃的野牛一样，打向后端存储。 6.2 如何防止缓存雪崩？ 保证缓存层服务高可用性。 和飞机都有多个引擎一样，如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如前面介绍过的 Redis Sentinel 和 Redis Cluster 都实现了高可用。 依赖隔离组件为后端限流并降级 无论是缓存层还是存储层都会有出错的概率，可以将它们视同为资源。作为并发量较大的系统，假如有一个资源不可用，可能会造成线程全部 hang 在这个资源上，造成整个系统不可用。降级在高并发系统中是非常正常的：比如推荐服务中，如果个性化推荐服务不可用，可以降级补充热点数据，不至于造成前端页面是开天窗。 在实际项目中，我们需要对重要的资源 ( 例如 Redis、 MySQL、 Hbase、外部接口 ) 都进行隔离，让每种资源都单独运行在自己的线程池中，即使个别资源出现了问题，对其他服务没有影响。但是线程池如何管理，比如如何关闭资源池，开启资源池，资源池阀值管理，这些做起来还是相当复杂的，这里推荐一个 Java 依赖隔离工具 Hystrix。超出范围了。不再赘述。 7. 热点key重建优化 7.1 问题 热点key( 例如一个热门的娱乐新闻）+较长的重建时间（可能是一个复杂计算，例如复杂的 SQL、多次 IO、多个依赖等） 就是说在高并发的情况下，某个key在缓存中重建时间太长，以至于高并发下缓存查不到，都去DB进行查询。对于DB压力很大，并且响应时间长。 三个目标：要减少缓存重建次数、数据尽可能一致、减少潜在危险。 两个解决：互斥锁、永远不过期 7.2 互斥锁—setex,setnx 存在问题：有等待时间。 伪代码： (1) 从 Redis 获取数据，如果值不为空，则直接返回值，否则执行 (2.1) 和 (2.2)。 (2) 如果 set(nx 和 ex) 结果为 true，说明此时没有其他线程重建缓存，那么当前线程执行缓存构建逻辑。 (2.2) 如果 setnx(nx 和 ex) 结果为 false，说明此时已经有其他线程正在执行构建缓存的工作，那么当前线程将休息指定时间 ( 例如这里是 50 毫秒，取决于构建缓存的速度 ) 后，重新执行函数，直到获取到数据。 7.3 永远不过期 这里我想了很久到底是什么意思，，，我感觉这是一个场景：保证数据的定期更新。对于热点key,无非是并发特别大并且重建缓存时间比较长，如果直接设置过期时间，那么时间到的时候，巨大的访问量会压迫到数据库上，所以我们实际上，是不给他设置过期时间，但是不设置过期时间，怎么做到定时更新呢？这里的方案是给热点key的val增加一个逻辑过期时间字段，并发访问的时候，判断这个逻辑字段的时间值是否大于当前时间，大于了说明要对缓存进行更新了，那么这个时候，依然让所有线程访问老的缓存，因为缓存并没有设置过期，但是另开一个线程对缓存进行重构。等重构成功，即执行了redis set操作之后，所有的线程就可以访问到重构后的缓存中的新的内容了。不知道我的理解是不是正确。 “永远不过期”包含两层意思： 从缓存层面来看，确实没有设置过期时间，所以不会出现热点 key 过期后产生的问题，也就是“物理”不过期。 从功能层面来看，为每个 value 设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。 2018/6/19 号补充：物理上缓存确实是不过期的，保证所有线程都能访问到，但是有可能是老的数据；逻辑上给 value 增加过期时间，如果当过期时间超过当前时间(每一个线程拿缓存数据的时候都会判断一下，也就是说这里仍然使用互斥锁，其中一个线程发现过期时间超过当前时间了，那么锁住，另开一个线程去完成数据重建)，新开一个线程去构建缓存，构建成功之后，设置新内容到缓存中并且删除老缓存，就完成了热点 key 的重建。 伪代码实现： 两种方案对比]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Cluster理论详解]]></title>
    <url>%2F2019%2F02%2F01%2Fredis%2FRedis-Cluster%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第八篇文章。上一篇我们学习了redis sentinel，知道了它是redis高可用的一种实现方案。但是面对要求很高的场景，一台master是一定不能解决问题的，redis 3.0给我们带来了服务端集群方案，解决了这个问题。 1. 数据分区 集群，那么就会涉及到数据是如何分片的。有两种方式：顺序分区和哈希分区 两者对比： 直接hash取模进行数据分片时，当节点增加，会有很多数据命中不了，需要重新映射。如果大多数数据在增加或者减少节点之后进行迁移的话，对于性能影响是很大的，因为数据迁移，那么缓存中现在是无法命中的，必须去数据库取，是灾难性的行为。 早期的做法就是这样，在客户端hash取余节点个数来进行数据分片。如果非要这样，采取翻倍扩容会稍微好一点，迁移数据量会小一点。不过无论如何，这种方式在大数据量情况下是不可行的。 2. 一致性hash算法 对于上面提到的直接hash取余的方式，会导致大量数据的迁移。那么有没有一种方式，在增加或减少节点时，只有少部分数据迁移呢？ 针对一致性hash算法，已经在简明理解一致性hash算法中详细说明了，不再赘述。 对于redis 3.0之前，客户端可以用这种方式来实现数据分片。在redis 3.0之后，就不需要客户端来实现分片算法了，而是直接给我们提供了服务端集群方案redis cluster. 3. 虚拟槽 redis cluster引入槽的概念，一定要与一致性hash的槽区分！这里每一个槽映射一个数据集。 CRC16(key) &amp; 16383 这里计算结果发送给redis cluster任意一个redis节点，这个redis节点发现他是属于自己管辖范围的，那就将它放进去；不属于他的槽范围的话，由于redis之间是相互通信的，这个节点是知道其他redis节点的槽的信息，那么会告诉他去那个redis节点去看看。 那么就实现了服务端对于槽、节点、数据的管理。 当master节点增加时，即扩容时，对于以上两种方案，都会出现数据迁移，那么只能作为缓存场景使用。但是redis cluster，由于每个节点维护的槽的范围是固定的，当有新加入的节点时，是不会干扰到其他节点的槽的，必须是以前的节点将使用槽的权利分配给你，并且将数据分配给你，这样，新的节点才会真正拥有这些槽和数据。这种实现还处于半自动状态，需要人工介入。-----主要的思想是：槽到集群节点的映射关系要改变，不变的是键到槽的映射关系. Redis集群，要保证16384个槽对应的node都正常工作，如果某个node发生故障，那它负责的slots也就失效，整个集群将不能工作。为了增加集群的可访问性，官方推荐的方案是将node配置成主从结构，即一个master主节点，挂n个slave从节点。这时，如果主节点失效，Redis Cluster会根据选举算法从slave节点中选择一个上升为主节点，整个集群继续对外提供服务。 4. 某个Master又怎么知道某个槽自己是不是拥有呢？ Master节点维护着一个16384/8字节的位序列，Master节点用bit来标识对于某个槽自己是否拥有。比如对于编号为1的槽，Master只要判断序列的第二位（索引从0开始）是不是为1即可。 如上面的序列，表示当前Master拥有编号为1，134的槽。集群同时还维护着槽到集群节点的映射，是由长度为16384类型为节点的数组实现的，槽编号为数组的下标，数组内容为集群节点，这样就可以很快地通过槽编号找到负责这个槽的节点。位序列这个结构很精巧，即不浪费存储空间，操作起来又很便捷。 具体参照：http://blog.jobbole.com/103258/ ,还提到了slot迁移的一些细节。 5. redis节点之间如何通信的？ gossip协议：节点之间彼此不断通信交换信息，一段时间后所有节点都会知道集群完整的信息。 节点与节点之间通过二进制协议进行通信。 客户端和集群节点之间通信和通常一样，通过文本协议进行。 集群节点不会代理查询。 6. 集群伸缩 这里6385为新加入的节点，一开始是没有槽的，所以进行slot的迁移。 集群伸缩：槽和数据在节点之间的移动。 迁移数据的流程图： 迁移key可以用pipeline进行批量的迁移。 对于扩容，原理已经很清晰了，至于具体操作，网上很多。至于缩容，也是先手动完成数据迁移，再关闭redis。 7. 客户端路由 7.1 moved重定向 其中，槽直接命中的话，就直接返回槽编号： 槽不命中，返回带提示信息的异常，客户端需要重新发送一条命令： 对于命令行的实验，用redis-cli去连接集群： redis -c -p 7000:加上-c，表示使用集群模式，帮助我们在第一次不命中的情况下自动跳转到对应的节点上： 如果不加-c的话，会返回moved异常，不会自动跳转： 7.2 ask重定向 在扩容缩容的时候，由于需要遍历这个节点上的所有的key然后进行迁移，是比较慢的，对客户端是一个挑战。因为假设一个场景，客户端访问某个key，节点告诉客户端这个key在源节点，当我们再去源节点访问的时候，却发现key已经迁移到目标节点。 7.3 moved重定向和ask重定向对比 两者都是客户端的重定向 moved：槽已经确定转移 ask:槽还在迁移中 问题：如果节点众多，那么让客户端随机访问节点，那么直接命中的概率只有百分之一，还有就是发生ask异常时（即节点正在迁移时）客户端如何还能高效运转？ 总结一句话就是redis cluster的客户端的实现会更复杂。 8. smart客户端 8.1 追求目标 追求性能，不会使用代理模式，而是直连对应节点。需要对moved异常和ask异常做兼容。也就是说，需要有一个这个语言对应的客户端来高效实现查找等操作。 8.2 smart原理 从集群中选一个可运行节点，使用cluster slots初始化槽和节点映射 将slot与node节点的结果映射到本地，为每个节点创建JedisPool 准备执行命令 第一步中将slot与node节点的对应关系放在了map中，形成一个映射关系；key是通过CRC16算法再取余得到slot，所以key与slot的映射关系也是确定的。我们就可以直接发送命令。只要后面集群没有发生数据迁移，那么就会连接成功。但是如果在连接的时候出现了连接出错，说明这个key已经迁移到其他的node上了。如果发现key不停地迁移，超过5次就报错。 在发生moved异常的时候，则需要刷新缓存，即一开始维护的map。 有一个情况比较全的图： java redis cluster客户端：jedisCluster基本使用–伪代码 jedisCluster内部已经封装好池的借还操作等。 先写一个JedisClusterFactory: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import redis.clients.jedis.HostAndPort;import redis.clients.jedis.JedisCluster;import redis.clients.jedis.JedisPoolConfig;import java.io.IOException;import java.util.HashSet;import java.util.List;import java.util.Set;public class JedisClusterFactory &#123; private JedisCluster jedisCluster; private List&lt;String&gt; hostPortList; //超时时间 private int timeout; public void init()&#123; //这里可以设置相关参数 JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); //从配置文件中读取ip:port的参数放进Set中 Set&lt;HostAndPort&gt; nodeSet = new HashSet&lt;HostAndPort&gt;(); for(String hostPort : hostPortList)&#123; String[] arr = hostPort.split(":"); if(arr.length != 2)&#123; continue; &#125; nodeSet.add(new HostAndPort(arr[0],Integer.parseInt(arr[1]))); &#125; try &#123; jedisCluster = new JedisCluster(nodeSet,timeout,jedisPoolConfig); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void destory()&#123; if(jedisCluster != null)&#123; try &#123; jedisCluster.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public JedisCluster getJedisCluster() &#123; return jedisCluster; &#125; //spring注入hostPortList和timeout public void setHostPortList(List&lt;String&gt; hostPortList) &#123; this.hostPortList = hostPortList; &#125; public void setTimeout(int timeout) &#123; this.timeout = timeout; &#125;&#125; hostPortList 放入spring bean中，spring自动完成注入。 8.3 多节点命令实现 有的时候我们想操作所有节点的数据。如何实现呢？ 8.4 批量操作 mget,mset必须在一个槽。这个条件比较苛刻，一般是不能保证的，那么如何实现批量的操作呢？ Redis Cluster的行为和Redis 的单节点不同，甚至和一个Sentinel 监控的主从模式也不一样。主要原因是集群自动分片，将一个key 映射到16384个槽中的一个，这些槽分布在多个节点上。因此操作多个key 的命令必须保证所有的key 都映射到同一个槽上，避免跨槽执行错误。更进一步说，今后一个单独的集群节点，只服务于一组专用的keys，请求一个命令到一个Server，只能得到该Server 上拥有keys 的对应结果。一个非常简单的例子是执行KEYS命令，当发布该命令到集群环境中的某个节时，只能得到该节点上拥有的keys，而不是集群中所有的keys。所以要得到集群中所有的keys，必须从集群的所有主节点上获取所有的keys。 对于分散在redis集群中不同节点的数据，我们如何比较高效地批量获取数据呢？？？？ 串行mget–原始方案，整一个for循环 串行IO 对key进行RCR16和取余操作得到slot，将slots按照节点进行分批传送： 并行IO hash_tag 不做任何改变的话，hash之后就比较均匀地散在每个节点上： 那么我们能不能像使用单机redis一样，一次IO将所有的key取出来呢？hash-tag提供了这样的功能，如果将上述的key改为如下，也就是用大括号括起来相同的内容，那么这些key就会到指定的一个节点上。 在mget的时候只需要在一台机器上去即可。 对比 方案三比较复杂，一般不用；方案四可能会出现数据倾斜，也不用。方案一在key小的时候可以用；方案二相对来说有一点优势； 为什么说是一点优势呢？pipeline批量处理不应该比串行处理好很多吗？ http://xiezefan.me/2015/12/13/redis_cluster_research_2/ http://trumandu.github.io/2016/05/09/RedisCluster构建批量操作探讨/ 9. 故障转移 9.1 故障发现 通过ping/pong消息实现故障发现：不需要sentinel 分为主观下线和客观下线 主观下线： 客观下线： pfail消息就是主观下线的信息，维护在一个链表中，链表中包含了所有其他节点对其他节点所有的主观信息，是有时间周期的，为了防止很早以前的主观下线信息还残留在这里。对这个链表进行分析，符合条件就尝试客观下线。 9.2 故障恢复 从节点接收到他的主节点客观下线的通知，则进行故障恢复的操作。 资格检查 选取出符合条件的从节点：当从节点和故障主节点的断线时间太长，会被取消资格。 准备选举时间 就是为了保证偏移量大的从节点优先被选举投票 选举投票 替换主节点 这些所有步骤加起来，差不多十几秒左右。最后如果故障节点又恢复功能了，就称为新的Master的slave节点。 10. 常见问题 10.1 集群完整性 cluster-require-full-coverage默认为yes - 要求所有节点都在服务，集群中16384个槽全部可用：保证集群完整性 - 节点故障或者正在故障转移：`(error)CLUSTERDOWN the cluster is down` 但是大多数业务都无法容忍。需要将cluster-require-full-coverage设置为no 10.2 带宽消耗 消息发送频率：节点发现与其他节点最后通信时间超过cluster-node-timeout/2时会直接发送Ping消息 消息数据量：slots槽数组(2k空间)和整个集群1、10的状态数据(10个节点状态数据约10k) 节点部署的机器规模：进去分布的机器越多且每台机器划分的节点数越均匀，则集群内整体的可用带宽越高。 优化：避免“大”集群，：避免多业务使用一个集群，大业务可用多集群；cluster-node-timeout时间设置要注意是带宽和故障转移速度的均衡；尽量均匀分配到多机器上：保证高可用和带宽。 10.3 PubSub广播 问题：publish在集群中每个节点广播：加重带宽。 解决：单独“走”一套redis sentinel。就是针对目标的几个节点构建redis sentinel，在这个里面实现广播。 10.4 数据倾斜 节点和槽分配不均匀 ./redis-trib.rb info ip:port查看节点、槽、键值分布 慎用rebalance命令 不同槽位对应键数量差异较大 CRC16正常情况下比较均匀 可能存在hash_tag cluster countKeysinslot {slot}获取槽对应键值个数 包含bigkey 例如大字符串、几百万的元素的hash、set等 在从节点上执行:redis-cli --bigkeys来查看bigkey情况 优化：优化数据结构 内存相关配置不一致 因为某种情况下，某个节点对hash或者Set这种数据结构进行了单独的优化，而其他节点都没有配置，会出现配置不一致的情况。 10.5 请求倾斜 热点key：重要的key或者bigkey 优化：避免bigkey;热键不使用hash_tag；当一致性不高时，可以用本地缓存+MQ 10.6 读写分离 只读连接：集群模式的从节点不接受任何读写请求 重定向到负责槽的主节点(对从节点进行读，都是重定向到主节点再返回信息) readonly命令可以读：连接级别命令(每次重新连接都要写一次) 上图可以看出，redis cluster 默认slave 也是不能读的，如果要读取，需要执行 readonly，就可以了。 读写分离：更加复杂（成本很高，尽量不要使用） 同样的问题：复制延迟、读取过期数据、从节点故障 修改客户端 10.7 数据迁移 分为离线迁移和在线迁移(唯品会redis-migrate-tool和豌豆荚redis-port)。 官方的方式：只能从单机迁移到集群、不支持在线迁移、不支持断点续传、单线程迁移影响速度 ./redis-trib.rb import --from 源ip:port --copy 目标ip:port 加入在迁移时再往源redis插入几条数据，这几条数据会丢失(丢失一部分) 10.8 集群vs单机 集群也有一定的限制： 分布式redis不一定是好的： 11. 简单总结]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简明理解一致性hash算法]]></title>
    <url>%2F2019%2F02%2F01%2Fmiscellany%2F15%E7%AE%80%E6%98%8E%E7%90%86%E8%A7%A3%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在分布式集群中，对机器的添加删除，或者机器故障后自动脱离集群这些操作是分布式集群管理最基本的功能。如果采用常用的hash(object)%N算法，那么在有机器添加或者删除后，很多原有的数据就无法找到了，这样严重的违反了单调性原则。接下来主要讲解一下一致性哈希算法是如何设计的。 环形Hash空间 按照常用的hash算法来将对应的key哈希到一个具有2^32 次方个桶的空间中，即0~(2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图 把数据通过一定的hash算法处理后映射到环上 现在我们将object1、object2、object3、object4四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上。如下图： 1234Hash(object1) = key1；Hash(object2) = key2；Hash(object3) = key3；Hash(object4) = key4； 将机器通过hash算法映射到环上 在采用一致性哈希算法的分布式集群中将新的机器加入，其原理是通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中。 假设现在有NODE1，NODE2，NODE3三台机器，通过Hash算法得到对应的KEY值，映射到环中，其示意图如下： 123Hash(NODE1) = KEY1;Hash(NODE2) = KEY2;Hash(NODE3) = KEY3; 通过上图可以看出对象与机器处于同一哈希空间中，这样按顺时针转动object1存储到了NODE1中，object3存储到了NODE2中，object2、object4存储到了NODE3中。在这样的部署环境中，hash环是不会变更的，因此，通过算出对象的hash值就能快速的定位到对应的机器中，这样就能找到对象真正的存储位置了。 机器的删除与添加 普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。下面来分析一下一致性哈希算法是如何处理的。 节点（机器）的删除 以上面的分布为例，如果NODE2出现故障被删除了，那么按照顺时针迁移的方法，object3将会被迁移到NODE3中，这样仅仅是object3的映射位置发生了变化，其它的对象没有任何的改动。如下图： 节点（机器）的添加 如果往集群中添加一个新的节点NODE4，通过对应的哈希算法得到KEY4，并映射到环中，如下图： 通过按顺时针迁移的规则，那么object2被迁移到了NODE4中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 平衡性 根据上面的图解分析，一致性哈希算法满足了单调性和负载均衡的特性以及一般hash算法的分散性，但这还并不能当做其被广泛应用的原由，因为还缺少了平衡性。下面将分析一致性哈希算法是如何满足平衡性的。 hash算法是不保证平衡的，如上面只部署了NODE1和NODE3的情况（NODE2被删除的图），object1存储到了NODE1中，而object2、object3、object4都存储到了NODE3中，这样就照成了非常不平衡的状态。在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。 “虚拟节点”（ virtual node ）是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。 以上面只部署了NODE1和NODE3的情况（NODE2被删除的图）为例，之前的对象在机器上的分布很不均衡，现在我们以2个副本（复制个数）为例，这样整个hash环中就存在了4个虚拟节点，最后对象映射的关系图如下： 根据上图可知对象的映射关系：object1-&gt;NODE1-1，object2-&gt;NODE1-2，object3-&gt;NODE3-2，object4-&gt;NODE3-1。通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，真正的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图： “虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值： 1Hash(“192.168.1.100”); 引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值为： 12Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2 整理自： 五分钟理解一致性哈希算法]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Sentinel实现高可用读写分离]]></title>
    <url>%2F2019%2F02%2F01%2Fredis%2FRedis-Sentinel%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第七篇文章。Redis Sentinel 是一个分布式系统，你可以在一个架构中运行多个 Sentinel 进程，这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息，并使用投票协议（agreement protocols）来决定是否执行自动故障迁移，以及选择哪个从服务器作为新的主服务器。 虽然 Redis Sentinel 是一个单独的可执行文件 redis-sentinel ，但实际上它只是一个运行在特殊模式下的 Redis 服务器，你可以在启动一个普通 Redis 服务器时通过给定 –sentinel 选项来启动 Redis Sentinel 。 启动方式一：使用sentinel可执行文件 redis-sentinel 程序来启动 Sentinel 系统，命令如下： 1redis-sentinel /path/to/sentinel.conf sentinel只是运行在特殊模式下的redis服务器，你可以用启动redis服务的命令来启动一个运行在 Sentinel 模式下的 Redis 服务器： 1redis-server /path/to/sentinel.conf --sentinel 1. redis sentinel 首先来看看什么是 redis sentinel，中文翻译是redis哨兵。顾名思义，哨兵是站岗监督突发情况的，那么这里具体的功能上很类似： 监控：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移：当一个主服务器不能正常工作时，Sentinel 会开始一次自动故障迁移操作，它会将失效主服务器的其中一个从服务器升级为新的主服务器，并让失效主服务器的其他从服务器改为复制新的主服务器；当客户端试图连接失效的主服务器时，集群也会向客户端返回新主服务器的地址，使得集群可以使用新主服务器代替失效服务器。 其中总结一下故障转移的基本原理： 多个sentinel发现并确认master有问题 选举出一个sentinel作为领导 选出一个可以成为新的master的slave 通知其他的slave称为新的master的slave 通知客户端主从变化 等待老的master复活称为新的master的slave 也支持多个master-slave结构： 2. 安装与配置 配置开启主从节点 配置开启sentinel监控主节点（sentinel是特殊的redis） 实际应该多台机器，但是演示方便，只用一台机器来搭建 详细配置节点 本地安装的结构图： 对于master:redis-7000.conf配置： 12345port 7000daemonize yespidfile /usr/local/redis/data/redis-7000.pidlogfile &quot;7000.log&quot;dir &quot;/usr/local/redis/data&quot; 对于slave:redis-7001和redis-7002配置： 123456port 7001daemonize yespidfile /usr/local/redis/data/redis-7001.pidlogfile &quot;7001.log&quot;dir &quot;/usr/local/redis/data&quot;slaveof 127.0.0.1 7000 启动redis服务： 1redis-server ../config/redis-7000.conf 访问7000端口的master redis: 1redis-cli -p 7000 info replication 显示他有两个从节点： 12345678910# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=7002,state=online,offset=99550,lag=1slave1:ip=127.0.0.1,port=7001,state=online,offset=99816,lag=0master_repl_offset:99816repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:99815 对于sentinel主要配置： master sentinel config: 123456port 26379daemonize yesdir &quot;/usr/local/redis/data&quot;logfile &quot;26379.log&quot;sentinel monitor mymaster 127.0.0.1 7000 2... 启动redis sentinel: 1redis-sentinel ../config/redis-sentinel-26379.conf 访问26379 redis sentinel master: 1redis-cli -p 26379 info sentinel 显示： 123456# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=127.0.0.1:7000,slaves=2,sentinels=3 1查看这六个进程是否都起来了：ps -ef | grep redis 注意，如果上面是配置在虚拟机的话，需要将127.0.0.1改为虚拟机的ip，要不然找不着。 3. 故障转移演练 3.1 java客户端程序 JedisSentinelPool只是一个配置中心，不需要具体连接某个redis，注意它不是代理。 1234567891011121314151617181920212223242526272829303132333435private Logger logger = LoggerFactory.getLogger(AppTest.class);@Testpublic void test4()&#123; //哨兵配置，我们访问redis，就通过sentinel来访问 String masername = "mymaster"; Set&lt;String&gt; sentinels = new HashSet&lt;&gt;(); sentinels.add("10.128.24.176:26379"); sentinels.add("10.128.24.176:26380"); sentinels.add("10.128.24.176:26381"); JedisSentinelPool sentinelPool = new JedisSentinelPool(masername,sentinels); //一个while死循环，每隔一秒往master塞入一个值，并且日志打印 while (true)&#123; Jedis jedis = null; try&#123; jedis = sentinelPool.getResource(); int index = new Random().nextInt(100000); String key = "k-" + index; String value = "v-" + index; jedis.set(key,value); logger.info("&#123;&#125; value is &#123;&#125;",key,jedis.get(key)); TimeUnit.MILLISECONDS.sleep(1000); &#125;catch (Exception e)&#123; logger.error(e.getMessage(),e); &#125;finally &#123; if(jedis != null)&#123; jedis.close(); &#125; &#125; &#125;&#125; maven依赖是： 123456789101112131415161718&lt;!--jedis--&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--slf4j日志接口--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt;&lt;/dependency&gt;&lt;!--logback日志实现--&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; 启动程序，发现是正常写入： 123456789101112131416:16:01.424 [main] INFO com.njupt.swg.AppTest - k-54795 value is v-5479516:16:02.426 [main] INFO com.njupt.swg.AppTest - k-55630 value is v-5563016:16:03.429 [main] INFO com.njupt.swg.AppTest - k-70642 value is v-7064216:16:04.430 [main] INFO com.njupt.swg.AppTest - k-42978 value is v-4297816:16:05.431 [main] INFO com.njupt.swg.AppTest - k-96297 value is v-9629716:16:06.433 [main] INFO com.njupt.swg.AppTest - k-4220 value is v-422016:16:07.435 [main] INFO com.njupt.swg.AppTest - k-34103 value is v-3410316:16:08.436 [main] INFO com.njupt.swg.AppTest - k-9177 value is v-917716:16:09.437 [main] INFO com.njupt.swg.AppTest - k-24389 value is v-2438916:16:10.439 [main] INFO com.njupt.swg.AppTest - k-32325 value is v-3232516:16:11.440 [main] INFO com.njupt.swg.AppTest - k-68538 value is v-6853816:16:12.441 [main] INFO com.njupt.swg.AppTest - k-36233 value is v-3623316:16:13.443 [main] INFO com.njupt.swg.AppTest - k-305 value is v-30516:16:14.444 [main] INFO com.njupt.swg.AppTest - k-59279 value is v-59279 我们将现在的端口为7000的redis master 给kill掉 kill -9 master的pid 我们会发现：客户端报异常，但是在大概十几秒之后，就继续正常塞值了。原因是服务端的哨兵机制的选举matser需要一定的时间。 4. 三个定时任务 4.1 每10秒每个sentinel对master和slave执行Info 发现slave节点 确认主从关系 4.2 每2秒每个sentinel通过master节点的channel交换信息(pub/sub) 通过__sentinel__:hello进行频道交互 交互对节点的“看法”和自身信息 4.3 每1秒每个sentinel对其他sentinel和redis执行ping 心跳监测，失败判定依据 5. 主观下线和客观下线 对于之前的Sentinel配置文件中有两条配置： 监控master redis节点，这里是当超过两个sentinel认为master挂了，则认为master挂了。 sentinel monitor &lt;masterName&gt; &lt;masterIp&gt; &lt;msterPort&gt; &lt;quorum&gt; sentinel monitor mymaster 127.0.0.1 6379 2 这里是每秒sentinel都会去Ping周围的master redis，超过30秒没有任何响应，说明其挂了。 sentinel down-after-milliseconds &lt;masterName&gt; &lt;timeout&gt; sentinel down-after-milliseconds mymaster 300000 5.1 主观下线 主观下线：每个sentinel节点对Redis节点失败的“偏见” 这是一种主观下线。因为在复杂的网络环境下，这个sentinel与这个master不通，但是master与其他的sentinel都是通的呢？所以是一种“偏见” 这是依靠的第三种定时：每秒去ping一下周围的sentinel和redis。对于slave redis,可以使用这个主观下线，因为他不需要进行故障转移。 5.2 客观下线 客观下线：所有sentinel节点对master Redis节点失败“达成共识”（超过quorum个则统一） 这是依靠的第二种定时：每两秒，sentinel之间进行“商量”，传递的消息是:sentinel is-master-down-by-addr 对于master redis的下线，必须要达成共识才可以，因为涉及故障转移，仅仅依靠一个sentinel判断是不够的。 6. 领导者选举 原因：只有一个sentinel节点完成故障转移 选举：通过sentinel is-master-down-by-addr命令都希望成为领导者 每个做主观下线的sentinel节点向其他sentinel节点发送命令，要求将它设置为领导者 收到命令的sentinel节点如果还没有同意过其他semtinel节点发送的命令，那么将同意该请求，否则拒绝 如果该sentinel节点发现自己的票数已经超过sentinel集合半数并且超过quorum，那么它将成为领导者。 如果此过程中多个sentinel节点成为了领导者，那么将等待一段时间重新进行选举 7. 故障转移 从slave节点中选出一个“合适的”节点作为新的master节点 对上述的slave节点执行“slaveof no one”命令使其成为master节点 向剩余的slave节点发送命令，让它们成为新master节点的slave节点，复制规则和parallel-syncs参数一样 更新对原来的master节点配置为slave，并保持着对其“关注”，当恢复后命令他去复制新的master节点 那么，如何选择“合适”的slave节点呢？ 选择slave-priority(slave节点优先级)最高的slave节点，如果存在则返回，不存在则继续。 选择复制偏移量最大的slave节点(复制得最完整)，如果存在则返回，不存在则继续 选择run_id最小的slave节点(最早的节点) 8. 节点下线 主节点下线：sentinel failover &lt;masterName&gt; 从节点下线要注意读写分离问题。 9. 总结与思考 redis sentinel是redis高可用实现方案：故障发现、故障自动转移、配置中心、客户端通知。 redis sentinel从redis2.8版本才正式生产可用，之前版本不可生产用。 尽可能在不同物理机上部署redis sentinel所有节点。 redis sentinel中的sentinel节点个数应该大于等于3且最好是奇数。 redis sentinel中的数据节点和普通数据节点没有区别。每个sentinel节点在本质上还是一个redis实例，只不过和redis数据节点不同的是，其主要作用是监控redis数据节点 客户端初始化时连接的是sentinel节点集合，不再是具体的redis节点，但sentinel只是配置中心不是代理。 redis sentinel通过三个定时任务实现了sentinel节点对于主节点、从节点、其余sentinel节点的监控。 redis sentinel在对节点做失败判定时分为主观下线和客观下线。 看懂redis sentinel故障转移日志对于redis sentinel以及问题排查非常有用。 redis sentinel实现读写分离高可用可以依赖sentinel节点的消息通知，获取redis数据节点的状态变化。 redis sentinel可以实现高可用的读写分离，高可用体现在故障转移，那么实现高可用的基础就是要有从节点，主从节点还实现了读写分离，减少master的压力。但是如果是从节点下线了，sentinel是不会对其进行故障转移的，并且连接从节点的客户端也无法获取到新的可用从节点，而这些问题在Cluster中都得到了有效的解决。 对于性能提高、容量扩展的时候，这种方式是比较复杂的，比较推荐的是使用集群，就是下面讨论的redis cluster!]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数值计算精度丢失问题]]></title>
    <url>%2F2019%2F01%2F31%2Fmiscellany%2F14%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%B2%BE%E5%BA%A6%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[无论在什么业务中，钱是非常重要的东西，对账的时候一定要对的上，不能这边少一分那边多一分。对于数值的计算，尤其是小数，double和double都是禁止使用的。 阿里强制要求存放小数时使用 decimal，禁止使用 float 和 double。 说明：float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。 处理方式可以为：mysql 可以用 decimal ，如果你是用 java， 在商业计算中我们要用 java.math.BigDecimal，注意：如果需要精确计算，非要用String来够造BigDecimal不可！ 那么到底是什么情况？ 一个例子说明 废话不多说，上图： 问题原因 无论是我们本文提到的double，还是float，都是浮点数。 在计算机科学中，浮点（英语：floating point，缩写为FP）是一种对于实数的近似值数值表现法，由一个有效数字（即尾数）加上幂数来表示，通常是乘以某个基数的整数次指数得到。以这种表示法表示的数值，称为浮点数（floating-point number）。 其实我觉得很好理解，我们之前说过，计算机计算加减乘除啊，都是用的加法器，实质都是二进制的加法处理。那么这里就有一个二进制表示的问题。试想，4，2，8之流都是2的幂次方，可以完美用二进制表示，计算当然不会出现问题。对于0，1，3，5之类也都可以用二进制来表示出来，所以，正数肯定是没问题的。 但是对于小数呢？1、0.5、0.25那都是可以转换成二进制的小数，如十进制的0.1，就无法用二进制准确的表示出来。因此只能使用近似值的方式表达。 如果我们尝试着把10进制的0.1转化成二进制，会怎么转呢？ 在十进制中，0.1如何计算出来的呢？ 0.1 = 1 ÷ 10 那么二进制中也是同理： 1 ÷ 1010 我们回到小学的课堂，来列竖式吧： 123456789101112131415 0.000110011... ------------------1010 ) 1 0000 1010 ------ 1100 1010 ---- 10000 1010 ----- 1100 1010 ---- 10 很显然，除不尽，除出了一个无限循环小数：二进制的 0.0001100110011… 那么，如何在计算机中表示这个无限不循环的小数呢？只能考虑按照不同的精度保理不同的位数。 我们知道float是单精度的，double是双精度的。不同的精度，其实就是保留的有效数字位数不同，保留的位数越多，精度越高。 所以，浮点数在Java中是无法精确表示的，因为大部分浮点数转换成二进制是一个无限不循环的小数，只能通过保留精度的方式进行近似表示。 问题的解决 String 构造方法是完全可预知的：写入 newBigDecimal(&quot;0.1&quot;) 将创建一个 BigDecimal，它正好等于预期的 0.1。因此，比较而言，通常建议优先使用String构造方法。 使用BigDecimal(String val)！ 123456789101112131415161718192021222324252627//加法public static BigDecimal add(double v1, double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.add(b2);&#125;//减法public static BigDecimal sub(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.subtract(b2);&#125;//乘法public static BigDecimal mul(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.multiply(b2);&#125;//除法public static BigDecimal div(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.divide(b2,2,BigDecimal.ROUND_HALF_UP);//四舍五入,保留2位小数,应对除不尽的情况&#125; 那么，上面的精度丢失问题就迎刃而解了。但是除不尽怎么办？比如10.0除以这里的3.0，保留小数点后三位有效数字： 那么，每个用户得到的都是3.333元，三个用户加起来是得不到10块钱的。 对于除法，始终会产生除不尽的情况怎么办？有个词叫轧差 什么意思呢？举个简单例子。假如现在需要把10元分成3分，如果是10除以3这么除，会发现为3.33333无穷尽的3。这些数字完全无法在程序或数据库中进行精确的存储。 简单理解就是，当除不尽或需去除小数点的时候，前面的n-1笔（这里n=3）做四舍五入。最后一笔做兜底（总金额减去前面n-1笔之和）。这样保证总金额的不会丢失。 比如10块钱，三个用户分，前面两个用户只能各分到3。333块钱，最后一个用户分到3.334块钱。保证总额不变。 至于原理，有一点点数学化，以后再作探讨吧。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从复制]]></title>
    <url>%2F2019%2F01%2F31%2Fredis%2FRedis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第六篇文章。介绍redis主从复制功能实现原理。作为高可用的基础，了解一下其中的门道是有必要的。 1.单机有什么问题 机器故障 容量瓶颈 QPS瓶颈 2. 主从复制的作用 数据副本 扩展读性能，slave专门用来读 一个master可以有多个slave，一个salve只能有一个master 3. 两种实现方式 方式一：slaveof命令 slaveof masterIp masterPort slaveof no one(不会清除原来同步的数据，而是新的数据不会再同步给他) 方式二：配置 修改某一行的配置：slaveof ip port 从节点只做读操作：slave-read-only yes 对比 命令的优点：不需要重启 命令的缺点：不便于管理 配置的优点：统一配置 配置的缺点：需要重启 一个场景，假如6380是6379的一个从节点，然后将6380执行salveof no one，然后插入一些新的数据；再重新变成6379的从节点，那么里面的新数据会被清除掉。 查看run_id redis-cli -p 6379 info server | grep run 4. 全量复制 全量复制开销 bgsave时间 rdb网络传输时间 从节点清空数据的时间 从节点加载RDB的时间 可能的AOF重写时间 存在的问题 时间开销比较大 如果master和slave之间网络扰动甚至断开，那么master此间更新的数据对于slave是不知道的，最简单的方法就是再进行一次全量复制，但是显然，消耗太大了。 5. 部分复制 6. 开发与运维的问题 读写分离 master只做写操作，slave来做读操作，来分摊流量。但是会有一些问题： 复制数据延迟 读到过期数据 从节点故障 主从配置不一致 例如maxmemory不一致：丢失数据 数据结构优化参数：内存不一致 规避全量复制 第一次全量复制：不可避免—小主节点(maxmemroy不要太大)或者在低峰时进行操作 节点run_id不匹配（主节点重启，那么master的run_id会发生变化，slave发现其run_id变化，会进行全量复制）；我们可以用故障转移，例如哨兵或集群来避免全量复制。 复制积压缓冲区不足(网络中断，部分复制无法满足)，可以增大复制缓冲区配置size，网络增强 规避复制风暴 概念：主节点宕机造成大量的全量复制 单主节点复制风暴：主节点重启，多从节点复制；解决：更换复制拓扑 单机器复制风暴：机器宕机后（该机器全是Mater），大量全量复制。解决：master分散多机器。 说到底，还是需要有一种高可用的实现方式，在master出现故障之后，如何自动实现从slave晋升为master继续使用.而不是一直死守着原来老的master不放，因为老的master啥时候恢复不知道，恢复了可能会造成复制风暴，既然从节点本来是一直与master节点保持尽量的同步的，那么为什么不将数据最新的从节点升级为主节点呢？下一章继续来分析。]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2F2019%2F01%2F31%2Fredis%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第五篇文章。redis处理数据都是在内存中进行，所以速度特别快，同样，它也可以支持持久化，这里注意，并不是说redis要来充当mysql那种角色，其实更多的是为了在崩溃的时候快速恢复以及主从复制这样的功能。redis的持久化主要有两种方式，一种是RDB，一种是AOF，对于他们的原理和区别都是比较重要的面试考察点，需要掌握。 1. 什么是持久化 redis所有数据保持在内存中，对数据的更新将异步地保存到磁盘中。 2. 持久化的方式 快照—mysql dump或者redis rdb 写日志—mysql binlog或者hbase glog或者redis aof 3. RDB 什么是RDB 触发机制三种主要方式 save(同步持久化，会造成redis主线程的阻塞，不推荐使用) save是同步的，当保存的数据量很大时，可能造成redis的阻塞，即客户端访问redis被阻塞。 他的文件策略是：如果存在老的RDB文件，则新的替换老的。复杂度为O(n)。 bgsave(异步，fork一个子进程来进行持久化，不会造成主线程的阻塞) 一般情况下，fork是比较快的，但是也可以会慢，这时会阻塞redis。只要fork不慢，客户端不会被阻塞。 他的文件策略和复杂度与save是一样的。 save和bgsave两者对比： 自动 redis的自动保存的默认配置是： 配置 seconds changes save 900 1 save 300 10 save 60 10000 就是说，在60秒内改变了10000条数据，就自动保存；在300秒内有10条改变才自动保存；900秒内有1一条改变就保存。 RDB总结 RDB是Redis内存到硬盘的快照，用于持久化。 save通常会阻塞redis。 bgsave不会阻塞redis，但是会fork新进程。 save自动配置满足任一就会被执行。 有些触发机制不容忽视。 4. AOF RDB问题 全量数据存入磁盘 O(n)数据的备份，很耗时间；对于bgsave来说，fork()是一个很消耗内存的操作；将数据全写到硬盘，必然对硬盘IO占用很大。 宕机丢失数据多 还有一点是：某个时间点宕机，那么在某个时间段的数据就丢失了。 AOF原理 将对redis的操作追加到aof文件中。当redis宕机之后，使用aof恢复所有的操作继而实现数据的恢复。 AOF三种策略 always everysec redis出现故障，有可能丢失一秒的数据。redis默认方式。 no 三种策略的比较 AOF重写 好处是：减少硬盘占用、减少数据丢失 下面是AOF的bgrewirteaof的过程： 注意：这里的重写并不是上面演示的，将原来的aof文件进行重写，而是根据redis现在的内存数据进行一次回溯。 aof重写流程 也就是说，子进程在执行 AOF 重写时，主进程需要执行以下三个工作： 1.处理命令请求； 2.将写命令追加到现有的 AOF 文件中； 3.将写命令追加到 AOF 重写缓存中。 如此可以保证： 现有的AOF功能继续执行，即使 AOF 重写期间发生停机，也不会有任何数据丢失； 所有对数据库进行修改的命令都会被记录到 AOF 重写缓存中。 当子进程完成对 AOF 文件重写之后，它会向父进程发送一个完成信号，父进程接到该完成信号之后，会调用一个信号处理函数，该函数完成以下工作：(阻塞) 将 AOF 重写缓存中的内容全部写入到新的 AOF 文件中；(现有 AOF 文件、新的 AOF 文件和数据库三者的状态就完全一致了) 对新的 AOF 文件进行改名，覆盖原有的 AOF 文件。(执行完毕后，程序就完成了新旧两个 AOF 文件的替换) 当这个信号处理函数执行完毕之后，主进程就可以继续像往常一样接收命令请求了。在整个 AOF 后台重写过程中，只有最后的“主进程写入命令到AOF缓存”和“对新的 AOF 文件进行改名，覆盖原有的 AOF 文件”这两个步骤会造成主进程阻塞，在其他时候， AOF 后台重写都不会对主进程造成阻塞，这将 AOF 重写对性能造成的影响降到最低。 小结： AOF 重写的目的是轻量地保存数据库状态，整个重写过程基本上不影响 Redis 主进程处理命令请求； AOF在redis宕机的时候最多丢失一秒的数据，比RDB要好一点，并且可读性高，基本上能看得懂 AOF 重写其实是一个有歧义的名字，实际上重写工作是针对数据库的当前值来进行的，重写过程中不会读写、也不适用原来的 AOF 文件； AOF 可以由用户手动触发，也可以由服务器自动触发。 5. 持久化的取舍和选择 RDB和AOF对比 可以看出，世界上没有完美的东西，只有合适的东西。AOF同样存在一些问题：AOF文件的体积通常要大于RDB文件的体积、且恢复速度慢。 RDB最佳策略 “关”：建议关闭，但是后面主从复制功能是需要他的，因为需要主节点执行dbsave，然后将rdb文件传给从节点。所以说，关不是永久关。 “集中管理”：虽然RDB很重，但是对于数据备份是很重要的，按照小时或者天集中地进行备份比较好，因为他的文件很小，利于传输。 “主从，从开”：有时候从节点打开这个功能是比较好的，但是备份太频繁，取决于实际的场景。 AOF最佳策略 “开”：建议打开，如果仅仅是作为一个普通缓存，对于数据要求不是很高，这次数据丢了，下次可以从数据库取(数据库压力不是很大)，这种情况就建议关闭，因为AOF还是有性能开销的。 “everysec” Redis4 Redis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。 RDB和AOF共存的情况下如何恢复数据： 优点： 混合持久化结合了RDB持久化 和 AOF 持久化的优点, 由于绝大部分都是RDB格式，加载速度快，同时结合AOF，增量的数据以AOF方式保存了，数据更少的丢失。 缺点： 兼容性差，一旦开启了混合持久化，在4.0之前版本都不识别该aof文件，同时由于前部分是RDB格式，阅读性较差 策略是： 6. 总结 http://www.ywnds.com/?p=4876]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入Nginx原理]]></title>
    <url>%2F2019%2F01%2F30%2Fmiscellany%2F13%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Nginx%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Nginx是一个高性能的HTTP和反向代理服务器，及电子邮件（IMAP/POP3）代理服务器，同时也是一个非常高效的反向代理、负载平衡中间件。是非常常用的web server.我们需要理解它的原理，才能达到游刃有余的程度。 本篇文章需要对Nginx有基本的使用以及对IO复用模型有一定的了解。文章比较长。 1.正向代理和反向代理 正向代理的工作原理就像一个跳板，比如：我访问不了google.com，但是我能访问一个代理服务器A，A能访问google.com，于是我先连上代理服务器A，告诉他我需要google.com的内容，A就去取回来，然后返回给我。从网站的角度，只在代理服务器来取内容的时候有一次记录，有时候并不知道是用户的请求，也隐藏了用户的资料，这取决于代理告不告诉网站。 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 简单来说： 正向代理是不知道客户端是谁，代理是一个跳板，所有客户端通过这个跳板来访问到对应的内容。 反向代理是不知道服务端是谁，用户的请求被转发到内部的某台服务器去处理。 2.基本的工作流程 用户通过域名发出访问Web服务器的请求，该域名被DNS服务器解析为反向代理服务器的IP地址； 反向代理服务器接受用户的请求； 反向代理服务器在本地缓存中查找请求的内容，找到后直接把内容发送给用户； 如果本地缓存里没有用户所请求的信息内容，反向代理服务器会代替用户向源服务器请求同样的信息内容，并把信息内容发给用户，如果信息内容是缓存的还会把它保存到缓存中。 3.优点 保护了真实的web服务器，保证了web服务器的资源安全 节约了有限的IP地址资源 减少WEB服务器压力，提高响应速度(缓存功能) 请求的统一控制，包括设置权限、过滤规则等 实现负载均衡 区分动态和静态可缓存内容 … 4.使用场景 Nginx作为Http代理、反向代理 Nginx作为负载均衡器 Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 Nginx作为Web缓存 5.Nginx的Master-Worker模式 启动Nginx后，其实就是在80端口启动了Socket服务进行监听，如图所示，Nginx涉及Master进程和Worker进程。 6.Master进程的作用是？ 读取并验证配置文件nginx.conf；管理worker进程； 接收来自外界的信号 向各worker进程发送信号 监控worker进程的运行状态，当worker进程退出后(异常情况下)，会自动重新启动新的worker进程 7.Worker进程的作用是？ 每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 思考：Nginx如何做到热部署？ 所谓热部署，就是配置文件nginx.conf修改后，不需要stop Nginx，不需要中断请求，就能让配置文件生效！（nginx -s reload 重新加载/nginx -t检查配置/nginx -s stop） 通过上文我们已经知道worker进程负责处理具体的请求，那么如果想达到热部署的效果，可以想象： 方案一： 修改配置文件nginx.conf后，主进程master负责推送给woker进程更新配置信息，woker进程收到信息后，更新进程内部的线程信息。（有点volatile的味道） 方案二： 修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx采用的就是方案二来达到热部署的！ 思考：Nginx如何做到高并发下的高效处理？ 上文已经提及Nginx的worker进程个数与CPU绑定、worker进程内部包含一个线程高效回环处理请求，这的确有助于效率，但这是不够的。 作为专业的程序员，我们可以开一下脑洞：BIO/NIO/AIO、异步/同步、阻塞/非阻塞… 要同时处理那么多的请求，要知道，有的请求需要发生IO，可能需要很长时间，如果等着它，就会拖慢worker的处理速度。 Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。 思考：Nginx挂了怎么办？ Nginx既然作为入口网关，很重要，如果出现单点问题，显然是不可接受的。 答案是：Keepalived+Nginx实现高可用。 Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用。（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合） Keepalived+Nginx实现高可用的思路： 第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP） 第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,从而实现Nginx故障切换） 6.nginx.conf 第一：location可以进行正则匹配，应该注意正则的几种形式以及优先级。（这里不展开） 第二：Nginx能够提高速度的其中一个特性就是：动静分离，就是把静态资源放到Nginx上，由Nginx管理，动态请求转发给后端。 第三：我们可以在Nginx下把静态资源、日志文件归属到不同域名下（也即是目录），这样方便管理维护。 第四：Nginx可以进行IP访问控制，有些电商平台，就可以在Nginx这一层，做一下处理，内置一个黑名单模块，那么就不必等请求通过Nginx达到后端在进行拦截，而是直接在Nginx这一层就处理掉。 除了可以映射静态资源，上面已经说了，可以作为一个代理服务器来使用。 所谓反向代理，很简单，其实就是在location这一段配置中的root替换成proxy_pass即可。root说明是静态资源，可以由Nginx进行返回；而proxy_pass说明是动态请求，需要进行转发，比如代理到Tomcat上。 反向代理，上面已经说了，过程是透明的，比如说request -&gt; Nginx -&gt; Tomcat，那么对于Tomcat而言，请求的IP地址就是Nginx的地址，而非真实的request地址，这一点需要注意。不过好在Nginx不仅仅可以反向代理请求，还可以由用户自定义设置HTTP HEADER。 负载均衡【upstream】 上面的反向代理中，我们通过proxy_pass来指定Tomcat的地址，很显然我们只能指定一台Tomcat地址，那么我们如果想指定多台来达到负载均衡呢？ 第一，通过upstream来定义一组Tomcat，并指定负载策略（IPHASH、加权论调、最少连接），健康检查策略（Nginx可以监控这一组Tomcat的状态）等。 第二，将proxy_pass替换成upstream指定的值即可。 负载均衡可能带来的问题？ 负载均衡所带来的明显的问题是，一个请求，可以到A server，也可以到B server，这完全不受我们的控制，当然这也不是什么问题，只是我们得注意的是：用户状态的保存问题，如Session会话信息，不能在保存到服务器上。 7.惊群现象 定义：惊群效应就是当一个fd的事件被触发时，所有等待这个fd的线程或进程都被唤醒。 Nginx的IO通常使用epoll，epoll函数使用了I/O复用模型。与I/O阻塞模型比较，I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。Nginx的epoll工作流程如下： master进程先建好需要listen的socket后，然后再fork出多个woker进程，这样每个work进程都可以去accept这个socket 当一个client连接到来时，所有accept的work进程都会受到通知，但只有一个进程可以accept成功，其它的则会accept失败，Nginx提供了一把共享锁accept_mutex来保证同一时刻只有一个work进程在accept连接，从而解决惊群问题 当一个worker进程accept这个连接后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完成的请求就结束了 8.Nginx架构及工作流程 Nginx真正处理请求业务的是Worker之下的线程。worker进程中有一个ngx_worker_process_cycle()函数，执行无限循环，不断处理收到的来自客户端的请求，并进行处理，直到整个Nginx服务被停止。 当一个 worker 进程在 accept() 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，一个完整的请求。一个请求，完全由 worker 进程来处理，而且只能在一个 worker 进程中处理。 这样做带来的好处： 节省锁带来的开销。每个 worker 进程都是独立的进程，不共享资源，不需要加锁。同时在编程以及问题查上时，也会方便很多。 独立进程，减少风险。采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快重新启动新的 worker 进程。当然，worker 进程的也能发生意外退出。 9.nginx为什么高性能 因为nginx是多进程单线程的代表，多进程模型每个进程/线程只能处理一路IO，那么 Nginx是如何处理多路IO呢？ 如果不使用 IO 多路复用，那么在一个进程中，同时只能处理一个请求，比如执行 accept()，如果没有连接过来，那么程序会阻塞在这里，直到有一个连接过来，才能继续向下执行。 而多路复用，允许我们只在事件发生时才将控制返回给程序，而其他时候内核都挂起进程，随时待命。 核心：Nginx采用的 IO多路复用模型epoll epoll通过在Linux内核中申请一个简易的文件系统（文件系统一般用什么数据结构实现？B+树），其工作流程分为三部分： 调用 int epoll_create(int size)建立一个epoll对象，内核会创建一个eventpoll结构体，用于存放通过epoll_ctl()向epoll对象中添加进来的事件，这些事件都会挂载在红黑树中。 调用 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) 在 epoll 对象中为 fd 注册事件，所有添加到epoll中的事件都会与设备驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个sockfd的回调方法，将sockfd添加到eventpoll 中的双链表 调用 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) 来等待事件的发生，timeout 为 -1 时，该调用会阻塞直到有事件发生 这样，注册好事件之后，只要有 fd 上事件发生，epoll_wait() 就能检测到并返回给用户，用户就能”非阻塞“地进行 I/O 了。 epoll() 中内核则维护一个链表，epoll_wait 直接检查链表是不是空就知道是否有文件描述符准备好了。（epoll 与 select 相比最大的优点是不会随着 sockfd 数目增长而降低效率，使用 select() 时，内核采用轮训的方法来查看是否有fd 准备好，其中的保存 sockfd 的是类似数组的数据结构 fd_set，key 为 fd，value 为 0 或者 1。） 能达到这种效果，是因为在内核实现中 epoll 是根据每个 sockfd 上面的与设备驱动程序建立起来的回调函数实现的。那么，某个 sockfd 上的事件发生时，与它对应的回调函数就会被调用，来把这个 sockfd 加入链表，其他处于“空闲的”状态的则不会。在这点上，epoll 实现了一个”伪”AIO。但是如果绝大部分的 I/O 都是“活跃的”，每个 socket 使用率很高的话，epoll效率不一定比 select 高（可能是要维护队列复杂）。 可以看出，因为一个进程里只有一个线程，所以一个进程同时只能做一件事，但是可以通过不断地切换来“同时”处理多个请求。 例子：Nginx 会注册一个事件：“如果来自一个新客户端的连接请求到来了，再通知我”，此后只有连接请求到来，服务器才会执行 accept() 来接收请求。又比如向上游服务器（比如 PHP-FPM）转发请求，并等待请求返回时，这个处理的 worker 不会在这阻塞，它会在发送完请求后，注册一个事件：“如果缓冲区接收到数据了，告诉我一声，我再将它读进来”，于是进程就空闲下来等待事件发生。 这样，基于 多进程+epoll， Nginx 便能实现高并发。 10.几种负载均衡的算法介绍 轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 weight 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 ip_hash 每个请求按访问ip的hash结果分配，这样每个访客固定访问同一个后端服务器，可以解决session的问题。但是不能解决宕机问题。 前三种是nginx自带的，直接在配置文件中配置即可使用。 fair（第三方） 按后端服务器的相应时间来分配请求，相应时间短的优先分配。 url_hash（第三方） 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 11.基于不同层次的负载均衡 七层就是基于URL等应用层信息的负载均衡； 同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。 换句话说: 二层负载均衡会通过一个虚拟MAC地址接受请求，然后再分配到真是的MAC地址； 三层负载均衡会通过一个虚拟IP地址接收请求，然后再分配到真实的IP地址； 四层通过虚拟的URL或主机名接收请求，然后再分配到真是的服务器。 所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。 七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。 负载均衡器通常称为四层交换机或七层交换机。四层交换机主要分析IP层及TCP/UDP层，实现四层流量负载均衡。七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息。 负载均衡设备也常被称为&quot;四到七层交换机&quot;，那么四层和七层两者到底区别在哪里？ 第一，技术原理上的区别。 所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 第二，应用场景的需求。 七层应用负载的好处，是使得整个网络更&quot;智能化&quot;。例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。 另外一个常常被提到功能就是安全性。 12.总结 理解正向代理和反向代理的概念 nginx的优点和使用场景 master和work两种进程的作用 如何热部署 Nginx单点故障的预防 映射静态文件、反向代理跳转到后端服务器处理的写法 惊群现象 Nginx 采用的是多进程（单线程） &amp; 多路IO复用模型(底层依靠epoll实现) 几种负载均衡的算法 四层的负载均衡和七层的负载均衡]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis为什么快]]></title>
    <url>%2F2019%2F01%2F30%2Fredis%2FRedis%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第四篇文章，本文主要攻克面试题-Redis为什么这么快。这就涉及Redis的线程模型啦。 完全基于内存 Redis是纯内存数据库，相对于读写磁盘，读写内存的速度就不是几倍几十倍了，一般，hash查找可以达到每秒百万次的数量级。 多路复用IO “多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗）。 Redis为什么是单线程的？ 因为CPU不是Redis的瓶颈。Redis的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 为什么 Redis 中要使用 I/O 多路复用这种技术呢？ 首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的。 假设你是一个老师，让30个学生解答一道题目，然后检查学生做的是否正确，你有下面几个选择： 第一种选择：按顺序逐个检查，先检查A，然后是B，之后是C、D。。。这中间如果有一个学生卡主，全班都会被耽误。这种模式就好比，你用循环挨个处理socket，根本不具有并发能力。 第二种选择：你创建30个分身，每个分身检查一个学生的答案是否正确。 这种类似于为每一个用户创建一个进程或者线程处理连接。 第三种选择，你站在讲台上等，谁解答完谁举手。这时C、D举手，表示他们解答问题完毕，你下去依次检查C、D的答案，然后继续回到讲台上等。此时E、A又举手，然后去处理E和A。。。 第三种就是IO复用模型，Linux下的select、poll和epoll就是干这个的。将用户socket对应的fd注册进epoll，然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用非阻塞模式。这样，整个过程只在调用select、poll、epoll这些调用的时候才会阻塞，收发客户消息是不会阻塞的，整个进程或者线程就被充分利用起来，这就是事件驱动，所谓的reactor模式。 所以，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。 这里还涉及一个名词：fd文件描述符。 Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行I/O操作的系统调用都会通过文件描述符。 redis的线程模型？ Redis 服务采用 Reactor 的方式来实现文件事件处理器。 文件事件处理器使用 I/O 多路复用模块同时监听多个 FD，当 accept、read、write 和 close 文件事件产生时，文件事件处理器就会回调 FD 绑定的事件处理器。 虽然整个文件事件处理器是在单线程上运行的，但是通过 I/O 多路复用模块的引入，实现了同时对多个 FD 读写的监控，提高了网络通信模型的性能，同时也可以保证整个 Redis 服务实现的简单。 上面简单理解就是：多个网络连接并发读写redis的时候，先将对应的fd注册到epoll上，I/O多路复用模块会监听这些网络请求的情况，一旦有一个网络连接产生了accept、read、write 和 close 文件事件，I/O多路复用模块就会向文件事件分派器传送那些产生了事件的网络连接。 当然了，上面的文件事件可能会并发产生，这时的策略是，将所有产生事件的套接字（对应上面的网络连接）都入队到一个队列里面， 然后通过这个队列， 以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字： 当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字。再看看下图，与上图使一样的： 文件事件分派器接收 I/O 多路复用程序传来的套接字， 并根据套接字产生的事件的类型， 调用相应的事件处理器。 服务器会为执行不同任务的套接字关联不同的事件处理器， 这些处理器是一个个函数， 它们定义了某个事件发生时， 服务器应该执行的动作。 整个模块使 Redis 能以单进程运行的同时服务成千上万个文件描述符，避免了由于多进程应用的引入导致代码实现复杂度的提升，减少了出错的可能性，单线程还减少线程切换和调度，实现更加简单 最后总结一下，为什么redis比较快大概思路通俗的说就是：Redis是纯内存数据库，读取快，瓶颈在于IO上，如果使用阻塞式IO，因为是单线程的缘故，就会停止等待。所以采用IO多路复用监听文件描述符的状态，将对redis的开关读写换成事件，加入队列进行相应的事件处理，吞吐量比较大。 IO复用模型的选择 因为 Redis 需要在多个平台上运行，同时为了最大化执行的效率与性能，所以会根据编译平台的不同选择不同的 I/O 多路复用函数作为子模块，提供给上层统一的接口； 因为 select 函数是作为 POSIX 标准中的系统调用，在不同版本的操作系统上都会实现，所以将其作为保底方案： Redis 会优先选择时间复杂度为 O(1) 的 I/O 多路复用函数作为底层实现，包括 Solaries 10 中的 evport、Linux 中的 epoll 和 macOS/FreeBSD 中的 kqueue，上述的这些函数都使用了内核内部的结构，并且能够服务几十万的文件描述符。 但是如果当前编译环境没有上述函数，就会选择 select 作为备选方案，由于其在使用时会扫描全部监听的描述符，所以其时间复杂度较差 O(n)，并且只能同时服务 1024 个文件描述符，所以一般并不会以 select 作为第一方案使用。 reids在linux下的安装 Redis对于Linux是官方支持的，安装起来也非常地简单，直接编译源码然后进行安装即可。 这里以centos为例，大概说一下步骤： 下载redis编译工具:yum install gcc和yum install g++ 解压redis.tar.gz文件，进去之后进行编译:make 然后安装：make install PREFIX=/usr/local/redis 安装成功之后进入/usr/local/redis/bin下启动redis ./redis-server]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis其他的功能介绍]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2FRedis%E5%85%B6%E4%BB%96%E7%9A%84%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第三篇文章，本文主要介绍redis一些其他的功能。遇到某些场景的时候可以想到redis是不是可以实现。 一、慢查询日志 1.1 什么是慢查询日志 慢查询日志帮助开发和运维人员定位系统存在的慢操作。慢查询日志就是系统在命令执行前后计算每条命令的执行时间，当超过预设阀值，就将这条命令的相关信息（慢查询ID，发生时间戳，耗时，命令的详细信息）记录下来。 1.2 redis一条命令简单的生命周期 慢查询只会出现在【3.执行命令】这个阶段，即慢查询只记录命令执行时间，并不包括命令排队时间和网络传输时间。 1.3 慢查询配置参数 慢查询的预设阀值 slowlog-log-slower-than slowlog-log-slower-than参数就是预设阀值，单位是微秒,默认值是10000，如果一条命令的执行时间超过10000微妙(10毫秒)，那么它将被记录在慢查询日志中。 如果slowlog-log-slower-than的值是0，则会记录所有命令。 如果slowlog-log-slower-than的值小于0，则任何命令都不会记录日志。 redis的操作一般是微妙级，slowlog-log-slower-than不要设置太大，一般设置为1毫秒。支持动态设置。 慢查询日志的长度slowlog-max-len slowlog-max-len只是说明了慢查询日志最多存储多少条。 Redis使用一个列表来存储慢查询日志，showlog-max-len就是列表的最大长度。 当慢查询日志已经到达列表的最大长度时，又有慢查询日志要进入列表，则最早插入列表的日志将会被移出列表，新日志被插入列表的末尾。 默认是128，但是slowlog-max-len不要设置太小，可以设置为1000以上. 慢查询日志是一个先进先出队列，慢查询较多的情况下，可能会丢失部分慢查询命令，可以定期执行slow get命令将慢查询日志持久化到其他存储中。然后制作可视化界面查询。 二、pipeline 2.1 为什么会出现Pipeline 用普通的get和set，如果同时需要执行大量的命令，那就是等待上一条命令应答后再执行，这中间不仅仅多了RTT（Round Time Trip），而且还频繁的调用系统IO，发送网络请求。 对于多条命令不是有mget和mset吗？确实对于一批的get和set可以用mget和mset，但是它的问题在于如果我们需要同时传输get和hget呢？此时pipeline(流水线)就出现了。 所以流水线解决的问题是N条命令网络通信的减少。 为什么说网络耗费时间大呢？这里给出一个极端的例子。 pipeline与原生M操作的对比。 原生M操作是一个原子操作。 pipeline非原子命令。 当某个命令的执行需要依赖前一个命令的返回结果时，无法使用pipeline。 12mset a “a1” b “b” c “c1” mget a b c mget和mset命令也是为了减少网络连接和传输时间所设置的，其本质和pipeline的应用区别不大，但是在特定场景下只能用pipeline实现，例如： 12get aset b ‘1’ pipeline适合执行这种连续，且无相关性的命令。 2.2 一个demo 搭建一个quickstart的maven工程。过程略。 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 直接再单元测试中进行测试：普通的直接hset 10000条数据： 1234567891011@Testpublic void test1()&#123; Jedis jedis = new Jedis("127.0.0.1",6379); long before = System.currentTimeMillis(); for(int i=0;i&lt;10000;i++)&#123; jedis.hset("hashkey"+i,"filed"+i,"value"+i); &#125; long after = System.currentTimeMillis(); System.out.println("一共耗时: "+(after-before)+"ms");&#125; 运行结果： 一共耗时: 1526ms 但是用pipeline后： 123456789101112131415@Testpublic void test2()&#123; Jedis jedis = new Jedis("127.0.0.1",6379); long before = System.currentTimeMillis(); //分为10次批量发送 for(int i=0;i&lt;10;i++)&#123; Pipeline pipeline = jedis.pipelined(); for(int j=1000*i;j&lt;(i+1)*1000;j++)&#123; pipeline.hset("hashkey:"+j,"field:"+j,"value:"+j); &#125; pipeline.syncAndReturnAll(); &#125; long after = System.currentTimeMillis(); System.out.println("使用pipeline一共耗时: "+(after-before)+"ms");&#125; 运行结果：使用pipeline一共耗时: 139ms 可以预见，对于更多的传输次数，pipeline的优势将越来越明显。但是pipeline每次只能作用在一个redis节点上。 三、发布订阅 3.1 角色 发布者----频道----订阅者 3.2 模型 注意，新订阅的，是不能收到之前的消息的。 订阅者1：subscribe mytopic 订阅者2：subscribe mytopic 订阅者3：subscribe mytopic 发布者：publish mytopic “hello” 缺点是不能保证消息可达，所以还是用专业的消息队列传达比较保障。 与发布订阅模型很类似的是消息队列模型。 只有一个是可以收到消息的。 四、bitMap 4.1 位图是什么 就是通过一个bit位来表示某个元素对应的值或者状态,其中的key就是对应元素本身。我们知道8个bit可以组成一个Byte，所以bitmap本身会极大的节省储存空间。 Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32长度的位数组。 比如我们执行 set hello big 那么这个big其实是这个形态： 执行getbit hello 0 得到0； 执行getbit hello 1 得到1 setbit hello 7 1，那么再get hello 将得到cig 4.2 位图有什么用呢？ 位图除了getbit和setbit之外，还有bitcount key [start end]，就是获取执行范围内的1的个数。 bitop作用是做多个Bitmap的and,or,not,xor操作。 以一个场景为例：日活跃用户 每次用户登录时会执行一次redis.setbit(daily_active_users, user_id, 1) 因为日活跃用户每天都变化，所以需要每天创建一个新的bitmap。我们简单地把日期（年月日）添加到key后面，以后就可以根据年月日这个key找到某天活跃用户。实现了这个功能。 第二个场景：用户签到情况 将那天所代表的网站的上线日作为offset参数， 比如,如果今天是网站上线的第100天,而用户$uid=10001在今天阅览过网站, 那么执行命令SETBIT peter 100 1. 如果明天$uid=10001也继续阅览网站,那么执行命令SETBIT peter 101 1 ,以此类推. 仔细想想，用位图，一天签到一次只要占一个bit，8天才占一个字节。那么一年这个用户签到占的数据是365/8=45.625个字节.如果不用位图实现，保存一条记录将远远大于一个比特吧，那么当用户量很大的时候，差距将会特别大。 五、hyperLogLog 基于HyperLogLog算法：极小空间完成独立数量统计。本质还是字符串。 pfadd key element [element...]:向hyperloglog添加元素 pfcount key [key...]:计算hyperloglog的独立总数 pfmerge destkey sourcekey [sourcekey...]:合并多个hyperloglog api例子 为什么要用hyperLogLog呢 我们上面例子可以看到，他的功能类似于去重，统计出所有不一样元素的个数。 他的优点是：占用内存极小。 缺点也有： 他可能会出错，错误率为0.81%，看你是否能够容忍错误了 不能拿到单条数据 六、geo 存储经纬度、计算两地距离、范围计算等。 提到LBS(Location Based Service)，基于位置的服务。我立即想起Mongodb的GEO实现地理坐标查询等功能，具体介绍为地理位置附近查询的GEOHASH解决方案。 mongodb最大的特点是灵活，因为其数据是以json的格式存储，所以字段随时可以增加或减少；Redis的特点是快，适合单一的，简单的，大量数据的存储；HBase我没有做深入研究，它的特点是大，适合做离线缓存。在处理社交这种关系复杂的数据存储时，依然还是需要用mysql这种关系型数据库，nosql并不能完全替代。 七、总结 首先是慢查询日志，可以定时地持久化，并且用一个可视化页面进行监测。 pipeline解决的是对没有相互依赖的操作的批量执行，减少网络传输和IO时间。但是呢，需要注意一般只能往一个节点放数据，面对集群的时候，就需要采取一些策略了。mset、mget，目前只支持具有相同slot值的key执行批量操作。后文再讲。 可以实现发布订阅模型以及消息队列，但是消息是无状态的，不能保证消息一定送达，所以需要用专业的MQ来实现。 位图，可以实现极小的空间完成对大量用户信息的统计。 地理坐标服务]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构和操作]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2FRedis%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第二篇文章，本文主要介绍redis如何启动，以及基本的键命令和五种基本数据类型的操作。部分图片可能看不清楚，可以拖到新窗口打开。 一、启动方式 我的环境是windows，那么直接进入redis的解压目录中，分别执行redis-server.exe和redis-cli.exe两个可执行的程序。也可以通过cmd启动： 不要直接用crtl+C关闭server，在linux下，直接停掉server的话，会导致数据的丢失。正确的做法是在客户端执行 redis-cli.exe shutdown 还可以指定端口启动：./redis-server.exe --port 6380 那么对应客户端连接也要指定相应 的端口才能连接。关闭服务端也要指定相应的端口才行： -h指定远程redis的ip 通过配置文件启动,可以在下面这个文件中指定端口号： 结合配置文件启动: 还可以设置密码： 那么客户端连接就必须要密码验证了： 二、命令 1、基础命令 info:查看系统信息 select (0-15)，redis一共有16个工作区间，一般默认从0开始，到15. flushdb：清空当前选择的空间 flushall：清空所有 dbsize：当前空间里面key-value键值对的数目 save：人工实现redis的持久化 quit：退出 2、键命令 del key成功返回1，失败返回0. exits key ttl和expire type key 查看key的类型 randomkey: rename oldkey newkey 如果是重命名为已经存在的key呢？ renamenx: 三、redis数据结构 1、String字符串 setex&amp;psetex getrange&amp;getset mset&amp;mget&amp;strlen setnx&amp;msetnx 数值操作 2、hash 3、list 4、set 5、sorted set]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初步认识Redis]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2F%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86Redis%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第一篇文章，主要从感性层面来认识一下这个开发中的宠儿，无论是什么应用，只要有点用户量的，不上redis是不可能的。作为当今最优秀的缓存中间件，没有理由不去深入了解它！ 一、redis是什么 redis很快，官方宣称QPS(每秒查询率)达10万。 Redis是一个开源的使用ANSI C语言编写、支持网络、单进程单线程、可基于内存亦可持久化、一个高性能的key-value数据库。 简而言之，就是一个缓存数据库，基于内存，也可以持久化，速度贼快，几乎所有互联网公司都在使用。 有的初学者可能看到数据库这个字眼，就把他归类于mysql之类，其实不是，mysql是一种关系型数据库，是存在磁盘中的。核心的数据是一定要落地到mysql之类的数据库中的。redis其实使用最多的功能是缓存，既然能存东西，那么必然也有数据库的功能，但是有可能会造成数据的缺失。所以，数据一定是要落入数据库才保险，redis可以作为缓存，缓存热点数据或者只读数据，提高性能并保护数据库。 二、为什么要用redis 好了，我们已经知道它是一个高性能的缓存中间件。那么必然一大功能是作为缓存使用。那为什么要用缓存呢？直接从数据库查不就行了码？ 在实际的业务场景中，用户量一上来，数据库是吃不消的。数据库是性能的一大瓶颈，如果不采取措施，用户的操作将卡在数据库处理这一块，最终可能导致不可用。 那么，此时，加入缓存，比如商城首页有很多很多内容，这些内容不可能经常变化，至少也要两三天吧？所以，可以将这些数据放到redis中，用户进商城之后，数据直接从redis中获取即可。速度极快，提高了用户的体验。 既然是缓存，那么必定会存在数据不一致的情况，所以缓存最适合于读多写少的情况，当然啦，要修改缓存肯定是可以的，但是要注意热点key的问题，比如微博最火的一片新闻，此时有几百万人再看，你却要修改一下，肯定是要注意点什么东西才行的，后续的文章会讲到如何处理热点key修改的问题。 三、Redis与其他key-value存储有什么不同 这里先简单说说，后面会有文章详细比价一下。 多样的数据结构和原子性操作 Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。redis中的单个命令都是原子性的，什么是原子性，就是该命令不可分割。 运行于内存+持久化于磁盘 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。另一个优点是， 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。 同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 注：我们要知道，对于磁盘的顺序访问速度是远大于随机访问的。这种在硬盘上顺序追加效率很高。 四、redis特点 速度快： 基于内存,这是快的最主要原因。 持久化： 可以同步或异步保存到磁盘中 多种数据结构： 除了五种基本数据类型，还支持位图、HyperLogLog，GEO等 支持多种编程语言客户端： java，python，ruby，Lua… 功能丰富： 可以实现发布-订阅，支持事务、Lua脚本 简单： 不依赖与外部库、单线程模型 主从复制： 主服务器同步数据到从服务器，是高可用的基础 高可用、分布式： 高可用：redis-Sentinel(v2.8版本)；分布式：redis-cluster(v3.0版本) 五、redis典型应用场景 缓存系统：这个就不多说了，redis作为高速缓存是其主要存在价值。 计数器：因为是原子操作incr+单线程，作为计数器永远不会出错 消息队列系统：数据结构list可以实现这种生产者-消费者模式的消息队列。 排行榜：有序集合sorted set就可以实现 社交网络：redis与社交网络就是一家，非常方便用set就能实现诸如共同好友这些功能。 六、redis优势 缓存管理：可以在必要时将无效的旧数据从内存中删除，为新数据腾出新的空间 提供更大的灵活性：redis支持多种类型，并且采用key-value 的形式存储，key和value的大小限制都是512Mb,与编码无关，所以数据安全。但是memcached限制key最大为250字节，value为1MB，况且只支持String类型。 redis提供主从复制：实现高可用的cache系统，支持集群中多个服务器之间的数据同步。 数据持久化：redis可以通过两种方式将数据进行持久化，一定程度上规避缓存中的数据不稳定的问题，也可以在重启服务器时最快的恢复缓存中所需的数据，提高了效率的同时减轻了主数据库系统的开销。 与传统的Memcached相比，优势还是很大的，两者的具体对比我会在后续的文章中详细说明。这里注意存在即合理，Memcached也有不可替代的适用场景： 存储一些粒度比较小的静态数据，比如一些html片段，Memcached便是我们更好的选择。相对于redis而言，Memcached的元数据metadata更小些，所以相对来讲对于数据存储管理的性能更高，额外开销更小。 Memcached的特点：Memcached唯一支持的数据类型是String,所以更适合存储只读数据，因为字符串并不会因为额外的处理造成额外的开销。毕竟Memcached每次更新一个对象时，都需要重复执行下面的操作：获取整个字符串-&gt;反序列化为对象-&gt;修改其中的值-&gt;再次序列化该对象-&gt;在缓存中将整个字符串替换为新字符串。这样一来，更新存储数据就会有更高的消耗，可能就不是我们的最佳选择了。 七、总结 只要记住redis三个关键字：快、持久化、高可用和分布式]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地理位置附近查询的GEOHASH解决方案]]></title>
    <url>%2F2019%2F01%2F29%2Fmiscellany%2F12%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%E9%99%84%E8%BF%91%E6%9F%A5%E8%AF%A2%E7%9A%84GEOHASH%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[地理位置附近查询的GEOHASH解决方案 1.需求场景 现今互联网确实从方方面面影响我们的生活。现在我们可以足不出户就能买到我们心仪的衣服，找到附近的美食。当我们点开一个外卖的app就能看到自己附近的餐厅，那我们有没有想过这是怎么实现的呢？ 2.尝试解决 首先我们能想到的就是把所有餐厅的经纬度存下来 然后当用户选择附近餐厅时 我们先获取用户的经纬度，然后到数据库中查出所有的经纬度，依次计算它们和用户间的距离。 最后根据用户输入的距离范围过滤出合适的餐厅，并根据距离做一个升序排列。 这样貌似能查出附近的餐厅，但是餐厅的数量这么多，直接全查出来内存也要爆掉，即使分批处理计算量也十分大。这样用户等待的时间就会特别长。那有什么办法能减少我们的计算量呢？ 其实很简单，我们应该只计算用户关心的那一片数据，而不是计算所有的。例如用户在北京，那完全没必要计算海南，黑龙江，新疆，浙江等其它地区的数据。如果我们能快速定位到北京甚至某个区，那么我们的计算量将大大减少。我们发现这其实就是索引的功能，但是MySQL对这种二维的地理位置的索引支持并不友好（mongodb有直接的地理位置索引），它对一维的像字符串这样的支持很好。那如果我们的数据在MySQL中，有没有什么方法能将我们的二维坐标转换为一种可比较的字符串呢？这就是我们今天要介绍的geohash算法。 3.基本思想 geohash简单来说就是将一个地理坐标转换为一个可比较的字符串的算法。不过生成的字符串表示的是一个矩形的范围，并不是一个点。 比如西二旗地铁附近这一片矩形区域就可以用wx4eyu82这个字符串表示，并且越靠前的编码表示额范围越大，比如中国绝大部分地区可以用w这个字母表示的矩形区域内。像wx4eyu82表示的区域一定在wx4e表示的区域范围内。利用这些特性我们就可以实现附近餐厅的功能了，比如我们希望查看西二旗地铁附近的餐厅就可以这样查询：select * from table where geohash like 'wx4eyu82%'; 这样就可以利用索引，快速查询出相关餐厅的信息了。并且我们还可以用wx4eyu82为key，餐厅信息为value做缓存。 通过上面的介绍我们知道了GeoHash就是一种将经纬度转换成字符串的方法，并且使得在大部分情况下，字符串前缀匹配越多的距离越近. 4.GeoHash算法的步骤 首先我们将经度和纬度都单独转换为一个二进制编码 得到经度和纬度的二进制编码后，我们按照奇数位放纬度，偶数为放经度的规则（我们这里奇数偶数下标是从0开始）将它们合成一个二进制编码 最后我们需要将这个二进制编码转换为base32编码 举例 地球纬度区间是[-90,90]， 北海公园的纬度是39.928167，可以通过下面算法对纬度39.928167进行逼近编码: 区间[-90,90]进行二分为[-90,0),[0,90]，称为左右区间，可以确定39.928167属于右区间[0,90]，给标记为1； 接着将区间[0,90]进行二分为 [0,45),[45,90]，可以确定39.928167属于左区间 [0,45)，给标记为0； 递归上述过程39.928167总是属于某个区间[a,b]。随着每次迭代区间[a,b]总在缩小，并越来越逼近39.928167； 如果给定的纬度x（39.928167）属于左区间，则记录0，如果属于右区间则记录1，这样随着算法的进行会产生一个序列1011100，序列的长度跟给定的区间划分次数有关。 通过上述计算，纬度产生的编码为10111 00011，经度产生的编码为11010 01011。偶数位放经度，奇数位放纬度，把2串编码组合生成新串：11100 11101 00100 01111。 最后使用用0-9、b-z（去掉a, i, l, o）这32个字母进行base32编码，首先将11100 11101 00100 01111转成十进制，对应着28、29、4、15，十进制对应的编码就是wx4g。 5.缺陷-geohash的边界问题 比如红色的点是我们的位置，绿色的两个点分别是附近的两个餐馆，但是在查询的时候会发现距离较远餐馆的GeoHash编码与我们一样（因为在同一个GeoHash区域块上），而较近餐馆的GeoHash编码与我们不一致。 目前比较通行的做法就是我们不仅获取当前我们所在的矩形区域，还获取周围8个矩形块中的点。那么怎样定位周围8个点呢？关键就是需要获取周围8个点的经纬度，那我们已经知道自己的经纬度，只需要用自己的经纬度减去最小划分单位的经纬度就行。因为我们知道经纬度的范围,又知道需要划分的次数，所以很容易就能计算出最小划分单位的经纬度。 6.几种实现geohash方案的对比 6.1支持二维索引的存储数据库：mongodb mongoDB支持二维空间索引,使用空间索引,mongoDB支持一种特殊查询,如某地图网站上可以查找离你最近的咖啡厅,银行等信息。这个使用mongoDB的空间索引结合特殊的查询方法很容易实现。 API直接支持，很方便 支持按照距离排序，并支持分页。支持多条件筛选。 可满足实时性需求。 资源占用大，数据量达到百万级请流量在10w左右查询速度明显下降。 6.2升级Mysql至5.7，支持Geohash MySQL 5.7.5 增加了对GeoHash的支持，提供了一系列geohash的函数，但是其实Mysql并没有提供类似mogodb类型near这样的函数，仅仅提供了一些经纬度转hash、hash取经纬度的一些函数。 优点:函数直接调用，生成目标hash、根据hash获取经纬度。 缺点：不支持范围查询函数，需要自行处理周边8点的问题，需要补充geo的算法 6.3Redis Commands: Geography Edition GEO 特性在 Redis 3.2 版发布， 这个功能可以将用户给定的地理位置信息储存起来， 并对这些信息进行操作，GEO通过如下命令来完成GEO需求. 命令 描述 geoadd 添加一个或多个经纬度地理位置 georadius 获取指定范围内的对象，也可以增加参数withdistance直接算出距离，也可以增加参数descending/ascending 进行距离排序 georadiusbymember 通过指定的对象，获取其周边对象 geoencode 转换为geohash，52-bit，同时返回该区域最小角的geohash,最大角的geohash，及中心点 geodecode 同上逆操作 优点:效率高，API丰富 缺点：3.2版本是否稳定？ 面试的时候，问到geohash算法以及技术选型大概也能说一说了… 本文章借鉴很多优秀文章，七拼八凑而出。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补码的前世今生]]></title>
    <url>%2F2019%2F01%2F29%2Fjava-basic%2F%E8%A1%A5%E7%A0%81%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[计算机如何来保存负数呢？其实只要达到这样的目的：正数负数都有一个唯一标识即可，但是，正如人类用+1和-1来表示可以提高效率一样，也得有一个比较适当的适合我们的计算机识别的一个方式。下面来详细讲解。 问题的由来 下面为了表示方便，先假设存储一个整型数字用4个bit。 举例来说，+2在计算机中表示为二进制的0010，那么-2怎么表示呢？ 很容易想到，可以将一个二进制位（bit）专门规定为符号位，它等于0时就表示正数，等于1时就表示负数。比如，在8位机中，规定每个字节的最高位为符号位。那么，+2就是0010，而-8则是1010。 更多的例子如下： 这就是直接用原码的方式来存储，虽然说这种方式理论上是可行的，毕竟每个数我都唯一标识了。 但是这种方式存在问题，我们希望+1和-1相加为0，但是通过这个方式算出来的是：0001+1001=1010 (-2)。也就是说，按照正常一步头的方式得不到我们想要的结果。 为了解决了“正负相加等于0”的问题，人们发明了反码。 反码 “反码”表示方式是用来处理负数的，符号位置不变，其余位置相反。 此时，我们再来算一下+1和-1相加，变成了0001+1110=1111，刚好反码表示方式中，1111象征-0。 此时，好像是解决了这个问题，但是我们发现，0这个时候有了两种表达：0000和1111。 即在用反码表示的情况下，0竟然可以用两个值来表示，这显然不好吧。毕竟+0和-0就是同一个玩意啊。 这个时候补码闪亮登场。 补码 很简单，在刚才反码的基础上加1。 此时，我们这里假定整形只有4位。那么-0表示为1111+1=10000，显然溢出了，就需要丢弃最高位，变成0000. 此时，神奇地发现，达到了统一，+0和-0都是用0000来表示了。 此时，也满足正负数相加为0的条件。比如+2为0010，-2为1110.此时两者相加为：0010+1110=(0)0000，丢掉最高位就是0000 那么对于普通情况，比如7+(-4)呢?即0111+1100=0011，就是3。OK，大功告成。 补码怎么求 上面已经说的很详细啦，比如-4，就是在4(0100)的基础上取反(1011)再加一(1100). 上面也解释了为什么要用补码。即保证了对称的正负数相加为0并且0只有一种表示方式。 还有一个重要的点就是，我们注意到，7-4其实我们都是转换成7+(-4)，也就是说，在计算机中，减法都是用加法的逻辑实现的。 即：一套加法的电路实现加减法。此外，乘法和除法其实都是加法这套电路实现的。 补码的本质 这里假设存储一个整型用8个bit。 要将正数转成对应的负数，其实只要用0减去这个数就可以了。比如，-8其实就是0-8。 则8的二进制是00001000，-8就可以用下面的式子求出： 123 ００００００００－００００１０００－－－－－－－－－ 因为00000000（被减数）小于0000100（减数），所以不够减。请回忆一下小学算术，如果被减数的某一位小于减数，我们怎么办？很简单，问上一位借1就可以了。 所以，0000000也问上一位借了1，也就是说，被减数其实是100000000，算式也就改写成： 1234１００００００００－００００１０００－－－－－－－－－ １１１１１０００ 进一步观察，可以发现100000000 = 11111111 + 1，所以上面的式子可以拆成两个： 1234567 １１１１１１１１－００００１０００－－－－－－－－－ １１１１０１１１＋０００００００１－－－－－－－－－ １１１１１０００ 通过这一步，我们就从数学上知道了为什么补码是取反加一了。 你看，求任何一个负数，都是0-正数，那么就用借位的思想来，则变成100000000。 100000000则可以分解为11111111+00000001。 此时求负数的过程就就变成11111111-X+1 而先用11111111来减这个正数，这个结果就是对正数取反。 此时再加上另外一个1. 这与我们求补码的过程是一样的，这也解释了为什么要这样求补码。 证明(可不看) 将上面的特例抽象一下，用统一表达式来证明一下。 我们要证明的是，X-Y或X+(-Y)可以用X加上Y的补码完成。 Y的补码等于(11111111-Y)+1。所以，X加上Y补码，就等于： 1X + (11111111-Y) + 1 我们假定这个算式的结果等于Z，即 1Z = X + (11111111-Y) + 1 接下来，分成两种情况讨论。 第一种情况，如果X小于Y，那么Z是一个负数。 由Y的补码等于(11111111-Y)+1，标记为F=(11111111-Y)+1,那么如何根据F逆向求Y呢？ 1Y=1111111-(F-1) OK,因为此时Z是一个负数，那么Z进行补码的逆运算就可以求出它的绝对值，即正数。再加一个符号，两者相等。 1Z = -[11111111-(Z-1)] = -[11111111-(X + (11111111-Y) + 1-1)] = X - Y 第二种情况，如果X大于Y 这意味着Z肯定大于11111111，但是我们规定了这是8位机，最高的第9位是溢出位，必须被舍去，这相当于减去100000000。所以， 1Z = Z - 100000000 = X + (11111111-Y) + 1 - 100000000 = X - Y 这就证明了，在正常的加法规则下，可以利用2的补码得到正数与负数相加的正确结果。换言之，计算机只要部署加法电路和补码电路，就可以完成所有整数的加法。 本文整理自： 关于2的补码 知乎第一条评论]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot使用logback实现日志按天滚动]]></title>
    <url>%2F2019%2F01%2F28%2Fmiscellany%2F11SpringBoot%E4%BD%BF%E7%94%A8logback%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E6%8C%89%E5%A4%A9%E6%BB%9A%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[日志是任何一个系统都必备的东西，日志的重要程度丝毫不亚于代码。而springboot中经常使用的是logback，那么今天我们就来学习一下在springboot下如何配置logback日志。理解了这里的配置，对于任何的日志都是一样的。 需求 日志按天滚动分割 info和error日志输出到不同文件 为什么使用Logback Logback是Log4j的升级版，作者为同一个人，作者不想再去改Log4j，所以写了Logback 使用日志框架的最佳实践是选择一款日志门面+一款日志实现，这里选择Slf4j+Logback,Slf4j作者也是Logback的作者 SpringBoot从1.4版本开始，内置的日志框架就是Logback Logback在SpringBoot中配置方式一 可以直接在applicatin.properties或者application.yml中配置 以在application.yml中配置为例 123456logging: pattern: console: "%d - %msg%n" file: /var/log/tomcat/sell.log level: com.imooc.LoggerTest: debug 可以发现，这种配置方式简单，但能实现的功能也很局限，只能 定制输出格式 输出文件的路径 指定某个包下的日志级别 如果需要完成我们的需求，这就得用第二种配置了 Logback在SpringBoot中配置方式二 在resource目录下新建logback-spring.xml, 内容如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;configuration&gt; &lt;!--打印到控制台的格式--&gt; &lt;appender name="consoleLog" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt; &lt;pattern&gt; %d - %msg%n &lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--除了error级别的日志文件保存格式以及滚动策略--&gt; &lt;appender name="fileInfoLog" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--过滤器，将error级别过滤掉--&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;DENY&lt;/onMatch&gt; &lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt; %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;!--滚动策略--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--路径--&gt; &lt;fileNamePattern&gt;/var/log/tomcat/sell/info.%d.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;!--error级别日志文件保存格式以及滚动策略--&gt; &lt;appender name="fileErrorLog" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--只让error级别的日志进来--&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt; %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;!--滚动策略--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--路径--&gt; &lt;fileNamePattern&gt;/var/log/tomcat/sell/error.%d.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;root level="info"&gt; &lt;appender-ref ref="consoleLog" /&gt; &lt;appender-ref ref="fileInfoLog" /&gt; &lt;appender-ref ref="fileErrorLog" /&gt; &lt;/root&gt;&lt;/configuration&gt; 每一个appender你可以理解为一个日志处理策略。 第一个appender的name=&quot;consoleLog&quot;, 名字是自己随意取的，取这个名字，表示这个策略用于控制台的日志。 我们重点看第二个和第三个appender,因为要把info和error日志输入到不同文件，所以我们分别建了两个appender。 rollingPolicy是滚动策略，这里我们设置按时间滚动 filter是日志的过滤方式，我们在fileInfoLog里做了如下过滤 123&lt;level&gt;ERROR&lt;/level&gt;&lt;onMatch&gt;DENY&lt;/onMatch&gt;&lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; 上述代码翻译之后：拦截ERROR级别的日志。如果匹配到了，则禁用处理。如果不匹配，则接受，开始处理日志。 那有的同学要问了，不能这样写吗 1&lt;level&gt;INFO&lt;/level&gt; 这样不是只拦截INFO日志了吗？ 不对！ 这就得说一下日志级别了 DEBUG -&gt;INFO -&gt; WARN -&gt;ERROR 如果你设置的日志级别是INFO，那么是会拦截ERROR日志的哦。也就是说，如果直接写info，那么大于等于info级别的日志都会写进去，违背了我们的需求。 整理自： http://www.imooc.com/article/19005]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql面试高频理论知识]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2Fmysql%E9%9D%A2%E8%AF%95%E9%AB%98%E9%A2%91%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[整理一些面试题，简单看看。 目录 数据库三范式 事务 mysql数据库默认最大连接数 分页 触发器 存储过程 用jdbc怎么调用存储过程？ 对jdbc的理解 写一个简单的jdbc的程序。写一个访问oracle数据的jdbc程序 JDBC中的PreparedStatement相比Statement的好处 数据库连接池作用 选择合适的存储引擎 数据库优化-索引 数据库优化-分表 数据库优化-读写分离 数据库优化-缓存 数据库优化-sql语句优化的技巧 jdbc批量插入几百万数据怎么实现 聚簇索引和非聚簇索引 sql注入问题 mysql悲观锁和乐观锁 1. 数据库三范式 1.1 范式是什么 范式就是规范，要满足第二范式必须先满足第一范式，要满足第三范式，必须要先满足第二范式。 1NF(第一范式)：列数据不可分割，即一列不能有多个值 2NF(第二范式)：主键(每一行都有唯一标识) 3NF(第三范式)：外键(表中不包含已在其他表中包含的非主关键信息) 1.2 反三范式 反三范式：有时为了效率，可以设置重复或者推导出的字段，例如：订单总价格订单项的单价，这个订单总价虽然可以由订单项计算出来，但是当订单数目庞大时，效率比较低，所以订单的总价这个字段是必要的。 2. 事务 2.1 含义 事务时并发控制的单位，是用户定义的一个操作序列，要么都做，要么都不做，是不可分割的工作单位。 3. mysql数据库默认最大连接数 3.1 为什么需要最大连接数 特定服务器上的数据库只能支持一定数目同时连接，这时需要我们设置最大连接数（最多同时服务多少连接）。在数据库安装时会有一个默认的最大连接数。 my.ini中max_connections=100 4. 分页 4.1 为什么需要分页？ 在很多数据时，不可能完全显示数据。进行分段显示. 4.2 mysql如何分页 12String sql = "select * from students order by id limit " + pageSize*(pageNumber-1) + "," + pageSize; 4.3 oracle分页 是使用了三层嵌套查询。 1234String sql = &quot;select * from &quot; + (select *,rownum rid from (select * from students order by postime desc) where rid&lt;=&quot; + pagesize*pagenumber + &quot;) as t&quot; + &quot;where t&gt;&quot; + pageSize*(pageNumber-1); 5. 触发器 略。 6. 存储过程 6.1 数据库存储过程具有如下优点： 1、存储过程只在创建时进行编译，以后每次执行存储过程都不需再重新编译，而一般 SQL 语句每执行一次就编译一次，因此使用存储过程可以大大提高数据库执行速度。 2、通常，复杂的业务逻辑需要多条 SQL 语句。这些语句要分别地从客户机发送到服务器，当客户机和服务器之间的操作很多时，将产生大量的网络传输。如果将这些操作放在一个存储过程中，那么客户机和服务器之间的网络传输就会大大减少，降低了网络负载。 3、存储过程创建一次便可以重复使用，从而可以减少数据库开发人员的工作量。 4、安全性高，存储过程可以屏蔽对底层数据库对象的直接访问，使用 EXECUTE 权限调用存储过程，无需拥有访问底层数据库对象的显式权限。 6.2 定义存储过程: 12345678create procedure insert_Student (_name varchar(50),_age int ,out _id int)begin insert into student value(null,_name,_age); select max(stuId) into _id from student;end;call insert_Student('wfz',23,@id);select @id; 7. 用jdbc怎么调用存储过程？ 贾琏欲执事 加载驱动 获取连接 设置参数 执行 释放连接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.sql.CallableStatement;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.Types;public class JdbcTest &#123; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stub Connection cn = null; CallableStatement cstmt = null; try &#123; //这里最好不要这么干，因为驱动名写死在程序中了 Class.forName("com.mysql.jdbc.Driver"); //实际项目中，这里应用DataSource数据，如果用框架， //这个数据源不需要我们编码创建，我们只需Datasource ds = context.lookup() //cn = ds.getConnection(); cn = DriverManager.getConnection("jdbc:mysql:///test","root","root"); cstmt = cn.prepareCall("&#123;call insert_Student(?,?,?)&#125;"); cstmt.registerOutParameter(3,Types.INTEGER); cstmt.setString(1, "wangwu"); cstmt.setInt(2, 25); cstmt.execute(); //get第几个，不同的数据库不一样，建议不写 System.out.println(cstmt.getString(3)); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; /*try&#123;cstmt.close();&#125;catch(Exception e)&#123;&#125; try&#123;cn.close();&#125;catch(Exception e)&#123;&#125;*/ try &#123; if(cstmt != null) cstmt.close(); if(cn != null) cn.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 8. 对jdbc的理解 Java database connection java数据库连接.数据库管理系统(mysql oracle等)是很多，每个数据库管理系统支持的命令是不一样的。 Java只定义接口，让数据库厂商自己实现接口，对于我们者而言。只需要导入对应厂商开发的实现即可。然后以接口方式进行调用.(mysql + mysql驱动（实现）+jdbc) 9. 写一个简单的jdbc的程序。写一个访问oracle数据的jdbc程序 贾琏欲执事 加载驱动(com.mysql.jdbc.Driver,oracle.jdbc.driver.OracleDriver) 取连接(DriverManager.getConnection(url,usernam,passord)) 设置参数 Statement PreparedStatement cstmt.setXXX(index, value); 执行 executeQuery executeUpdate 释放连接(是否连接要从小到大，必须放到finnaly) 10. JDBC中的PreparedStatement相比Statement的好处 大多数我们都使用PreparedStatement代替Statement 1：PreparedStatement是预编译的，比Statement速度快 2：代码的可读性和可维护性 虽然用PreparedStatement来代替Statement会使代码多出几行,但这样的代码无论从可读性还是可维护性上来说.都比直接用Statement的代码高很多档次： 123456789stmt.executeUpdate("insert into tb_name (col1,col2,col2,col4) values('"+var1+"','"+var2+"',"+var3+",'"+var4+"')"); perstmt = con.prepareStatement("insert into tb_name (col1,col2,col2,col4) values (?,?,?,?)");perstmt.setString(1,var1);perstmt.setString(2,var2);perstmt.setString(3,var3);perstmt.setString(4,var4);perstmt.executeUpdate(); 3：安全性 PreparedStatement可以防止SQL注入攻击，而Statement却不能。 比如说： String sql = “select * from tb_name where name= '”+varname+&quot;’ and passwd=’&quot;+varpasswd+&quot;’&quot;; 如果我们把[' or '1' = '1]作为varpasswd传入进来.用户名随意,看看会成为什么? select * from tb_name = ‘随意’ and passwd = ‘’ or ‘1’ = ‘1’; 因为'1'='1'肯定成立，所以可以任何通过验证。 更有甚者：把[';drop table tb_name;]作为varpasswd传入进来,则： select * from tb_name = ‘随意’ and passwd = ‘’;drop table tb_name; 有些数据库是不会让你成功的，但也有很多数据库就可以使这些语句得到执行。 而如果你使用预编译语句你传入的任何内容就不会和原来的语句发生任何匹配的关系，只要全使用预编译语句你就用不着对传入的数据做任何过虑。而如果使用普通的statement,有可能要对drop等做费尽心机的判断和过虑。 11. 数据库连接池作用 1、限定数据库的个数，不会导致由于数据库连接过多导致系统运行缓慢或崩溃 2、数据库连接不需要每次都去创建或销毁，节约了资源 3、数据库连接不需要每次都去创建，响应时间更快。 12. 选择合适的存储引擎 在开发中，我们经常使用的存储引擎 myisam / innodb/ memory MyISAM存储引擎 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. 比如 bbs 中的 发帖表，回复表. INNODB存储引擎: 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表. Memory 存储 我们数据变化频繁，不需要入库，同时又频繁的查询和修改，我们考虑使用memory, 速度极快. MyISAM 和 INNODB的区别(主要) 事务安全 myisam不支持事务而innodb支持 查询和添加速度 myisam不用支持事务就不用考虑同步锁，查找和添加和添加的速度快 支持全文索引 myisam支持innodb不支持 锁机制 myisam支持表锁而innodb支持行锁(事务) 外键 MyISAM 不支持外键， INNODB支持外键. (通常不设置外键，通常是在程序中保证数据的一致) 下面是数据库的优化手段，但是只是表面，需要以后再好好探究 在项目自验项目转测试之前，在启动mysql数据库时开启慢查询，并且把执行慢的语句写到日志中，在运行一定时间后。通过查看日志找到慢查询语句。 1234567891011121314151617181920212223242526272829show variables like '%slow%'; #查看MySQL慢查询是否开启set global slow_query_log=ON; #开启MySQL慢查询功能show variables like "long_query_time"; #查看MySQL慢查询时间设置，默认10秒set global long_query_time=5; #修改为记录5秒内的查询select sleep(6); #测试MySQL慢查询show variables like "%slow%"; #查看MySQL慢查询日志路径show global status like '%slow%'; #查看MySQL慢查询状态或者vi /etc/my.cnf #编辑，在[mysqld]段添加以下代码slow-query-log = on #开启MySQL慢查询功能slow_query_log_file = /var/run/mysqld/mysqld-slow.log #设置MySQL慢查询日志路径long_query_time = 5 #修改为记录5秒内的查询，默认不设置此参数为记录10秒内的查询log-queries-not-using-indexes = on #记录未使用索引的查询:wq! #保存退出service mysqld restart #重启MySQL服务 13. 数据库优化-索引 13.1 索引的概念 索引（Index）是帮助DBMS高效获取数据的数据结构。 13.2 索引有哪些 分类：普通索引/唯一索引/主键索引/全文索引 普通索引:允许重复的值出现 唯一索引:除了不能有重复的记录外，其它和普通索引一样(用户名、用户身份证、email,tel) 主键索引：是随着设定主键而创建的，也就是把某个列设为主键的时候，数据库就会給改列创建索引。这就是主键索引.唯一且没有null值 全文索引:用来对表中的文本域(char，varchar，text)进行索引， 全文索引针对MyIsam explain select * from articles where match(title,body) against(‘database’);【会使用全文索引】 13.3 使用索引的注意事项 索引弊端 占用磁盘空间。 对dml(插入、修改、删除)操作有影响，变慢。 使用场景： 肯定在where条件经常使用,如果不做查询就没有意义 该字段的内容不是唯一的几个值(sex) 字段内容不是频繁变化. 注意事项 对于创建的多列索引（复合索引），不是使用的第一部分就不会使用索引。 123alter table dept add index my_ind (dname,loc); // dname 左边的列,loc就是右边的列explain select * from dept where dname='aaa'\G 会使用到索引explain select * from dept where loc='aaa'\G 就不会使用到索引 对于使用like的查询，查询如果是%aaa不会使用到索引而aaa%会使用到索引。 12explain select * from dept where dname like '%aaa'\G不能使用索引explain select * from dept where dname like 'aaa%'\G使用索引. 所以在like查询时，‘关键字’的最前面不能使用% 或者 _这样的字符，如果一定要前面有变化的值，则考虑使用 全文索引-&gt;sphinx. 索引列排序 MySQL查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来。否则不使用索引。 123expain select * from dept where dname=’111’;expain select * from dept where dname=111;（数值自动转字符串）expain select * from dept where dname=qqq;报错 也就是，如果列是字符串类型，无论是不是字符串数字就一定要用 ‘’ 把它包括起来. 如果mysql估计使用全表扫描要比使用索引快，则不使用索引。 表里面只有一条记录 索引不会包含有NULL值的列 只要列中包含有NULL值都将不会被包含在MySQL索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 使用短索引 对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 不要在列上进行运算，不使用NOT IN和&lt;&gt;操作，不支持正则表达式。 14. 数据库优化-分表 分表分为水平(按行)分表和垂直(按列)分表 水平分表情形： 根据经验，Mysql表数据一般达到百万级别，查询效率会很低，容易造成表锁，甚至堆积很多连接，直接挂掉；水平分表能够很大程度较少这些压力。 垂直分表情形： 如果一张表中某个字段值非常多(长文本、二进制等)，而且只有在很少的情况下会查询。这时候就可以把字段多个单独放到一个表，通过外键关联起来。考试详情，一般我们只关注分数，不关注详情。 水平分表策略： 1.按时间分表 这种分表方式有一定的局限性，当数据有较强的实效性，如微博发送记录、微信消息记录等，这种数据很少有用户会查询几个月前的数据，如需要就可以按月分表。 2.按区间范围分表 一般在有严格的自增id需求上，如按照user_id水平分表： 123table_1 user_id从1~100w table_2 user_id从101~200w table_3 user_id从201~300w 3.hash分表 通过一个原始目标的ID或者名称通过一定的hash算法计算出数据存储表的表名，然后访问相应的表。 15. 数据库优化-读写分离 一台数据库支持的最大并发连接数是有限的，如果用户并发访问太多。一台服务器满足不要要求是就可以集群处理。Mysql的集群处理技术最常用的就是读写分离。 主从同步 数据库最终会把数据持久化到磁盘，如果集群必须确保每个数据库服务器的数据是一直的。能改变数据库数据的操作都往主数据库去写，而其他的数据库从主数据库上同步数据。 读写分离 使用负载均衡来实现写的操作都往主数据去，而读的操作往从服务器去。 16. 数据库优化-缓存 什么是缓存 在持久层(dao)和数据库(db)之间添加一个缓存层，如果用户访问的数据已经缓存起来时，在用户访问时直接从缓存中获取，不用访问数据库。而缓存是在操作内存级，访问速度快。 作用 减少数据库服务器压力，减少访问时间。 Java中常用的缓存有 hibernate的二级缓存。该缓存不能完成分布式缓存。 可以使用redis(memcahe等)来作为中央缓存。对缓存的数据进行集中处理 17. 数据库优化-sql语句优化的技巧 DDL优化 通过禁用索引来提供导入数据性能，这个操作主要针对现有数据库的表追加数据 123456//去除键alter table test3 DISABLE keys;//批量插入数据insert into test3 ***//恢复键alter table test3 ENABLE keys; 关闭唯一校验 12set unique_checks=0 关闭set unique_checks=1 开启 修改事务提交方式(导入)（变多次提交为一次） 123set autocommit=0 关闭//批量插入set autocommit=1 开启 DML优化（变多次提交为一次） 12345insert into test values(1,2);insert into test values(1,3);insert into test values(1,4);//合并多条为一条insert into test values(1,2),(1,3),(1,4) DQL优化 Order by优化 多用索引排序 普通结果排序（非索引排序）Filesort group by优化 使用order by null,取消默认排序 等等等等… 18. jdbc批量插入几百万数据怎么实现 1、变多次提交为一次 2、使用批量操作 3、像这样的批量插入操作能不使用代码操作就不使用，可以使用存储过程来实现 mysql优化手段介绍到这里。 19. 聚簇索引和非聚簇索引 索引分为聚簇索引和非聚簇索引。 “聚簇索引” 以一本英文课本为例，要找第8课，直接翻书，若先翻到第5课，则往后翻，再翻到第10课，则又往前翻。这本书本身就是一个索引，即“聚簇索引”。 “非聚簇索引” 如果要找&quot;fire”这个单词，会翻到书后面的附录，这个附录是按字母排序的，找到F字母那一块，再找到&quot;fire”，对应的会是它在第几课。这个附录，为“非聚簇索引”。 由此可见，聚簇索引，索引的顺序就是数据存放的顺序，所以，很容易理解，一张数据表只能有一个聚簇索引。 聚簇索引要比非聚簇索引查询效率高很多，特别是范围查询的时候。所以，至于聚簇索引到底应该为主键，还是其他字段，这个可以再讨论。 1、MYSQL的索引 mysql中，不同的存储引擎对索引的实现方式不同，大致说下MyISAM和InnoDB两种存储引擎。 MyISAM存储引擎的索引实现 MyISAM的B+Tree的叶子节点上的data，并不是数据本身，而是数据存放的地址。主索引和辅助索引没啥区别，只是主索引中的key一定得是唯一的。这里的索引都是非聚簇索引。 MyISAM还采用压缩机制存储索引，比如，第一个索引为“her”，第二个索引为“here”，那么第二个索引会被存储为“3,e”，这样的缺点是同一个节点中的索引只能采用顺序查找。 InnoDB存储引擎的索引实现 InnoDB 的数据文件本身就是索引文件，B+Tree的叶子节点上的data就是数据本身，key为主键，这是聚簇索引。非聚簇索引，叶子节点上的data是主键 (所以聚簇索引的key，不能过长)。为什么存放的主键，而不是记录所在地址呢，理由相当简单，因为记录所在地址并不能保证一定不会变，但主键可以保证。 至于为什么主键通常建议使用自增id呢？ 2.聚簇索引 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。如果主键不是自增id，那么可以想 象，它会干些什么，不断地调整数据的物理地址、分页，当然也有其他一些措施来减少这些操作，但却无法彻底避免。但，如果是自增的，那就简单了，它只需要一 页一页地写，索引结构相对紧凑，磁盘碎片少，效率也高。 聚簇索引不但在检索上可以大大滴提高效率，在数据读取上也一样。比如：需要查询f~t的所有单词。 一个使用MyISAM的主索引，一个使用InnoDB的聚簇索引。两种索引的B+Tree检索时间一样，但读取时却有了差异。 因为MyISAM的主索引并非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。 不过，如果涉及到大数据量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的。 鉴于聚簇索引的范围查询效率，很多人认为使用主键作为聚簇索引太多浪费，毕竟几乎不会使用主键进行范围查询。但若再考虑到聚簇索引的存储，就不好定论了。 20. sql注入问题 20.1 什么是sql注入 sql注入大家都不陌生，是一种常见的攻击方式，攻击者在界面的表单信息或url上输入一些奇怪的sql片段，例如“or ‘1’=’1’”这样的语句，有可能入侵参数校验不足的应用程序。所以在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性很高的应用中，比如银行软件，经常使用将sql语句全部替换为存储过程这样的方式，来防止sql注入，这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 20.2 PrepareStatement解决SQL注入的问题 在使用JDBC的过程中，可以使用PrepareStatement进行预处理，预处理的优势就是预防绝大多数的SQL注入；而且针对多次操作数据库的情况，可以极大的提高访问数据库的效率。 那为什么它这样处理就能预防SQL注入提高安全性呢？其实是因为SQL语句在程序运行前已经进行了预编译。在程序运行时第一次操作数据库之前，SQL语句已经被数据库分析，编译和优化，对应的执行计划也会缓存下来并允许数据库以参数化的形式进行查询。当运行时动态地把参数传给PreprareStatement时，即使参数里有敏感字符如 or ‘1=1’，数据库也会作为一个参数一个字段的属性值来处理而不会作为一个SQL指令。如此，就起到了SQL注入的作用了！ 20.3 MyBatis如何防止sql注入 mybatis框架作为一款半自动化的持久层框架，其sql语句都要我们自己来手动编写，这个时候当然需要防止sql注入。其实Mybatis的sql是一个具有“输入+输出”功能，类似于函数的结构，如下： 12345&lt;select id=“getBlogById“ resultType=“Blog“ parameterType=”int”&gt; select id,title,author,content from blog where id=#&#123;id&#125; &lt;/select&gt; 这里，parameterType标示了输入的参数类型，resultType标示了输出的参数类型。回应上文，如果我们想防止sql注入，理所当然地要在输入参数上下功夫。上面代码中“#{id}”即输入参数在sql中拼接的部分，传入参数后，打印出执行的sql语句，会看到sql是这样的： select id,title,author,content from blog where id = ? 不管输入什么参数，打印出的sql都是这样的。这是因为mybatis启用了预编译功能，在sql执行前，会先将上面的sql发送给数据库进行编译，执行时，直接使用编译好的sql，替换占位符“？”就可以了。因为sql注入只能对编译过程起作用，所以这样的方式就很好地避免了sql注入的问题。 mybatis是如何做到sql预编译的呢？其实在框架底层，是jdbc中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的sql语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行一个sql时，能够提高效率，原因是sql已编译好，再次执行时无需再编译。 补充 12345&lt;select id=“orderBlog“ resultType=“Blog“ parameterType=”map”&gt; select id,title,author,content from blog order by $&#123;orderParam&#125; &lt;/select&gt; 仔细观察，内联参数的格式由“#{xxx}”变为了${xxx}。如果我们给参数“orderParam”赋值为”id”,将sql打印出来，是这样的： select id,title,author,content from blog order by id 显然，这样是无法阻止sql注入的。在mybatis中，”${xxx}”这样格式的参数会直接参与sql编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式，所以，这样的参数需要我们在代码中手工进行处理来防止注入。 21. mysql悲观锁和乐观锁 21.1 悲观锁 悲观锁（Pessimistic Lock），顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 Java synchronized 就属于悲观锁的一种实现，每次线程要修改数据时都先获得锁，保证同一时刻只有一个线程能操作数据，其他线程则会被block。 21.2 乐观锁 乐观锁（Optimistic Lock），顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。乐观锁适用于读多写少的应用场景，这样可以提高吞吐量。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 乐观锁一般来说有以下2种方式： 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 version 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 使用时间戳（timestamp）。乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。 Java JUC中的atomic包就是乐观锁的一种实现，AtomicInteger 通过CAS（Compare And Set）操作实现线程安全的自增。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂查询基础]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2F%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[这一节从group by和having两个关键语法入手，学习一下写sql的基本思路。 数据库准备： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677-- ------------------------------ 学生表-- ----------------------------DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `student_id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` varchar(8) DEFAULT NULL, PRIMARY KEY (`student_id`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4;-- ------------------------------ student表数据-- ----------------------------INSERT INTO `student` VALUES ('1', 'lilei', '19', 'female');INSERT INTO `student` VALUES ('2', 'huangmeimei', '18', 'male');INSERT INTO `student` VALUES ('3', 'pollu', '17', 'female');INSERT INTO `student` VALUES ('4', 'tom', '18', 'male');INSERT INTO `student` VALUES ('5', 'david', '17', 'male');INSERT INTO `student` VALUES ('6', 'lucy', '19', 'female');INSERT INTO `student` VALUES ('7', 'jacky', '20', 'male');-- ------------------------------ 课程表-- ----------------------------DROP TABLE IF EXISTS `course`;CREATE TABLE `course` ( `course_id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, PRIMARY KEY (`course_id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4;-- ------------------------------ course表数据-- ----------------------------INSERT INTO `course` VALUES ('1', 'chinese');INSERT INTO `course` VALUES ('2', 'math');INSERT INTO `course` VALUES ('3', 'english');INSERT INTO `course` VALUES ('4', 'physics');-- ------------------------------ 分数表-- ----------------------------DROP TABLE IF EXISTS `score`;CREATE TABLE `score` ( `student_id` int(11) DEFAULT NULL, `course_id` int(11) DEFAULT NULL, `score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;-- ------------------------------ score表数据-- ----------------------------INSERT INTO `score` VALUES ('1', '2', '78');INSERT INTO `score` VALUES ('1', '3', '67');INSERT INTO `score` VALUES ('1', '4', '67');INSERT INTO `score` VALUES ('2', '1', '52');INSERT INTO `score` VALUES ('2', '2', '81');INSERT INTO `score` VALUES ('2', '3', '92');INSERT INTO `score` VALUES ('2', '4', '67');INSERT INTO `score` VALUES ('3', '1', '52');INSERT INTO `score` VALUES ('3', '2', '47');INSERT INTO `score` VALUES ('3', '3', '88');INSERT INTO `score` VALUES ('3', '4', '67');INSERT INTO `score` VALUES ('4', '2', '88');INSERT INTO `score` VALUES ('4', '3', '90');INSERT INTO `score` VALUES ('4', '4', '67');INSERT INTO `score` VALUES ('5', '1', '52');INSERT INTO `score` VALUES ('5', '3', '78');INSERT INTO `score` VALUES ('5', '4', '67');INSERT INTO `score` VALUES ('6', '1', '52');INSERT INTO `score` VALUES ('6', '2', '68');INSERT INTO `score` VALUES ('6', '4', '67');INSERT INTO `score` VALUES ('1', '1', '52');INSERT INTO `score` VALUES ('5', '2', '72');INSERT INTO `score` VALUES ('7', '2', '72'); 第一个问题 查询所有同学的学号、选课数、总成绩 针对sql问题，不需要一口气全部写出来，我们先分解逐个击破，最后再合体。 分析题目，他要查询的是每个学生的学号，选课数，总成绩 那么先把关键字列出来： 12345#首先是要查询，肯定有select关键字select#要查询的几个关键字段student_id,count(course_id),sum(score) 我们看到，有一些函数在里面，比如count和sum，那么我们会想到一般情况下是与group by结合使用的。 因为是查询每个学生，那么必然是根据每个学生的id进行分组了。 1group by student_id 此时，因为涉及的student_id,course_id以及score只需要一张score表就可以解决,那么拼接起来就是： 我们进行explain分析一下： 123explain SELECT student_id,count(course_id),sum(score)from scoregroup by student_id 显示： 基本的原理就是：首先根据group by进行分组，分组出来的数据缓存到一张临时表中，然后再做count之类的计算显示。 并且，本题是针对一张表，所以有一个规则是：如果用group by，那么你的select语句中选出的列要么是group by里用到的列，要么就是sum min等列函数的列。所以这里group by和后面是student_id，所以select后面可以查询student_id，但是不能查询course_id等字段。 第二个问题 查询所有同学的学号、姓名、选课数、总成绩 注意观察，其实就是比上一个问题多一个字段name，但是区别比较大，因为一张score表已经不够用了。这个时候还需要student表了，即两张表联合查询。那么只要搞一个连接条件即可： 第三个问题 查询平均成绩大于60分的同学的学号和平均成绩 我们再来分解看看： 12345678#查询肯定用到selectselect #要查询的两个字段student_id,avg(score) #由于存在avg，那么必然要分组group by student_id 最后，有一个条件是：平均成绩大于60分，此时就需要对查询出来的分组进行过滤筛选了，此时having闪亮登场。 测试了一下，下面两条sql都是一样的效果： 12select * from course where course_id = 1select * from course having course_id = 1 第四个问题 查询没有学全所有课的同学的学号、姓名 这个稍微复杂一点点，我们还是分解来看看： 1234567891011#查询肯定用到selectselect #要查询的两个字段student_id,name#由于存在avg，那么必然要分组group by student_id#两张表连接，要起个别名where sc.student_id = stu.student_id 因为需要查询课程没有学满的学生，所以需要先查询所有课程的数量： 1select count(1) from course 此时，我们需要利用这个查询语句作为结果再进行查询，即子查询。对于上面的分组要进行筛选 1having count(sc.course_id) &lt; (select count(1) from course) 所以最终的语句是：]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL必知必会知识点提炼]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2FSQL%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[这是对《mysql必知必会》的知识提炼。 一、基础 模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。 主键的值不允许修改，也不允许复用（不能使用已经删除的主键值赋给新数据行的主键）。 SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。 SQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。 SQL 支持以下三种注释： 12345# 注释SELECT *FROM mytable; -- 注释/* 注释1 注释2 */ 二、创建表 123456CREATE TABLE mytable ( id INT NOT NULL AUTO_INCREMENT, col1 INT NOT NULL DEFAULT 1, col2 VARCHAR(45) NULL, col3 DATE NULL, PRIMARY KEY (`id`)); 三、修改表 添加列 12ALTER TABLE mytableADD col CHAR(20); 删除列 12ALTER TABLE mytableDROP COLUMN col; 删除表 1DROP TABLE mytable; 四、插入 普通插入 12INSERT INTO mytable(col1, col2)VALUES(val1, val2); 插入检索出来的数据 123INSERT INTO mytable1(col1, col2)SELECT col1, col2FROM mytable2; 将一个表的内容插入到一个新表 12CREATE TABLE newtable ASSELECT * FROM mytable; 五、更新 123UPDATE mytableSET col = valWHERE id = 1; 六、删除 12DELETE FROM mytableWHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。 1TRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。 七、查询 DISTINCT 相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。 12SELECT DISTINCT col1, col2FROM mytable; LIMIT 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。 返回前 5 行： 123SELECT *FROM mytableLIMIT 5; 123SELECT *FROM mytableLIMIT 0, 5; 返回第 3 ~ 5 行： 123SELECT *FROM mytableLIMIT 2, 3; 八、排序 ASC ：升序（默认） DESC ：降序 可以按多个列进行排序，并且为每个列指定不同的排序方式： 123SELECT *FROM mytableORDER BY col1 DESC, col2 ASC; 九、过滤 不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。 123SELECT *FROM mytableWHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符 操作符 说明 = 等于 &lt; 小于 &gt; 大于 &lt;&gt; != 不等于 &lt;= !&gt; 小于等于 &gt;= !&lt; 大于等于 BETWEEN 在两个值之间 IS NULL 为 NULL 值 应该注意到，NULL 与 0、空字符串都不同。 AND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。 IN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。 NOT 操作符用于否定一个条件。 十、通配符 通配符也是用在过滤语句中，但它只能用于文本字段。 % 匹配 &gt;=0 个任意字符； \_ 匹配 ==1 个任意字符； [ ] 可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。 使用 Like 来进行通配符匹配。 123SELECT *FROM mytableWHERE col LIKE '[^AB]%'; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。 十一、计算字段 在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。 计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。 12SELECT col1 * col2 AS aliasFROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。 12SELECT CONCAT(TRIM(col1), '(', TRIM(col2), ')') AS concat_colFROM mytable; 十二、函数 汇总 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。 使用 DISTINCT 可以让汇总函数值汇总不同的值。 12SELECT AVG(DISTINCT col1) AS avg_colFROM mytable; 文本处理 函数 说明 LEFT() 左边的字符 RIGHT() 右边的字符 LOWER() 转换为小写字符 UPPER() 转换为大写字符 LTRIM() 去除左边的空格 RTRIM() 去除右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。 123SELECT *FROM mytableWHERE SOUNDEX(col1) = SOUNDEX('apple') 日期和时间处理 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 1mysql&gt; SELECT NOW(); 12018-4-14 20:25:11 数值处理 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 十三、分组 分组就是把具有相同的数据值的行放在同一组中。 可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。 指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。 123SELECT col, COUNT(*) AS numFROM mytableGROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。 1234SELECT col, COUNT(*) AS numFROM mytableGROUP BY colORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。 12345SELECT col, COUNT(*) AS numFROM mytableWHERE col &gt; 2GROUP BY colHAVING num &gt;= 2; 分组规定： GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。 十四、子查询 子查询中只能返回一个字段的数据。 可以将子查询的结果作为 WHRER 语句的过滤条件： 1234SELECT *FROM mytable1WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次： 123456SELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_numFROM CustomersORDER BY cust_name; 十五、连接 连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。 连接可以替换子查询，并且比子查询的效率一般会更快。 可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。 内连接 内连接又称等值连接，使用 INNER JOIN 关键字。 123SELECT A.value, B.valueFROM tablea AS A INNER JOIN tableb AS BON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。 123SELECT A.value, B.valueFROM tablea AS A, tableb AS BWHERE A.key = B.key; 在没有条件语句的情况下返回笛卡尔积。 自连接 自连接可以看成内连接的一种，只是连接的表是自身而已。 一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。 子查询版本 123456SELECT nameFROM employeeWHERE department = ( SELECT department FROM employee WHERE name = "Jim"); 自连接版本 1234SELECT e1.nameFROM employee AS e1 INNER JOIN employee AS e2ON e1.department = e2.department AND e2.name = "Jim"; 自然连接 自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。 内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。 12SELECT A.value, B.valueFROM tablea AS A NATURAL JOIN tableb AS B; 外连接 外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。 检索所有顾客的订单信息，包括还没有订单信息的顾客。 123SELECT Customers.cust_id, Orders.order_numFROM Customers LEFT OUTER JOIN OrdersON Customers.cust_id = Orders.cust_id; customers 表： cust_id cust_name 1 a 2 b 3 c orders 表： order_id cust_id 1 1 2 1 3 3 4 3 结果： cust_id cust_name order_id 1 a 1 1 a 2 3 c 3 3 c 4 2 b Null 十六、组合查询 使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。 每个查询必须包含相同的列、表达式和聚集函数。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 1234567SELECT colFROM mytableWHERE col = 1UNIONSELECT colFROM mytableWHERE col =2; 十七、视图 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。 对视图的操作和对普通表的操作一样。 视图具有如下好处： 简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 1234CREATE VIEW myview ASSELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_colFROM mytableWHERE col5 = val; 十八、存储过程 存储过程可以看成是对一系列 SQL 操作的批处理； 使用存储过程的好处： 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。 包含 in、out 和 inout 三种参数。 给变量赋值都需要用 select into 语句。 每次只能给一个变量赋值，不支持集合的操作。 123456789101112delimiter //create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end //delimiter ; 12call myprocedure(@ret);select @ret; 十九、游标 在存储过程中使用游标可以对一个结果集进行移动遍历。 游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。 使用游标的四个步骤： 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标； 1234567891011121314151617181920delimiter //create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; # 定义了一个 continue handler，当 sqlstate '02000' 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate '02000' set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器 触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。 触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。 INSERT 触发器包含一个名为 NEW 的虚拟表。 1234CREATE TRIGGER mytrigger AFTER INSERT ON mytableFOR EACH ROW SELECT NEW.col into @result;SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。 UPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改地，而 OLD 是只读的。 MySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。 二十一、事务处理 基本术语： 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。 MySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。 通过设置 autocommit 为 0 可以取消自动提交，直到 autocommit 被设置为 1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。 如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。 1234567START TRANSACTION// ...SAVEPOINT delete1// ...ROLLBACK TO delete1// ...COMMIT 二十二、字符集 基本术语： 字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。 除了给表指定字符集和校对外，也可以给列指定： 123CREATE TABLE mytable(col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci )DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对： 123SELECT *FROM mytableORDER BY col COLLATE latin1_general_ci; 二十三、权限管理 MySQL 的账户信息保存在 mysql 这个数据库中。 12USE mysql;SELECT user FROM user; 创建账户 1CREATE USER myuser IDENTIFIED BY 'mypassword'; 新创建的账户没有任何权限。 修改账户名 1RENAME myuser TO newuser; 删除账户 1DROP USER myuser; 查看权限 1SHOW GRANTS FOR myuser; 授予权限 1GRANT SELECT, INSERT ON mydatabase.* TO myuser; 账户用 username@host 的形式定义，username@% 使用的是默认主机名。 删除权限 1REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; GRANT 和 REVOKE 可在几个层次上控制访问权限： 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.\*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 更改密码 必须使用 Password() 函数 1SET PASSWROD FOR myuser = Password('new_password'); 转自： SQL]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务核心问题]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文从事务引出了Mysql InnoDB RR隔离级别下是如何防止幻读的。 1. 数据库事务的四大特性 最重要的是ACID特性。 原子性(Atomicity)：事务要么都成功要么都失败 一致性(Consistency)：关系型数据库有很多约束，事务前后都要满足这些约束(不仅仅是数据库物理约束，还包括内部逻辑上的一些假设) 隔离性(Isolation)：两个事务互相独立，不能互相干扰 持久性(Durability)：事务执行成功之后结果可以持久化，永久存储下来(redo日志) 对于一致性，可能解释比较抽象，他的实际含义是：数据库的数据应满足完整性约束。拿转账业务来说，假设用户A和用户B一共有2000块钱，那么他们之间无论如何转账，总共的钱应该都是2000. 2. 事务并发访问引起的问题 更新丢失-mysql所有事务隔离级别在数据库层面均可避免 取款事务 存款事务 开始事务 开始事务 查询余额为100元 无 无 查询余额为100元 无 存入20，余额变为120元 无 提交事务 取出10元，余额改为90元 无 回滚事务，余额恢复为100元 更新丢失 脏读问题-一个事务读到另一个事务未提交的数据 不可重复读-事务A多次读取数据，未提交数据，此时事务B提交新的数据，导致A多次读取数据期间数据不一致，不满足隔离性 幻读-事务A受到另一个事务插入新的一行或者删除一行的影响，导致幻觉 不可重复读的重点是修改: 同样的条件的select, 你读取过的数据, 再次读取出来发现值不一样了 幻读的重点在于新增或者删除: 同样的条件的select, 第1次和第2次读出来的记录数不一样 具体可以自己设置不同的隔离级别进行演示。 3. 事务的隔离级别 Read uncommitted：读到其他事务未commit的值 Read committed：解决了脏读问题，但是会读到其他事务commit的值，读两次可能会读到两个值，所以又叫不可重复读 Repeatable Read：解决了不可重复读问题，可重复读，别人commit对我没有影响，但是对于别的事务插入操作，可能会产生幻读 Serializable：串行化，当发生两个事务同时提交，结果只可能有一个，相当于串行执行后的某个结果 级别越来越高，安全性也越来越高，但是但是性能越来越低。说明一下，出现幻读只是针对这种Repeatable Read隔离级别，但是InnoDB已经不存在幻读问题了，如何解决的呢？主要是用next-key锁来解决，下文会讲到。 4. 当前读和快照读 4.1 当前读 读取的都是当前数据的最新版本，并且在读的时候对其加锁，不允许其他事务进行修改操作。 select ... lock in share mode（共享锁）以及 select ... for update、update、delete、insert（排他锁）这些操作都是当前读。 为什么将 插入/更新/删除 操作，都归为当前读？可以看看下面这个 更新 操作，在数据库中的执行流程： 从图中，可以看到，一个Update操作的具体流程。当Update SQL被发给MySQL后，MySQL Server会根据where条件，发出current read 读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回，并加锁 (current read)。 待MySQL Server收到这条加锁的记录之后，会再发起一个Update请求，更新这条记录。 一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此，Update操作内部，就包含了一个当前读。 4.2 快照读 不加锁的非阻塞读，简单的select（前提是事务级别不是serializable，因为在serializable级别下都是串行读，普通的select也会退化为当前读即select ... lock in share mode） 快照读的实现是基于多版本并发控制（MVCC）实现，旨在提高性能。有可能读到的不是数据的最新版本。（创建快照的时机决定了读到的数据的版本，如果事务A先快照读，事务B修改，那么事务A再快照读就还是更新前的版本，事务A的当前读会读到最新的数据；而当事务B先更新，事务A再快照读，就会读到数据最新版本了） 4.3 MVCC MVCC在MySQL的InnoDB中的实现 在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下： SELECT时，读取创建版本号&lt;=当前事务版本号，删除版本号为空或&gt;当前事务版本号。 INSERT时，保存当前事务版本号为行的创建版本号 DELETE时，保存当前事务版本号为行的删除版本号 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 通过MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。 说白了，就是乐观锁的一种实现。免去了加锁解锁的过程，对于读多写少的场景特别适用。 5. RC，RR级别下的InnoDB非阻塞读（快照读）如何实现 通过数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID这三个字段 DB_TRX_ID，最后一次修改本行事务的ID DB_ROLL_PTR，即回滚指针,与undo日志配合 DB_ROW_ID，随着新行插入而单调递增的行号（innoDB中如果既没有主键索引也没有唯一索引的时候，就会自动生成一个隐藏主键，就是这个玩意） 这三个字段结合undo日志，这个日志里面记录的都是老版本的数据，这样，快照读就可以读出适合的一个版本的数据出来。在数据库中，日志是非常重要的东西，可以说其重要性是大于数据本身的，因为数据丢失可以通过日志找回来，但是日志丢失了，那么以后数据库出现崩溃等就麻烦了。 6. 日志 数据库数据存放的文件称为data file；日志文件称为log file；数据库数据是有缓存的，如果没有缓存，每次都写或者读物理disk，那性能就太低下了。数据库数据的缓存称为data buffer，日志（redo）缓存称为log buffer；既然数据库数据有缓存，就很难保证缓存数据（脏数据）与磁盘数据的一致性。比如某次数据库操作： 1update driver_info set driver_status = 2 where driver_id = 10001; 更新driver_status字段的数据会存放在缓存中，等待存储引擎将driver_status刷新data_file，并返回给业务方更新成功。如果此时数据库宕机，缓存中的数据就丢失了，业务方却以为更新成功了，数据不一致，也没有持久化存储。 上面的问题就可以通过事务的ACID特性来保证。 12345BEGIN trans；update driver_info set driver_status = 2 where driver_id = 10001;COMMIT; 这样执行后，更新要么成功，要么失败。业务方的返回和数据库data file中的数据保持一致。要保证这样的特性这就不得不说存储引擎innodb的redo和undo日志。 6.1 undo是啥 undo日志用于存放数据修改被修改前的值，假设修改 tba 表中 id=2的行数据，把Name=‘B’ 修改为Name = ‘B2’ ，那么undo日志就会用来存放Name='B’的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 对数据的变更操作，主要来自 INSERT UPDATE DELETE，而UNDO LOG中分为两种类型，一种是 INSERT_UNDO（INSERT操作，事务提交后可以立即丢弃），记录插入的唯一键值；一种是 UPDATE_UNDO（包含UPDATE及DELETE操作），记录修改的唯一键值以及old column记录。 6.2 redo是啥 存储引擎也会为redo undo日志开辟内存缓存空间，log buffer。磁盘上的日志文件称为log file，是顺序追加的，性能非常高，注：磁盘的顺序写性能比内存的写性能差不了多少。 redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。例如某一事务的事务序号为T1，其对数据X进行修改，设X的原值是5，修改后的值为15，那么Undo日志为&lt;T1, X, 5&gt;，Redo日志为&lt;T1, X, 15&gt;。 梳理下事务执行的各个阶段： 写undo日志到log buffer； 执行事务，并写redo日志到log buffer； 如果innodb_flush_log_at_trx_commit=1，则将redo日志写到log file，并刷新落盘。 提交事务。 那redo日志是写进去了，但是数据呢？ 在数据库的世界里，数据从来都不重要，日志才是最重要的，有了日志就有了一切。 因为data buffer中的数据会在合适的时间 由存储引擎写入到data file，如果在写入之前，数据库宕机了，根据落盘的redo日志，完全可以将事务更改的数据恢复。好了，看出日志的重要性了吧。先持久化日志的策略叫做Write Ahead Log，即预写日志。 6.3 Undo + Redo事务的简化过程 假设有A、B两个数据，值分别为1,2，开始一个事务，事务的操作内容为：把1修改为3，2修改为4，那么实际的记录如下（简化）： 事务开始. 记录A=1到undo log buffer. 修改A=3. 记录A=3到redo log buffer. 记录B=2到undo log buffer. 修改B=4. 记录B=4到redo log buffer. 将redo log写入磁盘。 事务提交 我们可以看到，2，4，5，7，8都是新增操作，但是2，4，5，7都是缓冲到buffer区，只有8是磁盘IO操作。为了保证Redo Log有较好的IO性能，设计一般有以下特点： 尽量保持Redo Log存储在一段连续的空间上。因此在系统第一次启动时就会将日志文件的空间完全分配。 以顺序追加的方式记录Redo Log,通过顺序IO来改善性能。 批量写入日志。日志并不是直接写入文件，而是先写入redo log buffer.当需要将日志刷新到磁盘时 (如事务提交),将许多日志一起写入磁盘. 并发的事务共享Redo Log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起， 以减少日志占用的空间。例如,Redo Log中的记录内容可能是这样的： 记录1: &lt;trx1, insert …&gt; 记录2: &lt;trx2, update …&gt; 记录3: &lt;trx1, delete …&gt; 记录4: &lt;trx3, update …&gt; 记录5: &lt;trx2, insert …&gt; 因为上一条的原因,当一个事务将Redo Log写入磁盘时，也会将其他未提交的事务的日志写入磁盘 Redo Log上只进行顺序追加的操作，当一个事务需要回滚时，它的Redo Log记录也不会从Redo Log中删除掉。 6.4 回滚 前面说到未提交的事务和回滚了的事务也会记录Redo Log，因此在进行恢复时,这些事务要进行特殊的的处理。有2种不同的恢复策略： 进行恢复时，只重做已经提交了的事务。 进行恢复时，重做所有事务包括未提交的事务和回滚了的事务。然后通过Undo Log回滚那些未提交的事务。 MySQL数据库InnoDB存储引擎使用了第二个策略。 InnoDB可重复读隔离级别下如何避免幻读 表象原因:快照读（非阻塞读）–伪MVCC 内在原因：next-key锁（行锁+gap锁） 6.5 next-key锁 在 RR 级别下，如果查询条件能使用上唯一索引，或者是一个唯一的查询条件，那么仅加行锁，如果是一个范围查询，那么就会给这个范围加上 gap 锁或者 next-key锁 (行锁+gap锁)。 那么gap锁啥时候出现呢？ 使用主键索引或者唯一索引时： 如果where条件全部命中，则不会用Gap锁，只会加记录锁 如果where条件部分命中或者全不命中，则会加Gap锁 在走非唯一索引或者不走索引的当前读中，也会出现Gap锁。对于不走索引的情况，那么就会锁住整张表。 总结一下：只有对唯一索引+全部命中才不会加gap锁。 具体来个例子说明间隙锁如何工作。 7. 例子-走唯一索引 7.1 准备工作 有这样一个表test，其中name为主键，id为唯一键。 123456CREATE TABLE `test` ( `name` varchar(11) primary key, `id` int, unique KEY `id` (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into test(name,id) values ("f",1), ("h",2), ("b",3), ("a",5), ("c",6),("d",9); name id f 1 h 2 b 3 a 5 c 6 d 9 首先验证一下使用主键索引或者唯一索引时会怎么样。 7.2 第一种情况：唯一索引+命中所有数据 session1执行 1delete from test where id = 3; session2执行： 1insert into test(name,id) values("swg",4); 此时由于id是唯一索引，并且是命中的，所以只是对这一行加排他锁，而没有加gap锁，所以session2是可以正常执行的，不能被阻塞。 7.3 第一种情况：唯一索引+不命中数据 session1执行 1delete from test where id = 7; session2执行： 1insert into test(name,id) values("swg",8); 此时session2会阻塞住，证明id=7周围加了gap锁。gap锁的范围遵从左开右闭的原则，这里就是(6,7）以及(7,9)都会被锁住。加上record锁组成next-key锁，所以next-key锁的范围是(6,7]以及(7,9]这个范围。 7.4 第三种情况：唯一索引+不命中所有数据 session1执行 1select * from test where id in (5,7,9) lock in share mode; 这里是一个范围，5和9都是存在的，但是7不存在，即部分数据不存在。 session2执行： 1234567insert into test(name,id) values("swg",4);&lt;!--可以--&gt;insert into test(name,id) values("swg",7);&lt;!--不可以--&gt;insert into test(name,id) values("swg",8);&lt;!--不可以--&gt;insert into test(name,id) values("swg",10);&lt;!--可以--&gt; 那么对于(5,9]的范围内就阻塞住了，那么部分命中就是部分加gap锁。 7.5 第四种情况：唯一索引+命中所有数据 session1执行 1select * from test where id in (5,6,9) lock in share mode; 这里全部命中，那么 session2执行： 123insert into test(name,id) values("swg",7);&lt;!--可以--&gt;insert into test(name,id) values("swg",8);&lt;!--可以--&gt; 这个时候就不会加gap锁了。 8. 例子-不走唯一索引或者不走索引 下面来看看不走非唯一索引的当前读是什么情况。 此时表的数据为： name id h 2 c 6 b 9 d 9 f 11 a 15 把id上的唯一索引换成了普通索引。 8.1 第五种情况：非唯一索引 session1执行 1delete from test where id = 9; session2执行： 1insert into test(name,id) values("swg",9); 此时session2是会被block住的。gap的范围是(6,9]以及(9,11]. 12345insert into test(name,id) values("swg",5);&lt;!--可以--&gt;insert into test(name,id) values("swg",7);&lt;!--不可以--&gt;insert into test(name,id) values("swg",12);&lt;!--可以--&gt; 上面的原理都是一样的，即只要是6和11之间的数，不包含临界值的时候，无论插入什么数据，都是会阻塞的。 但是关于临界值6和11，这里就比较特殊了，因为需要加上主键的值才能进行精准的判断。 123insert into test(name,id) values(&quot;bb&quot;,6);&lt;!--可以--&gt;insert into test(name,id) values(&quot;dd&quot;,6);&lt;!--不可以--&gt; 这是什么原因呢？ 我们将数据画成图： 这里的gap区间可能是(负无穷，2],(2,6],(6,9],(9,11],(11,15],(15,正无穷) 我们可以看到，id为6的行，对应的name为c(不要忘记name是主键，主键按照顺序排序)，那么主键中就是按照字母表的顺序进行排列的（ASCII码），如果插入的name小于c，那么就不在gap的范围内(c,)，就可以插入，但是dd在gap的范围内,所以就会阻塞住。 8.2 第五种情况：不走索引 这个时候，所有的间隙都会加上间隙锁，那么就是锁表了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锁模块]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E9%94%81%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[本章对行锁表所、共享锁排他锁进行详细说明。这是数据库锁的核心知识。 MySQL中几种重要的锁概念 共享锁（S）和 排他锁（X） InnoDB 实现了标准的行级锁，包括两种：共享锁（简称 s 锁）、排它锁（简称 x 锁） 共享锁允许持锁事务读取一行 排它锁允许持锁事务更新或者删除一行 如果事务 T1 持有行 r 的 s 锁，那么另一个事务 T2 请求 r行 的锁时，会做如下处理： T2 请求 s 锁立即被允许，结果 T1 T2 都持有 r 行的 s 锁 T2 请求 x 锁不能被立即允许 如果 T1 持有 r 的 x 锁，那么 T2 请求 r 的 x、s 锁都不能被立即允许，T2 必须等待T1释放 x 锁才行。 注意：排他锁指的是一个事务在一行数据加上排他锁后，其他事务不能再在其上加其他的锁。mysql InnoDB引擎默认的修改数据语句，update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果加排他锁可以使用select ...for update语句，加共享锁可以使用select ... lock in share mode语句。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select ...from...查询数据，因为普通查询没有任何锁机制。 意向锁 innodb的意向锁主要用户多粒度的锁并存的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。 举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。 说白了意向锁的主要作用是处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁” 意向排它锁（简称 IX 锁）表明一个事务意图在某个表中设置某些行的 x 锁 意向共享锁（简称 IS 锁）表明一个事务意图在某个表中设置某些行的 s 锁 例如， SELECT ... LOCK IN SHARE MODE 设置一个 IS 锁, SELECT ... FOR UPDATE 设置一个 IX 锁。 意向锁的原则如下： 一个事务必须先持有该表上的 IS 或者更强的锁才能持有该表中某行的 S 锁 一个事务必须先持有该表上的 IX 锁才能持有该表中某行的 X 锁 next-key锁 InnoDB有三种行锁的算法： Record Lock：单个行记录上的锁。分为S Lock和X Lock Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。 Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 在默认情况下，mysql的事务隔离级别是可重复读，并且innodb_locks_unsafe_for_binlog参数为0，这时默认采用next-key locks。所谓Next-Key Locks，就是Record lock和gap lock的结合，即除了锁住记录本身，还要再锁住索引之间的间隙。 例子：假设一个索引包含值 10,11,13和20，索引上可能的NK 锁包括如下几个区间（注意开闭区间） 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) Innodb使用NK 锁来进行索引搜索和扫描，阻止了幻读。 间隙锁在Innodb中是被“十足的抑制”的，也就是说，他们只阻止其他事务插入到间隙中，他们不阻止其他事物在同一个间隙上获得间隙锁。 下篇文章会详细介绍一下。 MyISAM和InnoDB关于锁方面的区别 结论： MyISAM默认使用的是表级锁，不支持行级锁 InnoDB默认使用的是行级锁，也支持表级锁 所谓表级锁，就是锁住整张表。开销小，加锁快；不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 MyISAM在执行select的时候会产生一个表共享读锁，当进行更新等操作的时候会产生表独占写锁（排他锁）。所以： myISAM表的读操作，不会阻塞其他用户对同一个表的读请求，但会阻塞对同一个表的写请求。 myISAM表的写操作，会阻塞其他用户对同一个表的读和写操作。 myISAM表的读、写操作之间、以及写操作之间是串行的。 这里的读是共享锁，也可以将其变为排他锁，语法是select … for update 上面说完了MyISAM的表锁，下面要说说InnoDB啦。InnoDB支持行级锁。 所谓行级锁，就是锁住一行数据。开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发性也最高。 SELECT … LOCK IN SHARE MODE 和 SELECT … FOR UPDATE 如果你在查询数据，然后在同一个事务里插入或者修改相关的数据，常规的 select 语句不会提供足够的保护。其他的事务可以修改或者删除你正在查询的行。InnoDB 支持两种可以提供安全机制的读取锁： SELECT ... LOCK IN SHARE MODE SELECT ... FOR UPDATE SELECT … LOCK IN SHARE MODE 在读取的行上设置一个共享锁，其他的session可以读这些行，但在你的事务提交之前不可以修改它们。如果这些行里有被其他的还没有提交的事务修改，你的查询会等到那个事务结束之后使用最新的值。 索引搜索遇到的记录，SELECT … FOR UPDATE 会锁住行及任何关联的索引条目，和你对那些行执行 update 语句相同。其他的事务会被阻塞在比如执行 update 操作，获取共享锁，或从某些事务隔离级别读取数据等操作。 使用 SELECT FOR UPDATE 为 update 操作锁定行，只适用于 autocommit 被禁用（当使用 START TRANSACTION 开始事务或者设置 autocommit 为0时）。如果 autocommit 已启用，符合规范的行不会被锁定。 以上是对官方文档的翻译解读。 SELECT … LOCK IN SHARE MODE ：共享锁(S锁, share locks)。其他事务可以读取数据，但不能对该数据进行修改，直到所有的共享锁被释放。 如果事务对某行数据加上共享锁之后，可进行读写操作；其他事务可以对该数据加共享锁，但不能加排他锁，且只能读数据，不能修改数据。 SELECT … FOR UPDATE：排他锁(X锁, exclusive locks)。如果事务对数据加上排他锁之后，则其他事务不能对该数据加任何的锁。获取排他锁的事务既能读取数据，也能修改数据。 注：普通 select 语句默认不加锁，而CUD操作默认加排他锁。 当前事务获取共享锁后，可以读写，其他事务是否可以进行读写操作和获取共享锁：可以读，可以获取共享锁，不可以写 两个事务同时获取共享锁后，是否可以进行update操作：不可以 当前事务获取排他锁后，其他事务是否可以进行读写操作和获取共享锁：其他事务可以读，不可以获取共享锁，不可以写 是否可对一条数据加多个排他锁：不可以 行锁和索引的关系：查询字段未加索引（主键索引、普通索引等）时，使用表锁 注：InnoDB行级锁基于索引实现。 未加索引时，两种行锁情况为（使用表锁）： 事务1获取某行数据共享锁，其他事务可以获取不同行数据的共享锁，不可以获取不同行数据的排他锁 事务1获取某行数据排他锁，其他事务不可以获取不同行数据的共享锁、排他锁 加索引后，两种行锁为（使用行锁）： 事务1获取某行数据共享锁，其他事务可以获取不同行数据的排他锁 事务1获取某行数据排他锁，其他事务可以获取不同行数据的共享锁、排他锁 索引数据重复率太高会导致全表扫描：当表中索引字段数据重复率太高，则MySQL可能会忽略索引，进行全表扫描，此时使用表锁。可使用 force index 强制使用索引。 总结（很重要） MyISAM默认使用的是表级锁，不支持行级锁 执行select的时候会产生一个表共享读锁 当进行更新等操作的时候会产生表独占写锁（排他锁） 读不会阻塞其他session的读以及获取表共享读锁 写会阻塞其他session读和写操作 写与读之间是串行的 InnoDB默认使用的是行级锁，也支持表级锁 InnoDB 支持两种可以提供安全机制的读取锁：SELECT … LOCK IN SHARE MODE以及SELECT … FOR UPDATE SELECT … LOCK IN SHARE MODE 在读取的行上设置一个共享锁 SELECT … FOR UPDATE：排他锁 一个session对某一行上共享锁，其他的session可以读这行，也可以获取共享锁，但是不允许写，更不允许获取写锁。对于其他行，可以读写其他行数据也可以上读写锁。 一个session对某一行上排他锁，其他的session则不能加任何锁，包括共享锁。允许读这一行，但是不能写。允许对其他行数据进行读写以及上读写锁。 InnoDB中行级锁基于索引实现，所以在不加索引的时候，这两者上的其实都是表锁；加上索引之后，使用行锁。 以上的内容都是从博客：https://blog.csdn.net/u012099869/article/details/52778728 中整理而来，具体的实验也在他的博客中进行了详细的展示。 MyISAM适合场景 频繁执行全表count语句(MyISAM已经用一个表保存了行数) 对数据进行增删改的频率不高，查询非常频繁 没有事务 InnoDB适合场景 数据增删改查都相当频繁 可靠性要求比较高，要求支持事务]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于索引失效和联合索引]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E5%85%B3%E4%BA%8E%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E5%92%8C%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[理解最左前缀原则，可以帮助我们避免索引失效。 索引失效 查询条件包含or 当or左右查询字段只有一个是索引，该索引失效，explain执行计划key=null；只有当or左右查询字段均为索引时，才会生效； 组合索引，不是使用第一列索引，索引失效 如果select * from key1=1 and key2= 2;建立组合索引（key1，key2）; select * from key1 = 1;组合索引有效； select * from key1 = 1 and key2= 2;组合索引有效； select * from key2 = 2;组合索引失效；不符合最左前缀原则 like 以%开头 使用like模糊查询，当%在前缀时，索引失效； 如何列类型是字符串，where时一定用引号括起来，否则索引失效 当全表扫描速度比索引速度快时，mysql会使用全表扫描，此时索引失效 最左前缀原则 建立以下sql： 123456789CREATE TABLE IF NOT EXISTS `test_index`( `id` int(4) NOT NULL AUTO_INCREMENT, `a` int(4) NOT NULL DEFAULT '0', `b` int(4) NOT NULL DEFAULT '0', `c` int(4) NOT NULL DEFAULT '0', `data` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `union_index` (`a`,`b`,`c`))ENGINE=InnoDB ROW_FORMAT=DYNAMIC DEFAULT CHARSET=binary; 测试的mysql版本是 5.7. 首先以列a作为条件查询数据，我们看到 type: ref 表示引用查找, key_len: 4 表示索引长度为4，也就是利用上了索引来进行查找: 123456789101112131415explain select data from test_index where a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.01 sec) 然后以列b作为条件查询数据，可以看到type: ALL表示全表查找, key_len: NULL 表示没有索引，也就说明如果只使用b作为查询条件，不能利用索引来加快查找速度. 123456789101112131415explain select data from test_index where b = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 接着以列c作为条件查询数据，可以看到type: ALL表示全表查找, key_len: NULL 表示没有索引，情况与用b作为条件一样，只使用c作为查询条件也不能利用索引来加快查找速度 123456789101112131415explain select data from test_index where c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 现在来测一下使用a、b作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 8 表示索引长度为8，也就是说我们利用上了a、b联合索引来进行查找 123456789101112131415explain select data from test_index where a = 1 and b = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 紧接着来测一下使用a、c作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 4 表示索引长度为4，这就奇怪了，按照最左原则来说，a、c上是不会建立索引的，为什么会有索引长度呢？其实与a、b上的索引一比较我们就能发现，a、c上的索引长度只有4，而且单独的c上是没有索引的，所以4字节长度的索引只能是a上的，也就是说这种情况我们只使用了a列上的索引来进行查找 123456789101112131415explain select data from test_index where a = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 为了进一步验证上面的想法，这一次测一下使用b、c作为条件的情况，我们看到 type: ALL 表示全表查找, key_len: NULL 表示没有索引可以使用，按照最左原则来说，b列上没有索引，c列上也没有索引，同时b、c的上也不存在联合索引，所以使用b、c作为查询条件时无法利用联合索引 123456789101112131415explain select data from test_index where b = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 1.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 测试完两个条件的情况，接下来测试一下使用a、b、c作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 12 表示索引长度为12，这完全符合联合索引的最左原则，同时使用3个条件查询可以利用联合索引 123456789101112131415explain select data from test_index where a = 1 and b = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 12 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 下面这种情况也能利用a、b上的联合索引，索引长度为8 123456789101112131415explain select data from test_index where b = 1 and a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 再来试试这种情况，按照最左原则，c上没有建立索引，a上有索引，c、a没有建立联合索引，所以只能使用a上的索引进行查找，结果索引长度只有4，验证了我们的想法，联合查询条件使用索引时满足“交换律” 123456789101112131415explain select data from test_index where c = 1 and a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 联合索引总结 联合索引的最左原则就是建立索引KEY union_index (a,b,c)时，等于建立了(a)、(a,b)、(a,b,c)三个索引，从形式上看就是索引向左侧聚集，所以叫做最左原则，因此最常用的条件应该放到联合索引的组左侧。 **对于&quot;=&quot;和&quot;in&quot;可以乱序。**利用联合索引加速查询时，联合查询条件符合“交换律”，也就是where a = 1 and b = 1 等价于 where b = 1 and a = 1。这归功于mysql查询优化器，mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。 mysql会一直向右匹配直到遇到范围查询(&lt;,&gt;,between,like)就停止匹配。比如a=3 and b=4 and c&gt;5 and d=6，如果建立(a,b,c,d)顺序的索引，d是用不到索引的。如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 对于最左匹配原则的理解 mysql索引最左匹配原则的理解?–沈杰的回答 其实我觉得只要理解一点就是，只要有最左边的索引元素，那么这个索引结构一定是按照最左索引元素排序的，后序的索引元素也是依赖于最左元素之后才有可能变得有意义。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL调优]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2FMySQL%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[本文介绍最基本的sql调优手段。 根据慢日志定位慢查询sql 12&lt;!--这里是用模糊查询查出关于查询的一些配置项--&gt;show variables like '%query%' 我们关注slow_query_log：OFF，表示慢查询处于关闭状态。关注long_query_time：超出这个时间就是慢查询，记录到slow_query_log_file文件中。 1show status like '%slow_queries%' 这一句作用是统计慢查询的数量。 如何打开慢查询呢？ 1234&lt;!--打开慢查询--&gt;set global slow_query_log = on;&lt;!--慢查询的标准是1秒--&gt;set global long_query_time = 1; 注意要重启一下客户端。或者在配置文件中设置，重启服务端就永久保留了。 explain分析慢日志 上一步时打开慢查询日志。下面要进行分析。 1explain select ... 对这个命令进行分析。有两个关键字段： type：表示mysql找到数据行的方式，下面的顺序是由快到慢： system&gt;const&gt; eq_ref&gt;ref&gt;fulltext&gt;ref_or_null&gt;index_merge&gt;unique_subquery&gt;index_subquery&gt;range&gt;index&gt;all 其中index和all为全表扫描。说明需要优化。 extra： using_filesort：表示MySQL会对结果使用一个外部索引排序，而不是从表里按索引次序读到相关内容。可能在内存或者磁盘上进行排序。MqSQL中无法利用索引完成的排序操作称为“文件排序” using temporary：表示MySQL在对查询结果排序时使用临时表。常见于排序order by 和分组查询 group by。 当extra中出现以上两项意味着MYSQL根本不能使用索引，效率会受到重大影响，应尽可能对此进行优化。 修改sql或者尽量让sql走索引 上一步分析完之后，就要采取一定的措施来修正。 如果是没有加索引，可以对其加上索引。extra就会变成using index，表示走了索引。 索引是越多越好吗 数据量小的表不需要建立索引，建立会增加额外的索引开销 数据变更需要维护索引，因此更多的索引意味着更多的维护成本 更多的索引意味着也需要更多的空间 可以理解为，一个几页的宣传手册 对于几页的宣传手册我们还需要建立一个目录吗？ 变更这个小的宣传手册里面的章节还要修改目录不是更烦吗？ 一个宣传手册内容就两页，结果你的目录就占了一页，这合理吗？]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引全面解读]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2FMySQL%E7%B4%A2%E5%BC%95%E5%85%A8%E9%9D%A2%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[彻底理解MySQL的存储引擎数据结构以及聚集索引。 1.为什么需要索引 如果不用索引，那么最简单的方案是：将数据全部或者分批次地加载到内存，因为数据是以页的形式存储的，所以我们可以轮询这些页，找出有没有我们需要的数据。 在大数据量的情况下，显然是会非常慢的。因为它要进行全表的扫描。 而索引的灵感来源于字典，我们知道，新华字典前面有按照拼音或者按偏旁部首排序的一个列表页，我们可以快速地根据这个索引目录迅速定位到某几页，然后我们到这个某几页找一下就可以找到了。不需要全表扫描。 2.索引的数据结构 2.1 二叉查找树上阵 二叉查找树的特点： 二分搜索树本质上是一棵二叉树。不需要是一棵完全二叉树。 每个节点的键值大于左孩子 每个节点的键值小于右孩子 以左右孩子为根的子树仍为二分搜索树 二叉查找树有点很明显，我们查询一个数据只需要O(logn)的时间。但是它存在一个致命问题： 我们有时会删除增加数据，搞的不好，会把他恶化成一个链表。 但是有的同学说，我们可以利用红黑树之类的数据结构来维持住平衡二叉树的特性，这样不就好了吗？ 这里还存在另一个问题，就是IO。我们知道从磁盘查询数据，影响性能的关键点是iO的次数，然后这种一个节点只有两个孩子，在海量数据里，IO的次数还是太多，影响性能。 我们这个时候就知道了方向，我们想找一个数据结构，它既包含了二叉树的优点，还要是平衡的树，还能使树的高度变矮，并且每个节点存储更多的数据。 2.2 BTree上阵 B数又叫平衡多路查找树。M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉 有几个特点： 根节点至少包含两个孩子 排序方式：所有节点关键字是按递增次序排列，并遵循左小右大原则 子节点数：树中每个节点最多包含m个孩子(m&gt;=2)；除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子 关键字数：枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于m-1个（分别比孩子数少一个） 所有叶子节点都位于同一层，即所有叶子节点高度都一样 我们结合这个图来理解。 可以看到，我们这是一个3路B树，根节点有3个孩子，有2个关键字。根据规则，子节点数最多为m个即3个，最少为ceil(1.5)个即2个；关键字数最多为m-1个即2个，最少为于ceil(1.5)-1个即1个. 那么我们进行插入的时候，关键字这里最多为2个，所以大于2就要进行拆分。 如何拆分呢？拿个例子来： 定义一个5阶树（平衡5路查找树;），现在我们要把3、8、31、11、23、29、50、28 这些数字构建出一个5阶树出来; 那么关键字最多为4个，超过4个就拆分。 先插入 3、8、31、11 再插入23、29 再插入50、28 大概就是这样的流程。总之要维护一个从左到右逐渐增大的一个特性，并且必须是平衡的。(大概忽略里面可能存在的一些小错误，理解其中意思即可) 对于删除也是如此，要满足以上的特性才行，这里就不再赘述了。 对B树总结一下： B树相对于平衡二叉树的不同是，每个节点包含的关键字增多了，特别是在B树应用到数据库中的时候，数据库充分利用了磁盘块的原理（磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来）把节点大小限制和充分使用在磁盘快大小范围；把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度; 2.3 B+树 是B树的变体，其定义基本上与B树是差不多的。除了： 非叶子节点的子树指针与关键字个数相同 非叶子节点仅用来索引，数据都保存在叶子节点中 所有叶子节点均有一个链指针指向下一个叶子节点 这就是B+树相对于B树的改进的几个点。 由于数据存在叶子节点，优点是非叶子节点保存的关键字更多了，树的高度就会更矮。 2.4 总结 B+树更适合用来做存储索引： B+树的磁盘读写代价更低（因为内部不存放数据，一次性读取的关键字更多，IO次数降低） B+树的查询效率更加稳定（任何关键字的查找都要到叶子节点，导致每个查询都差不多） B+树有利于对数据库扫描（遍历叶子节点就可以直接扫描整个表，这个适合做范围查询） 2.5 Hash索引也可以考虑一下 Hash结构可以一次性地定位到响应位置。如果遇到碰撞的情况，只需要遍历链表即可。那么性能这么高，为什么我们不用Hash索引呢？ 它也有缺点： 只能做等值操作，不能使用范围查询 hash索引不是按照索引值顺序存储，无法使用于排序。 不能利用部分索引键查询（比如组合索引，hash索引是对这几个索引一起hash计算的，而我们用组合索引中的部分索引时就无法用了） 不能避免表扫描（会出现hashs冲突，必然要扫描里面具体的数据才行） 遇到大量hash值相等的时候性能不一定比B树高(同上) 3. 聚集索引 3.1 什么是聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 3.2 MyISAM和InnoDB索引实现 第一个不同是：InnoDB的数据文件本身就是索引文件。而MyISAM的索引和数据是分开的。 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址，是没有任何顺序而言的，所以MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。 InnoDB也使用B+Tree作为索引结构。InnoDB的数据文件本身就是索引文件，即 InnoDB 表是基于聚簇索引建立的。 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址（这一点可以通过在data目录下查看数据库文件验证。Innodb每一个数据库只有一个数据文件，而Myisam则有三个（数据文件、索引文件、表结构文件））。 而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。聚集索引的确定规则为： 若一个主键被定义，该主键则作为聚集索引 若没有主键被定义，该表的第一个唯一非空索引作为聚集索引 若上述都找不到，innodb内部会生成一个隐藏主键(聚集索引) 非主键索引存储相关键位和其对应的主键值，包含两次查找 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。 换句话说，InnoDB的所有辅助索引都引用主键作为data域。 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择 4. 总结 本文首先介绍的是为什么要索引。这个问题很简单。 然后介绍了几种数据结构：二叉搜索树、二叉平衡树、B树以及B+树。一步一步引出为什么最终是B+树。面试的时候就看解决：为什么不能用二叉搜索树、为什么不用红黑树、B树和B+树各自的数据结构特点、B+树的优点 最后介绍了聚集索引，因为这是MyISAM与InnoDB索引结构最大的不同。 之前还是不能太准确理解聚集索引，这两种存储引擎都是以B+树数据结构建立索引结构的，但是InnoDB本身这个B+树就作为了索引文件，即索引与数据是放在一起的，所以逻辑上这样排的数据，它物理上也是这么排。 而MyISAM的索引结构(B+树)与数据是分离的，虽然B+树可能是按照主键有序地组织，但是表的数据在另一个地方是随机放的，找数据是根据地址来找即可，所以这种结构就不是聚集的。 理解了这个，下面就非常好理解了，InnoDB这个B+树，我们知道，叶子节点的核心数据就是主键。所以是按照主键递增的方式进行排列。这样子，无论是按照主键排序还是范围搜索，都会非常地快。 那么如果是非主键索引的辅助索引呢？InnoDB只能通过两次查询来实现了，首先第一步是根据这个辅助索引找到存放在叶子节点中的主键值，然后根据主键再去主键索引中去查找对应的数据。 而MyISAM索引，主键索引和辅助索引就区别不大了。都是单独一个索引结构，然后根据最后叶子节点中的该条数据的地址去找。 上面说的按照主键排列，就是这里所谓的聚集索引啦。当然了，如果没有指定主键，会按照上面所说的规则去构建聚集索引。 那么，面试的时候，就可以应对InnoDB与MyISAM索引结构的各自的实现和不同点啦。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引入门]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[索引是数据库中提高性能的一大利器。本篇入门索引的基本知识。 1. 什么是索引 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2. 为什么要用索引 索引主要就是为了提高查询速度用的。 3. 索引的一些缺点 第一，创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 第二，索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 第三，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4. 哪些字段适合用索引 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。（这同一） 5. 不应该创建索引的的这些列具有下列特点 第一，对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 第二，对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 第三，对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 第四，当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 6. 索引的分类 B-Tree 索引， Hash 索引， Fulltext 索引和R-Tree 索引 最主要关心的是B-Tree 索引。下面再提一下聚集索引，因为这是innodb最主要的组织方式。 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 下一节会详细讲到InnoDB和MyISAM的索引实现方式，他们最大的区别就是InnoDB是聚集索引，而MyISAM不是。 7. 局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 8.B树/B+树 想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是B tree或者 B+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然， 有的数据库也使用哈希桶作用索引的数据结构 ， 然而， 主流的RDBMS都是把平衡树当做数据表默认的索引数据结构的。 我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个加了主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。 如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。 假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。 这里不讲解平衡树的运行细节， 但是从上图能看出，树一共有三层， 从根节点至叶节点只需要经过三次查找就能得到结果。如下图 这一节先对索引入个门，关于B+树以及聚集索引下篇文章来具体分析。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一个关系型数据库]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[这是一个比较空的面试题，这里说一下如何回答。 如何设计一个关系型数据库 我们考虑开发一个数据库最重要的模块是什么。首先数据存储是其核心功能。因此会有一个存储模块来存储数据。介质主要是硬盘。 可是，光有存储是不行的。我们需要有以下程序模块对数据进行组织。 存储管理 我们需要对数据的格式和文件的分隔进行统一的管理，通过逻辑的形式来组合和表示出来。 我们知道程序处理，需要将数据先加载到内存中去，不可能直接在硬盘上进行处理。 我们通过io读取磁盘数据，磁盘的io是非常耗时的，所以硬盘以页的形式存储数据，根据局部性原理，往往用户要查询的数据周围的数据也会被查询到，所以取数据都是以页为单位查取多个数据，提高效率。 缓存机制 也就是上面提到的，一次IO不会只取用户所需要的一点数据，所以会涉及到缓存，缓存可能会不够放，那就涉及一些缓存淘汰的算法，比如比较常用的是LRU算法。 SQL解析 将SQL进行编译执行。如何提高SQL解析效率呢？可能也用缓存，缓存好SQL解析后的结果，下次再执行一样的SQL就可以免去解析的过程。 日志管理 要记录SQL操作，方便主从同步、灾难恢复等。这里要了解一下binlog. 权限划分 就是权限。 容灾机制 要对异常情况做好准备，比如数据库挂了怎么办。 索引管理 优化数据库执行效率。 锁模块 使得数据库支持并发操作。 总结 了解了上面的内容，我们就可以对这个问题做一个简单的总结性回答了，如何设计关系型数据库呢？首先数据库有一个存储的功能，使得它能存储在比如机械硬盘或者固态硬盘上面。其次，我们需要一个存储管理模块来映射程序逻辑与物理地址，实现存储管理。还需要缓存机制，对一些数据进行缓存提高效率，并且缓存不能太大，必须配备缓存淘汰机制；然后需要一个SQL解析模块，来解析SQL；然后需要日志管理来提供主从赋值、主从同步等功能；还需要一个权限划分模块，来提供给多用户使用场景；还需要容灾机制面对异常情况；最后，为了提高数据查询效率需要有索引管理模块；为了支持并发操作需要有锁模块。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[delete和truncate以及drop区别]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fdelete%E5%92%8Ctruncate%E4%BB%A5%E5%8F%8Adrop%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[这个题目我自己也被问过，这里简单整理一下。 先来个总结： drop直接删掉表； truncate删除的是表中的数据，再插入数据时自增长的数据id又重新从1开始； delete删除表中数据，可以在后面添加where字句。 日志是否记录 DELETE语句执行删除操作的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。 TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 是否可以回滚 delete 这个操作会被放到 rollback segment 中,事务提交后才生效。truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment 中，不能回滚. 所以在没有备份情况下，谨慎使用 drop 与 truncate。 表和索引占的空间 当表被 TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小。 而 DELETE 操作不会减少表或索引所占用的空间。 drop语句将表所占用的空间全释放掉。 TRUNCATE 和 DELETE 只删除数据，而 DROP 则删除整个表（结构和数据） 所以从干净程度，drop &gt; truncate &gt; delete ok，差不多了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最基础知识小结]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fmysql%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文介绍关于数据库的最最最最基本的一些语法知识，如果这些都不熟悉，建议多多练习，因为后续的文章会比较深入原理。 一、DDL语句 1、创建数据库：create database dbname; 2、删除数据库：drop database dbname; 3、创建表：create table tname; 4、删除表：drop table tname; 5、修改表：略，懒得看 二、DML语句 插入： 1insert into table(字段1，字段2，...) values (value1,value2,...) , (value3,value4,..) 更新： 1update table set 字段=value where ... 删除： 1delete from table where ... 这里要注意下delete和truncate以及drop三者的区别，下篇文章详解。 单表查询： 1select 字段 from table 连表查询方式1： 1select 别名1.字段,别名2.字段 from table1 别名1,table2 别名2 where ... 连表查询方式2： 1select 别名1.字段,别名2.字段 from table1 别名1 join table2 别名2 on ... 这是全连接，这里就要了解一下笛卡儿积，简单来说，最后行数是左边表的函数乘以右边表的行数。详细的可以自行google. 查询不重复的记录： 1select distinct 字段 from table ... 排序：默认是升序 1select 字段 from table where ... order by ... asc/desc limit：主要用于分页 1select * from table where ... order by ... asc/desc limit 起始偏移位置，显示条数 聚合： 1select count(*)/avg(..)/sum(...)/max(...)/min(...) from table group by ... having .... 注意这里的having和where的区别：where是对表结果进行筛选，having 是对查询结果进行筛选，与group by 合用 左连接和右连接 左连接意思就是左表中的记录都在，右表没有匹配项就以null显示。记录数等于左表的行数。 右连接与之同理，尽量转为左连接做。 子查询： 1select * from table where ... in (select ....) 所谓子查询就是根据另一个select的结果再进行筛选，常用的是in,not in,=,!=,exits,not exits union 主要用于两张表中的数据的合并： 1select 字段 from table1 union all select 字段 from table2 要想不重复用union]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常见的面试题]]></title>
    <url>%2F2019%2F01%2F25%2Fnetwork%2F8.%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[对这一块整理一些常见的面试题。 1.TCP三次握手、四次挥手 这部分略。前面已经说的很详细，包括握手为什么不是两次、为什么不是四次，为什么挥手要等2MSL的时间。 2.常见的HTTP状态码及其含义 200 OK：正常返回信息 400 Bad Reqest：客户端请求有语法错误，不能被服务器所理解 401 Unauthorized：请求未经授权，这个状态码必须与WWW-Authenticate报头域一起使用 403 Forbidden：服务器收到请求，但是拒绝提供服务 404 Not Found：请求资源不存在 500 Internal Server Error：服务器发生不可预期的错误 503 Server Unavilable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常 3.Get请求和Post请求的区别 Http报文层面：GET将请求信息放在URL，POST则放在报文体中 数据库层面：GET符合幂等性和安全性(查询不会改变数据库)，POST不符合 其他层面：GET可以被缓存、被存储为书签，而POST不行 4.Cookie和Session的区别 对于session，字面上理解是会话，可以理解为用户与服务端一对一的交互。是一个比较抽象的概念。 但是我们常说的session其实是这里抽象概念的一种实现方式罢了，我觉得没有必要咬文嚼字，下面直接从面试角度来分析一下。 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。 这个Session是保存在服务端的，有一个唯一标识，这个唯一标识对应一个用户。在服务端保存Session的方法很多，内存、数据库、文件都有。 服务端解决了用户标识问题，但是服务端怎么知道此时操作浏览器的用户是谁呢？ 这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。 实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端(放在响应头中返回)，需要在 Cookie 里面记录一个Session ID，以后每次请求(请求头)把这个会话ID发送到服务器，我就知道你是谁了。 有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 总结： Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中； Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 下面说一下很常见的一种写法。比如在单体应用中，我此时登陆你的网站了，你可以将我的信息保存在session中： 12User currentUserInfo = userService.getUserByUsernameAndPasswd(username,password);session.setAttribute("currentUser",currentUserInfo); 下次，我就可以在我们之间的会话中随时获取我的个人信息： 1User currentUser = session.getAttribute("currentUser"); 其实这些就是利用存放在Cookie中的JSESSIONID来实现的。 5.HTTP和HRTTPS的关系 来说一下SSL(Security Sockets Layer，安全套接层) 为网络通信提供安全及数据完整性的一种安全协议 是操作系统对外的API，SSL3.0之后更名为TLS 采用身份验证和数据加密保证网络通信的安全和数据的完整性 HTTPS数据传输流程： 浏览器将支持的加密算法信息发送给服务器 服务器选择一套浏览器支持的加密算法，以证书的形式回发浏览器 浏览器验证证书合法性，并结合证书公钥加密信息发给服务器 服务器使用私钥解密信息，验证哈希，加密响应消息回发浏览器 浏览器解密响应消息，并对消息进行验证，之后进行加密交互数据 这个也就不赘述了，下面直接说说区别。 HTTPS需要到CA申请证书，HTTP不需要 HTTPS密文传输，HTTP明文传输 连接方式不同，HTTPS默认使用443端口，HTTP使用80端口 HTTPS=HTTP+加密+认证+完整性保护，更安全 但是仍然存在一定的风险： 浏览器默认填充http://，请求需要进行跳转，有被劫持的风险 可以使用HSTS(HTTP Strict Transport Security)优化（这个还不未主流，面试问的少） Socket简介 我们知道，进程与进程直接的通信最基本的要求是：可以唯一确定进程。 在本地进程通信中，可以用PID来唯一标识一个进程。 但是PID只在本地唯一，网络中PID冲突的几率还是存在的。 我们知道，到IP层就可以唯一定位到一台主机了，TCP层(tcp协议+端口号)可以唯一定位一台主机中的一个进程。 这样，我们可以通过ip地址+协议+端口号可以唯一标识一台主机的一个进程。这样就可以通过socket进行网络通信了。 socket是对TCP/IP协议的抽象，是操作系统对外开放的接口。 socket起源于unix，而unix是遵从一切皆文件的哲学。Socket是一种基于从打开、读/写、关闭的模式实现的。客户端和服务器各自维护一个文件，在连接建立后，可以供对方读取或者读取对方内容。 socket相关题目 编写一个网络程序，有客户端和服务端，客户端向服务端发送一个字符串，服务器收到字符串之后打印到命令行上，然后向客户端返回该字符串的长度，最后，客户端输出服务端返回的该字符串的长度，分别用TCP和UDP两种方式去实现。 代码地址：https://github.com/sunweiguo/TcpAndUdp/]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础核心-理解类、对象、面向对象编程、面向接口编程]]></title>
    <url>%2F2019%2F01%2F24%2Fjava-basic%2FJAVA%E5%9F%BA%E7%A1%80%E6%A0%B8%E5%BF%83-%E7%90%86%E8%A7%A3%E7%B1%BB%E3%80%81%E5%AF%B9%E8%B1%A1%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E3%80%81%E9%9D%A2%E5%90%91%E6%8E%A5%E5%8F%A3%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是类，什么是对象，什么是面向对象编程，什么是面向接口编程。学习面向对象思想的语言，比如java，第一关可能就是要理解这些概念。下面就来好好琢磨一下。 类和对象的概念 首先总结一下：类是一个模板，对象就是用这个模板创造出来的东西。 比如，男孩，他就是一个模板，男的就行，那么对象是什么呢？就是具体某个男孩，比如男孩BOB，男孩fourColor. 请看下面一张图： 男孩女孩是比较抽象的概念，是模板，左边一排就是其具体的一些对象。你看长的都不一样，有的黑，有的白，有的高，有的矮，国家地区也不一样。但是他们都属于男孩或者女孩。 那么同理，人就是一个类，男孩女孩就是人的子类，因为人可能不仅包括男孩女孩，还包括第三性别这个类。 这里还引出了JAVA特性中的继承。继承简单理解就是父类有的东西(访问级别不能是private)的，那都是你的。比如你老爸的房子，就是属于你的，你出入自由。 人还可以分为胖人和瘦人这个子类。所以只要是抽象的模板，就是一个类。 对象：对象是类的一个实例（对象不是找个女朋友），有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。 类：类是一个模板，它描述一类对象的行为和状态。 下面就拿狗这个类来说事。狗是动物这个类的子类。 Java中创建类 构造器方法说明 需要创造一个类对象出来的时候，要用到这个类的构造器方法，那么啥是构造器方法呢？构造器方法就是创造类时的初始化方法，和类同名的方法，你可以在里面写自己的代码 1234567891011121314151617181920//模版class 类名称 &#123; 访问权限 构造方法名称()&#123; &#125;&#125;//例子public class Dog&#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125;&#125; 一个相对比较完整的类 1234567891011121314151617181920212223242526272829//模版class 类名称 &#123; //构造器方法 //声明成员变量---这个变量属于这个类 //声明成员方法 //在方法里面定义的变量是局部变量，区别于成员变量&#125;//例子public class Dog &#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125; //狗的颜色--成员属性 public String color;//一般是private，赋值用set方法，取值用get方法，这里只是演示 //狗的行为，它会叫---成员方法 private void say()&#123; int i = 0;//局部变量 System.out.println("我会叫：汪汪汪~"); &#125;&#125; 创建对象 语法： 类名 对象名 = new 类名() ; 举例： 12Dog fourcolor ; // 先声明一个 Dog 类的对象 fourcolorfourcolor = new Dog(&quot;fourcolor&quot;) ; // 用 new 关键字实例化 Dog 的对象 fourcolor,此时调用构造方法二 通过Dog这个类可以创造出fourcolor对象.下面我才能操作这个对象： 1234//让它的颜色为黑色fourcolor.color = &quot;black&quot;;//让它叫fourcolor.say(); 面向对象 在理解了什么是类，什么是对象，就可以来说说面向对象到底是什么了。 先来说说面向过程，大家都学习过C语言。C语言就是典型的面向过程的语言。 举个例子：要把大象装进冰箱里，这件事，面向过程的程序员是这样思考的： 把冰箱门儿打开。 把大象装进去。 把冰箱门儿关上。 上面的每一件事都用一个函数来实现。抽象为下面三个函数： openTheDoor()； pushElephant()； closeTheDoor()； 这样不挺好的吗？为什么不用面向过程的这种思维来编程呢，还要搞出什么面向对象来。 需求又来啦： 「我要把大象装微波炉里」 「我要把狮子也装冰箱里」 「我要把大象装冰箱，但是门别关，敞着就行」 这个时候，面向过程的程序员就悲剧了，来一个需求我就写一个函数，我还能下班吗？ 面向对象从另一个角度来解决这个问题。它抛弃了函数，把「对象」作为程序的基本单元。 面向对象的世界里，到处都是对象。即：万物皆对象。 比如人这个类，每个具体的人(对象)都要有这样的属性：身高、体重、年龄。每个人都有这样的行为：吃饭、睡觉、上厕所。 那么，这些通用的属性+方法可以构建一个模板：人这个类。因为每个具体的人（对象）都需要这些基本的东西。当然了，每个人具体什么身高、什么体重、吃什么都是不一样的，所以每个对象一般都是不一样的。但是模板是一样的。 那么，回到刚才的需求，面向对象是如何思考这件事的呢？ 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 就是说，我不用亲自管开门的细节，我只要叫他开门即可。 我们创建的对象，应该是刚刚好能做完它能做的事情，不多做，不少做。多做了容易耦合，各种功能杂糅在一个对象里。比如我有一个对象叫「汽车」，可以「行驶」，可以「载人」，现在的需求是要实现「载人飞行」，就不能重用这个对象，必须新定义一个对象「飞机」来做。如果你给「汽车」插上了翅膀，赋予了它「飞行」的能力，那么新来的同学面对你的代码就会莫名其妙，无从下手。 但是不禁要问：怎么实现这种下达命令就可以自动去执行的效果呢？或者说，我怎么知道它有这个功能啊！ 面向接口编程 现在我们把「数据」和「行为」都封装到了对象里，相当于对象成了一个黑匣子，那我们怎么知道对象具有什么样的能力呢？这个问题的关键就是接口。 因为无论是把大象装进洗衣机还是冰箱，都要求洗衣机或者冰箱有开门和关门的功能。这个时候，我们就可以抽象出来一个接口：【自动门】。这个接口里面定义两个能力：【开门】和【关门】。 让洗衣机、冰箱、微波炉这些带门的东西全部实现【自动门】接口。 这个时候，每个具体的实现可能略有不同，比如冰箱开门是往外拽，但是洗衣机开门可能是往上翻盖子。 此时，我有一个需求，把大象放进冰箱。我一看，冰箱实现了【自动门】这个接口，里面有【开门】和【关门】两个方法，ok，我知道冰箱是可以开门和关门了，那就好办了。我直接下达命令即可。还是跟上面一样的步骤. 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，需要将狮子也装冰箱里。那还是一样： 向冰箱下达「开门」的命令。 向狮子下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，我要把大象装冰箱，但是门别关，敞着就行，那就： 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 是不是很方便？冰箱也可以换，我可以换成任何东西，只要实现了这个接口，这些东西就都有这些能力，那我才不管里面到底怎么实现的呢，直接下达【开门】【关门】命令即可。 这也引入了JAVA特性中另一个特性：封装。外界不知道里面实现细节，只需要知道它的功能和入参即可。 这就是面向过程和面向对象编程的区别，也顺带地理解了什么是面向接口编程。这是学习JAVA最基础也是最核心的点。 整理自： https://tryenough.com/java05 http://www.woshipm.com/pmd/294180.html]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串核心一网打尽]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%B8%E5%BF%83%E4%B8%80%E7%BD%91%E6%89%93%E5%B0%BD%2F</url>
    <content type="text"><![CDATA[对字符串中最核心的点：对象创建和动态加入常量池这些点进行深入分析。 比如有两个面试题： Q1：String s = new String(&quot;abc&quot;); 定义了几个对象。 Q2：如何理解String的intern方法？ A1：对于通过 new 产生的对象，会先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象，然后在堆中创建一个常量池中此 “abc” 对象的拷贝对象。所以答案是：一个或两个。如果常量池中原来没有 ”abc”, 就是两个。如果原来的常量池中存在“abc”时，就是一个。 A2：当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； 字面量和运行时常量池 JVM为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化。为了减少在JVM中创建的字符串的数量，字符串类维护了一个字符串常量池。 在JVM运行时区域的方法区中，有一块区域是运行时常量池，主要用来存储编译期生成的各种字面量和符号引用。 了解过JVM就会知道，在java代码被javac编译之后，文件结构中是包含一部分Constant pool的。比如以下代码： 123public static void main(String[] args) &#123; String s = "abc";&#125; 经过编译后，常量池内容如下： 1234567891011Constant pool: #1 = Methodref #4.#20 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #21 // abc #3 = Class #22 // StringDemo #4 = Class #23 // java/lang/Object ... #16 = Utf8 s .. #21 = Utf8 abc #22 = Utf8 StringDemo #23 = Utf8 java/lang/Object 上面的Class文件中的常量池中，比较重要的几个内容： 123#16 = Utf8 s#21 = Utf8 abc#22 = Utf8 StringDemo 上面几个常量中，s就是前面提到的符号引用，而abc就是前面提到的字面量。而Class文件中的常量池部分的内容，会在运行期被运行时常量池加载进去。 new String创建了几个对象 下面，我们可以来分析下String s = new String(&quot;abc&quot;);创建对象情况了。 这段代码中，我们可以知道的是，在编译期，符号引用s和字面量abc会被加入到Class文件的常量池中。由于是new的方式，在类加载期间，先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象。 在运行期间，在堆中创建一个常量池中此 “abc” 对象的拷贝对象。 运行时常量池的动态扩展 编译期生成的各种字面量和符号引用是运行时常量池中比较重要的一部分来源，但是并不是全部。那么还有一种情况，可以在运行期像运行时常量池中增加常量。那就是String的intern方法。 当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； intern()有两个作用，第一个是将字符串字面量放入常量池（如果池没有的话），第二个是返回这个常量的引用。 一个例子： 123456789String s1 = "hello world";String s2 = new String("hello world");System.out.println("s==s1:"+(s==s1));String s3 = new String("hello world").intern();System.out.println("s==s2:"+(s==s2)); 运行结果是： 12s1==s2:falses2==s3:true 你可以简单的理解为String s1 = &quot;hello world&quot;;和String s3 = new String(&quot;hello world&quot;).intern();做的事情是一样的（但实际有些区别，这里暂不展开）。都是定义一个字符串对象，然后将其字符串字面量保存在常量池中，并把这个字面量的引用返回给定义好的对象引用。 对于String s3 = new String(&quot;hello world&quot;).intern();，在不调intern情况，s3指向的是JVM在堆中创建的那个对象的引用的（如s2）。但是当执行了intern方法时，s3将指向字符串常量池中的那个字符串常量。 由于s1和s3都是字符串常量池中的字面量的引用，所以s1==s3。但是，s2的引用是堆中的对象，所以s2!=s1。 intern的正确用法 不知道，你有没有发现，在String s3 = new String(&quot;abc&quot;).intern();中，其实intern是多余的？ 因为就算不用intern，“abc&quot;作为一个字面量也会被加载到Class文件的常量池”&quot;，进而加入到运行时常量池中，为啥还要多此一举呢？到底什么场景下才会用到intern呢? 在解释这个之前，我们先来看下以下代码： 1234String s1 = "hello";String s2 = "world";String s3 = s1 + s2;String s4 = "hello" + "world"; 在经过反编译后，得到代码如下： 1234String s1 = "hello";String s2 = "world";String s3 = (new StringBuilder()).append(s1).append(s2).toString();String s4 = "helloworld"; 这就是阿里巴巴文档里为什么规定循环拼接字符串不准使用&quot;+&quot;而必须使用StringBuilder，因为反编译出的字节码文件显示每次循环都会 new 出一个 StringBuilder 对象，然后进行append 操作，最后通过 toString 方法返回 String 对象，造成内存资源浪费。 不恰当的方式形如： 1234String str = "start";for (int i = 0; i &lt; 100; i++) &#123; str = str + "hello";&#125; 好了，言归正传，可以发现，同样是字符串拼接，s3和s4在经过编译器编译后的实现方式并不一样。s3被转化成StringBuilder及append，而s4被直接拼接成新的字符串。 如果你感兴趣，你还能发现，String s4 = s1 + s2; 经过编译之后，常量池中是有两个字符串常量的分别是 hello、world（其实hello和world是String s1 = &quot;hello&quot;;和String s2 = &quot;world&quot;;定义出来的），拼接结果helloworld并不在常量池中。 如果代码只有String s4 = &quot;hello&quot; + &quot;world&quot;;，那么常量池中将只有helloworld而没有hello和 world。 究其原因，是因为常量池要保存的是已确定的字面量值。也就是说，对于字符串的拼接，纯字面量和字面量的拼接，会把拼接结果作为常量保存到字符串。 如果在字符串拼接中，有一个参数是非字面量，而是一个变量的话，整个拼接操作会被编译成StringBuilder.append，这种情况编译器是无法知道其确定值的。只有在运行期才能确定。 那么，有了这个特性了，intern就有用武之地了。那就是很多时候，我们在程序中用到的字符串是只有在运行期才能确定的，在编译期是无法确定的，那么也就没办法在编译期被加入到常量池中。 这时候，对于那种可能经常使用的字符串，使用intern进行定义，每次JVM运行到这段代码的时候，就会直接把常量池中该字面值的引用返回，这样就可以减少大量字符串对象的创建了。 总结 第一种情况： 12String str1 = "abc"; System.out.println(str1 == "abc"); 栈中开辟一块空间存放引用str1； String池中开辟一块空间，存放String常量&quot;abc&quot;； 引用str1指向池中String常量&quot;abc&quot;； str1所指代的地址即常量&quot;abc&quot;所在地址，输出为true 第二种情况： 12String str2 = new String("abc"); System.out.println(str2 == "abc"); 栈中开辟一块空间存放引用str2； 堆中开辟一块空间存放一个新建的String对象&quot;abc&quot;； 引用str2指向堆中的新建的String对象&quot;abc&quot;； str2所指代的对象地址为堆中地址，而常量&quot;abc&quot;地址在池中，输出为false； 第三、四种情况 1234567891011121314//（3）String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//str1 和 str2 是字符串常量，所以在编译期就确定了。//str3 中有个 str1 是引用，所以不会在编译期确定。//又因为String是 final 类型的，所以在 str1 + "b" 的时候实际上是创建了一个新的对象，在把新对象的引用传给str3。//（4）final String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//这里和(3)的不同就是给 str1 加上了一个final，这样str1就变成了一个常量。//这样 str3 就可以在编译期中就确定了 这里的细节在上面已经详细说明了。 第五种情况 1234String str1 = "ab"；String str2 = new String("ab");System.out.println(str1== str2);//falseSystem.out.println(str2.intern() == str1);//true 整理自： 我终于搞清楚了和String有关的那点事儿 https://www.jianshu.com/p/2624036c9daa]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[关于java字符串不可变特性的深入理解。 什么是不可变对象？ 众所周知， 在Java中， String类是不可变的。那么到底什么是不可变的对象呢？ 可以这样认为：如果一个对象，在它创建完成之后，不能再改变它的状态，那么这个对象就是不可变的。不能改变状态的意思是，不能改变对象内的成员变量，包括基本数据类型的值不能改变，引用类型的变量不能指向其他的对象，引用类型指向的对象的状态也不能改变。 区分对象和对象的引用 对于Java初学者， 对于String是不可变对象总是存有疑惑。看下面代码： 12345String s = "ABCabc"; System.out.println("s = " + s); s = "123456"; System.out.println("s = " + s); 打印结果: 12s = ABCabcs = 123456 首先创建一个 String 对象 s ，然后让 s 的值为 ABCabc ， 然后又让 s 的值为 123456 。 从打印结果可以看出，s 的值确实改变了。那么怎么还说 String 对象是不可变的呢？ 其实这里存在一个误区：s只是一个String对象的引用，并不是对象本身。 对象在内存中是一块内存区，成员变量越多，这块内存区占的空间越大。 引用只是一个4字节的数据，里面存放了它所指向的对象的地址，通过这个地址可以访问对象。 也就是说，s 只是一个引用，它指向了一个具体的对象，当 s=“123456”; 这句代码执行过之后，又创建了一个新的对象“123456”， 而引用s重新指向了这个新的对象，原来的对象“ABCabc”还在内存中存在，并没有改变。内存结构如下图所示： 为什么String对象是不可变的？ 要理解 String 的不可变性，首先看一下 String 类中都有哪些成员变量。 在JDK1.6中，String 的成员变量有以下几个： 1234567891011121314public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 在JDK1.7和1.8中，String 类做了一些改动，主要是改变了substring方法执行时的行为，这和本文的主题不相关。JDK1.7中 String 类的主要成员变量就剩下了两个： 1234567public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 由以上的代码可以看出， 在Java中 String 类其实就是对字符数组的封装。 JDK6中， value是String封装的数组，offset是String在这个value数组中的起始位置，count是String所占的字符的个数。 在JDK7中，只有一个value变量，也就是value中的所有字符都是属于String这个对象的。这个改变不影响本文的讨论。 除此之外还有一个hash成员变量，是该 String 对象的哈希值的缓存，这个成员变量也和本文的讨论无关。在Java中，数组也是对象。 所以value也只是一个引用，它指向一个真正的数组对象。其实执行了 1String s = “ABCabc"; 这句代码之后，真正的内存布局应该是这样的： value，offset和count这三个变量都是private的，并且没有提供setValue， setOffset和setCount等公共方法来修改这些值，所以在String类的外部无法修改String。也就是说一旦初始化就不能修改， 并且在String类的外部不能访问这三个成员。 此外，value，offset和count这三个变量都是final的， 也就是说在 String 类内部，一旦这三个值初始化了， 也不能被改变。所以可以认为 String 对象是不可变的了。 那么在 String 中，明明存在一些方法，调用他们可以得到改变后的值。这些方法包括substring， replace， replaceAll， toLowerCase等。例如如下代码： 1234String a = "ABCabc"; System.out.println("a = " + a); //ABCabca = a.replace('A', 'a'); System.out.println("a = " + a); //aBCabc 那么a的值看似改变了，其实也是同样的误区。再次说明， a只是一个引用， 不是真正的字符串对象，在调用a.replace('A', 'a')时， 方法内部创建了一个新的String对象，并把这个心的对象重新赋给了引用a。String中replace方法的源码可以说明问题： 1234567891011121314151617181920212223242526public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(buf, true);//new出了新的String对象 &#125; &#125; return this;&#125; String对象真的不可变吗？ 从上文可知String的成员变量是private final 的，也就是初始化之后不可改变。那么在这几个成员中， value比较特殊，因为他是一个引用变量，而不是真正的对象。 value是final修饰的，也就是说final不能再指向其他数组对象，那么我能改变value指向的数组吗？ 比如将数组中的某个位置上的字符变为下划线“_”。 至少在我们自己写的普通代码中不能够做到，因为我们根本不能够访问到这个value引用，更不能通过这个引用去修改数组。 那么用什么方式可以访问私有成员呢？ 没错，用反射， 可以反射出String对象中的value属性， 进而改变通过获得的value引用改变数组的结构。下面是实例代码： 123456789101112131415161718192021public static void testReflection() throws Exception &#123; //创建字符串"Hello World"， 并赋给引用s String s = "Hello World"; System.out.println("s = " + s); //Hello World //获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField("value"); //改变value属性的访问权限 valueFieldOfString.setAccessible(true); //获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); //改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println("s = " + s); //Hello_World &#125; 在这个过程中，s始终引用的同一个 String 对象，但是再反射前后，这个 String 对象发生了变化， 也就是说，通过反射是可以修改所谓的“不可变”对象的。但是一般我们不这么做。 这个反射的实例还可以说明一个问题：如果一个对象，他组合的其他对象的状态是可以改变的，那么这个对象很可能不是不可变对象。例如一个Car对象，它组合了一个Wheel对象，虽然这个Wheel对象声明成了private final 的，但是这个Wheel对象内部的状态可以改变， 那么就不能很好的保证Car对象不可变。 参考： https://blog.csdn.net/zhangjg_blog/article/details/18319521]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer拆箱和装箱]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2FInteger%E6%8B%86%E7%AE%B1%E5%92%8C%E8%A3%85%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[由于笔试经常遇到，所以这里整理一下。将Integer这一块一网打尽。 拆箱和装箱 这里以面试笔试经常出现的Integer类型为例，请看下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; /*第一组*/ Integer i = new Integer(128); Integer i2 = 128; System.out.println(i == i2); Integer i3 = new Integer(127); Integer i4 = 127; System.out.println(i3 == i4); /*第二组*/ Integer i5 = 128; Integer i6 = 128; System.out.println(i5 == i6); Integer i7 = 127; Integer i8 = 127; System.out.println(i7 == i8); /*第三组*/ Integer i9 = new Integer(128); int i10 = 128; System.out.println(i9 == i10); Integer i11 = new Integer(127); int i12 = 127; System.out.println(i11== i12); /*第四组*/ Integer i13 = new Integer(128); Integer i14 = Integer.valueOf(128); System.out.println(i13 == i14); Integer i15 = new Integer(127); Integer i16 = Integer.valueOf(127); System.out.println(i13 == i14); /*第五组*/ Integer i17 = Integer.valueOf(128); Integer i18 = 128; System.out.println(i17 == i18); Integer i19 = Integer.valueOf(127); Integer i20 = 127; System.out.println(i19 == i20);&#125; 执行结果为： 1234567891011121314falsefalsefalsetruetruetruefalsefalsefalsetrue 翻开源码(jdk8)，我们可以看到一个私有的静态类，叫做整形缓存。顾名思义，就是缓存某些整型值，我们可以看到，它默认将-128-127之间数字封装成对象，放进一个常量池中，以后定义类似于Integer a = 1里面的a就可以直接从这个常量池中取对象即可，不需要重新new 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 对于Integer.valueOf(): 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 我们可以看到，也是先看看是不是在-128到127之间的范围，是的话就从 cache 中取出相应的 Integer 对象即可。 第一组中 第一种情况，i 是创建的一个Integer的对象，取值是128。i2 是进行自动装箱的实例，因为这里超出了-128到127的范围，所以是创建了新的Integer对象。由于==比较的是地址，所以两者必然不一样。 第二种情况就不一样了，i4是不需要自己new，而是可以直接从缓存中取，但是i3是new出来的，地址还是不一样。 第二组中 第一种情况是都超出范围了，所以都要自己分别去new，所以不一样 第二种情况是在范围内，都去缓存中取，实际上都指向同一个对象，所以一样 第三组中 i10和i12都是int型，i9和i11与它们比较的时候都要自动拆箱，所以比较的是数值，所以都一样 第四组中 与第一组原理一样 四五组中 与第二组原理一样 所以啊，new Integer()是每次都直接new对象出来，而Integer.valueOf()可能会用到缓存，所以后者效率高一点。 总结 int 和 Integer 在进行比较的时候， Integer 会进行拆箱，转为 int 值与int` 进行比较。 Integer 与 Integer 比较的时候，由于直接赋值的时候会进行自动的装箱，那么这里就需要注意两个问题，一个是 -128&lt;= x&lt;=127 的整数，将会直接缓存在 IntegerCache 中，那么当赋值在这个区间的时候，不会创建新的 Integer 对象，而是从缓存中获取已经创建好的 Integer 对象。二：当大于这个范围的时候，直接 new Integer 来创建 Integer 对象。 new Integer(1) 和 Integer a = 1 不同，前者会创建对象，存储在堆中，而后者因为在-128到127的范围内，不会创建新的对象，而是从 IntegerCache 中获取的。那么 Integer a = 128, 大于该范围的话才会直接通过 new Integer(128)创建对象，进行装箱。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务解决方案思考]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F10%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[在分布式系统中，最头疼的就是分布式事务问题，处理起来一定要小心翼翼。由于没有此方面实战，本文就从理论上看看比较好的分布式事务处理方案。 什么是分布式事务 众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？ 比如用户下单过程。当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 方案1：基于可靠消息服务的分布式事务 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程中，如果任务A处理失败，那么需要进入回滚流程: 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 注意，这个方案需要消息队列具有事务消息的能力，阿里的RocketMQ可以实现这个目标。其他的MQ还不行。 方案2：最大努力通知（定期校对） 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。 如果这两步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第一种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。还有其他的一些解决思路，这里就暂时只描述这些。后续再学习。 参考自：https://juejin.im/post/5aa3c7736fb9a028bb189bca]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[库存扣减问题]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F09%E5%BA%93%E5%AD%98%E6%89%A3%E5%87%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[库存扣减问题一直是秒杀中最关键的一个点。如果把控不好，扣成负数，那可就麻烦了，如么如何保证不会出现超卖还能保证性能呢？ 一、扣减库存问题分析 在提交订单的时候，要扣减库存，对于sql，是这么写的： 1update t_stcok set stock = stock-2 where sku_id = 1 首先这条sql存在超卖问题，很有可能会减成负数。可能会改成如下： 1update t_stcok set stock = stock-2 where sku_id = 1 and stock &gt; 2 这样好像解决了超卖问题。但是引入了新的问题。由于库存牵涉进货、补货等系统，所以是个独立的服务。 并且，比如我是通过MQ去通知库存进行扣减库存，但是由于网络抖动，请求扣减库存没有结果，这个时候可能需要进行重试。重试之后，可能成功了，这个时候，有可能这两次都成功了。那么，一个用户买一样东西，但是库存扣了两遍。这就是幂等。如果不做幂等处理，重试会出现上述这种致命问题。 那么如何做到幂等呢？ 实际上就是追求数据一致性。那么就可以考虑锁来保证，比如我这里用乐观锁来实现： 123select stock,version from t_stock;if(stock &gt; 用户购买数量) update t_stcok set stock = stock-2 where sku_id = 1 and version = last_version 但是，一旦出现并发，那么可能这个用户是执行update失败的，所以还需要去重试(guava retry或者spring retry都可以优雅地实现重试)，直到成功或者库存已经不足。 那么，在少量并发的情况下，可以考虑乐观锁，要不然会大量失败，此时需要用悲观锁： 12select * from t_stock for update;下面执行update操作。。。 在一个事务内，第一句为select for update，那么这一行数据就会被本线程锁住，整个事务执行完才能允许其他线程进来。 存在的问题：一个线程锁住这行数据，那么其他线程都要等待，效率很低。 那么，如何保证数据一致性，还可以提高效率呢？ 对于扣减库存，往往是先在redis中进行扣减库存。redis是单线程，是高速串行执行，不存在并发问题。 如果是单机redis，可以在同一个事务中保证一次性执行: 12345watch stockmultiif stock &gt; count stock = stock - count;exec 但是不能在集群中用（分布在不同节点上时），所以用watch不通用。 redis都是原子操作，比如自增:incrby，用这个就可以判断库存是否够。就是所谓的redis预减库存。 但是在实际中，库存表里有两个字段：库存和锁定库存。 锁定库存是表示多少用户真正下单了，但是还没有支付。锁定库存+库存=总库存，等用户真正支付之后，就可以将锁定库存减掉。那么，此时，redis中需要存库存和锁定库存这两个值，上面单一的原子操作就不行了。 解决方案：redis+lua 为什么要用lua呢？可以用lua将一系列操作封装起来执行，输入自己的参数即可。lua脚本在redis中执行是串行的、原子性的。 OK，下面就实战一波：根据skuId查询缓存中的库存值。 二、查询库存（设置库存） 首先，我们要明确一点，redis中的库存初始值是由后台的系统人工提前配置好的，在进行商品销售时（用户下单时），直接从redis中先进行库存的扣减。 这里呢，我们没有进行初始化，而是在程序中进行判断：如果redis已经有了这个库存值，就将他查询出来返回；否则，就去数据库查询，然后对redis进行初始化。 这里的一个问题是：如果存在并发问题，但是我们初始化两个值（库存值和库存锁定值），这里采用lua脚本，在lua脚本中完成初始化，并且对于两个用户同时进行初始化库存的问题，可以在lua中进行判断,因为redis是单线程，lua也是单线程，不用担心会同时初始化两次。 下面首先写一个接口，根据skuid查询库存(库存和锁定库存)： 123456789101112@RequestMapping("/query/&#123;skuId&#125;")public ApiResult&lt;Stock&gt; queryStock(@PathVariable long skuId)&#123; ApiResult&lt;Stock&gt; result = new ApiResult(Constants.RESP_STATUS_OK,"库存查询成功"); Stock stock = new Stock(); stock.setSkuId(skuId); int stockCount = stockService.queryStock(skuId); stock.setStock(stockCount); result.setData(stock); return result;&#125; service层： 123456789101112131415161718192021222324252627282930@Overridepublic int queryStock(long skuId) &#123; //先查redis Stock stock ; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+skuId; String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+skuId; //只需要查询一个即可，比如我这里只查询库存就行 Object stockObj = redisTemplate.opsForValue().get(stockKey); Integer stockInRedis = null ; if(stockObj!=null)&#123; stockInRedis = Integer.valueOf(stockObj.toString()); &#125; //没有，那么我就需要将数据库中的数据初始化到redis中 if(stockInRedis==null)&#123; //去数据库查询 然后对redis进行初始化 stock = stockMapper.selectBySkuId(skuId); //两个key和两个库存值通过lua脚本塞到redis中 //这里如果发生两个用户并发初始化redis，脚本中会进行判断，如果已经初始化了，脚本就会停止执行 // 设置库存不应该在这配置，应该是后台管理系统进行设置，所以正常情况下，这里redis中应该是必然存在的 //如果是在后台配置，就没有必要这么复杂了 redisUtils.skuStockInit(stockKey,stockLockKey,stock.getStock().toString(),stock.getLockStock().toString()); &#125;else&#123; return stockInRedis;//缓存中有就直接返回 &#125; //缓存结果可能会返回设置不成功，所以还是返回数据库查询结果 return stock.getStock();&#125; 那么这个工具类为： 123456789101112131415161718192021222324252627282930313233/** * 查看redis是否已经初始化好库存初始值，没有就初始化 */public static final String STOCK_CACHE_LUA = "local stock = KEYS[1] " + "local stock_lock = KEYS[2] " + "local stock_val = tonumber(ARGV[1]) " + "local stock_lock_val = tonumber(ARGV[2]) " + "local is_exists = redis.call(\"EXISTS\", stock) " + "if is_exists == 1 then " + " return 0 " + "else " + " redis.call(\"SET\", stock, stock_val) " + " redis.call(\"SET\", stock_lock, stock_lock_val) " + " return 1 " + "end"; /** * @Description 缓存sku库存 以及锁定库存 */public boolean skuStockInit(String stockKey,String stockLockKey,String stock,String stockLock)&#123; //用jedis去执行lua脚本 输入的参数要注意顺序 都是写死的 第一组是key，第二组是stock Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_CACHE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stock, stockLock))); &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 对于lua脚本进行稍微的解释一下： 12345678910111213141516171819202122//第一组数据是key数组；第二组数据是args数组，是与key数组对应的值，就是库存//我们这里第一组为[stockKey,stockLockKey],就是存在redis中的名字，这里是在service层中定义好了//第二组为[50,0]，这个值就是可以从数据库表t_stock中查询出来的//因为执行这段lua脚本的话，说明redis中没有缓存的数据，所以需要先查询数据库，然后将缓存设置好//lua中定义变量用locallocal stock = KEYS[1]local stock_lock = KEYS[2]local stock_val = tonumber(ARGV[1])local stock_lock_val = tonumber(ARGV[2])//再查询一遍缓存是否存在，防止两个线程同时进来设置缓存//存在就不用设置缓存了，否则就设置缓存local is_exists = redis.call("EXISTS", stock)if is_exists == 1 then return 0else redis.call("SET", stock, stock_val) redis.call("SET", stock_lock, stock_lock_val) return 1end 那么，启动工程mama-buy-stock：假如我去查询skuId=1的商品： 第一次库存不存在，那么就会去查询数据库： 我们再来看看redis中的数据： 三、扣减库存 下面来看看扣减库存是如何实现的。因为提交订单后，往往是不止一件商品的，往往购物车内有很多件商品，同时过来，假设有五件商品，但是其中只有一件暂时没有库存了，那么我还是希望其他的四件商品能够卖出去，只是没有库存的商品就不算钱了。所以扣减库存用一个map来装，即Map&lt;skuId,count&gt; controller层： 1234567@RequestMapping("/reduce")public ApiResult&lt;Map&lt;Long,Integer&gt;&gt; reduceStock(@RequestBody List&lt;StockReduce&gt; stockReduceList)&#123; ApiResult result = new ApiResult(Constants.RESP_STATUS_OK,"库存扣减成功"); Map&lt;Long,Integer&gt; resultMap = stockService.reduceStock(stockReduceList); result.setData(resultMap); return result;&#125; service层： 1234567891011121314151617181920212223242526272829303132333435@Override@Transactionalpublic Map&lt;Long, Integer&gt; reduceStock(List&lt;StockReduce&gt; stockReduceList) &#123; Map&lt;Long, Integer&gt; resultMap = new HashMap&lt;&gt;(); //遍历去减redis中库存，增加锁定库存 stockReduceList.stream().forEach(param -&gt; &#123; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+param.getSkuId(); String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+param.getSkuId(); Object result = redisUtils.reduceStock(stockKey, stockLockKey, param.getReduceCount().toString(),//incrby一个负数，就是减 String.valueOf(Math.abs(param.getReduceCount())));//incrby一个正数，就是加 if(result instanceof Long)&#123; //库存不存在或者不足 扣减失败 sku下单失败 记录下来 resultMap.put(param.getSkuId(),-1); &#125;else if (result instanceof List)&#123; //扣减成功 记录扣减流水 List resultList = ((List) result); int stockAftChange = ((Long)resultList.get(0)).intValue(); int stockLockAftChange = ((Long) resultList.get(1)).intValue(); StockFlow stockFlow = new StockFlow(); stockFlow.setOrderNo(param.getOrderNo()); stockFlow.setSkuId(param.getSkuId()); stockFlow.setLockStockAfter(stockLockAftChange); stockFlow.setLockStockBefore(stockLockAftChange+param.getReduceCount()); stockFlow.setLockStockChange(Math.abs(param.getReduceCount())); stockFlow.setStockAfter(stockAftChange); stockFlow.setStockBefore(stockAftChange+Math.abs(param.getReduceCount())); stockFlow.setStockChange(param.getReduceCount()); stockFlowMapper.insertSelective(stockFlow); resultMap.put(param.getSkuId(),1); &#125; &#125;); return resultMap;&#125; 对于redis的操作，基本与上一致： 12345678910111213141516171819202122232425262728293031323334/** * @Description 扣减库存lua脚本 * @Return 0 key不存在 错误 -1 库存不足 返回list 扣减成功 */public static final String STOCK_REDUCE_LUA= "local stock = KEYS[1]\n" + "local stock_lock = KEYS[2]\n" + "local stock_change = tonumber(ARGV[1])\n" + "local stock_lock_change = tonumber(ARGV[2])\n" + "local is_exists = redis.call(\"EXISTS\", stock)\n" + "if is_exists == 1 then\n" + " local stockAftChange = redis.call(\"INCRBY\", stock,stock_change)\n" + " if(stockAftChange&lt;0) then\n" + " redis.call(\"DECRBY\", stock,stock_change)\n" + " return -1\n" + " else \n" + " local stockLockAftChange = redis.call(\"INCRBY\", stock_lock,stock_lock_change)\n" + " return &#123;stockAftChange,stockLockAftChange&#125;\n" + " end " + "else \n" + " return 0\n" + "end"; public Object reduceStock(String stockKey,String stockLockKey,String stockChange,String stockLockChange)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_REDUCE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stockChange, stockLockChange))); &#125;); return result;&#125; 此时，一旦数据库发生异常，那么就会回滚，但是redis中是无法回滚的。这个问题不用担心，因为数据库发生异常是及其严重的问题，是很少会发生的，一旦发生，只需要去这个流水的表中去查看情况，然后去执行脚本去初始化这个redis即可。所以是可以补救的。 但是接口的幂等性还没有做。重复尝试调用这个接口（通常是发生在MQ的失败重传机制，客户端的连续点击一般是可以避免的），可能会重复减redis库存并且重复地去插入流水记录。这个问题该如何解决呢？ 四、redis分布式锁来实现幂等性 主流的方案，比如有用一张表来控制，比如以这个orderID为唯一主键，一旦插入成功，就可以根据这个唯一主键的存在与否判断是否为重复请求（也就是说，这里的扣减库存和插入去重表放在一个事务里，去重表中有一个字段为orderId，全局唯一不重复，用唯一索引进行约束，那么插入的时候判断这个去重表是否可以插入成功，如果不成功，那么数据库操作全部回滚）。 可以用redis分布式锁给这个订单上锁。以订单id为锁，不会影响其他线程来扣减库存，所以不影响性能。 针对这个订单，第一次肯定是可以去扣减库存的，但是第二次再接收到这个请求，那么就要返回已经成功了，不要再重复扣减。 对于reduceStock()这个方法最前面增加锁： 123456789//防止扣减库存时MQ正常重试时的不幂等//以订单ID 加个缓存锁 防止程序短时间重试 重复扣减库存 不用解锁 自己超时Long orderNo = stockReduceList.get(0).getOrderNo();boolean lockResult = redisUtils.distributeLock(Constants.ORDER_RETRY_LOCK+orderNo.toString(),orderNo.toString(),300000);if(!lockResult)&#123; //锁定失败 重复提交 返回一个空map return Collections.EMPTY_MAP;&#125;... 123456789101112131415161718192021222324252627282930313233343536373839404142private static final String LOCK_SUCCESS = "OK";private static final String SET_IF_NOT_EXIST = "NX";private static final String SET_WITH_EXPIRE_TIME = "PX";private static final Long EXCUTE_SUCCESS = 1L;/**lua脚本 在redis中 lua脚本执行是串行的 原子的 */private static final String UNLOCK_LUA= "if redis.call('get', KEYS[1]) == ARGV[1] then " + " return redis.call('del', KEYS[1]) " + "else " + " return 0 " + "end";/** * @Description 获取分布式锁 * @Return boolean */public boolean distributeLock(String lockKey, String requestId, int expireTime)&#123; String result = redisTemplate.execute((RedisCallback&lt;String&gt;) redisConnection -&gt; &#123; JedisCommands commands = (JedisCommands)redisConnection.getNativeConnection(); return commands.set(lockKey,requestId,SET_IF_NOT_EXIST,SET_WITH_EXPIRE_TIME,expireTime);//一条命令实现setnx和setexpire这些操作，原子性 &#125;); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125;/** * @Description 释放分布式锁 * @Return boolean */public boolean releaseDistributelock(String lockKey, String requestId)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(UNLOCK_LUA, Collections.singletonList(lockKey), Collections.singletonList(requestId));//lua脚本中原子性实现：get查询和delete删除这两个操作 &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 注意，这里不需要我们主动去释放分布式锁，只要设置一个大于重试时间的过期时间即可。让它自己删除。 注意redis在集群下做分布式锁，最好要用Redission。这里如果用于集群，如何lua脚本在一个事务里同时操作多个key的时候，如果要保证这个事务生效，就需要保证这几个key都要在同一个节点上。但是，比如我们这里的两个key： 12public static final String CACHE_PRODUCT_STOCK = "product:stock";public static final String CACHE_PRODUCT_STOCK_LOCK = "product:stock:lock"; 因为我们这里要同时对库存和锁定库存这两个key进行操作，需要放在一个事务内执行，不处理的话，一旦他们不在一个节点，那么事务就不会生效，解决方案： 12public static final String CACHE_PRODUCT_STOCK = "&#123;product:stock&#125;";public static final String CACHE_PRODUCT_STOCK_LOCK = "&#123;product:stock&#125;:lock"; 如果加上花括号，那么在进行计算hash值的时候，他们两就会是一样的，会被投放到同一个slot中，自然就保证了在同一个节点上。 五、测试一下]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK平台搭建]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F08ELK%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[因为要完成产品的全文搜索这个功能，所以需要准备一下ES的环境。本节安装ELK。 ELK由Elasticsearch、Logstash和Kibana三部分组件组成。 前言 Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 简单来说，他是个全文搜索引擎，可以快速地储存、搜索和分析海量数据。 Logstash是一个完全开源的工具，它可以把分散的、多样化的日志日志，或者是其他数据源的数据信息进行收集、分析、处理，并将其存储供以后使用。 Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。 你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。 你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。 Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。 一、安装ES 1.1 首先是安装JDK： 12345cd /opt/wget --no-cookies --no-check-certificate --header &quot;Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie&quot; &quot;http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz&quot;tar xzf jdk-8u141-linux-x64.tar.gz 1.2 添加环境变量： 123456789101112vim /etc/profileJAVA_HOME=/opt/jdk1.8.0_141JAVA_JRE=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATHsource /etc/profilejava -version 1.3 下载6.2.4版本： 123456wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gztar -xzvf elasticsearch-6.2.4.tar.gztar -zxvf elasticsearch-6.2.4.tar.gzmv elasticsearch-6.2.4 elasticsearch 1.4 配置sysctl.conf 1234567891011#修改sysctl配置vim /etc/sysctl.conf #添加如下配置vm.max_map_count=262144 #让配置生效sysctl -p #查看配置的数目sysctl -a|grep vm.max_map_count 1.5 elasticsearch从5.0版本之后不允许root账户启动 1234567891011121314151617181920#添加用户adduser dev #设定密码passwd dev #添加权限chown -R dev /opt/elasticsearch #切换用户su dev #查看当前用户who am i #启动./elasticsearch/bin/elasticsearch #后台启动./elasticsearch/bin/elasticsearch -d 1.6 配置limits.conf 123456789101112131415vim /etc/security/limits.conf 把* soft nofile 65535* hard nofile 65535 改为* soft nofile 65536* hard nofile 65536 #切换用户su dev #查看配置是否生效ulimit -Hn 1.7 配置所有用户访问 1vim /opt/elasticsearch/config/elasticsearch.yml 1.8 添加一下内容 1network.host: 0.0.0.0 1.9 重启 12ps -ef | grep elastickill -9 xxxx 1.10 测试： 1curl http://localhost:9200/ 显示： 123456789101112131415&#123; &quot;name&quot; : &quot;MmiaBfA&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;zjX-q5PDRLyrWMy5TiBDkw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.2.4&quot;, &quot;build_hash&quot; : &quot;ccec39f&quot;, &quot;build_date&quot; : &quot;2018-04-12T20:37:28.497551Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.2.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 就说明成功了。 二、安装Kibana 6.2.4 1234567wget https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gztar -zxvf kibana-6.2.4-linux-x86_64.tar.gzmv kibana-6.2.4-linux-x86_64 kibanavim /opt/kibana/config/kibana.yml 2.1 添加以下内容： 123server.port: 5601server.host: &quot;0.0.0.0&quot;elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 2.2 切换到bin目录下，启动即可。 12345#不能关闭终端./kibana #可关闭终端nohup ./kibana &amp; 2.3 开放防火墙和安全组对应的这个端口 浏览器访问：http://106.14.163.235:5601 看到一个控制台页面就成功啦。 2.4 关闭这个进程 1234567891011121314ps -ef|grep kibana ps -ef|grep 5601 都找不到 尝试 使用 fuser -n tcp 5601 kill -9 端口 启动即可 ./kibana或者去这个目录下的.out日志中可以看到看到它占用的pid 三、logstash 1234567891011# 下载wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.4.tar.gz# 解压tar -zxvf logstash-6.2.4.tar.gz# 重命名mv logstash-6.2.4.tar.gz logstash# 进入cd logstash 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 新建一个配置文件 我这里是mysqltones.confinput &#123; stdin &#123; &#125; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/mama-buy-trade&quot; jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;root&quot; jdbc_driver_library =&gt; &quot;/opt/logstash/mysql-connector-java-5.1.46-bin.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; # mysql文件, 也可以直接写SQL语句在此处，如下： statement =&gt; &quot;SELECT * from t_product&quot; # statement_filepath =&gt; &quot;/opt/logstash/conf/jdbc.sql&quot; # 这里类似crontab,可以定制定时操作，比如每10分钟执行一次同步(分 时 天 月 年) schedule =&gt; &quot;*/10 * * * *&quot; type =&gt; &quot;jdbc&quot; # 是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中 record_last_run =&gt; &quot;true&quot; # 是否需要记录某个column 的值,如果record_last_run为真,可以自定义我们需要 track 的 column 名称，此时该参数就要为 true. 否则默认 track 的是 timestamp 的值. use_column_value =&gt; &quot;true&quot; # 如果 use_column_value 为真,需配置此参数. track 的数据库 column 名,该 column 必须是递增的. 一般是mysql主键 tracking_column =&gt; &quot;id&quot; last_run_metadata_path =&gt; &quot;/opt/logstash/conf/last_id&quot; # 是否清除 last_run_metadata_path 的记录,如果为真那么每次都相当于从头开始查询所有的数据库记录 clean_run =&gt; &quot;false&quot; # 是否将 字段(column) 名称转小写 lowercase_column_names =&gt; &quot;false&quot; &#125;&#125;# 此处我不做过滤处理filter &#123;&#125;output &#123; # 输出到elasticsearch的配置 elasticsearch &#123; hosts =&gt; [&quot;127.0.0.1:9200&quot;] index =&gt; &quot;jdbc&quot; # 将&quot;_id&quot;的值设为mysql的autoid字段 document_id =&gt; &quot;%&#123;id&#125;&quot; template_overwrite =&gt; true &#125; # 这里输出调试，正式运行时可以注释掉 stdout &#123; codec =&gt; json_lines &#125;&#125; 12# 启动./bin/logstash -f ./mysqltones.conf 看到这个就说明成功了： 安装mysql数据库 这一步要在执行logstash之前搞定，我的是阿里云centos7.3版本，mysql版本是5.7，安装过程如下： 123456789101112131415161718192021222324252627282930313233343536373839# 下载MySQL源安装包: wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm# 安装MySQL源：yum localinstall mysql57-community-release-el7-8.noarch.rpm # 检查MySQL源安装情况： yum repolist enabled | grep &quot;mysql.*-community.*&quot;# 安装MySQL: yum install mysql-community-server# 启动MySQL: systemctl start mysqld# 查看MySQL状态: systemctl status mysqld# 设置开机启动MySQL：systemctl enable mysqld systemctl daemon-reload# 查找并修改MySQL默认密码（注意密码要符合规范，否则会失败）：grep &apos;temporary password&apos; /var/log/mysqld.log mysql -uroot -p alter user root@localhost identified by &apos;你的新密码&apos;;# 远程连接测试添加远程账户：GRANT ALL PRIVILEGES ON *.* TO &apos;用户&apos;@&apos;%&apos; IDENTIFIED BY &apos;密码&apos; WITH GRANT OPTION;# 立即生效：flush privileges;# 退出MySQL：exit# 最后远程将数据给导入数据库 安装分词器 ik_max_word是分词比较细腻的一款，我们就用它来做分词，首先需要安装一下： 12345678# 直接安装./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.4/elasticsearch-analysis-ik-6.2.4.zip # 重新启动ESps -ef | grep elastickill -9 xxxxsu dev./bin/elasticsearch -d 对这个分词器在kibana中进行测试： 下面结合数据库模拟一下：]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Curator]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F07Curator%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[从技术角度出发，注册一个网站，再高并发的时候，有可能出现用户名重复这样的问题（虽然一般情况下不会出现这种问题），如何解决呢？ 从数据库角度，对于单表，我可以用select .. for update悲观锁实现，或者用version这种乐观锁的思想。 更好的方法是将这个字段添加唯一索引，用数据库来保证不会重复。一旦插入重复，那么就会抛出异常，程序就可以捕获到。 但是，假如我们这里分表了，以上都是针对单表，第一种方案是锁表，不行，设置唯一索引是没有用。怎么办呢？ 解决方案：用ZK做一个分布式锁。 首先准备一个ZK客户端，用的是Curator来连接我们的ZK： 1234567891011121314151617181920@Componentpublic class ZkClient &#123; @Autowired private Parameters parameters; @Bean public CuratorFramework getZkClient()&#123; CuratorFrameworkFactory.Builder builder= CuratorFrameworkFactory.builder() .connectString(parameters.getZkHost()) .connectionTimeoutMs(3000) .retryPolicy(new RetryNTimes(5, 10)); CuratorFramework framework = builder.build(); framework.start(); return framework; &#125;&#125; 注册用一个分布式锁来控制： 1234567891011121314151617181920212223242526272829303132333435@Overridepublic void registerUser(User user) throws Exception &#123; InterProcessLock lock = null; try&#123; lock = new InterProcessMutex(zkClient, Constants.USER_REGISTER_DISTRIBUTE_LOCK_PATH); boolean retry = true; do&#123; if (lock.acquire(3000, TimeUnit.MILLISECONDS))&#123; //查询重复用户 User repeatedUser = userMapper.selectByEmail(user.getEmail()); if(repeatedUser!=null)&#123; throw new MamaBuyException("用户邮箱重复"); &#125; user.setPassword(passwordEncoder.encode(user.getPassword())); user.setNickname("码码购用户"+user.getEmail()); userMapper.insertSelective(user); //跳出循环 retry = false; &#125; //可以适当休息一会...也可以设置重复次数，不要无限循环 &#125;while (retry); &#125;catch (Exception e)&#123; log.error("用户注册异常",e); throw e; &#125;finally &#123; if(lock != null)&#123; try &#123; lock.release(); log.info(user.getEmail()+Thread.currentThread().getName()+"释放锁"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 思路非常简单，就是先尝试上锁，即acquire，但是有可能失败，所以这里用一个超时时间，即3000ms之内上不了锁就失败，进入下一次循环。最后释放锁即可。 ok，这里要来说说ZK实现分布式锁了。这里用了开源客户端Curator，他对于实现分布式锁进行了封装，但是，我还是想了解一下它的实现原理： 每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。 也就是说，最小的那个节点就是Leader，进来判断是不是为那个节点，是的话就可以获取到锁，反之不行。 为什么不能通过大家一起创建节点，如果谁成功了就算获取到了锁。 多个client创建一个同名的节点，如果节点谁创建成功那么表示获取到了锁，创建失败表示没有获取到锁。 答：使用临时顺序节点可以保证获得锁的公平性，及谁先来谁就先得到锁，这种方式是随机获取锁，会造成无序和饥饿。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Session]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F06Spring%20Session%2F</url>
    <content type="text"><![CDATA[在单体应用中，我们经常用http session去管理用户信息，但是到了分布式环境下，显然是不行的，因为session对于不同的机器是隔离的，而http本身是无状态的，那么就无法判断出用户在哪一个服务器上登陆的。这个时候就需要有一个独立的地方存储用户session。spring session可以做到无代码侵入的方式实现分布式session存储。 在spring boot开发中，我们先注册相应bean并且打开相应的注解： 1234567891011121314151617181920212223242526272829@Configuration@EnableRedisHttpSession //(maxInactiveIntervalInSeconds = 604800)//session超时public class HttpSessionConfig &#123; @Autowired private Parameters parameters; @Bean public HttpSessionStrategy httpSessionStrategy()&#123; return new HeaderHttpSessionStrategy(); &#125; @Bean public JedisConnectionFactory connectionFactory()&#123; JedisConnectionFactory connectionFactory = new JedisConnectionFactory(); String redisHost = parameters.getRedisNode().split(":")[0]; int redisPort = Integer.valueOf(parameters.getRedisNode().split(":")[1]); connectionFactory.setTimeout(2000); connectionFactory.setHostName(redisHost); connectionFactory.setPort(redisPort);// connectionFactory.setPassword(parameters.getRedisAuth()); return connectionFactory; &#125;&#125; ok，这样子其实就配置好了，一开始我也云里雾里的，这是啥玩意？ 其实官网的文档中讲的是最准确的。所以还是官网看看吧！ ok，来spring session的官网(https://spring.io/projects/spring-session)来看看把，我们来看看1.3.4GA版本的文档(https://docs.spring.io/spring-session/docs/1.3.4.RELEASE/reference/html5/#httpsession-rest). spring session可以存在很多介质中，比如我们的数据源，比如redis，甚至是mongodb等。但是我们常用的是存在redis中，结合redis的过期机制来做。 所以其实我们只要关心如何跟redis整合，以及restful接口。 我们可以看到一开始文档就告诉我们要配置一下HttpSessionStrategy和存储介质。从HttpSessionStrategy语义就能大致看出配置的是它的策略，是基于header的策略。这个是什么意思，下面会提到。 那么我们就来看看文档吧！ 好了，我们知道了它的基本原理，下面来看看是如何在restful接口中实现用户session的管理的： 也就是说要想在restful接口应用中用这种方式，直接告诉spring session:return new HeaderHttpSessionStrategy();即可。进入源码我们就会知道，它默认给这个header里面放置的一条类似于token的名字是private String headerName = &quot;x-auth-token&quot;;。 那么在用户登陆成功之后，到底存到是什么呢，先来看看响应数据的header里面是什么： 这一串数字正好可以跟redis中对应上，我们可以先来redis中看看到底在里面存储了啥玩意： 我们已经看到了想要看到的一串字符串，这里解释一下redis中存储的东西： spring:session是默认的Redis HttpSession前缀（redis中，我们常用’:’作为分割符） 每一个session都会有三个相关的key，第一个key(spring:session:sessions:37...)最为重要，它是一个HASH数据结构，将内存中的session信息序列化到了redis中。如本项目中用户信息,还有一些meta信息，如创建时间，最后访问时间等。 另外两个key，一个是spring:session:expiration，还有一个是spring:session:sessions:expires，前者是一个SET类型，后者是一个STRING类型，可能会有读者发出这样的疑问，redis自身就有过期时间的设置方式TTL，为什么要额外添加两个key来维持session过期的特性呢？redis清除过期key的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致session不被清除。于是引入了专门的expiresKey，来专门负责session的清除，包括我们自己在使用redis时也需要关注这一点。 这样子，就可以用独立的redis来存储用户的信息，通过前端传来的header里面的token，就可以到redis拿出当前登陆用户的信息了。 OK，在解决了spring session的问题之后，下面就可以来实现登陆啦： controller: 1234567891011121314@RequestMapping("login")public ApiResult login(@RequestBody @Valid User user, HttpSession session)&#123; ApiResult&lt;UserElement&gt; result = new ApiResult&lt;&gt;(Constants.RESP_STATUS_OK,"登录成功"); UserElement ue= userService.login(user); if(ue != null)&#123; if(session.getAttribute(Constants.REQUEST_USER_SESSION) == null)&#123; session.setAttribute(Constants.REQUEST_USER_SESSION,ue); &#125; result.setData(ue); &#125; return result;&#125; 就跟以前一样，将session直接存进去就可以了。 12345678910111213141516171819202122@Overridepublic UserElement login(User user) &#123; UserElement ue = null; User userExist = userMapper.selectByEmail(user.getEmail()); if(userExist != null)&#123; //对密码与数据库密码进行校验 boolean result = passwordEncoder.matches(user.getPassword(),userExist.getPassword()); if(!result)&#123; throw new MamaBuyException("密码错误"); &#125;else&#123; //校验全部通过，登陆通过 ue = new UserElement(); ue.setUserId(userExist.getId()); ue.setEmail(userExist.getEmail()); ue.setNickname(userExist.getNickname()); ue.setUuid(userExist.getUuid()); &#125; &#125;else &#123; throw new MamaBuyException("用户不存在"); &#125; return ue;&#125;]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式ID生成策略]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F05%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[分布式环境下如何保证ID的不重复呢？一般我们可能会想到用UUID来实现嘛。但是UUID一般可以获取当前时间的毫秒数再加点随机数，但是在高并发下仍然可能重复。最重要的是，如果我要用这种UUID来生成分表的唯一ID的话，重复不谈，这种随机的字符串对于我们的innodb存储引擎的插入效率是很低的。所以我们生成的ID如果作为主键，最好有两种特性：分布式唯一和有序。 唯一性就不用说了，有序保证了对索引字段的插入的高效性。我们来具体看看ShardingJDBC的分布式ID生成策略是如何保证。 snowflake算法 sharding-jdbc的分布式ID采用twitter开源的snowflake算法，不需要依赖任何第三方组件，这样其扩展性和维护性得到最大的简化；但是snowflake算法的缺陷（强依赖时间，如果时钟回拨，就会生成重复的ID）。 雪花算法是由Twitter公布的分布式主键生成算法，它能够保证不同进程主键的不重复性，以及相同进程主键的有序性。 在同一个进程中，它首先是通过时间位保证不重复，如果时间相同则是通过序列位保证。 同时由于时间位是单调递增的，且各个服务器如果大体做了时间同步，那么生成的主键在分布式环境可以认为是总体有序的，这就保证了对索引字段的插入的高效性。例如MySQL的Innodb存储引擎的主键。 使用雪花算法生成的主键，二进制表示形式包含4部分，从高位到低位分表为：1bit符号位、41bit时间戳位、10bit工作进程位以及12bit序列号位。 雪花算法主键的详细结构见下图。 符号位(1bit) 预留的符号位，恒为零。 时间戳位(41bit) 41位的时间戳可以容纳的毫秒数是2的41次幂，一年所使用的毫秒数是：365 * 24 * 60 * 60 * 1000。通过计算可知： 1Math.pow(2, 41) / (365 * 24 * 60 * 60 * 1000L); 结果约等于69.73年。ShardingSphere的雪花算法的时间纪元从2016年11月1日零点开始，可以使用到2086年，相信能满足绝大部分系统的要求。 工作进程位(10bit) 该标志在Java进程内是唯一的，如果是分布式应用部署应保证每个工作进程的id是不同的。该值默认为0，可通过调用静态方法DefaultKeyGenerator.setWorkerId()设置。 序列号位(12bit) 该序列是用来在同一个毫秒内生成不同的ID。如果在这个毫秒内生成的数量超过4096(2的12次幂)，那么生成器会等待到下个毫秒继续生成。 时钟回拨 服务器时钟回拨会导致产生重复序列，因此默认分布式主键生成器提供了一个最大容忍的时钟回拨毫秒数。 如果时钟回拨的时间超过最大容忍的毫秒数阈值，则程序报错；如果在可容忍的范围内，默认分布式主键生成器会等待时钟同步到最后一次主键生成的时间后再继续工作。 最大容忍的时钟回拨毫秒数的默认值为0，可通过调用静态方法DefaultKeyGenerator.setMaxTolerateTimeDifferenceMilliseconds()设置。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springMVC全局异常+spring包扫描包隔离+spring事务传播]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F04springMVC%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%2Bspring%E5%8C%85%E6%89%AB%E6%8F%8F%E5%8C%85%E9%9A%94%E7%A6%BB%2Bspring%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[在开发中，springMVC全局异常+spring包扫描包隔离+spring事务传播这三个不可能不会遇到。下面来好好说说他们吧。 1、全局异常引入原因 假设在我们的login.do的controller方法中第一行增加一句： 1int i = 1/0; 重新启动服务器进行用户登录操作，那么就会抛出异常： 123java.lang.ArithmeticException: / by zero com.swg.controller.portal.UserController.login(UserController.java:37) ...其他的堆栈信息 这些信息会直接显示在网页上，如果是关于数据库的错误，同样，会详细地将数据库中的字段都显示在页面上，这对于我们的项目来说是存在很大的安全隐患的。这个时候，需要用全局异常来处理，如果发生异常，我们就对其进行拦截，并且在页面上显示我们给出的提示信息。 对于SpringBoot，一般全局异常是: 1234567891011121314151617@ControllerAdvice@ResponseBody@Slf4jpublic class ExceptionHandlerAdvice &#123; @ExceptionHandler(Exception.class) public ServerResponse handleException(Exception e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(Constants.RESP_STATUS_INTERNAL_ERROR,"系统异常，请稍后再试"); &#125; @ExceptionHandler(SnailmallException.class) public ServerResponse handleException(SnailmallException e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(e.getExceptionStatus(),e.getMessage()); &#125;&#125; 2、引入全局异常 12345678910111213@Slf4j@Componentpublic class ExceptionResolver implements HandlerExceptionResolver&#123; @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) &#123; log.error("exception:&#123;&#125;",httpServletRequest.getRequestURI(),e); ModelAndView mv = new ModelAndView(new MappingJacksonJsonView()); mv.addObject("status",ResponseEnum.ERROR.getCode()); mv.addObject("msg","接口异常，详情请查看日志中的异常信息"); mv.addObject("data",e.toString()); return mv; &#125;&#125; 那么，再执行登陆操作之后，就不会在页面上直接显示异常信息了。有效地屏蔽了关键信息。 3、spring和springmvc配置文件的优化 3.1 包隔离优化 在编写全局异常之前，先进行了包隔离和优化，一期中的扫描包的写法是： 12345&lt;!--spring:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt;&lt;!--springmvc:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt; 即spring和springmvc扫描包下面的所有的bean和controller.优化后的代码配置： 1234567891011#spring&lt;context:component-scan base-package="com.swg" annotation-config="true"&gt;&lt;!--将controller的扫描排除掉--&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt;#springmvc&lt;context:component-scan base-package="com.swg.controller" annotation-config="true" use-default-filters="false"&gt;&lt;!--添加白名单，只扫描controller，总之要将service给排除掉即可--&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt; 这样做的原因是：Spring和SpringMVC是有父子容器关系的，而且正是因为这个才往往会出现包扫描的问题。 针对包扫描只要记住以下几点即可： spring是父容器，springmvc是子容器，子容器可以访问父容器的bean,父容器不能访问子容器的bean。 只有顶级容器（spring）才有加强的事务能力，而springmvc容器的service是没有的。 如果springmvc不配置包扫描的话，页面404. 3.2 事务的传播机制 针对事务，不得不展开说明spring事务的几种传播机制了。在 spring 的 TransactionDefinition 接口中一共定义了七种事务传播属性： PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择（默认）。 PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 4、补充 Spring默认情况下，会对运行期例外(RunTimeException)，即uncheck异常，进行事务回滚。如果遇到checked异常就不回滚。如何改变默认规则： 让checked例外也回滚：在整个方法前加上 @Transactional(rollbackFor=Exception.class) 让unchecked例外不回滚： @Transactional(notRollbackFor=RunTimeException.class) 不需要事务管理的(只查询的)方法：@Transactional(propagation=Propagation.NOT_SUPPORTED) 5、那么什么是嵌套事务呢？ 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫做save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点在于那个save point，看以下几个问题： 问题1：如果子事务回滚，会发生什么？ 父事务会回到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。 问题2：如果父事务回滚，会发生什么？ 父事务回滚，子事务也会跟着回滚，为什么呢？因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理/ 问题3：父事务先提交，然后子事务再提交；还是子事务先提交，然后父事务再提交呢？ 答案是第二种情况，子事务是父事务的一部分，由父事务同意提交。 6、spring配置文件的一些理解： 容器 在Spring整体框架的核心概念中，容器是核心思想，就是用来管理Bean的整个生命周期的，而在一个项目中，容器不一定只有一个，Spring中可以包括多个容器，而且容器有上下层关系，目前最常见的一种场景就是在一个项目中引入Spring和SpringMVC这两个框架，那么它其实就是两个容器，Spring是父容器，SpringMVC是其子容器，并且在Spring父容器中注册的Bean对于SpringMVC容器中是可见的，而在SpringMVC容器中注册的Bean对于Spring父容器中是不可见的，也就是子容器可以看见父容器中的注册的Bean，反之就不行。 1&lt;context:component-scan base-package="com.springmvc.test" /&gt; 我们可以使用统一的如下注解配置来对Bean进行批量注册，而不需要再给每个Bean单独使用xml的方式进行配置。 从Spring提供的参考手册中我们得知该配置的功能是扫描配置的base-package包下的所有使用了@Component注解的类，并且将它们自动注册到容器中，同时也扫描@Controller，@Service，@Respository这三个注解，因为他们是继承自@Component。 1&lt;context:annotation-config/&gt; 其实有了上面的配置，这个是可以省略掉的，因为上面的配置会默认打开以下配置。以下配置会默认声明了@Required、@Autowired、 @PostConstruct、@PersistenceContext、@Resource、@PreDestroy等注解。 1&lt;mvc:annotation-driven /&gt; 这个是SpringMVC必须要配置的，因为它声明了@RequestMapping、@RequestBody、@ResponseBody等。并且，该配置默认加载很多的参数绑定方法，比如json转换解析器等。 7、总结 在实际工程中会包括很多配置，我们按照官方推荐根据不同的业务模块来划分不同容器中注册不同类型的Bean：Spring父容器负责所有其他非@Controller注解的Bean的注册，而SpringMVC只负责@Controller注解的Bean的注册，使得他们各负其责、明确边界。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis实现分布式锁]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F03redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[为了讲解redis分布式锁，我将引入一个场景：定时关单。因为往往订单服务是一个集群，那么定时器会同时触发这些集群去取消订单，显然是浪费机器资源的，所以目的是：只让其中一台机器去执行取消订单即可。这里可以用分布式锁来实现。 项目是从练手项目中截取出来的，框架是基于SSM的XML形式构成，所以下面还涉及一点XMl对于定时器spring schedule的配置内容。 1、引入目标 定时自动对超过两个小时还未支付的订单对其进行取消，并且重置库存。 2、配置 首先是spring配置文件引入spring-schedule 123456xmlns:task="http://www.springframework.org/schema/task"...http://www.springframework.org/schema/taskhttp://www.springframework.org/schema/task/spring-task.xsd...&lt;task:annotation-driven/&gt; 补充：针对applicationContext-datasource.xml中的dataSource读取配置文件的信息无法展现的问题，在spring的配置文件中增加一条配置： 1&lt;context:property-placeholder location="classpath:datasource.properties"/&gt; 3、定时调度代码 此代码的主要功能是：定时调用取消订单服务。 1234567891011121314@Component@Slf4jpublic class CloseOrderTask &#123; @Autowired private OrderService orderService; @Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍的时候执行 public void closeOrderTaskV1()&#123; log.info("关闭订单定时任务启动"); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); log.info("关闭订单定时任务结束"); &#125;&#125; @Component一定要加，否则spring扫描不到。 close.order.task.time.hour 也是配置在snailmall.properties中的，这里配置的是默认的2，即两个小时，下订单超过两个小时仍然不支付，就取消该订单。 对于orderService里面的具体方法： 这里是关单的具体逻辑，细节是行锁。这段代码只要知道他是具体关单的逻辑即可，不需要仔细了解代码。 1234567891011121314151617181920212223242526@Overridepublic void closeOrder(int hour) &#123; Date closeDateTime = DateUtils.addHours(new Date(),-hour); //找到状态为未支付并且下单时间是早于当前检测时间的两个小时的时间,就将其置为取消 //SELECT &lt;include refid="Base_Column_List"/&gt; from mmall_order WHERE status = #&#123;status&#125; &lt;![CDATA[ and create_time &lt;= #&#123;date&#125; ]]&gt; order by create_time desc List&lt;Order&gt; orderList = orderMapper.selectOrderStatusByCreateTime(Const.OrderStatusEnum.NO_PAY.getCode(),DateTimeUtil.dateToStr(closeDateTime)); for(Order order:orderList)&#123; List&lt;OrderItem&gt; orderItemList = orderItemMapper.getByOrderNo(order.getOrderNo()); for(OrderItem orderItem:orderItemList)&#123; //一定要用主键where条件，防止锁表。同时必须是支持MySQL的InnoDB. Integer stock = productMapper.selectStockByProductId(orderItem.getProductId()); if(stock == null)&#123; continue; &#125; //更新产品库存 Product product = new Product(); product.setId(orderItem.getProductId()); product.setStock(stock+orderItem.getQuantity()); productMapper.updateByPrimaryKeySelective(product); &#125; //关闭order //UPDATE mmall_order set status = 0 where id = #&#123;id&#125; orderMapper.closeOrderByOrderId(order.getId()); log.info("关闭订单OrderNo:&#123;&#125;",order.getOrderNo()); &#125;&#125; 这样，debug启动项目，一分钟后就会自动执行closeOrderTaskV1方法了。找一个未支付的订单，进行相应测试。 4、存在的问题 经过实验发现，同时部署两台tomcat服务器，执行定时任务的时候是两台都同时执行的，显然不符合我们集群的目标，我们只需要在同一时间只有一台服务器执行这个定时任务即可。那么解决方案就是引入redis分布式锁。 redis实现分布式锁，核心命令式setnx命令。所以阅读下面，您需要对redis分布式锁的基本实现原理必须要先有一定的认识才行。 5、第一种方案 第一步：setnx进去，如果成功，说明塞入redis成功，抢占到锁 第二步：抢到锁之后，先设置一下过期时间，即后面如果执行不到delete，也会将这个锁自动释放掉，防止死锁 第三步：关闭订单，删除redis锁 存在的问题：如果因为tomcat关闭或tomcat进程在执行closeOrder()方法的时候，即还没来得及设置锁的过期时间的时候，这个时候会造成死锁。需要改进。 123456789101112131415161718192021222324252627//第一个版本，在突然关闭tomcat的时候有可能出现死锁@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV2()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; log.info("关闭订单定时任务结束");&#125;private void closeOrder(String lockName) &#123; //给锁一个过期时间，如果因为某个原因导致下面的锁没有被删除，造成死锁 RedisShardPoolUtil.expire(lockName,50); log.info("获取&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); //关闭订单之后就立即删除这个锁 RedisShardPoolUtil.del(lockName); log.info("释放&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); System.out.println("=============================================");&#125; 6、改进 图看不清，可以重新打开一个窗口看。具体的逻辑代码： 12345678910111213141516171819202122232425262728@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV3()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; //在没有拿到锁的情况下，也要进行相应的判断，确保不死锁 String lockValueStr = RedisShardPoolUtil.get(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); //如果判断锁是存在的并且现在已经超时了，那么我们这个进程就有机会去占有这把锁 if(lockValueStr != null &amp;&amp; System.currentTimeMillis() &gt; Long.parseLong(lockValueStr))&#123; //当前进程进行get set操作，拿到老的key，再塞进新的超时时间 String getSetResult = RedisShardPoolUtil.getset(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); //如果拿到的是空的，说明老的锁已经释放，那么当前进程有权占有这把锁进行操作； //如果拿到的不是空的，说明老的锁仍然占有，并且这次getset拿到的key与上面查询get得到的key一样的话，说明没有被其他进程刷新，那么本进程还是有权占有这把锁进行操作 if(getSetResult == null || (getSetResult != null &amp;&amp; StringUtils.equals(lockValueStr,getSetResult)))&#123; closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125; log.info("关闭订单定时任务结束");&#125; 这样两次的防死锁措施，不仅可以防止死锁，还可以提高效率。 7、扩展 mysql四种事务隔离机制 read uncommitted:读取未提交内容 两个线程，其中一个线程执行了更新操作，但是没有提交，另一个线程在事务内就会读到该线程未提交的数据。 read committed:读取提交内容（不可重复读） 针对第一种情况，一个线程在一个事务内不会读取另一个线程未提交的数据了。但是，读到了另一个线程更新后提交的数据，也就是说重复读表的时候，数据会不一致。显然这种情况也是不合理的，所以叫不可重复读。 repeatable read:可重复读（默认） 可重复读，显然解决2中的问题，即一个线程在一个事务内不会再读取到另一个线程提交的数据，保证了该线程在这个事务内的数据的一致性。 对于某些情况，这种方案会出现幻影读，他对于更新操作是没有任何问题的了，但是对于插入操作，有可能在一个事务内读到新插入的数据（但是MySQL中用多版本并发控制机制解决了这个问题），所以默认使用的就是这个机制，没有任何问题。 serializable:序列化 略。 存储引擎 MySQL默认使用的是InnoDB，支持事务。还有例如MyISAM,这种存储引擎不支持事务，只支持只读操作，在用到数据的修改的地方，一般都是用默认的InnoDB存储引擎。 索引的一个注意点 一般类型为normal和unique，用btree实现，对于联合索引(字段1和字段2)，在执行查询的时候，例如 1select * from xxx where 字段1="xxx" ... 是可以利用到索引的高性能查询的，但是如果是 1select * from xxx where 字段2="xxx" ... 效率跟普通的查询时一样的，因为用索引进行查询，最左边的那个字段必须要有，否则无效。 扩展的内容知识顺便提一下，在数据库这一块，会详细介绍一下。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson实现Redis分布式锁原理]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F02Redisson%E5%AE%9E%E7%8E%B0Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[我们可以自己来实现一个redis分布式锁，但是如何用Redisson优雅地实现呢？本文探讨一下它的原理。 用Redisson来实现分布式锁异常地简单，形如： 还支持redis单实例、redis哨兵、redis cluster、redis master-slave等各种部署架构，都可以给你完美实现。 加锁 原理图： 现在某个客户端要加锁。如果该客户端面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。紧接着，就会发送一段lua脚本到redis上，那段lua脚本如下所示： 为啥要用lua脚本呢？因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的原子性。 解释一下这段脚本的意思。 这里的KEYS[1]代表的是你加锁的那个key的名字。这个key就是我们常看到的： 1RLock lock = redisson.getLock("myLock"); 中的myLock，我就是对这个key进行加锁。 这里的ARGV[1]代表的就是锁key的默认生存时间，默认30秒。ARGV[2]代表的是加锁的客户端的ID:比如8743c9c0-0795-4907-87fd-6c719a6b4586:1 第一段if判断语句，就是相当于用exists myLock命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。如何加锁呢？很简单，用下面的命令：hset myLock。 执行完hest之后，设置了一个hash数据结构：8743c9c0-0795-4907-87fd-6c719a6b4586:1 1，这行命令执行后，会出现一个类似下面的数据结构： 紧接着会执行pexpire myLock 30000命令，设置myLock这个锁key的生存时间是30秒。好了，到此为止，ok，加锁完成了。 锁互斥 那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？很简单，第一个if判断会执行exists myLock，发现myLock这个锁key已经存在了。接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。 所以这个客户端2两个if都不能进入，只能执行最后的pttl myLock，返回值代表了myLock这个锁key的剩余生存时间。比如还剩15000毫秒的生存时间。此时客户端2会进入一个while循环，不停的尝试加锁。 watch dog自动延期机制 客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？ 简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制 看一下代码，相同的客户进来，会进入第二个if，会执行hincrby，即增1，那么这个hash结构就会变成： 释放锁 如果执行lock.unlock()，就可以释放分布式锁，此时的业务逻辑也是非常简单的。其实说白了，就是每次都对myLock数据结构中的那个加锁次数减1。如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用：del myLock命令，从redis里删除这个key。然后呢，另外的客户端2就可以尝试完成加锁了。 这就是所谓的分布式锁的开源Redisson框架的实现机制。 存在的问题 其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。 但是复制的这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。 假设客户端1在redis master上获得锁，然后主机宕机，redis slave成为新的redis master，但是还未同步到redis slave上，但是客户端1已经觉得自己获取到了锁。 此时，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，此时就会发生多个客户端完成对一个key的加锁。这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring事务的传播行为]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F01%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BC%A0%E6%92%AD%E8%A1%8C%E4%B8%BA%2F</url>
    <content type="text"><![CDATA[经常听到别人说事务传播行为，那到底什么是事务的传播行为呢？ 1.什么是事务？ 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。 这里注意，其实事务就是数据库才能保证的，所以抛开数据库层面来谈事务本身就是不存在的，所以事务的概念就是数据库一系列操作的一个完整单元。 2.什么是ACID？ ACID是指数据库管理系统在写入或更新资料的过程中，为保证事务是正确可靠的，所必须具备的四个特性：原子性、一致性、隔离性、持久性。 Atomicity：一个事务中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 Isolation：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔 离分为不同级别，包括读未提交(Read uncommitted)、读提交(read committed)、可重复读(repeatable read)和串行化(Serializable)。 Durability：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 3.spring事务传播行为 在我们用SSM开发项目的时候，我们一般都是将事务设置在Service层 那么当我们调用Service层的一个方法的时候它能够保证我们的这个方法中执行的所有的对数据库的更新操作保持在一个事务中，在事务层里面调用的这些方法要么全部成功，要么全部失败。那么事务的传播特性也是从这里说起的。 如果你在你的`Service`层的这个方法中，除了调用了`Dao`层的方法之外，还调用了本类的其他的`Service`方法，那么在调用其他的`Service`方法的时候，这个事务是怎么规定的呢，我必须保证我在我方法里调用的这个方法与我本身的方法处在同一个事务中，否则如果保证事物的一致性。事务的传播特性就是解决这个问题的. 在Spring中有针对传播特性的多种配置我们大多数情况下只用其中的一种:PROPGATION_REQUIRED：这个配置项的意思是说当我调用service层的方法的时候开启一个事务(具体调用那一层的方法开始创建事务，要看你的aop的配置),那么在调用这个service层里面的其他的方法的时候,如果当前方法产生了事务就用当前方法产生的事务，否则就创建一个新的事务。这个工作使由Spring来帮助我们完成的。 默认情况下当发生`RuntimeException`的情况下，事务才会回滚，所以要注意一下：如果你在程序发生错误的情况下，有自己的异常处理机制定义自己的`Exception`，必须从`RuntimeException`类继承，这样事务才会回滚！ 4.事务隔离级别 1、Serializable：最严格的级别，事务串行执行，资源消耗最大； 2、REPEATABLE READ：保证了一个事务不会修改已经由另一个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 3、READ COMMITTED:大多数主流数据库的默认事务等级，保证了一个事务不会读到另一个并行事务已修改但未提交的数据，避免了“脏读取”。该级别适用于大多数系统。 4、Read Uncommitted：保证了读取过程中不会读取到非法数据。 5.总结 本文的重点是在于理解事务的传播行为这个概念，从事务的概念，到事务的ACID介绍，引出事务传播传播行为和隔离级别这两个概念加以理解。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap]]></title>
    <url>%2F2019%2F01%2F22%2Fjava-collection%2F12.ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[对于并发场景下，推荐使用线程安全的 concurrentHashMap ，而不是 HashMap 或者是 HashTable .concurrentHashMap在JDK7和JDK8中的实现原理是不一样的。本文分别对其核心思想和方法进行阐述。 一、JDK7实现 ConcurrentHashMap 的内部细分了若干个小的 HashMap ，称之为段（ SEGMENT ）。 ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment ，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel ( Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 1.1 get方法 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值. 内部 HashEntry 类 ： 12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 1.2 put方法 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。 首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。 而 ConcurrentHashMap 不一样，它是在将数据插入之前检查是否需要扩容，之后再做插入操作。 1.3 size方法 每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。 但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。 在 JDK7 中，第一种方案他会使用不加锁的模式去尝试多次计算 ConcurrentHashMap 的 size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的。 第二种方案是如果第一种方案不符合，他就会给每个 Segment 加上锁，然后计算 ConcurrentHashMap 的 size 返回。其源码实现: 12345678910111213141516171819202122232425262728293031323334353637public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 其中 12// 锁之前重试次数static final int RETRIES_BEFORE_LOCK = 2; 二、JDK8实现 jdk8 中的 ConcurrentHashMap 数据结构和实现与 jdk7 还是有着明显的差异。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 也将 jdk7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。 其中的 val next 都用了 volatile 修饰，保证了可见性。 2.1 put方法 重点来看看 put 函数： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足(不需要初始化、Node不为空、不需要扩容)，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 2.2 get方法 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 都不满足那就按照链表的方式遍历获取值。 2.3 size方法 JDK8 实现相比 JDK7 简单很多，只有一种方案，我们直接看 size() 代码： 123456789101112131415161718192021public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; //获取baseCount值 long sum = baseCount; //遍历CounterCell数组全部加到baseCount上，它们的和就是size if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; 可能你会有所疑问，ConcurrentHashMap 中的 baseCount 属性不就是记录的所有键值对的总数吗？直接返回它不就行了吗？ 之所以没有这么做，是因为我们的 addCount 方法用于 CAS 更新 baseCount，但很有可能在高并发的情况下，更新失败，那么这些节点虽然已经被添加到哈希表中了，但是数量却没有被统计。 还好，addCount 方法在更新 baseCount 失败的时候，会调用 fullAddCount 将这些失败的结点包装成一个 CounterCell 对象，保存在 CounterCell 数组中。那么整张表实际的 size 其实是 baseCount 加上 CounterCell数组中元素的个数。 三、总结 并发情况下请使用concurrentHashMap 在jdk7中，用的是分段锁，默认是12段，那么并发量最多也就12. get不加锁，第一次hash定位到segment，第二次hash定位到元素，元素值是用volatile保证内存可见性 put需要加锁，hash定位到segment后，先检查是否需要扩容再插入。 size先使用不加锁的模式去尝试多次计算size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入；如果不一致，给每个 Segment 加上锁再依次去计算个数 在jdk8中，采用了 CAS + synchronized 来保证并发安全性 put的过程比较复杂，简单来说是：先计算hash定位到node—》判断是否初始化—》如果node为空则表示可以插入，用cas插入—》判断是否需要扩容—》如果不需要初始化、Node不为空、不需要扩容，则利用 synchronized 锁写入数据—》判断是否需要转换为红黑树 get就比较简单，直接根据hash定位到node，然后以链表或者红黑树的方式拿到 size方法就一种方案：baseCount+CounterCell[]中所有元素 整理自： https://crossoverjie.top/JCSprout/#/thread/ConcurrentHashMap?id=size-方法 https://www.jianshu.com/p/e99e3fcface4]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap死循环问题]]></title>
    <url>%2F2019%2F01%2F21%2Fjava-collection%2F11.HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JDK1.7或者更老的版本中在多线程情况下是会存在死循环问题，究其原因是put过程中的resize方法在调用transfer方法的时候导致的死锁。这次我们来看看到底是哪里出了问题！ 核心源码 在JDK8中，内部已经调整，解决了死循环问题，是如何解决的呢？将JDK7中头插入法改为末端插入。就是这么简单。关于这个，可以查看jdk8源码中的resize方法。 上面提到是由于put时出现问题，那么先来到put()中看看： 我们看到，put一个不存在的新元素，必然增加一个节点，我们进入这个增加节点的方法： 检查是否需要扩容，需要的话就resize: 下面就是对链表数据进行迁移： 核心的代码就是这么多，首先要强调一下：两个线程进来，是分别建立了两个独立的扩容后的数组，比如这里是两个长度为4的数组。老的数组为2个数就是唯一的。所以在第一步，线程2运行结束时，老的数组元素已经空了。 下面先演示一下正常的rehash过程。 正常情况 假设了我们的hash算法就是简单的用 key mod 一下数组(hash表)的大小 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。 接下来的三个步骤是Hash表 resize 成4，然后所有的 &lt;key,value&gt; 重新 rehash 的过程 注意到，在JDK7中，是按照头插入法依次插入的。所以7插到了3前面。 并发情况 1.初始情况 假设我们有两个线程。我用红色和浅蓝色标注了一下。 对于第一个线程先执行完这一行，然后挂起，此时 e 和 next 都附好值了： 而让线程二执行完成。于是我们有下面的这个样子： 因为Thread1的 e 指向了 key(3) ，而 next 指向了 key(7) ，其在 Thread2 rehash后，指向了 Thread2 重组后的链表。 2.Thread1被调度回来执行 先是执行 newTalbe[i] = e ：此时线程1的第三个位置就是指向元素3; 然后是 e = next，导致了 e 指向了 key(7) ; 而下一次循环的 next = e.next 导致了 next 指向了 key(3) ; 3.一切安好 线程一接着工作。把 key(7) 摘下来，放到 newTable[i] 的第一个，然后把 e 和 next 往下移。 4.环形链接出现 e.next = newTable[i] 导致 key(3).next 指向了 key(7) 注意：此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 自己的简单整理 这里还是比较绕的，理解的最好方式左边放源码，右边放图，中间用草稿纸画一画。 那么，这里我在对其过程尽可能地讲明白一点。我们先确定7和3会全部落到扩容后的下标为3的位置(3%4=3,7%4=3)。 规定线程1开辟的数组为 arr1 ，线程2开辟的数组为 arr2; 1. 初始状态 线程一： e -&gt; key3 , next -&gt; key7 线程二： 数组3号位置 arr2[3] -&gt; key7 -&gt; key3 注意此时 key7 指向 key3 . 我们要明确一下，发生死循环，是指在put操作完毕之后，最终生成的数组中有死循环引用才行，千万不要一开始看线程一种key3指向key7，然后线程二种key7指向key3就是死循环了。。。 2. 线程一继续执行 i = 3 e.next = key7,此时 e=key3 ,所以是 key3.next = key7（这是线程1的初始状态决定的） arr1[3] 指向 key3 e 为 key7 3.由于e不为空，所以还会循环： 上一步 e 为 key7，所以 next = key7.next ，到线程2中一看是 key3 ，所以 next = key3（线程2中key7.next就是key3） i = 3 e.next = key3------注意，这里就是Key7指向了key3,key7的next引用下面没有变过，所以这里做一下记录，即key7指向key3 newTable[3] = key7 e = key3 4.由于e不为空，所以还会循环： 上一步 e=key3 , next=null i=3 key3.next = key7，注意,由于key7已经指向了key3，此时key3又指向key7,发生死循环 newTable[3] = key3 e = null 5.e为null，跳出循环。 此时发现key3又指向了key7。发生死循环。 整理自:https://coolshell.cn/articles/9606.html/comment-page-2#comments]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux重要的一些命令]]></title>
    <url>%2F2019%2F01%2F21%2Flinux%E9%87%8D%E8%A6%81%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[很多命令之所以重要，是因为用它们会大幅提高工作效率。所以，熟悉它们是我们必须要做的一件事情。下面着重提取了find、grep、管道、wc、awk以及sed等几个命令来看看如何使用。 一、Linux体系结构 对这幅图进行详细说明一下。 体系结构主要分为用户态(用户上层活动)和内核态 内核：本质是一段管理计算机硬件设备的程序，这个程序直接管理硬件：包括CPU、内存空间、硬盘接口、网络接口等。所有的计算机操作都要通过内核来操作。 系统调用：内核的访问接口，是一种不能再简化的操作(可以认为系统调用已经是最小的原子操作，上层完成一个功能要依托于若干系统调用才能完成) 由于系统调用比较基础，要完成一个功能需要很多系统调用组合才能实现，对于程序员来说比较复杂。这个时候怎么办呢？我们可以调用公共函数库：系统调用的组合拳。简化程序员操作。 Shell也是一种特殊的应用程序，是一个命令解释器，可以编程。 Shell下通系统调用，上通各种应用，是上层和下层之间粘合的胶水，让不同程序可以偕同工作。 在没有图形界面之前，用户通过shell命令行或者可编程的shell脚本可以完成很多事情。 二、如何根据文件名检索文件 find 在指定目录下查找文件 语法 find path [options] params 2.1 精确查找文件 比如我在当前目录(可能在子目录下)下找一个文件叫做test.java： 1find -name "test.java" 这个指令就可以在本目录以及子目录下递归查找这个文件了。 实例：精确查询名字叫snailmall-api-gateway-8080.jar这个文件： 2.2 全局搜索 如果是全局查找，也很简单，无非是从根目录开始递归查找。 1find / -name "test.java" 实例：我对这台服务器全局查找文件名以snailmall开头的所有文件： 2.3 模糊查询 如果找以test打头的文件： 1find -name "test*" 即用 * 通配符就可以模糊查询到以 test 打头的文件。 实例，我的这台服务器上部署了几个关于商城的服务，这个目录下我放了jar包和相应的启动信息文件。我对其进行模糊查询： 2.4 忽略大小写 1find -iname "test*" 三、如何根据文件内的内容进行检索 3.1 grep命令 grep 查找文件里符合条件的字符串 语法：grep [options] pattern file 比如 test.java 中有一句话是： 1System.out.println("i love java"); 那么如何查找 test.java 中的 java 呢？ 1grep "java" test* 这句话意思就是查找以 test 打头的文件中的包含 java 字符串所在的行。 直接输入： 1grep "hello" 会等待用户输入文本。然后再对输入的内容进行检索。 3.2 管道操作符 | 可将指令连接起来，前一个指令的输出作为后一个指令的输入。 1find / | grep "test" 作用同： 1find / -name "test" 注意： 只有前一个指令正确才会再处理。 管道右边命令必须能接收标准输入流，否则传递过程中数据会被抛弃 3.3 grep结合管道 1grep 'xxx' hello.info 可以将 xxx 所在的行全部筛选出来，但是还是特别多，我比如关心这每一行中某个字段的信息，比如是 param[xx12]这种信息。如何实现筛选呢？ 1grep 'xxx' hello.info | grep -o 'param\[[0-9a-z]*\]' 这样就只把类似于 param[xx12] 这样的信息清晰地展现出来。 如何过滤掉不要的信息呢？可以用： 1grep -v 比如我们查询 tomcat 进程信息： 1ps -ef | grep tomcat 我们会发现，不仅 tomcat 的信息展现出来了，执行 grep 命令本身的进程信息也展示出来了。我们要将这个 grep 命令过滤掉，只展现 tomcat 进程信息，可以： 1ps -ef | grep tomcat | grep -v "grep" 这样就把 grep 进程信息过滤掉了。 四、如何对文件内容做统计 awk 一次读取一行文本，按输入分隔符进行切片，切成多个组成部分 将切片直接保存再内建的变量中，$1$2…($0表示行的全部) 支持对单个切片的判断，支持循环判断，默认分隔符为空格 语法：awk [options] ‘cmd’ file 有这样一个文件text1.txt： 123456789proto Recv-Q Send-Q Local Address Foreign Address statetcp 0 48 111.34.134.2:ssh 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.13.2:s0 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 1 48 localhots:webcac 124.213.2.12:12565 ESTABLISHEDtcp 1 48 111.34.134.2:s1 124.213.2.12:12565 ESTABLISHEDudp 1 48 111.34.134.2:s2 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.134.2:s3 124.213.2.12:12565 ESTABLISHED 列出切分出来的第一列和第二列： 1awk '&#123;print $1,$2&#125;' test1.txt 结果： 筛选出第一列为tcp和第二列为1的所在行，将这些行数据全部打印出来： 1awk '$1="tcp" &amp;&amp; $2==1&#123;print $0&#125;' test1.txt 结果： 打印带有表头的数据： 1awk '($1="tcp" &amp;&amp; $2==1) || NR==1 &#123;print $0&#125;' test1.txt 默认是以空格分隔，那么以逗号或者其他符号可以吗？答案当然是可以。对于这样的文件text2.txt： 12345adas,123wqe,54412321,dddfsdaasd,1235465547,fjigj 1awk -F "," '&#123;print $2&#125;' text2.txt 五、WC统计 有一个文件test2.txt，里面的内容是： 123swg123eh shwfshsfswg7 121 32n dswg17328 123swg1 2h1jhwjqbsjwqbsh ddddh wg ehdedhd dhsjh 六、sed命令 sed是一个很好的文件处理工具，本身是一个管道命令，主要是以行为单位进行处理，可以将数据行进行替换、删除、新增、选取等特定工作 sed [-n/e/f/r/i] ‘command’ 输入文本 常用选项： -n∶使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN的资料一般都会被列出到萤幕上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e∶直接在指令列模式上进行 sed 的动作编辑； -f∶直接将 sed 的动作写在一个档案内， -f filename 则可以执行 filename 内的sed 动作； -r∶sed 的动作支援的是延伸型正规表示法的语法。(预设是基础正规表示法语法) -i∶直接修改读取的档案内容，而不是由萤幕输出。 常用命令： a ∶新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ∶取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ∶删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ∶插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ∶列印，亦即将某个选择的资料印出。通常 p 会与参数 sed -n 一起运作～ s ∶取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 这里我就主要看一下批量替换这个功能。 如果只是给一个文件中的若干字符串批量替换，只需要： 1sed -i "s/oldstring/newstring/g" filename 如果是对某一路径下很多的文件批量替换： 1sed -i &quot;s/oldstring/newstring/g&quot; `grep oldstring -rl path` 其中，oldstring是待被替换的字符串，newstring是待替换oldstring的新字符串，grep操作主要是按照所给的路径查找oldstring，path是所替换文件的路径； -i选项是直接在文件中替换，不在终端输出； -r选项是所给的path中的目录递归查找； -l选项是输出所有匹配到oldstring的文件； 这里只是模拟一下，将目录下的所有文件进行批量修改：]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【站点文章汇总】]]></title>
    <url>%2F2019%2F01%2F21%2F%E7%AB%99%E7%82%B9%E6%96%87%E7%AB%A0%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[👉请先移步这里☝️☝️☝️🔝🔝🔝本文为置顶文章，为了方便管理和查阅，在这里详细展示目录索引，看完索引，你就知道本站的大体内容啦！我相信一定会给小伙伴们一些收获！🏃🏃🏃🏃持续更新中！ 注1：最好用Chrome浏览器打开浏览。 注2：浏览前先刷新一下页面，因为我这段时间一直在更新。 注3：star或follow一下呗，关注不迷路。 注4：不免有错误，请在下面留言。 注5：导航里面有锚功能，点击即可传送。 "Repetition is the mother of all learning." 导航 🐀 🐂 🐅 🐇 🐉 网络相关 JAVA容器 Linux JVM java基础 🐍 🐎 🐏 🐒 🐒 java多线程 redis mysql 算法 剑指offer题解 🐒 🐒 🐓 🐓 🐕 leetcode精简 leetcode刷题 spring面试 spring源码 设计模式 🐖 🦓 🦒 🐦 🐟 spring cloud相关 zookeeper 杂记 随笔 github 🐫 🐙 前端学习 大数据相关 计算机网络🐀🐀🐀top👆 这一部分主要是关于HTTP和TCP的必备知识。 《计算机网络相关系列》： 《从下到上看五层模型》 《从上到下看五层模型》 《HTTP的前世今生》 《TCP协议入门》 《TCP三次握手和四次挥手》 《HTTP基础知识提炼》 《一步一步理解HTTPS》 《一些常见的面试题》 JAVA容器🐂🐂🐂top👆 这一部分是JAVA容器一系列文章，主要讲了常用JAVA容器的源码和一些特性，面试必问点。 《JAVA容器》： 《ArrayList/Vector》 《LinkedList》 《CopyOnWriteArrayList》 《HashCode/Equals》 《HashMap》 《HashSet》 《LinkedHashMap》 《HashMap和LinkedHashMap遍历机制》 《HashTable》 《LinkedHashSet》 《JDK7中HashMap死循环原因剖析》 《ConcurrentHashMap》 Linux相关🐅🐅🐅top👆 一些必备的Liunx相关的知识点整理。 《Linux》： 《Linux面试重要命令》 《操作系统相关》： 《面试-进程与线程》 《Socket》： 《Socket基础》： JVM相关🐇🐇🐇top👆 主要是介绍JVM相关知识。轻松应付面试。 《JVM》： 《Java如何执行一个最简单的程序》 《浅谈ClassLoader》 《双亲委派模型》 《细谈loadClass》 《JAVA内存模型-线程私有》 《JAVA内存模型-线程共享》 《JAVA内存模型常问面试题》 《GC相关》 《垃圾收集器介绍》 《内存分配和回收策略》 《类的初始化过程》 《静态分派和动态分派》 《实例说明类加载过程》 以下是扩展阅读部分，主要是对Class文件的结构进行详细的解读 《补充阅读1-Class类文件结构》 《补充阅读2-Class文件中的常量池》 《补充阅读3-Class文件中的访问标志、类索引、父类索引、接口索引集合》 《补充阅读4-Class文件中的字段表集合–field字段在class文件中是怎样组织的》 《补充阅读5-Class文件中的方法表集合–method方法在class文件中是怎样组织的》 JAVA基础🐉🐉🐉top👆 主要是介绍比较核心的JAVA基础知识，属于JAVA基础进阶。 《java基础》： 《Integer拆箱和装箱》 《String为什么不可变》 《java字符串核心一网打尽》 《JAVA基础核心-理解类、对象、面向对象编程、面向接口编程》 《补码的前世今生》 《数值计算精度丢失问题》 《彻底理解java反射机制》 《java基础之冰川表面》 《java基础之注解》 《java基础之JDK动态代理》 《java基础之异常》 《java基础之克隆》 《java基础之泛型上》 《java基础之泛型下》 《java基础之NIO》 《java基础之ThreadLocal详解》 《java基础之ThreadLocal内存泄漏问题》 《java基础之ThreadLocal自问自答》 JAVA多线程🐍🐍🐍top👆 多线程这一块比较棘手，且学且保重。 《JAVA多线程和并发》： 《线程基本知识梳理》 《java多线程之传参和返回值处理》 《线程的状态》 《线程重要的相关方法》 《从卖票程序初步看synchronized的特性》 《从底层理解synchronized》 《volatile详解》 《从ReentrantLock引出AQS的原理》 《AQS实现的一些并发工具类》 《读写锁ReentrantReadWriteLock》 《从CAS到Atomic包原理》 《线程池原理详解》 《JUC组件拓展-ForkJoin简介》 《JUC组件拓展-BlockingQueue》 《线程间通信方式总结》 《实现生产者消费者模式》 《Condition详解》 《自己实现一个简单的web服务器》 Redis🐎🐎🐎top👆 系统学习redis的笔记整理。 《Redis》： 《初步认识Redis》 《Redis基本数据结构和操作》 《Redis其他的功能介绍》 《Redis为什么快》 《Redis持久化》 《Redis主从复制》 《Redis-Sentinel实现高可用读写分离》 《Redis-Cluster理论详解》 《Redis缓存设计与优化》 《Redis缓存更新问题》 《Redis事务》 《几种主流缓存框架介绍》 《关于Redis一些重要的面试点》 MySQL数据库🐏🐏🐏top👆 作为必备技能，用法和原理都要会。 《MySQL》： 《mysql最基础知识小结》 《SQL必知必会知识点提炼》 《复杂查询基础》 《内连接和外连接》 《delete和truncate以及drop区别》 《如何设计一个关系型数据库》 《数据库索引入门》 《MySQL索引全面解读》 《MySQL调优》 《关于索引失效和联合索引》 《锁模块》 《数据库事务核心问题》 《mysql面试高频理论知识》 算法🐒🐒🐒top👆 算法与数据结构从来都不是分开谈的，在基础算法笔记系类中直击常见面试算法，穿插重要的数据结构说明。下面的计划是将《剑指offer》和《leetcode》上的题目进行刷题训练，把自认为比较好的题解放上来，配以注释。 😄《基础算法学习》： 《基础算法01-递归入门》 《基础算法02-汉诺塔问题》 《基础算法03-循环控制》 《基础算法04-二分查找算法》 《基础算法05-二分搜索树》 《基础算法06-关于二叉树的经典面试题分析》 《基础算法07-基本排序之冒泡、选择、插入》 《基础算法08-归并排序》 《基础算法09-快速排序》 《基础算法10-堆排序》 《基础算法11-排序之计数排序、桶排序、基数排序》 《基础算法12-排序总结》 《基础算法13-动态规划入门》 《基础算法14-创建二叉搜索树》 😆《剑指offer题解》：top👆 《【题01-二维数组中的查找】》 《【题02-替换空格】》 《【题03-从尾到头打印链表】》 《【题04-重建二叉树】》 《【题05-用两个栈实现队列】》 《【题06-旋转数组的最小数字】》 《【题07-斐波那契数列】》 《【题08-跳台阶】》 《【题09-变态跳台阶】》 《【题10-矩形覆盖】》 《【题11-二进制中1的个数】》 《【题12-数值的整数次方】》 《【题13-调整数组顺序使奇数位于偶数前面】》😭 《【题14-链表中倒数第k个结点】》 《【题15-反转链表】》 《【题16-合并两个排序的链表】》😭 《【题17-树的子结构】》😭 《【题18-二叉树的镜像】》 《【题19-顺时针打印矩阵】》😭 《【题20-包含min函数的栈】》 《【题21-栈的压入、弹出序列】》 《【题22-从上往下打印二叉树】》 《【题23-二叉搜索树后序遍历序列】》 《【题24-二叉树中和为某一值的路径】》😭 《【题25-复杂链表的复制】》 《【题26-二叉搜索树与双向链表】》 《【题27-字符串的排列】》😭 《【题28-数组中出现次数朝超过一半的数字】》 《【题29-最小的k个数】》😭 《【题30-连续子数组的最大和】》 《【题31-整数中1出现的次数】》😭 《【题32-把数组排成最小的数】》 《【题33-丑数】》 《【题34-第一个只出现一次的字符位置】》 《【题35-数组中的逆序对】》 《【题36-两个链表的第一个公共结点】》 《【题37-数字在排序数组中出现的次数】》 《【题38-二叉树的深度】》 《【题39-平衡二叉树】》 《【题40-数组中只出现一次的数字】》 《》 《》 《》 《》 《》 《》 《》 《》 《》 《》 《》 《》 😃《leetcode分类经典题目提炼》：top👆 😏《未来计划-leetcode刷题记录》：top👆 Spring🐓🐓🐓top👆 大厂必问啊啊啊啊，源码终究还是要读的~ 😄《spring面试》： 《IOC基本原理》 《IOC的用法》 《AOP基本使用和原理》 《Spring Bean》 《注解–组件注册》 《注解-生命周期》 《注解-属性赋值和自动装配》 😆《spring源码研读-占坑》： 设计模式🐕🐕🐕top👆 设计模式一般是必问的，这里尝试对每个设计模式进行理解，这也是未来计划，不是一朝一夕完成的，首先着重对UML图、软件设计原则、常见的单例模式、工厂模式进行探讨，其他的设计模式慢慢参悟。 《设计模式-占坑》： SpringCloud🐖🐖🐖top👆 这一块就比较偏实践了。分布式。。。路漫漫。。。 😄《从天气项目入门微服务》： 《1.天气预报系统-简单接口调用》 《2.天气预报系统-redis提升性能》 《3.天气预报系统-天气数据同步》 《4.天气预报系统-前端样式》 《5.天气预报系统-服务拆分和业务建模》 《6.天气预报系统-拆分本系统》 《7.天气预报系统-微服务的注册和发现》 《8.天气预报系统-微服务的消费》 《9.天气预报系统-API网关》 《10.天气预报系统-集中化配置》 《11.天气预报系统-熔断机制》 😆《springcloud组件系统学习》： 《1.Eureka服务治理》 《2.Ribbon客户端负载均衡》 《3.Hystrix请求熔断服务降级》 《4.Hystrix请求合并》 《5.Feign声明式服务调用》暂时没什么可说的，看上面个就行 《6.Zuul网关服务》 《7.Config分布式配置管理》 《8.Bus消息总线》 《9.Stream消息驱动》 《10.Sleuth服务追踪》 Zookeeper🦓🦓🦓top👆 作为当今分布式协调中心，核心的Paxos算法你不想了解一下吗？ 《zookpeeper学习笔记》： 《Zookeeper笔记1-CAP和BASE理论》 《Zookeeper笔记2-2PC&amp;3PC》 《Zookeeper笔记3-paxos算法》 《Zookeeper笔记4-Zookeeper介绍》 《Zookeeper笔记5-ZAB协议》 《Zookeeper笔记6-zk安装和集群搭建》 《Zookeeper笔记7-ZK的基本操作以及权限控制》 《Zookeeper笔记8-典型应用场景详解》 《Zookeeper笔记9-原生Java API使用》 《Zookeeper笔记10-Apache Curator客户端的使用（一）》 《Zookeeper笔记11-Apache Curator客户端的使用（二）》 《Zookeeper笔记12-分布式锁》 杂记🦒🦒🦒top👆 在这个板块，不划分类别，文章尽可能地简短，也可谓之记忆碎片。 《技术短文杂记》： 《spring事务的传播行为》 《Redisson实现Redis分布式锁原理》 《redis实现分布式锁》 《springMVC全局异常+spring包扫描包隔离+spring事务传播》 《分布式ID生成策略》 《Spring Session》 《Curator》 《ELK平台搭建》 《库存扣减问题》 《分布式事务解决方案思考》 《SpringBoot使用logback实现日志按天滚动》 《地理位置附近查询的GEOHASH解决方案》 《深入探究Nginx原理》 《简明理解一致性hash算法》 《SpringBoot通用知识深入–切面、异常、单元测试》 随笔🐦🐦🐦top👆 《随笔》： 《2019年展望》 实战作品🐟🐟🐟top👆 记录一些实战作品，代码主要存放在github上。 《我的实战》： 《快乐蜗牛商城代码》 《码码购分布式电商实战代码》 前端🐫🐫🐫top👆 《前端基础》 《HTML标签学习》 《CSS选择器相关》 《CSS显示模式》 《小练习》 《CSS之图片》 《CSS之盒子模型》 《CSS之浮动》 《CSS之定位》 大数据相关🐙🐙🐙top👆]]></content>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F10.LinkedHashSet%2F</url>
    <content type="text"><![CDATA[HashSet 和 LinkedHashSet 的关系类似于 HashMap 和 LinkedHashMap 的关系，即后者维护双向链表，实现迭代顺序可为插入顺序或是访问顺序。所以也就轻松加愉快快速了解一下即可。 从源码中可以看到其空的构造函数为： 123public LinkedHashSet() &#123; super(16, .75f, true);&#125; 这个super即父类是HashSet，从它的继承关系就可以显然看到： 123public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable 那么HashSet内部的数据结构就是一个 HashMap，其方法的内部几乎就是在调用 HashMap 的方法。 LinkedHashSet 首先我们需要知道的是它是一个 Set 的实现，所以它其中存的肯定不是键值对，而是值。此实现与 HashSet 的不同之处在于，LinkedHashSet 维护着一个运行于所有条目的双向循环链表。 这一切都与LinkedHashMap类似。 LinkedHashSet 内部有个属性 accessOrder 控制着遍历次序。默认情况下该值为 false ,即按插入排序访问。如果将该值设置为 true 的话，则按访问次序排序(即最近最少使用算法，最近最少使用的放在链表头部，最近访问的则在链表尾部)。 一、 示例 HashSet的遍历： 123456789101112public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new HashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125;&#125; 输出结果是： aaa ccc bbb eee LinkedHashSet的遍历： 1234567891011121314public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new LinkedHashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); linkedHashSet.add(null); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; 输出结果是： aaa eee ccc bbb null 可以看到与输入顺序是一致的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashtable]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F9.HashTable%2F</url>
    <content type="text"><![CDATA[Hashtable 是个过时的集合类，不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换。但这并不是我们不去了解它的理由。最起码 Hashtable 和 HashMap 的面试题在面试中经常被问到。 一、前言 Hashtable和HashMap，从存储结构和实现来讲基本上都是相同的。 它和HashMap的最大的不同是它是线程安全的，另外它不允许key和value为null。 为了能在哈希表中成功地保存和取出对象，用作key的对象必须实现hashCode方法和equals方法。 二、fail-fast机制 iterator方法返回的迭代器是fail-fast的。如果在迭代器被创建后hashtable被结构型地修改了，除了迭代器自己的remove方法，迭代器会抛出一个ConcurrentModificationException异常。 因此，面对在并发的修改，迭代器干脆利落的失败，而不是冒险的继续。 关于这个的理解，其实在上一章讲LinkedHashMap中的第八点提到： 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 简单说，就是两个线程同时分别进行修改和遍历时，会抛出这个异常。 面试题：集合在遍历过程中是否可以删除元素，为什么迭代器就可以安全删除元素？ 集合在使用 for 循环迭代的过程中不允许使用，集合本身的 remove 方法删除元素，如果进行错误操作将会导致 ConcurrentModificationException 异常的发生 Iterator 可以删除访问的当前元素(current)，一旦删除的元素是Iterator 对象中 next 所正在引用的，在 Iterator 删除元素通过 修改 modCount 与 expectedModCount 的值，可以使下次在调用 remove 的方法时候两者仍然相同因此不会有异常产生。 迭代器的fail-fast机制并不能得到保证，它不能够保证一定出现该错误。一般来说，fail-fast会尽最大努力抛出ConcurrentModificationException异常。因此，为提高此类操作的正确性而编写一个依赖于此异常的程序是错误的做法，正确做法是：ConcurrentModificationException 应该仅用于检测 bug。 Hashtable是线程安全的。如果不需要线程安全的实现是不需要的，推荐使用HashMap代替Hashtable。如果需要线程安全的实现，推荐使用java.util.concurrent.ConcurrentHashMap代替Hashtable。 二、继承关系 123public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable&#123;&#125; extends Dictionary&lt;K,V&gt;：Dictionary类是一个抽象类，用来存储键/值对，作用和Map类相似。 implements Map&lt;K,V&gt;：实现了Map，实现了Map中声明的操作和default方法。 hashMap以及TreeMap的源码，都没有继承于这个类。不过当我看到注释中的解释也就明白了，其 Dictionary 源码注释是这样的：NOTE: This class is obsolete. New implementations should implement the Map interface, rather than extending this class. 该话指出 Dictionary 这个类过时了，新的实现类应该实现Map接口。 三、属性 1234567891011121314//哈希表private transient Entry&lt;?,?&gt;[] table;//记录哈希表中键值对的个数private transient int count;//扩容的阈值private int threshold;//负载因子private float loadFactor;//hashtable被结构型修改的次数。private transient int modCount = 0; HashTable并没有像HashMap那样定义了很多的常量，而是直接写死在了方法里。 Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。 四、构造函数 12345678/** * 使用默认初始化容量（11）和默认负载因子（0.75）来构造一个空的hashtable. * * 这里可以看到，Hashtable默认初始化容量为16，而HashMap的默认初始化容量为11。 */public Hashtable() &#123; this(11, 0.75f);&#125; 我们可以获取到这些信息：HashTable默认的初始化容量为11（与HashMap不同），负载因子默认为0.75（与HashMap相同）。而正因为默认初始化容量的不同，同时也没有对容量做调整的策略，所以可以先推断出，HashTable使用的哈希函数跟HashMap是不一样的（事实也确实如此）。 五、重要方法 5.1 get方法 12345678910111213public synchronized V get(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //通过哈希函数，计算出key对应的桶的位置 int index = (hash &amp; 0x7FFFFFFF) % tab.length; //遍历该桶的所有元素，寻找该key for (Entry&lt;?,?&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return (V)e.value; &#125; &#125; return null;&#125; 这里可以看到，Hashtable和HashMap确认key在数组中的索引的方法不同。 Hashtable通过index = (hash &amp; 0x7FFFFFFF) % tab.length;来确认 HashMap通过i = (n - 1) &amp; hash;来确认 跟HashMap相比，HashTable的get方法非常简单。我们首先可以看见get方法使用了synchronized来修饰，所以它能保证线程安全。并且它是通过链表的方式来处理冲突的。另外，我们还可以看见HashTable并没有像HashMap那样封装一个哈希函数，而是直接把哈希函数写在了方法中。而哈希函数也是比较简单的，它仅对哈希表的长度进行了取模。 5.2 put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public synchronized V put(K key, V value) &#123; // 确认value不为null if (value == null) &#123; throw new NullPointerException(); &#125; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //找到key在table中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") //获取key所在索引的entry Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，判断key是否已经存在 for(; entry != null ; entry = entry.next) &#123; //如果key已经存在 if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; //保存旧的value V old = entry.value; //替换value entry.value = value; //返回旧的value return old; &#125; &#125; //如果key在hashtable不是已经存在，就直接将键值对添加到table中，返回null addEntry(hash, key, value, index); return null;&#125;private void addEntry(int hash, K key, V value, int index) &#123; modCount++; Entry&lt;?,?&gt; tab[] = table; //哈希表的键值对个数达到了阈值，则进行扩容 if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; //把新节点插入桶中（头插法） tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++;&#125; 从代码中可以总结出Hashtable的put方法的总体思路： 确认value不为null。如果为null，则抛出异常 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key已经存在，替换value，返回旧的value 如果key在hashtable不是已经存在，就直接添加，否则直接将键值对添加到table中，返回null 在方法中可以看到，在遍历桶中元素时，是按照链表的方式遍历的。可以印证，HashMap的桶中可能为链表或者树。但Hashtable的桶中只可能是链表。 5.3 remove方法 12345678910111213141516171819202122232425public synchronized V remove(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //计算key在hashtable中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，如果entry中存在key为参数key的键值对，就删除键值对，并返回键值对的value for(Entry&lt;K,V&gt; prev = null ; e != null ; prev = e, e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; modCount++; if (prev != null) &#123; prev.next = e.next; &#125; else &#123; tab[index] = e.next; &#125; count--; V oldValue = e.value; e.value = null; return oldValue; &#125; &#125; //如果不存在key为参数key的键值对，返回value return null;&#125; 从代码中可以总结出Hashtable的remove方法的总体思路： 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key存在，删除key映射的键值对，返回旧的value 如果key在hashtable不存在，返回null 5.4 rehash方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 增加hashtable的容量，为了更有效地存放和找到它的entry。 * 当键值对的数量超过了临界值（capacity*load factor）这个方法自动调用 * 长度变为原来的2倍+1 * */@SuppressWarnings("unchecked")protected void rehash() &#123; //记录旧容量 int oldCapacity = table.length; //记录旧桶的数组 Entry&lt;?,?&gt;[] oldMap = table; // overflow-conscious code //新的容量为旧的容量的2倍+1 int newCapacity = (oldCapacity &lt;&lt; 1) + 1; //如果新的容量大于容量的最大值MAX_ARRAY_SIZE if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; //如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; //如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE newCapacity = MAX_ARRAY_SIZE; &#125; //创建新的数组，容量为新容量 Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; //结构性修改次数+1 modCount++; //计算扩容的临界值 threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; //将旧的数组中的键值对转移到新数组中 for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125;&#125; 看完代码，我们可以总结出rehash的总体思路为： 新建变量新的容量，值为旧的容量的2倍+1 如果新的容量大于容量的最大值MAX_ARRAY_SIZE 如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE 创建新的数组，容量为新容量 将旧的数组中的键值对转移到新数组中 这里可以看到，一般情况下，HashMap扩容后容量变为原来的两倍，而Hashtable扩容后容量变为原来的两倍加一。 HashTable的rehash方法相当于HashMap的resize方法。跟HashMap那种巧妙的rehash方式相比，HashTable的rehash过程需要对每个键值对都重新计算哈希值，而比起异或和与操作，取模是一个非常耗时的操作，所以这也是导致效率较低的原因之一。 六、遍历 可以使用与HashMap一样的遍历方式，但是由于历史原因，多了Enumeration的方式。 针对Enumeration，这里与iterator进行对比一下。 相同点 Iterator和Enumeration都可以对某些容器进行遍历。 Iterator和Enumeration都是接口。 不同点 Iterator有对容器进行修改的方法。而Enumeration只能遍历。 Iterator支持fail-fast，而Enumeration不支持。 Iterator比Enumeration覆盖范围广，基本所有容器中都有Iterator迭代器，而只有Vector、Hashtable有Enumeration。 Enumeration在JDK 1.0就已经存在了，而Iterator是JDK2.0新加的接口。 七、Hashtable与HashMap对比 HashTable的应用非常广泛，HashMap是新框架中用来代替HashTable的类，也就是说建议使用HashMap。 下面着重比较一下二者的区别： 1.继承不同 Hashtable是基于陈旧的Dictionary类的，HashMap是java1.2引进的Map接口的一个实现。 2.同步 Hashtable 中的方法是同步的，保证了Hashtable中的对象是线程安全的。 HashMap中的方法在缺省情况下是非同步的,HashMap中的对象并不是线程安全的。在多线程并发的环境下，可以直接使用Hashtable，但是要使用HashMap的话就要自己增加同步处理了。 3.效率 单线程中, HashMap的效率大于Hashtable。因为同步的要求会影响执行的效率，所以如果你不需要线程安全的集合，HashMap是Hashtable的轻量级实现，这样可以避免由于同步带来的不必要的性能开销，从而提高效率。 4.null值 Hashtable中，key和value都不允许出现null值，否则出现NullPointerException。 在HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键，而应该用containsKey()方法来判断。 5.遍历方式 Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable可以使用Enumeration的方式。 6.容量 Hashtable和HashMap它们两个内部实现方式的数组的初始大小和扩容的方式。 HashTable中hash数组默认大小是11，增加的方式是 old*2+1。 HashMap中hash数组的默认大小是16，而且一定是2的指数。 八、总结 无论什么时候有多个线程访问相同实例的可能时，就应该使用Hashtable，反之使用HashMap。非线程安全的数据结构能带来更好的性能。 如果在将来有一种可能—你需要按顺序获得键值对的方案时，HashMap是一个很好的选择，因为有HashMap的一个子类 LinkedHashMap。 所以如果你想可预测的按顺序迭代（默认按插入的顺序），你可以很方便用LinkedHashMap替换HashMap。反观要是使用的Hashtable就没那么简单了。 如果有多个线程访问HashMap，Collections.synchronizedMap（）可以代替，总的来说HashMap更灵活，或者直接用并发容器ConcurrentHashMap。 整理自： http://blog.csdn.net/panweiwei1994/article/details/77428710 http://blog.csdn.net/u013124587/article/details/52655042]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap和LinkedHashMap遍历机制]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F8.HashMap%E5%92%8CLinkedHashMap%E9%81%8D%E5%8E%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本篇单独讲一下HashMap和LinkedHashMap遍历方式。 一、对HashMap和LinkedHashMap遍历的几种方法 这里以HashMap为例，LinkedHashMap一样的方式。 12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println("key=" + next.getKey() + " value=" + next.getValue());&#125; 123456Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println("key=" + key + " value=" + map.get(key));&#125; 123map.forEach((key,value)-&gt;&#123; System.out.println("key=" + key + " value=" + value);&#125;); 强烈建议使用第一种 EntrySet 进行遍历。 第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。 我们知道，HashMap的输出顺序与元素的输入顺序无关，LinkedHashMap可以按照输入顺序输出，也可以根据读取元素的顺序输出。这一现象，已经在上一篇中展示出来了。 二、HashMap的遍历机制 HashMap 提供了两个遍历访问其内部元素Entry&lt;k,v&gt;的接口： Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()-------&gt;返回此映射所包含的映射关系的 Set 视图。 Set&lt;K&gt; keySet()--------&gt;返回此映射中所包含的键的 Set 视图。 实际上，第二个接口表示的Key的顺序，和第一个接口返回的Entry顺序是对应的，也就是说：这两种接口对HashMap的元素遍历的顺序相相同的。 那么，HashMap遍历内部Entry&lt;K,V&gt; 的顺序是什么呢？ 搞清楚这个问题，先要知道其内部结构是怎样的。 HashMap在存储Entry对象的时候，是根据Key的hash值判定存储到Entry[] table数组的哪一个索引值表示的链表上。 对HashMap遍历Entry对象的顺序和Entry对象的存储顺序之间没有任何关系。 HashMap散列图、Hashtable散列表是按“有利于随机查找的散列(hash)的顺序”。并非按输入顺序。遍历时只能全部输出，而没有顺序。甚至可以rehash()重新散列，来获得更利于随机存取的内部顺序。 所以对HashMap的遍历，由内部的机制决定的，这个机制是只考虑利于快速存取，不考虑输入等顺序。 三、LinkedHashMap 的遍历机制 LinkedHashMap 是HashMap的子类，它可以实现对容器内Entry的存储顺序和对Entry的遍历顺序保持一致。 为了实现这个功能，LinkedHashMap内部使用了一个Entry类型的双向链表，用这个双向链表记录Entry的存储顺序。当需要对该Map进行遍历的时候，实际上是遍历的是这个双向链表。 LinkedHashMap内部使用的LinkedHashMap.Entry类继承自Map.Entry类，在其基础上增加了LinkedHashMap.Entry类型的两个字段，用来引用该Entry在双向链表中的前面的Entry对象和后面的Entry对象。 它的内部会在Map.Entry类的基础上，增加两个Entry类型的引用：before，after。LinkedHashMap使用一个双向连表，将其内部所有的Entry串起来。 1234LinkedHashMap linkedHashMap = new LinkedHashMap(); linkedHashMap.put("name","louis"); linkedHashMap.put("age","24"); linkedHashMap.put("sex","male"); 对LinkedHashMap进行遍历的策略： 从 header.after 指向的Entry对象开始，然后一直沿着此链表遍历下去，直到某个entry.after == header 为止，完成遍历。 根据Entry&lt;K,V&gt;插入LinkedHashMap的顺序进行遍历的方式叫做：按插入顺序遍历。 另外，LinkedHashMap还支持一种遍历顺序，叫做：Get读取顺序。 如果LinkedHashMap的这个Get读取遍历顺序开启，那么，当我们在LinkedHashMap上调用get(key) 方法时，会导致内部key对应的Entry在双向链表中的位置移动到双向链表的最后。 四、遍历机制的总结 HashMap对元素的遍历顺序跟Entry插入的顺序无关，而LinkedHashMap对元素的遍历顺序可以跟Entry&lt;K,V&gt;插入的顺序保持一致：从双向。 当LinkedHashMap处于Get获取顺序遍历模式下，当执行get() 操作时，会将对应的Entry&lt;k,v&gt;移到遍历的最后位置。 LinkedHashMap处于按插入顺序遍历的模式下，如果新插入的&lt;key,value&gt; 对应的key已经存在，对应的Entry在遍历顺序中的位置并不会改变。 除了遍历顺序外，其他特性HashMap和LinkedHashMap基本相同。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F7.LinkedHashMap%2F</url>
    <content type="text"><![CDATA[大多数情况下，只要不涉及线程安全问题， Map 基本都可以使用 HashMap ，不过 HashMap 有一个问题，就是迭代 HashMap 的顺序并不是 HashMap 放置的顺序，也就是无序。 HashMap 的这一缺点往往会带来困扰，因为有些场景，我们期待一个有序的 Map。 篇幅有点长，但是在理解了HashMap之后就比较简单了。 这个时候，LinkedHashMap就闪亮登场了，它虽然增加了时间和空间上的开销，但是可以解决有排序需求的场景。 它的底层是继承于 HashMap 实现的，由一个双向循环链表所构成。 LinkedHashMap 的排序方式有两种： 根据写入顺序排序。 根据访问顺序排序。 其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。 一、LinkedHashMap数据结构 LinkedHashMap是通过哈希表和双向循环链表实现的，它通过维护一个双向循环链表来保证对哈希表迭代时的有序性，而这个有序是指键值对插入的顺序。 我们可以看出，遍历所有元素只需要从header开始遍历即可，一直遍历到下一个元素是header结束。 另外，当向哈希表中重复插入某个键的时候，不会影响到原来的有序性。也就是说，假设你插入的键的顺序为1、2、3、4，后来再次插入2，迭代时的顺序还是1、2、3、4，而不会因为后来插入的2变成1、3、4、2。（但其实我们可以改变它的规则，使它变成1、3、4、2） LinkedHashMap的实现主要分两部分，一部分是哈希表，另外一部分是链表。哈希表部分继承了HashMap，拥有了HashMap那一套高效的操作，所以我们要看的就是LinkedHashMap中链表的部分，了解它是如何来维护有序性的。 二、demo示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public static void main(String[] args) &#123; /** * HashMap插入数据，遍历输出无序 */ System.out.println("----------HashMap插入数据--------"); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("apple", "a"); map.put("watermelon", "b"); map.put("banana", "c"); map.put("peach", "d"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap插入数据，遍历，默认以插入顺序为序 */ System.out.println("----------LinkedHashMap插入数据,按照插入顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap = new LinkedHashMap&lt;&gt;(); linkedHashMap.put("apple", "a"); linkedHashMap.put("watermelon", "b"); linkedHashMap.put("banana", "c"); linkedHashMap.put("peach", "d"); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = linkedHashMap.entrySet().iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; /** * LinkedHashMap插入数据，设置accessOrder=true实现使得其遍历顺序按照访问的顺序输出，这里先用get方法来演示 */ System.out.println("----------LinkedHashMap插入数据,accessOrder=true:按照访问顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap2 = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); linkedHashMap2.put("apple", "aa"); linkedHashMap2.put("watermelon", "bb"); linkedHashMap2.put("banana", "cc"); linkedHashMap2.put("peach", "dd"); linkedHashMap2.get("banana");//banana移动到了内部的链表末尾 linkedHashMap2.get("apple");//apple移动到了内部的链表末尾 Iterator iter2 = linkedHashMap2.entrySet().iterator(); while (iter2.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter2.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap的put方法在accessOrder=true的情况下 */ System.out.println("-----------"); linkedHashMap2.put("watermelon", "bb");//watermelon移动到了内部的链表末尾 linkedHashMap2.put("stawbarrey", "ee");//末尾插入新元素stawbarrey linkedHashMap2.put(null, null);//插入新的节点 null Iterator iter3 = linkedHashMap2.entrySet().iterator(); while (iter3.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter3.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; &#125; 输出结果是: 12345678910111213141516171819202122----------HashMap插入数据--------banana=capple=apeach=dwatermelon=b----------LinkedHashMap插入数据,按照插入顺序进行排序--------apple=awatermelon=bbanana=cpeach=d----------LinkedHashMap插入数据,按照访问顺序进行排序--------watermelon=bbpeach=ddbanana=cc//banana到了末尾apple=aa//apple到了末尾-----------peach=ddbanana=ccapple=aawatermelon=bb//watermelon到了链表末尾stawbarrey=ee//新插入的放在末尾null=null//新插入的放在末尾 三、属性 LinkedHashMap可以认为是HashMap+LinkedList，即它既使用HashMap操作数据结构，又使用LinkedList维护插入元素的先后顺序 3.1 继承关系 123public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; LinkedHashMap是HashMap的子类，自然LinkedHashMap也就继承了HashMap中所有非private的方法。所以它已经从 HashMap 那里继承了与哈希表相关的操作了，那么在LinkedHashMap中，它可以专注于链表实现的那部分，所以与链表实现相关的属性如下。 3.2 属性介绍 1234567891011121314151617//LinkedHashMap的链表节点继承了HashMap的节点，而且每个节点都包含了前指针和后指针，所以这里可以看出它是一个双向链表static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125;//头指针transient LinkedHashMap.Entry&lt;K,V&gt; head;//尾指针transient LinkedHashMap.Entry&lt;K,V&gt; tail;//默认为false。当为true时，表示链表中键值对的顺序与每个键的插入顺序一致，也就是说重复插入键，也会更新顺序//简单来说，为false时，就是上面所指的1、2、3、4的情况；为true时，就是1、3、4、2的情况final boolean accessOrder; 五、构造方法 1234public LinkedHashMap() &#123; super(); accessOrder = false;&#125; 其实就是调用的 HashMap 的构造方法: HashMap 实现： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap init();&#125; 可以看到里面有一个空的 init()，具体是由 LinkedHashMap 来实现的： 12345@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 其实也就是对 header 进行了初始化。 六、添加元素 LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法. newNode() 会在HashMap的putVal() 方法里被调用，putVal() 方法会在批量插入数据putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) 或者插入单个数据public V put(K key, V value)时被调用。 LinkedHashMap重写了newNode(),在每次构建新节点时，通过linkNodeLast(p);将新节点链接在内部双向链表的尾部。 12345678910111213141516171819//在构建新节点时，构建的是`LinkedHashMap.Entry` 不再是`Node`.Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125;//将新增的节点，连接在链表的尾部private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; //集合之前是空的 if (last == null) head = p; else &#123;//将新节点连接在链表的尾部 p.before = last; last.after = p; &#125;&#125; 以及HashMap专门预留给LinkedHashMap的afterNodeAccess() 、afterNodeInsertion() 、afterNodeRemoval() 方法。 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 如果你没有注意到注释的解释的话，你可能会很奇怪为什么会有三个空方法，而且有不少地方还调用过它们。其实这三个方法表示的是在访问、插入、删除某个节点之后，进行一些处理，它们在LinkedHashMap有各自的实现。LinkedHashMap正是通过重写这三个方法来保证链表的插入、删除的有序性。 123456789101112131415161718//回调函数，新节点插入之后回调,判断是否需要删除最老插入的节点。//如果实现LruCache会用到这个方法。void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; //LinkedHashMap 默认返回false 则不删除节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;//LinkedHashMap 默认返回false 则不删除节点。 //返回true 代表要删除最早的节点。//通常构建一个LruCache会在达到Cache的上限是返回trueprotected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; void afterNodeInsertion(boolean evict)以及boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) 是构建LruCache需要的回调，在这可以忽略它们。 七、删除元素 LinkedHashMap也没有重写remove() 方法，因为它的删除逻辑和HashMap并无区别。 但它重写了afterNodeRemoval() 这个回调方法。该方法会在Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) 方法中回调，removeNode() 会在所有涉及到删除节点的方法中被调用，上文分析过，是删除节点操作的真正执行者。 1234567891011121314151617//在删除节点e时，同步将e从双向链表上删除void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //待删除节点 p 的前置后置节点都置空 p.before = p.after = null; //如果前置节点是null，则现在的头结点应该是后置节点a if (b == null) head = a; else//否则将前置节点b的后置节点指向a b.after = a; //同理如果后置节点时null ，则尾节点应是b if (a == null) tail = b; else//否则更新后置节点a的前置节点为b a.before = b;&#125; 八、查询元素 LinkedHashMap重写了get()和getOrDefault() 方法： 12345678910111213141516public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125;public V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return defaultValue; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 对比HashMap中的实现,LinkedHashMap只是增加了在成员变量(构造函数时赋值)accessOrder为true的情况下，要去回调void afterNodeAccess(Node&lt;K,V&gt; e) 函数。 1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 在afterNodeAccess() 函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。 12345678910111213141516171819202122232425262728293031void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last;//原尾节点 //如果accessOrder 是true ，且原尾节点不等于e if (accessOrder &amp;&amp; (last = tail) != e) &#123; //节点e强转成双向链表节点p LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //p现在是尾节点， 后置节点一定是null p.after = null; //如果p的前置节点是null，则p以前是头结点，所以更新现在的头结点是p的后置节点a if (b == null) head = a; else//否则更新p的前直接点b的后置节点为 a b.after = a; //如果p的后置节点不是null，则更新后置节点a的前置节点为b if (a != null) a.before = b; else//如果原本p的后置节点是null，则p就是尾节点。 此时 更新last的引用为 p的前置节点b last = b; if (last == null) //原本尾节点是null 则，链表中就一个节点 head = p; else &#123;//否则 更新 当前节点p的前置节点为 原尾节点last， last的后置节点是p p.before = last; last.after = p; &#125; //尾节点的引用赋值成p tail = p; //修改modCount。 ++modCount; &#125;&#125; 图示(注意这个图，1和6也应该是连在一起的，因为是双向循环链表，所以视为一个小错误)： 说明：从图中可以看到，结点3链接到了尾结点后面。 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 九、判断元素是否存在 它重写了该方法，相比HashMap的实现，更为高效。 123456789public boolean containsValue(Object value) &#123; //遍历一遍链表，去比较有没有value相等的节点，并返回 for (LinkedHashMap.Entry&lt;K,V&gt; e = head; e != null; e = e.after) &#123; V v = e.value; if (v == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; return false;&#125; 对比HashMap，是用两个for循环遍历，相对低效。 12345678910111213public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false;&#125; 十、替换某个元素 1234567891011121314// 用dst替换srcprivate void transferLinks(LinkedHashMap.Entry&lt;K,V&gt; src, LinkedHashMap.Entry&lt;K,V&gt; dst) &#123; LinkedHashMap.Entry&lt;K,V&gt; b = dst.before = src.before; LinkedHashMap.Entry&lt;K,V&gt; a = dst.after = src.after; if (b == null) head = dst; else b.after = dst; if (a == null) tail = dst; else a.before = dst;&#125; 十二、总结 LinkedHashMap相对于HashMap的源码比，是很简单的。因为大树底下好乘凉。它继承了HashMap，仅重写了几个方法，以改变它迭代遍历时的顺序。这也是其与HashMap相比最大的不同。 在每次插入数据，或者访问、修改数据时，会增加节点、或调整链表的节点顺序。以决定迭代时输出的顺序。 accessOrder默认是false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。为true时，可以在这基础之上构建一个LruCache. LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法.在每次构建新节点时，将新节点链接在内部双向链表的尾部 accessOrder=true的模式下,在afterNodeAccess()函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。值得注意的是，afterNodeAccess()函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 nextNode() 就是迭代器里的next()方法 。该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。 而双链表节点的顺序在LinkedHashMap的增、删、改、查时都会更新。以满足按照插入顺序输出，还是访问顺序输出。 它与HashMap比，还有一个小小的优化，重写了containsValue()方法，直接遍历内部链表去比对value值是否相等。 整理自： http://blog.csdn.net/zxt0601/article/details/77429150 http://wiki.jikexueyuan.com/project/java-collection/linkedhashmap.html http://blog.csdn.net/u013124587/article/details/52659741 http://www.cnblogs.com/leesf456/p/5248868.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F6.HashSet%2F</url>
    <content type="text"><![CDATA[HashSet 是一个不允许存储重复元素的集合，它是基于 HashMap 实现的， HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成。所以只要理解了 HashMap，HashSet 就水到渠成了。 成员变量 首先了解下HashSet的成员变量: 1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 发现主要就两个变量: map ：用于存放最终数据的。 PRESENT ：是所有写入map的value值。 构造方法 1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 构造函数很简单，利用了HashMap初始化了map。 add 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 比较关键的就是这个 add() 方法。 可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会受到影响，这样就保证了 HashSet 中只能存放不重复的元素。 该方法如果添加的是在 HashSet 中不存在的，则返回 true；如果添加的元素已经存在，返回 false。其原因在于我们之前提到的关于 HashMap 的 put 方法。该方法在添加 key 不重复的键值对的时候，会返回 null。 总结 HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。 所以 HashMap 会出现的问题 HashSet 依然不能避免。 对于 HashSet 中保存的对象，请注意正确重写其 equals 和 hashCode 方法，以保证放入的对象的唯一性。这两个方法是比较重要的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F5.HashMap%2F</url>
    <content type="text"><![CDATA[HashMap基本是面试必问的点，因为这个数据结构用的太频繁了，jdk1.8中的优化也是比较巧妙。有必要去深入探讨一下。但是涉及的内容比较多，这里只先探讨jdk8中HashMap的实现，至于jdk7中HashMap的死循环问题、红黑树的原理等都不会在本篇文章扩展到。其他的文章将会再去探讨整理。 本篇文章较长，高能预警。 一、前言 之前的List，讲了ArrayList、LinkedList，最后讲到了CopyOnWriteArrayList，就前两者而言，反映的是两种思想： （1）ArrayList以数组形式实现，顺序插入、查找快，插入、删除较慢 （2）LinkedList以链表形式实现，顺序插入、查找较慢，插入、删除方便 那么是否有一种数据结构能够结合上面两种的优点呢？有，答案就是HashMap。 HashMap是一种非常常见、方便和有用的集合，是一种键值对（K-V）形式的存储结构，在有了HashCode的基础后，下面将还是用图示的方式解读HashMap的实现原理。 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 二、HashMap的结构 其中哈希表是一个数组，我们经常把数组中的每一个节点称为一个桶，哈希表中的每个节点都用来存储一个键值对。 在插入元素时，如果发生冲突（即多个键值对映射到同一个桶上）的话，就会通过链表的形式来解决冲突。 因为一个桶上可能存在多个键值对，所以在查找的时候，会先通过key的哈希值先定位到桶，再遍历桶上的所有键值对，找出key相等的键值对，从而来获取value。 如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数： 容量 负载因子 容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。 三、继承关系 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 说明： HashMap继承自AbstractMap，AbstractMap是Map接口的骨干实现，AbstractMap中实现了Map中最重要最常用和方法，这样HashMap继承AbstractMap就不需要实现Map的所有方法，让HashMap减少了大量的工作。 四、属性 123456789101112131415161718192021222324252627//默认的初始容量为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//最大的容量上限为2^30static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的负载因子为0.75static final float DEFAULT_LOAD_FACTOR = 0.75f;//变成树型结构的临界值为8static final int TREEIFY_THRESHOLD = 8;//恢复链式结构的临界值为6static final int UNTREEIFY_THRESHOLD = 6;/** * 哈希表的最小树形化容量 * 当哈希表中的容量大于这个值时，表中的桶才能进行树形化 * 否则桶内元素太多时会扩容，而不是树形化 * 为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD */static final int MIN_TREEIFY_CAPACITY = 64;//哈希表transient Node&lt;K,V&gt;[] table;//哈希表中键值对的个数transient int size;//哈希表被修改的次数transient int modCount;//它是通过capacity*load factor计算出来的，当size到达这个值时，就会进行扩容操作int threshold;//负载因子final float loadFactor; 4.1 几个属性的详细说明 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍（为什么是两倍下文会说明）。 默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子`Load factor`的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子`loadFactor`的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意size和table的长度length、容纳最大键值对数量threshold的区别。 而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，因为常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考 http://blog.csdn.net/liuqiyao_01/article/details/14475159 ，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。下文会说明。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，并且链表的长度超过64时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 这里着重提一下MIN_TREEIFY_CAPACITY字段，容易与TREEIFY_THRESHOLD打架，TREEIFY_THRESHOLD是指桶中元素达到8个，就将其本来的链表结构改为红黑树，提高查询的效率。MIN_TREEIFY_CAPACITY是指最小树化的哈希表元素个数，也就是说，小于这个值，就算你(数组)桶里的元素数量大于8了，还是要用链表存储，只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。注意扩容是数组的复制。 4.2 Node结构 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 例如程序执行下面代码： 1map.put("美团","小美"); 系统将调用&quot;美团&quot;这个key的hashCode()方法得到其hashCode值（该方法适用于每个Java对象）。 然后再通过Hash算法来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。 当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。 那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法(5.4节)和扩容机制(5.5节)。下文会讲到。 五、方法 5.1 get方法 123456789101112131415161718192021222324252627282930//get方法主要调用的是getNode方法，所以重点要看getNode方法的实现public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //如果哈希表不为空 &amp;&amp; key对应的桶上不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //是否直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //判断是否有后续节点 if ((e = first.next) != null) &#123; //如果当前的桶是采用红黑树处理冲突，则调用红黑树的get方法去获取节点 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //不是红黑树的话，那就是传统的链式结构了，通过循环的方法判断链中是否存在该key do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 实现步骤大致如下： 通过hash值获取该key映射到的桶。 桶上的key就是要查找的key，则直接命中。 桶上的key不是要查找的key，则查看后续节点： 如果后续节点是树节点，通过调用树的方法查找该key。 如果后续节点是链式节点，则通过循环遍历链查找该key。 5.2 put方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法的具体实现也是在putVal方法中，所以我们重点看下面的putVal方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果哈希表为空，则先创建一个哈希表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果当前桶没有碰撞冲突，则直接把键值对插入，完事 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果桶上节点的key与当前key重复，那你就是我要找的节点了 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果是采用红黑树的方式处理冲突，则通过红黑树的putTreeVal方法去插入这个键值对 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //否则就是传统的链式结构 else &#123; //采用循环遍历的方式，判断链中是否有重复的key for (int binCount = 0; ; ++binCount) &#123; //到了链尾还没找到重复的key，则说明HashMap没有包含该键 if ((e = p.next) == null) &#123; //创建一个新节点插入到尾部 p.next = newNode(hash, key, value, null); //如果链的长度大于TREEIFY_THRESHOLD这个临界值，则把链变为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //找到了重复的key if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //这里表示在上面的操作中找到了重复的键，所以这里把该键的值替换为新值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //判断是否需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 1234567891011121314151617181920final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; put方法比较复杂，实现步骤大致如下： 先通过hash值计算出key映射到哪个桶。 如果桶上没有碰撞冲突，则直接插入。 如果出现碰撞冲突了，则需要处理冲突： 如果该桶使用红黑树处理冲突，则调用红黑树的方法插入。 否则采用传统的链式方法插入。如果链的长度到达临界值，则把链转变为红黑树。 如果桶中存在重复的键，则为该键替换新值。 如果size大于阈值，则进行扩容。 5.3 remove方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//remove方法的具体实现在removeNode方法中，所以我们重点看下面的removeNode方法public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果当前key映射到的桶不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //如果桶上的节点就是要找的key，则直接命中 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //如果是以红黑树处理冲突，则构建一个树节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); //如果是以链式的方式处理冲突，则通过遍历链表来寻找节点 else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //比对找到的key的value跟要删除的是否匹配 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //通过调用红黑树的方法来删除节点 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //使用链表的操作来删除节点 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 5.4 hash方法(确定哈希桶数组索引位置) 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。 注意get方法和put方法源码中都需要先计算key映射到哪个桶上，然后才进行之后的操作，计算的主要代码如下： 1(n - 1) &amp; hash 上面代码中的n指的是哈希表的大小，hash指的是key的哈希值，hash是通过下面这个方法计算出来的，采用了二次哈希的方式，其中key的hashCode方法是一个native方法： 123456static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 总结就是：由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2^n 。这样用 2^n - 1 做位运算与取模效果一致，并且效率还要高出许多。这样回答了上文中：好的Hash算法到底是什么。 5.5 resize方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //计算扩容后的大小 if (oldCap &gt; 0) &#123; //如果当前容量超过最大容量，则无法进行扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //没超过最大值则扩为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //新的resize阈值 threshold = newThr; //创建新的哈希表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; //遍历旧哈希表的每个桶，重新计算桶里元素的新位置 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //如果桶上只有一个键值对，则直接插入 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果是通过红黑树来处理冲突的，则调用相关方法把树分离开 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果采用链式处理冲突 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //通过上面讲的方法来计算节点的新位置 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。 例如，原来的容量为32，那么应该拿hash跟31（0x11111）做与操作；在扩容扩到了64的容量之后，应该拿hash跟63（0x111111）做与操作。新容量跟原来相比只是多了一个bit位，假设原来的位置在23，那么当新增的那个bit位的计算结果为0时，那么该节点还是在23；相反，计算结果为1时，则该节点会被分配到23+31的桶上。 这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。回答了上文中好的扩容机制。 六、总结 HashMap的结构底层是一个数组，每个数组元素是一个桶，后面可能会连着一串因为碰撞而聚在一起的(key,value)节点，以链表的形式或者树的形式挂着 按照原来的拉链法来解决冲突，如果一个桶上的冲突很严重的话，是会导致哈希表的效率降低至O（n），而通过红黑树的方式，可以把效率改进至O（logn）。相比链式结构的节点，树型结构的节点会占用比较多的空间，所以这是一种以空间换时间的改进方式。 threshold是数组长度扩容的临界值 modCount字段主要用来记录HashMap内部结构发生变化的次数，这里结构变化必须是新的值塞进来或者某个值删除这种类型，而不是仅仅是覆盖 只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。 好的hash算法：由于在计算中位运算比取模运算效率高的多，所以HashMap规定数组的长度为 2^n 。这样用 2^n - 1 与 hash 做位运算与取模效果一致，并且效率还要高出许多。 好的扩容机制：因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。 还有就是要记住put的过程。 整理自： https://zhuanlan.zhihu.com/p/21673805 http://blog.csdn.net/u013124587/article/details/52649867]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashcode/Equals]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F4.hashcode%E5%92%8Cequals%2F</url>
    <content type="text"><![CDATA[hashcode涉及到集合HashMap等集合，此篇侧重于了解hashcode和equals方法的作用的原理。有助于下一篇HashMap的理解。 一、Hash是什么 Hash是散列的意思，就是把任意长度的输入，通过散列算法变换成固定长度的输出，该输出就是散列值。这个玩意还可以做加密。 不同关键字经过散列算法变换后可能得到同一个散列地址，这种现象称为碰撞。 如果两个Hash值不同（前提是同一Hash算法），那么这两个Hash值对应的原始输入必定不同 二、什么是hashcode HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的。 如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置。 如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写。为什么呢？下文会说。 三、HashCode有什么用 比方说Set里面已经有1000个元素了，那么第1001个元素进来的时候，最多可能调用1000次equals方法，如果equals方法写得复杂，对比的东西特别多，那么效率会大大降低。 使用HashCode就不一样了，比方说HashSet，底层是基于HashMap实现的，先通过HashCode取一个模，这样一下子就固定到某个位置了，如果这个位置上没有元素，那么就可以肯定HashSet中必定没有和新添加的元素equals的元素，就可以直接存放了，都不需要比较； 如果这个位置上有元素了，逐一比较，比较的时候先比较HashCode，HashCode都不同接下去都不用比了，肯定不一样，HashCode相等，再equals比较，没有相同的元素就存，有相同的元素就不存。 如果原来的Set里面有相同的元素，只要HashCode的生成方式定义得好（不重复），不管Set里面原来有多少元素，只需要执行一次的equals就可以了。这样一来，实际调用equals方法的次数大大降低，提高了效率。 当俩个对象的`hashCode`值相同的时候，`Hashset`会将对象保存在同一个位置，但是他们`equals`返回`false`，所以实际上这个位置采用链式结构来保存多个对象。 四、为什么重写Object的equals()方法尽量要重写Object的hashCode()方法 面临问题：若两个对象equals相等，但由于不在一个区间，因为hashCode的值在重写之前是对内存地址计算得出，所以根本没有机会进行比较，会被认为是不同的对象(这就是为什么还要重写hashcode方法了)。所以Java对于eqauls方法和hashCode方法是这样规定的： 1 如果两个对象相同(equals为true)，那么它们的hashCode值一定要相同。也告诉我们重写equals方法，一定要重写hashCode方法，也就是说hashCode值要和类中的成员变量挂上钩，对象相同–&gt;成员变量相同—-&gt;hashCode值一定相同。 2 如果两个对象的hashCode相同(只是映射到同一个位置而已)，它们并不一定相同，这里的对象相同指的是用eqauls方法比较。 简单来说，如果只重写equals方法而不重写hashcode方法，会导致重复元素的产生。具体通过下面的例子进行说明。 五、举例 6.1 Student类 很简单，定义了id和name两个字段，无参和有参构造函数，toString方法。 1234567891011121314151617181920public class Student &#123; private int id; private String name; get(),set()略... public Student()&#123;&#125; public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; @Override public String toString() &#123; return "Student [id=" + id + ", name=" + name + "]"; &#125;&#125; 6.2 main方法 1234567891011121314151617181920public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 执行结果： 1234set集合容量为: 3Student [id=1, name=hh]---1735600054Student [id=1, name=hh]---356573597Student [id=2, name=gg]---21685669 我们可以看到，只要是new的对象，他们的hashcode是不一样的。所以，就会认为他们是不一样的对象。所以，集合里面数量为3. 6.3 只重写equals()方法，而不重写HashCode()方法 输出： 1234set集合容量为: 3Student [id=2, name=gg]---2018699554Student [id=1, name=hh]---366712642Student [id=1, name=hh]---1829164700 结论：覆盖equals（Object obj）但不覆盖hashCode(),导致数据不唯一性。 在这里，其实我们可以看到，student1和student2其实是一个对象，但是由于都是new并且没有重写hashcode导致他们变成了两个不一样的对象。 分析： （1）当执行set.add(student1)时，集合为空，直接存入集合； （2）当执行set.add(student2)时，首先判断该对象（student2）的hashCode值所在的存储区域是否有相同的hashCode，因为没有覆盖hashCode方法，所以jdk使用默认Object的hashCode方法，返回内存地址转换后的整数，因为不同对象的地址值不同，所以这里不存在与student2相同hashCode值的对象，因此jdk默认不同hashCode值，equals一定返回false，所以直接存入集合。 （3）当执行set.add(student3)时,与2同理。 （4）当最后执行set.add(student1)时，因为student1已经存入集合，同一对象返回的hashCode值是一样的，继续判断equals是否返回true，因为是同一对象所以返回true。此时jdk认为该对象已经存在于集合中，所以舍弃。 6.4 只重写HashCode()方法，equals()方法直接返回false 1234set集合容量为: 3Student [id=1, name=hh]---4320Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 按照上面的分析，可能会觉得里面应该装4个，因为两次add的student1，虽然他们的hashcode一样，但是equals直接返回false，那么应该判定为两个不同的对象。但是结果确跟我们预想的不一样。 分析： 首先student1和student2的对象比较hashCode，因为重写了HashCode方法，所以hashcode相等,然后比较他们两的equals方法，因为equals方法始终返回false,所以student1和student2也是不相等的，所以student2也被放进了set 首先student1(student2)和student3的对象比较hashCode，不相等，所以student3放进set中 最后再看最后重复添加的student1,与第一个student1的hashCode是相等的，在比较equals方法，因为equals返回false,所以student1和student4不相等;同样，student2和student4也是不相等的;student3和student4的hashcode都不相等，所以肯定不相等的，所以最后一个重复的student1应该可以放到set集合中，那么结果应该是size:4,那为什么会是3呢？ 这时候我们就需要查看HashSet的源码了，下面是HashSet中的add方法的源码： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 这里我们可以看到其实HashSet是基于HashMap实现的，我们在点击HashMap的put方法，源码如下： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 首先是判断hashCode是否相等，不相等的话，直接跳过，相等的话，然后再来比较这两个对象是否相等或者这两个对象的equals方法，因为是进行的或操作，所以只要有一个成立即可，那这里我们就可以解释了，其实上面的那个集合的大小是3,因为最后的一个r1没有放进去，以为r1==r1返回true的，所以没有放进去了。所以集合的大小是3，如果我们将hashCode方法设置成始终返回false的话，这个集合就是4了。 6.5 同时重写 我的写法是： 12345678910111213141516171819@Overridepublic int hashCode() &#123; int result = 17; result = result * 31 + name.hashCode(); result = result * 31 + id; return result;&#125;@Overridepublic boolean equals(Object obj) &#123; if(obj == this) return true; if(!(obj instanceof Student)) return false; Student o = (Student)obj; return o.name.equals(name) &amp;&amp; o.id == id;&#125; 结果： 123set集合容量为: 2Student [id=2, name=gg]---118515Student [id=1, name=hh]---119506 达到我们预期的效果。 六、内存泄露 我们上面实验了重写equals和hashcode方法，执行main，执行结果是： 123set集合容量为: 2Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 将main方法改为： 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); //------新增的开始------- student3.setId(11); set.remove(student3); System.out.println("set集合容量为: "+set.size()); //------新增的结束------- Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 运行结果是： 1234set集合容量为: 2set集合容量为: 2Student [id=1, name=hh]---4320Student [id=11, name=gg]---4598 我们调用了remove删除student3对象，以为删除了student3,但事实上并没有删除，这就叫做内存泄露，就是不用的对象但是他还在内存中。所以我们多次这样操作之后，内存就爆了。 原因： 在调用remove方法的时候，会先使用对象的hashCode值去找到这个对象，然后进行删除，这种问题就是因为我们在修改了对象student3的id属性的值，又因为RectObject对象的hashCode方法中有id值参与运算,所以student3对象的hashCode就发生改变了，所以remove方法中并没有找到student3了，所以删除失败。即student3的hashCode变了，但是他存储的位置没有更新，仍然在原来的位置上，所以当我们用他的新的hashCode去找肯定是找不到了。 总结： 上面的这个内存泄露告诉我一个信息：如果我们将对象的属性值参与了hashCode的运算中，在进行删除的时候，就不能对其属性值进行修改，否则会出现严重的问题。 七、总结 hashCode是为了提高在散列结构存储中查找的效率，在线性表中没有作用。 equals和hashCode需要同时覆盖。 若两个对象equals返回true，则hashCode有必要也返回相同的int数。 若两个对象equals返回false，则hashCode不一定返回不同的int数,但为不相等的对象生成不同hashCode值可以提高哈希表的性能。 若两个对象hashCode返回相同int数，则equals不一定返回true。 同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 整理自： http://blog.csdn.net/haobaworenle/article/details/53819838 http://www.cnblogs.com/xrq730/p/4842028.html http://blog.csdn.net/qq_21688757/article/details/53067814 http://blog.csdn.net/fyxxq/article/details/42066843]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CopyOnWriteArrayList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F3.CopyOnWriteArrayList%2F</url>
    <content type="text"><![CDATA[CopyOnWriteArrayList是ArrayList的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 CopyOnWriteArrayList是一个写时复制的容器，采用了读写分离的思想。通俗点来讲，在对容器进行写操作时，不直接修改当前容器，而是先对当前容器进行拷贝得到一个副本，然后对副本进行写操作，最后再将原容器的引用指向拷贝出来的副本。这样做的好处就是可以对容器进行并发读而不用进行加锁。 一、类的继承关系 12public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 含义不需要再赘述了。 二、类的属性 12345678910111213141516171819202122/** 用于在对数组产生写操作的方法加锁. */final transient ReentrantLock lock = new ReentrantLock();/** 底层的存储结构. */private transient volatile Object[] array;/** 反射机制. */private static final sun.misc.Unsafe UNSAFE;/** lock域的内存偏移量.是通过反射拿到的 */private static final long lockOffset;static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = CopyOnWriteArrayList.class; lockOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("lock")); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 三、数组末尾添加一个元素 12345678910111213141516171819202122public boolean add(E e) &#123; // 可重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 元素数组 Object[] elements = getArray(); // 数组长度 int len = elements.length; // 复制数组 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将要添加的元素放到副本数组的末尾去 newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 基本原理很简单，就是对当前数组加锁，内部复制一个新数组，处理完毕，修改引用即可，达到最终一致的效果。 四、如果没有这个元素则添加 12345public boolean addIfAbsent(E e) &#123; Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false : addIfAbsent(e, snapshot);&#125; 该函数用于添加元素（如果数组中不存在，则添加；否则，不添加，直接返回）。如何可以保证多线程环境下不会重复添加元素？ 答案：通过快照数组和当前数组进行对比来确定是否一致，确保添加元素的线程安全 12345678910111213141516171819202122232425262728293031323334private boolean addIfAbsent(E e, Object[] snapshot) &#123; // 重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 获取数组 Object[] current = getArray(); // 数组长度 int len = current.length; if (snapshot != current) &#123; // 快照不等于当前数组，对数组进行了修改 // 取较小者 int common = Math.min(snapshot.length, len); for (int i = 0; i &lt; common; i++) // 遍历 if (current[i] != snapshot[i] &amp;&amp; eq(e, current[i])) // 当前数组的元素与快照的元素不相等并且e与当前元素相等 // 表示在snapshot与current之间修改了数组，并且设置了数组某一元素为e，已经存在 // 返回false return false; if (indexOf(e, current, common, len) &gt;= 0) // 在当前数组中找到e元素 // 返回false return false; &#125; // 复制数组 Object[] newElements = Arrays.copyOf(current, len + 1); // 对数组len索引的元素赋值为e newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 该函数的流程如下： 获取锁，获取当前数组为current，current长度为len，判断数组之前的快照snapshot是否等于当前数组current，若不相等，则进入步骤2；否则，进入步骤3 不相等，表示在snapshot与current之间，对数组进行了修改，直接返回false结束; 说明当前数组等于快照数组，说明数组没有被改变。在当前数组中索引指定元素，若能够找到，说明已经存在此元素，直接返回false结束；否则进入4 说明没有当前要插入的元素，通过数组复制的方式添加到末尾 无论如何，都要释放锁 五、获取指定索引的元素 1234567public E get(int index) &#123; return get(getArray(), index);&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 通过写时复制的方式，CopyOnWriteArrayList 的 get 方法不用加锁也可以保证线程安全，所以 CopyOnWriteArrayList 并发读的效率是非常高的，它是直接通过数组下标获取元素的。 六、总结 简单而言要记住它的三个特点： CopyOnWriteArrayList 是一个并发的数组容器，它的底层实现是数组。 CopyOnWriteArrayList 采用写时复制的方式来保证线程安全。 通过写时复制的方式，可以高效的进行并发读，但是对于写操作，每次都要进行加锁以及拷贝副本，效率非常低，所以 CopyOnWriteArrayList 仅适合读多写少的场景。 Vector虽然是线程安全的，但是只是一种相对的线程安全而不是绝对的线程安全，它只能够保证增、删、改、查的单个操作一定是原子的，不会被打断，但是如果组合起来用，并不能保证线程安全性。 CopyOnWriteArrayList在并发下不会产生任何的线程安全问题，也就是绝对的线程安全 另外，有两点必须讲一下。 我认为CopyOnWriteArrayList这个并发组件，其实反映的是两个十分重要的分布式理念： （1）读写分离 我们读取CopyOnWriteArrayList的时候读取的是CopyOnWriteArrayList中的Object[] array，但是修改的时候，操作的是一个新的Object[] array，读和写操作的不是同一个对象，这就是读写分离。这种技术数据库用的非常多，在高并发下为了缓解数据库的压力，即使做了缓存也要对数据库做读写分离，读的时候使用读库，写的时候使用写库，然后读库、写库之间进行一定的同步，这样就避免同一个库上读、写的IO操作太多 （2）最终一致 对CopyOnWriteArrayList来说，线程1读取集合里面的数据，未必是最新的数据。因为线程2、线程3、线程4四个线程都修改了CopyOnWriteArrayList里面的数据，但是线程1拿到的还是最老的那个Object[] array，新添加进去的数据并没有，所以线程1读取的内容未必准确。不过这些数据虽然对于线程1是不一致的，但是对于之后的线程一定是一致的，它们拿到的Object[] array一定是三个线程都操作完毕之后的Object array[]，这就是最终一致。最终一致对于分布式系统也非常重要，它通过容忍一定时间的数据不一致，提升整个分布式系统的可用性与分区容错性。当然，最终一致并不是任何场景都适用的，像火车站售票这种系统用户对于数据的实时性要求非常非常高，就必须做成强一致性的。 最后总结一点，随着CopyOnWriteArrayList中元素的增加，CopyOnWriteArrayList的修改代价将越来越昂贵，因此，CopyOnWriteArrayList适用于读操作远多于修改操作的并发场景中。 感谢 http://www.cnblogs.com/xrq730/p/5020760.html http://blog.csdn.net/u013124587/article/details/52863533 https://www.cnblogs.com/leesf456/p/5547853.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F2.LinkedList%2F</url>
    <content type="text"><![CDATA[提到ArrayList，就会比较与LinkedList的区别。本文来看看LinkedList的核心原理。 如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。 一、LinkedList属性 123456//链表的节点个数.transient int size = 0;//Pointer to first node.transient Node&lt;E&gt; first;//Pointer to last node.transient Node&lt;E&gt; last; 二、Node的结构 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next;//后置指针 Node&lt;E&gt; prev;//前置指针 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 三、添加元素 3.1 LinkedList表头添加一个元素 当向表头插入一个节点时，很显然当前节点的前驱一定为 null，而后继结点是 first 指针指向的节点，当然还要修改 first 指针指向新的头节点。除此之外，原来的头节点变成了第二个节点，所以还要修改原来头节点的前驱指针，使它指向表头节点，源码的实现如下： 1234567891011121314151617public void addFirst(E e) &#123; linkFirst(e);&#125;private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); //新节点作为新的first节点 first = newNode; if (f == null) last = newNode;//初始就是个空LinkedList的话，last指向当前新节点 else f.prev = newNode;//初始值不为空，将其前置指针指向新节点 size++; modCount++;&#125; 3.2 LinkedList表尾添加一个元素 当向表尾插入一个节点时，很显然当前节点的后继一定为 null，而前驱结点是 last 指针指向的节点，然后还要修改 last 指针指向新的尾节点。此外，还要修改原来尾节点的后继指针，使它指向新的尾节点，源码的实现如下： 123456789101112131415161718public void addLast(E e) &#123; linkLast(e);&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); //新节点作为新的last节点 last = newNode; //如果原来有尾节点，则更新原来节点的后继指针，否则更新头指针 if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 3.3 LinkedList在指定节点前添加一个元素 12345678910111213141516171819202122232425262728293031323334public void add(int index, E element) &#123; //判断数组是否越界 checkPositionIndex(index); if (index == size) linkLast(element);//直接插在最后一个 else linkBefore(element, node(index));//在index节点之前插入&#125;private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; //找到索引位置的前面一个元素pred final Node&lt;E&gt; pred = succ.prev; //新节点，前置指针指向pred,后置指针指向索引处元素 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); //修改索引出元素的前置指针为新节点 succ.prev = newNode; if (pred == null) first = newNode;//说明是插在表头 else pred.next = newNode;//说明是插在非表头位置，修改pred后置指针为新指针 size++; modCount++;&#125; 可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。 四、删除元素 删除操作与添加操作大同小异，例如删除指定节点的过程如下图所示，需要把当前节点的前驱节点的后继修改为当前节点的后继，以及当前节点的后继结点的前驱修改为当前节点的前驱。 就不赘述了。 五、获取元素 12345678910111213141516171819202122//获取指定索引对应的元素public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;//寻找元素的方向是根据index在表中的位置决定的Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123;//索引小于表长的一半，从表头开始往后找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123;//索引大于表长的一半，从表尾往前开始找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 上述代码，利用了双向链表的特性，如果index离链表头比较近，就从节点头部遍历。否则就从节点尾部开始遍历。使用空间（双向链表）来换取时间。 node()会以O(n/2)的性能去获取一个结点 如果索引值大于链表大小的一半，那么将从尾结点开始遍历 这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 总结 1、理论上无容量限制，只受虚拟机自身限制影响，所以没有扩容方法。 2、和ArrayList一样，LinkedList也是是未同步的，多线程并发读写时需要外部同步，如果不外部同步，那么可以使用Collections.synchronizedList方法对LinkedList的实例进行一次封装。 3、和ArrayList一样，LinkedList也对存储的元素无限制，允许null元素。 4、顺序插入速度ArrayList会比较快，因为ArrayList是基于数组实现的，数组是事先new好的，只要往指定位置塞一个数据就好了；LinkedList则不同，每次顺序插入的时候LinkedList将new一个对象出来，如果对象比较大，那么new的时间势必会长一点，再加上一些引用赋值的操作，所以顺序插入LinkedList必然慢于ArrayList 5、基于上一点，因为LinkedList里面不仅维护了待插入的元素，还维护了Entry的前置Entry和后继Entry，如果一个LinkedList中的Entry非常多，那么LinkedList将比ArrayList更耗费一些内存 6、数据遍历的速度，看最后一部分，这里就不细讲了，结论是：使用各自遍历效率最高的方式，ArrayList的遍历效率会比LinkedList的遍历效率高一些 7、有些说法认为LinkedList做插入和删除更快，这种说法其实是不准确的： LinkedList做插入、删除的时候，慢在寻址，快在只需要改变前后Entry的引用地址 ArrayList做插入、删除的时候，慢在数组元素的批量copy，快在寻址 所以，如果待插入、删除的元素是在数据结构的前半段尤其是非常靠前的位置的时候，LinkedList的效率将大大快过ArrayList，因为ArrayList将批量copy大量的元素；越往后，对于LinkedList来说，因为它是双向链表，所以在第2个元素后面插入一个数据和在倒数第2个元素后面插入一个元素在效率上基本没有差别，但是ArrayList由于要批量copy的元素越来越少，操作速度必然追上乃至超过LinkedList。 从这个分析看出，如果你十分确定你插入、删除的元素是在前半段，那么就使用LinkedList；如果你十分确定你删除、删除的元素在比较靠后的位置，那么可以考虑使用ArrayList。如果你不能确定你要做的插入、删除是在哪儿呢？那还是建议你使用LinkedList吧，因为一来LinkedList整体插入、删除的执行效率比较稳定，没有ArrayList这种越往后越快的情况；二来插入元素的时候，弄得不好ArrayList就要进行一次扩容，记住，ArrayList底层数组扩容是一个既消耗时间又消耗空间的操作. 8、ArrayList使用最普通的for循环遍历，LinkedList使用foreach循环比较快.注意到ArrayList是实现了RandomAccess接口而LinkedList则没有实现这个接口.关于RandomAccess这个接口的作用，看一下JDK API上的说法： 9、如果使用普通for循环遍历LinkedList，在大数据量的情况下，其遍历速度将慢得令人发指 整理自： 1、http://www.cnblogs.com/xrq730/p/5005347.html 2、http://blog.csdn.net/u013124587/article/details/52837848 3、http://blog.csdn.net/u011392897/article/details/57115818 4、http://blog.csdn.net/fighterandknight/article/details/61476335]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList/Vector]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F1.ArrayList%E5%92%8CVector%2F</url>
    <content type="text"><![CDATA[面试中，关于java的一些容器，ArrayList是最简单也是最常问的，尤其是里面的扩容机制。 ArrayList 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 构造函数为： 123456789101112131415161718//用初始容量作为参数的构造方法public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //初始容量大于0，实例化数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //初始容量等于0，赋予空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125;//无参的构造方法public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 从构造方法中我们可以看见，默认情况下，elementData是一个大小为0的空数组，当我们指定了初始大小的时候，elementData的初始大小就变成了我们所指定的初始大小了。 ArrayList相当于动态数据，其中最重要的两个属性分别是: elementData 数组，以及 size 大小。 在调用 add() 方法的时候： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话： 12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码: 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 也是一个数组复制的过程，ArrayList每次扩容都是扩1.5倍，然后调用Arrays类的copyOf方法，把元素重新拷贝到一个新的数组中去。 由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。 序列化 由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 从实现中可以看出 ArrayList 只序列化了被使用的数据。 Vector Vector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 Vector比ArrayList多了一个属性： 1protected int capacityIncrement; 这个属性是在扩容的时候用到的，它表示每次扩容只扩capacityIncrement个空间就足够了。该属性可以通过构造方法给它赋值。先来看一下构造方法： 123456789101112131415public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 从构造方法中，我们可以看出Vector的默认大小也是10，而且它在初始化的时候就已经创建了数组了，这点跟ArrayList不一样。再来看一下grow方法： 12345678910private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从grow方法中我们可以发现，newCapacity默认情况下是两倍的oldCapacity，而当指定了capacityIncrement的值之后，newCapacity变成了oldCapacity+capacityIncrement。 以下是 add() 方法： 123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据: 12345678910111213public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年的最后一天，对商城项目的架构做个改造]]></title>
    <url>%2F2019%2F01%2F20%2F2018%E5%B9%B4%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%A4%A9%EF%BC%8C%E5%AF%B9%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%9E%B6%E6%9E%84%E5%81%9A%E4%B8%AA%E6%94%B9%E9%80%A0%2F</url>
    <content type="text"><![CDATA[一直以来都是学习慕课的实战视频，虽然也跟着做出了一些东西，但是思路都是别人提供好的，脱离了老师，我一直在问自己一个问题：能不能独立地按照自己的思路做出一些东西来？ 前方图片高能…更有几十兆gif演示动画，图片全部存放于七牛云上。 在去年，即2017年年底，我在慕课上学习了这两门课程： 第一期项目实现了比较简单的电商业务，整合SSM，并且部署到云端。 第二期实现了tomcat集群，配合redis实现分布式session，还有一些定时任务、redis分布式锁、maven环境隔离的一些东西，还涉及很多spring和springmvc的有用的机巧。 整体感觉是：一期实现业务，二期对于一期的提高不是太大，跟分布式无太大关系，仅仅实现了单点登陆和分布式session存储而已。 个人感觉下一期的课程应该是springCloud的分布式改造，进行服务拆分和治理。所以，在整合这两个课程的基础上用springCloud进行微服务治理。 项目详细描述 项目源码地址：https://github.com/sunweiguo/MMall 整体效果演示： 下面贴个小一点的gif: 部分页面截图： 经过一遍遍测试，商城还是存在一些无伤大雅的bug，但是主要还是锻炼自己的能力嘛！ 下订单的时候，报错：商品不存在或库存不足，是因为我模拟的秒杀，所以商品的库存要提前预置于redis中，后端管理系统的商品管理页面有预置库存的按钮。 新增商品的时候，对于上传图片，需要耐心等待一会，需要等待FTP服务器上传完毕，给一个返回信息(Map，是图片的文件名)才能真正显示（对于富文本中的图片上传，在上传之后需要等待一会，时间与小图上传差不多，否则直接保存不报错，但是前端看不到图，因为还没上传完毕,url还没回传回来） 普通注册的账号，没有管理员权限，所以不能登陆后台管理系统。 后来做了eureka集群，但是配置文件还是只指向其中一个eureka 在学习的视频中，一期只是实现业务功能，单体架构，一个tomcat。二期对其做了集群，并且解决了集群模式下session存储问题，实现了比较简单的单点登陆功能。架构如下： 对于上面的架构来说，只是做了一些集群进行优化，随着业务的发展，用户越来越多，用户服务等其他服务必然要拆分出来独立成为一个服务，这样做的好处是，一方面一个团队负责一个服务可以提高开发效率，另外，对于扩展性也是非常有利的，但是也是有缺点的，会带来很多的复杂性，尤其是引入了分布式事务，所以不能为了分布式而分布式，而是针对不同的业务场景而采用合适的架构。 微服务的实现，主要有两种，国内是阿里系的以dubbo+zookeeper为核心的一套服务治理和发现生态。另一个则是大名鼎鼎的spring cloud栈。 spring cloud并不是像spring是一个框架，他是解决微服务的一种方案，是由各种优秀开源组件共同配合而实现的微服务治理架构。下面的图是我构思的项目结构图： 最前面是Nginx，这里就作为一个静态资源映射和负载均衡，nginx中有几个配置文件，分别为www.oursnail.cn.conf，这个主要是对zuul网关地址做一个负载均衡，指向网关所在的服务器，并且找到前台页面所在位置对页面进行渲染。admin.oursnail.cn.conf，这个主要是配置后端以及后端的页面文件；img.oursnail.cn.conf是对图片服务器地址进行映射。 然后是zuul网关，这里主要是用来限流、鉴权以及路由转发。 再后面就是我们的应用服务器啦。对服务器进行了服务追踪(sleuth)，实现了动态刷新配置(spring cloud config+bus)等功能。以http restful的方式进行通信(openFeign),构建起以eureka为注册中心的分布式架构。 每个服务都是基于springboot打造，结合mybatis持久层操作的框架，完成基本的业务需求。springboot基于spring，特点是快速启动、内置tomcat以及无xml配置。将很多东西封装起来，引入pom就可以直接使用，比如springMVC就基本上引入starter-web即可。 由于资源的原因，只有三台最低配的服务器，所以本来想做的基于ES的全文检索服务没有做，也没有分库分表。 至于定时任务以及Hystrix服务熔断和降级，比较简单，就不做了。 项目的接口文档详见wiki：https://github.com/sunweiguo/spring-cloud-for-snailmall/wiki 项目的数据库表设计：snailmall.sql 下面详细介绍每个模块实现的大体思路（仅供参考，毕竟应届生，真实项目没做过）： 用户模块 关于用户模块，核心的功能是登陆。再核心是如何验证以及如何存储用户信息。这里采取的方案为： 对于用户注册，我这里就是用户名（昵称），那么如何保证不重复呢（高并发）？这里还是用了分布式锁来保证的。 对于未登陆章台下用户修改密码，逻辑为： 购物车模块 订单模块 针对这些问题，我想说一下我的思路。 对于幂等性，这里产生幂等性的主要原因在于MQ的重传机制，可能第一个消息久久没有发出去，然后重新发送一条，结果第一条消息突然又好了，那么就会重复发两跳，对于用户来说，只下一次单，但是服务器下了多次订单。网上解决这个问题的思路是创建一张表，如果是重复的订单号，就不可能创建多次了。还有一种可能方案是用分布式锁对该订单号锁住一段时间，由于只是锁住订单号，所以不影响性能，在这一段时间内是不可以再放同一个订单号的请求进来。 对于MQ消息不丢失，只能是订阅模式了。消息发出去之后，消费端给MQ回复一个接收到的信息，MQ本次消费成功，给订阅者一个回复。 对于全局唯一ID生成，这里用的是雪花算法，具体介绍可以看我的笔记 对于分布式事务，比较复杂，这里其实并没有真正处理，对于数据库扣减库存和数据库插入订单，他们在不同的数据库，廖师兄比较倾向的方式是： 这一切的基础还是需要有一个可靠的消息服务，确保消息要能送达。 针对redis预减库存存在的并发问题，这里的思路是用lua+redis，在预减之前判断库存是否够，这两个操作要在一个原子操作里面才行，lua恰好可以实现原子性、顺序性地操作。 支付模块 这里对接的是支付宝-扫码支付，用到是支付宝沙箱环境。支付的扫码支付详细流程在这里聊一聊哈。 商户前台将商品参数发送至商户后台，商户后台生成内部订单号并用于请求支付平台创建预下单，支付平台创建完预订单后将订单二维码信息返还给商户，此时用户即可扫取二维码进行付款操作。 内部订单号：这是相对于支付宝平台而言的，这个订单号是我们商城自己生成的，对于我们商城来说是内部订单号，但是对于支付宝来说是外部订单号。 将一系列的数据按照支付宝的要求发送给支付宝平台，包括商品信息，生成的验签sign，公钥；支付宝去将sign解密，进行商品的各种信息校验。校验通过，同步返回二维码串。 支付业务流程图： 在获取支付的二维码串之后，用工具包将其转换未二维码展示给用户扫码。 用户扫码后，会收到第一次支付宝的回调，展示要支付的金额，商品信息等。 用户输入密码成功后，正常情况会收到支付宝的第二次回调，即支付成功信息。 但是也可能会由于网络等原因，迟迟收不到支付宝的回调，这个时候就需要主动发起轮询去查看支付状态。 在支付成功之后，接收回调的接口要记得返回success告诉支付宝我已经收到你的回调了，不要再重复发给我了。接收回调的接口也要做好去除重复消息的逻辑。 这个流程是多么地简单而理所当然！ 对应于代码层面，其实就是两个接口，一个是用户点击去支付按钮，此时发起预下单，展示付款二维码，另一个是接收支付宝回调： 预下单： 接收支付宝支付状态回调： 项目进展 [x] 2018/12/31 完成了聚合工程的创建、Eureka服务注册中心、spring cloud config+gitHub+spring cloud bus（rabbitMQ）实现配置自动刷新–v1.0 [x] 2018/12/31 将Eureka注册中心(单机)和配置中心部署到服务器上，这比较固定，所以先部署上去，以后本地就直接用这两个即可，对配置进行了一点点修改 [ ] 2018/12/31 关于配置的自动刷新，用postman发送post请求是可以的，但是用github webhook不行，不知道是不是这个版本的问题 [x] 2018/12/31 用户模块的逻辑实现,首先增加了一些pom文件的支持，整合mybatis，测试数据库都通过，下面就可以真正去实现业务代码了 [x] 2019/1/1 完成用户注册、登陆、校验用户名邮箱有效性、查看登陆用户信息、根据用户名去拿到对应的问题、校验答案是否正确、重置密码这个几个接口，在注册这个接口，增加一个ZK分布式锁来防止在高并发场景下出现用户名或邮箱重复问题 [x] 2019/1/2 上午完成门户用户模块所有接口–v2.0 [x] 2019/1/2 下午完成品类管理模块，关于繁琐的获取用户并且鉴权工作，这里先放每个接口里面处理，后面放到网关中去实现–v3.0 [x] 2019/1/3 上午引入网关服务，将后台重复的权限校验统一放到网关中去做，并且加了限流，解决了一下跨域问题。–v4.0 [x] 2019/1/3 下午和晚上完成门户和后台的商品管理模块所有的接口功能，除了上传文件的两个接口没有测试以外，其他接口都进行了简单的测试，其中还用Feign去调用了品类服务接口–v5.0 [x] 2019/1/3 初步把购物车模块和模块引入，通过基础测试，后面在此基础上直接开发代码即可，明天下午看《大黄蜂》，晚上师门聚餐吃火锅，明天早上赶一赶吧，今天任务结束！ [x] 2019/1/4 整理了接口文档，并且画了一下购物车模块的流程图以及订单服务的流程图，针对订单服务中，记录了需要一些注意的问题，尽可能地完善，提高可用性和性能。并且完成购物车模块的controller层。 [x] 2019/1/5 完成购物车模块，并且进行了简单的测试，这里进行了两处改造，一个是判断了一下是否需要判断库存；另一个是商品信息从redis中取，取不出来则调用商品服务初始化值 [x] 2019/1/5 收货地址管理模块，这个模块就是个增删改查，没啥东西写，这里就不加缓存了。 [x] 2019/1/6 完成了后台订单管理模块并且进行了测试，调用收货地址服务时，发现收货地址服务无法读取到cookie，通过这个方法(https://blog.csdn.net/WYA1993/article/details/84304243) 暂时解决了问题 [x] 2019/1/6 预置所有商品库存到redis中；预置所有商品到redis中；大概确定好订单服务思路：预减库存（redis判断库存）—对userID增加分布式锁防止用户重复提交订单–MQ异步下订单 [x] 2019/1/6 新增全局唯一ID生成服务，雪花算法实现 [x] 2019/1/7 完善订单服务-这一块涉及跨库操作，并且不停地调用其他服务，脑子都快晕了，这里采取的策略是：用到购物车的时候，去调用购物车服务获取；产品详情从redis中获取。首先将商品以及商品库存全部缓存到redis中，然后用户下单，先从redis中判断库存，够则减，判断 和扣减放在lua脚本中原子执行，然后MQ异步出去生成订单（生成订单主表和订单详情表放在一个本地事务中），这两步操作成功之后，再用MQ去异步删除购物车。MQ消费不成功则重试。 对于扣减库存这一步，想法是用定时任务，定时与redis中进行同步。这里是模拟了秒杀场景，预减库存+MQ异步，提交订单–&gt;redis判断并且减库存–&gt;调用cart-service获取购物车–&gt;MQ异步(userId,shippingId)生成订单主表和详情表–&gt;上面都成功，则MQ异步(userId) 去清除购物车，库存用定时任务去同步(未做)，理想的做法是：MQ异步扣减库存，订单服务订阅扣减库存消息，一旦库存扣减成功，则进行订单生成。 [x] 2019/1/8 继续完善订单接口，完成支付服务，就直接放在订单服务里面了，因为与订单逻辑紧密，就放在一起了。 [x] 2019/1/8 使用了一下swagger，发现代码侵入比较强，每一个接口上面都要手动打上响应的注解 [x] 2019/1/8 关于hystrix熔断与降级，可以引入hystrix的依赖，用@HystrixCommand注解来控制超时时间、服务降级以及服务熔断。也可以直接再@FeignClient接口中指定服务降级的类，这里不演示了，因为设置比如超时时间，我还要重新测试，写起来很简单，测起来有点儿麻烦 [x] 2019/1/9 服务跟踪，服务端是直接用的线程的，只需要下载：wget -O zipkin.jar ‘https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec’，然后nohup java -jar zipkin.jar &gt; zipkin.server.out &amp; ，开放9411端口，打开浏览器http://ip:9411看到页面即可。 客户端只需要添加相应依赖和配置文件即可。用客户端测试，发现死活不出现我的请求，经过搜索，发现需要增加spring.zipkin.sender.type= web这个配置项才行. [x] 2019/1/10 初步把项目部署到服务器上，进行测试，bug多多，修改中… [x] 2019/1/10 改了一天的bug，其中网关的超时时间以及feign的超时时间都要改大一点，否则会超时报错。最终成功，花了三台服务器，部署了11个服务。后面把部署过程写一下。 [x] 2019/1/11 将注册中心做成集群，因为早上一起来，注册中心挂了？？？ [ ] 2019/1/11 docker部署(商城第四期的改造目标:容器化+容器编排)，本期改造结束。 [x] 2019/1/11 完善readme文档 [x] 2019/1/12 两次发现redis数据被莫名其妙清空，我确定不是缓存到期，为了安全起见，设置了redis的密码，明天看缓存数据还在不在。 [x] 2019/1/14 redis数据没有再丢失，修复用户更新信息的bug 项目启动 安装redis、zookeeper、mysql、jdk、nginx以及rabbitMQ。 对代码进行maven-package操作。打包成jar包。将其放到服务器上： 执行nohup java -jar snailmall-user-service.8081 &gt; user-service.out &amp;后台启动即可。 补充：针对配置刷新，修改了github信息，用postman请求http://xxxxx:8079/actuator/bus-refresh 触发更新。 本改造是基于快乐慕商城一期和快乐慕商城二期的基础上进行改造。所以需要在其业务基础上改造会比较顺手。关于微服务，尤其是电商中的一些处理手段，很多思路都是学习于码吗在线中分布式电商项目。再加上慕课网廖师兄的spring cloud微服务实践。 前台项目 只要阅读readme文档即可。代码仓库为：https://github.com/sunweiguo/snailmall-front 学习不仅要有输入，更要有自己的输出，实践是提升的捷径！]]></content>
      <tags>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-进程与线程]]></title>
    <url>%2F2019%2F01%2F19%2F%E9%9D%A2%E8%AF%95-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[都是操作系统管理的对象，比较容易混淆，但是又是两样完全不同的东西，因此区别很多。从他们区别也可以发散出很多关于操作系统比较重要的知识。所以面试比较常问。 1. 进程到底是什么？ 翻了一下书：《操作系统概念》第三章中提及了进程的概念。他是这样说的： 进程是执行中的程序，这是一种非正式说法。进程不只是程序代码（代码块），进程还包含了当前活动，通过程序计数器的值和处理器寄存器的内容来表示。此外，进程还包含进程堆栈段（包括临时数据，如函数参数、返回地址和局部变量）和数据段（包括全局变量）。进程还可能包含堆，是在进程运行期间动态分配的内存。 程序本身不是进程。程序只是被动实体，如存储在磁盘上包含一系列指令的文件内容。而进程是活动实体，它有一个程序计数器来表示下一个要执行的命令和相关资源集合。当一个可执行文件被装入内存时，一个程序才能成为进程。 总结一下进程是什么，是我个人理解：它是一个活动实体，运行在内存上。然后它占用很多独立的资源，比如：内存资源、程序运行肯定涉及CPU计算、占用的端口资源（公共的）、文件资源（公共的）、网络资源（公共的）等等等。要想执行这个进程，首先要有一个可执行文件，有了这个可执行文件，还要有相应的执行需要的资源。所以将可执行文件、当前进程的上下文、内存等资源结合起来，才是一个真正的进程。 那么，我们就可以理解一句话：进程是资源分配的基本单位。 进程中的内存空间（虽然空间大小都一样，下文会说明）是独立的，否则就会出现一种情况：修改自己程序中的某个指针就可以指向其他程序中的地址，然后拿到里面的数据，岂不是很恐怖的场景？ 如上图，进程中包含了线程。操作系统可能会运行几百个进程，进程中也可能有几个到几百个线程在运行。 文件和网络句柄是所有进程共享的，多个进程可以去打开同一个文件，去抢占同一个网络端口。 图中还有个内存。这个内存不是我们经常说的内存条，即物理内存，而是虚拟内存，是进程独立的，大小与实际物理内存无关。 2. 寻址空间 比如8086只有20根地址线，那么它的寻址空间就是1MB，我们就说8086能支持1MB的物理内存，及时我们安装了128M的内存条在板子上，我们也只能说8086拥有1MB的物理内存空间。 以前叫卖的32位的机子，32位是指寻址空间为2的32次方。32位的386以上CPU就可以支持最大4GB的物理内存空间了。 3. 为什么会有虚拟内存和物理内存的区别 正在运行的一个进程，他所需的内存是有可能大于内存条容量之和的，比如你的内存条是256M，你的程序却要创建一个2G的数据区，那么不是所有数据都能一起加载到内存（物理内存）中，势必有一部分数据要放到其他介质中（比如硬盘），待进程需要访问那部分数据时，在通过调度进入物理内存。 所以，虚拟内存是进程运行时所有内存空间的总和，并且可能有一部分不在物理内存中，而物理内存就是我们平时所了解的内存条。 关键的是不要把虚拟内存跟真实的插在主板上的内存条相挂钩，虚拟内存它是“虚拟的”不存在，假的啦，它只是内存管理的一种抽象！ 4. 虚拟内存地址和物理内存地址是如何映射呢 假设你的计算机是32位，那么它的地址总线是32位的，也就是它可以寻址0 ~ 0xFFFFFFFF（4G）的地址空间，但如果你的计算机只有256M的物理内存0x~0x0FFFFFFF（256M），同时你的进程产生了一个不在这256M地址空间中的地址，那么计算机该如何处理呢？回答这个问题前，先说明计算机的内存分页机制。 计算机会对虚拟内存地址空间（32位为4G）分页产生页（page），对物理内存地址空间（假设256M）分页产生页帧（page frame），这个页和页帧的大小是一样大的，所以呢，在这里，虚拟内存页的个数势必要大于物理内存页帧的个数。 在计算机上有一个页表（page table），就是映射虚拟内存页到物理内存页的，更确切的说是页号到页帧号的映射，而且是一对一的映射。但是问题来了，虚拟内存页的个数 &gt; 物理内存页帧的个数，岂不是有些虚拟内存页的地址永远没有对应的物理内存地址空间？ 不是的，操作系统是这样处理的。操作系统有个页面失效（page fault）功能。操作系统找到一个最少使用的页帧，让他失效，并把它写入磁盘，随后从磁盘中把把需要访问的数据所在的页放到最少使用的页帧中，并修改页表中的映射（即修改页号指向当前页帧），这样就保证所有的页都有被调度的可能了。这就是处理虚拟内存地址到物理内存的步骤。 至于里面如何实现的细节，我没有过多去探究。 5. 什么是虚拟内存地址和物理内存地址 虚拟内存地址由页号和偏移量组成。页号就是上面所说的。偏移量就是我上面说的页（或者页帧）的大小，即这个页（或者页帧）到底能存多少数据。 举个例子，有一个虚拟地址它的页号是4，偏移量是20，那么他的寻址过程是这样的：首先到页表中找到页号4对应的页帧号（比如为8），如果找不到对应的页桢，则用失效机制调入页。如果存在，把页帧号和偏移量传给MMU（CPU的内存管理单元）组成一个物理上真正存在的地址，接着就是访问物理内存中的数据了。 6. 线程里面有什么 写到这里，好像还与本标题无关，即进程和线程到底是什么关系和区别等。但是我们要知道，面试或者学习一个知识点，不是为了学习这个区别而学习， 我们应该学习为什么有进程和线程，有了进程还需要线程吗？有了线程还要进程吗？你说进程是资源分配的单位，分配的是什么资源呢？进程中的内存是咋管理的呢？虚拟内存和物理内存是什么？什么是虚拟内存地址和物理内存地址？等等等，所以面试是千变万化的，重要的是我们尽可能地多问自己几个为什么，然后从为什么开始去逐个击破，形成一个体系。 说说这个栈，我们知道，执行程序从主程序入口进入开始，可能会调用很多的函数，那么这些函数的参数和返回地址都会被压入栈中，包括这些函数中定义的临时局部变量都会压入栈中，随着函数的执行完毕，再逐层地弹出栈，回到主函数运行的地方，再继续执行。 PC(program counter)，就是程序计数器，指向的下一条指令执行的地址。 由此可见，操作系统运行的其实是一个一个的线程，而进程只是一个隔离资源的容器。 上面说到，PC是指向下一条指令执行的地址。而这些指令是放在内存中的。 我们的计算机大多数是存储程序型的。就是说数据和程序是同时存储在同一片内存里的。 所以我们经常会听到一个漏洞叫做“缓冲区溢出”：比如有一个地方让用户输入用户名，但是黑客输入很长很长的字符串进去，那么很有可能就会超出存放这个用户名的一片缓冲区，而直接侵入到存放程序的地方，那么黑客就可以植入程序去执行。解决方案就是限制输入的用户名长度，不要超过缓冲区大小。 还有一块是TLS(thread local storage)，我们知道进程有自己独立的内存，那么我们的线程能不能也有一小块属于自己的内存区域呢？ 这个东西，其实很简单，就是说，比如new一个对象，往往是在堆中开辟空间的，但是现在的情况是：在一个函数内，new出来一个对象，这个对象不引用外部对象，也不会被外部引用，是纯粹属于这个函数段，可以理解为这个对象是属于这个函数的局部临时变量。 此时，new这个对象就不需要再去堆中开辟空间了，因为一方面不需要共享，另一方面是在堆中开辟是比较慢的，并且可能有很多函数，这种局部对象零零总总加起来还是很多的，在堆中开辟会浪费空间。 所以，能不能在栈中就可以new出这个对象，反正用完就扔。TLS可以是现在这个。栈中直接new多方便多快，因为不需要走垃圾回收机制，还避免了线程安全问题。可以去搜索：栈上分配和逃逸分析 7. 线程VS进程 到这里，就清晰了很多。我们也可以多多少少理解他们的区别。 可以做个简单的比喻，便于记忆：进程=火车，线程=车厢 线程在进程下行进（单纯的车厢无法运行） 一个进程可以包含多个线程（一辆火车可以有多个车厢） 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘） 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易） 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源） 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢） 进程可以拓展到多机，进程最多适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上） 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－“互斥锁” 进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量” 再补充几句。 线程是调度的基本单位，进程是资源分配的基本单位 进程间没有共享内存，所以交互要通过TCP/IP端口的等方式来实现。线程间由于有共享内存，所以交互比较方便。 线程占用很多资源，而线程只需要分配栈和PC即可。 8. 针对虚拟内存和物理内存的总结 每个进程都有自己独立的4G(32位系统下)内存空间，各个进程的内存空间具有类似的结构 一个新进程建立的时候，将会建立起自己的内存空间，此进程的数据，代码等从磁盘拷贝到自己的进程空间（建立一个进程，就要把磁盘上的程序文件拷贝到进程对应的内存中去，对于一个程序对应的多个进程这种情况，浪费内存！），哪些数据在哪里，都由进程控制表中的task_struct记录 每个进程的4G内存空间只是虚拟内存空间，每次访问内存空间的某个地址，都需要把地址翻译为实际物理内存地址 所有进程共享同一物理内存，每个进程只把自己目前需要的虚拟内存空间映射并存储到物理内存上。 进程要知道哪些内存地址上的数据在物理内存上，哪些不在，还有在物理内存上的哪里，需要用页表来记录 页表的每一个表项分两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址（如果在的话） 当进程访问某个虚拟地址，去看页表，如果发现对应的数据不在物理内存中，则缺页异常 缺页异常的处理过程，就是把进程需要的数据从磁盘上拷贝到物理内存中，如果内存已经满了，没有空地方了，那就找一个页覆盖，当然如果被覆盖的页曾经被修改过，需要将此页写回磁盘 9. 关于进程和线程更深的认识 关于为什么要分进程和线程，先抛出结论： 进程process：进程就是时间总和=执行环境切换时间+程序执行时间------&gt;CPU加载执行环境-&gt;CPU执行程序-&gt;CPU保存执行环境 线程thread：线程也是时间总和=执行环境切换时间（共享进程的）+程序模块执行时间------&gt;CPU加载执行环境（共享进程的）-&gt;CPU执行程序摸块-&gt;CPU保存执行环境（共享进程的） 进程和线程都是描述CPU工作的时间段，线程是更细小的时间段。 那么，如果CPU时间片临幸本进程，那么这个进程在恢复执行环境之后，执行里面的若干线程就不需要再不停地切换执行环境了，所以说，线程相比于进程是比较轻量的。 在CPU看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。。 进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文 线程是什么呢？进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成：程序A得到CPU =》CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。 这里a，b，c的执行是共享了A的上下文，CPU在执行的时候没有进行上下文切换的。这里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境、更为细小的CPU时间段。 进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同。 整理自： https://www.zhihu.com/question/25532384 https://blog.csdn.net/moshenglv/article/details/52242153 https://blog.csdn.net/u012861978/article/details/53048077]]></content>
      <tags>
        <tag>操作系统相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步一步理解HTTPS]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F7.%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3HTTPS%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第七篇文章。HTTPS（SSL/TLS）的加密机制是前端后端ios安卓等都应了解的基本问题。也是面试经常问的点。 一、为什么需要加密？ 小时候看谍战片，情报发过来了之后，用一个小本本进行翻译，然后解密出情报。加密就是防止明文被别人看到甚至篡改嘛！ 回到互联网，因为http的内容是明文传输的，明文数据会经过中间代理服务器、路由器、wifi热点、通信服务运营商等多个物理节点，如果信息在传输过程中被劫持，传输的内容就完全暴露了，他还可以篡改传输的信息且不被双方察觉，这就是中间人攻击。所以我们才需要对信息进行加密。最简单容易理解的就是对称加密 。 二、什么是对称加密？ 小明写个求爱信给小红，小明担心小红的妈妈看到这封信的内容，他灵机一动，对信加个密，并确定好我用这个密钥加密的，小红收到之后也用这个密钥解密才行。 但是呢，这里有个麻烦的地方就是，小明和小红不在一个学校，这个钥匙呢，不方便直接送到手里。所以呢，小明得想办法把这个钥匙寄一个送给小红，好吧，就用最贵的顺丰吧！ 就是有一个密钥，它可以对一段内容加密，加密后只能用它才能解密看到原本的内容，和我们日常生活中用的钥匙作用差不多。 三、用对称加密可行吗？ 顺丰快递到了，结果小红不在家，小红的妈妈收到了，一看是个男同学寄的，怎么能忍住，赶紧打开，以看是一把钥匙，作为程序猿，妈妈得意一笑：哼哼，还能逃过我的眼睛？我赶紧复制一把藏着，我倒要看看他后面要寄啥来，还要加密？！ 果然小红的妈妈等到了来自小明寄过来的情书，解密一看，实锤早恋。 同样地，小明这边也非常危险，快递员刚出发，就被小明的妈妈拦截了，拿到了这个钥匙，那小明还没寄出的信已经被妈妈看光了。 所以问题的根本就是，这把钥匙要传输，传输就可能被截取。 回到互联网，如果通信双方都各自持有同一个密钥，且没有别人知道，这两方的通信安全当然是可以被保证的（除非密钥被破解）。 然而最大的问题就是这个密钥怎么让传输的双方知晓，同时不被别人知道。 如果由服务器生成一个密钥并传输给浏览器，那这个传输过程中密钥被别人劫持弄到手了怎么办？ 换种思路？试想一下，如果浏览器内部就预存了网站A的密钥，且可以确保除了浏览器和网站A，不会有任何外人知道该密钥，那理论上用对称加密是可以的，这样浏览器只要预存好世界上所有HTTPS网站的密钥就行啦！这么做显然不现实。 怎么办？所以我们就需要神奇的非对称加密。 四、什么是非对称加密？ 有两把密钥，通常一把叫做公钥、一把叫做私钥。 用公钥加密的内容必须用私钥才能解开，同样，私钥加密的内容只有公钥能解开。 五、用非对称加密可行吗？ 公钥呢，还是要通过快递员送给小红的。OK，假设小红要回信，写好了用公钥加密，小红的妈妈因为拿不到私钥，看不到信的内容。 OK，但是反过来呢？小明用私钥加密传给小红，那么小红的妈妈可就能解密了（因为公钥可能会被小红的妈妈拿到）。 回到互联网，服务器先把公钥直接明文传输给浏览器，之后浏览器向服务器传数据前都先用这个公钥加密好再传，这条数据的安全似乎可以保障了！因为只有服务器有相应的私钥能解开这条数据。 然而由服务器到浏览器的这条路怎么保障安全？ 如果服务器用它的的私钥加密数据传给浏览器，那么浏览器用公钥可以解密它，而这个公钥是一开始通过明文传输给浏览器的，这个公钥被谁劫持到的话，他也能用该公钥解密服务器传来的信息了。 所以目前似乎只能保证由浏览器向服务器传输数据时的安全性（其实仍有漏洞，下文会说）。 六、改良的非对称加密方案，似乎可以？ 小明和小红年纪不大，但是很聪明，针对这个情况，还是迅速升级加密方法。他们想到既然一组公钥私钥不够，那两组呢？ OK，小明和小红各造了一对。下面就是互相交换公钥。那么就变成： 下面就好办啦，小明写信用公钥B加密，那么信的内容只有小红能破解，因为小红是随身携带私钥B。相反，小红用公钥A对信加密，这样只有小明能破解，因为小明也是随身携带私钥A。好像很安全啦！除了下面提到的漏洞，唯一的缺点可能是：小红得花半天时间才能解密完这封信，有点受不了。 回到互联网。请看下面的过程： 某网站拥有用于非对称加密的公钥A、私钥A；浏览器拥有用于非对称加密的公钥B、私钥B。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器把公钥B明文传输给服务器。 之后浏览器向服务器传输的所有东西都用公钥A加密，服务器收到后用私钥A解密。由于只有服务器拥有这个私钥A可以解密，所以能保证这条数据的安全。 服务器向浏览器传输的所有东西都用公钥B加密，浏览器收到后用私钥B解密。同上也可以保证这条数据的安全。 的确可以！抛开这里面仍有的漏洞不谈（下文会讲），HTTPS的加密却没使用这种方案，为什么？最主要的原因是非对称加密算法非常耗时，特别是加密解密一些较大数据的时候有些力不从心。 七、非对称加密+对称加密？ 小明也知道，这个信很长，用非对称加密，太慢！办法也有，没有必要对那么长的信加密，我只要保证这个真正解密的钥匙不被别人拿到就行，那么他灵机一动想到这个方法： 小明和小红利用非对称加密对钥匙加密，姑且认为是这个钥匙被放在了一个盒子里，这个盒子也被锁起来了，只有小红或者小明才能打开盒子，再用钥匙去解密。 这个真正用于对称加密解密的钥匙别人就拿不到啦！ 自从用了这个方案，感觉又安全，解密又快，感情又深温了呢！ 回到互联网，步骤如下： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都用密钥X加密解密。 HTTPS的基本思想就是基于这个。但是这个方案也存在上面一直在说的漏洞。 八、中间人攻击 像妈妈这样级别的程序猿可能是那他们两没办法啦，但是呢，校区有个看门的大爷，以前是个黑客，也不知道咋回事，明明才50岁，但是看起来像80岁，头上光溜溜的，冬天冷呢。整天在那胡言乱语：docker牛逼啊，spring cloud牛逼啊，这个开源软件XXX写的真好，跟周围的老大爷老大妈根本谈不到一起去。 他也是闲的蛋疼，非要掺和，因为据说他以前单身30年，苦逼敲代码，不知道谈恋爱是啥滋味，姑且认为他好奇心重吧。 在小明第一次寄公钥A的时候，大爷出手了，截取下来。换成自己做的公钥B。然后送给小红。 小红哪里会知道这公钥被掉包了呢，所以直接就用了，按照正常步骤，小红想了一个随机字符串，这次就叫xiaomingwoxuanni吧，OK，用这个公钥B对这个字符串加个密，这个字符串就被锁进了用大爷公钥B锁的盒子里。 老大爷在门口守着呢，一看到小红寄东西了，又偷偷地截取下来，用自己的私钥B来解密这个盒子。轻易地拿到了里面的字符串，OK，怕小明察觉，再用小明寄来的公钥A加密传给小明，这样双方都不知道他们的钥匙已经被大爷给获取了。 小明和小红之间的信就用xiaomingwoxuanni这个钥匙进行对称加密和对称解密，完全不知道有个大爷就天天拿着这个字符串去解密信件，看的不亦乐乎，甚至还偷偷改几个字呢。 回到互联网。中间人的确无法得到浏览器生成的密钥B，这个密钥本身被公钥A加密了，只有服务器才有私钥A解开拿到它呀！然而中间人却完全不需要拿到密钥A就能干坏事了。请看： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器向网站服务器请求，服务器把公钥A明文给传输浏览器。 中间人劫持到公钥A，保存下来，把数据包中的公钥A替换成自己伪造的公钥B（它当然也拥有公钥B对应的私钥B）。 浏览器随机生成一个用于对称加密的密钥X，用公钥B（浏览器不知道公钥被替换了）加密后传给服务器。 中间人劫持后用私钥B解密得到密钥X，再用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样在双方都不会发现异常的情况下，中间人得到了密钥B。根本原因是浏览器无法确认自己收到的公钥是不是网站自己的。只要解决了这个公钥一定是这个网站发来的，那么基本就OK了 九、如何证明浏览器收到的公钥一定是该网站的公钥？ 现实生活中，如果想证明某身份证号一定是小明的，怎么办？看身份证。这里政府机构起到了“公信”的作用，身份证是由它颁发的，它本身的权威可以对一个人的身份信息作出证明。互联网中能不能搞这么个公信机构呢？给网站颁发一个“身份证”？ 十、数字证书 网站在使用HTTPS前，需要向“CA机构”申请颁发一份数字证书，即SSL证书，数字证书里有证书持有者、证书持有者的公钥等信息，服务器把证书传输给浏览器，浏览器从证书里取公钥就行了，证书就如身份证一样，可以证明“该公钥对应该网站”。然而这里又有一个显而易见的问题了，证书本身的传输过程中，如何防止被篡改？即如何证明证书本身的真实性？身份证有一些防伪技术，数字证书怎么防伪呢？解决这个问题我们就基本接近胜利了！ SSL证书内容： 证书的发布机构CA 证书的有效期 公钥 证书所有者 签名 十一、如何放防止数字证书被篡改？ 我们把证书内容生成一份“签名”，比对证书内容和签名是否一致就能察觉是否被篡改。这种技术就叫数字签名。 提到数字签名，其实原理很简单啦，就是比如我要传输一句话叫：“你给我转100块钱，我的账号是123456，转完了告诉我一声。”，如果不做任何处理，被刚才的老大爷截取了，他偷偷地改一下内容“你给我转200块钱，我的账号是654321，不要告诉任何人，尤其是你嫂子。” 是不是太坏了，弄不好被抓，大爷可不敢做大的，只敢骗个喝酒钱。 那么怎么防止大爷这种猥琐技术又高的人篡改呢？数字签名排上用场啦！ 以后再传消息就是“你给我转100块钱，我的账号是123456，转完了告诉我一声。”+“！……@&amp;@%#……！￥@￥！@%……#￥！%……”,后面那一串东西就是数字签名，简单来说，就是想办法对前面的内容进行非对称加密（这样别人根本不知道你加密的私钥是什么，也就伪装不了签名了）。传过去之后，我要对其进行解密，与传过来的明文一一对比参数，看有没有被改动过。一旦发现哪里不对应，说明已经被篡改了。 “CA机构”制作签名的过程： CA拥有非对称加密的私钥和公钥。 CA对证书明文信息进行hash。 对hash后的值用私钥加密，得到数字签名。 明文和数字签名共同组成了数字证书，这样一份数字证书就可以颁发给网站了。网站把这个数字证书传给浏览器。 那浏览器拿到服务器传来的数字证书后，如何验证它是不是真的？（有没有被篡改、掉包） 浏览器验证过程： 拿到证书，得到明文T，数字签名S。 用CA机构的公钥对S解密（由于是浏览器信任的机构，所以浏览器保有它的公钥。详情见下文），得到S’。 用证书里说明的hash算法对明文T进行hash得到T’。 比较S’是否等于T’，等于则表明证书可信。 为什么这样可以证明证书可信呢？我们来仔细想一下。 十二、中间人有可能篡改该证书吗？ 老大爷就算有天大的能耐，也拿不到加密的私钥，那么只是单纯地篡改明文，只会造成校验不通过。 回到互联网，假设中间人篡改了证书的原文，由于他没有CA机构的私钥，所以无法得到此时加密后签名，无法相应地篡改签名。 浏览器收到该证书后会发现原文和签名解密后的值不一致，则说明证书已被篡改，证书不可信，从而终止向服务器传输信息，防止信息泄露给中间人。 十三、中间人有可能把证书掉包吗？ 假设有另一个网站B也拿到了CA机构认证的证书，它想搞垮网站A，想劫持网站A的信息。于是它成为中间人拦截到了A传给浏览器的证书，然后替换成自己的证书，传给浏览器，之后浏览器就会错误地拿到B的证书里的公钥了，会导致上文提到的漏洞。 其实这并不会发生，因为证书里包含了网站A的信息，包括域名，浏览器把证书里的域名与自己请求的域名比对一下就知道有没有被掉包了。 总结：因为一个网站域名对应一个证书，你的证书根其他人的证书肯定是不一样的，那么你就算拿到了其他人的证书再掉包成自己的，也没用，毕竟浏览器那边只要看一下是不是我要查看的域名。 十四、为什么制作数字签名时需要hash一次？ 最显然的是性能问题，前面我们已经说了非对称加密效率较差，证书信息一般较长，比较耗时。而hash后得到的是固定长度的信息（比如用md5算法hash后可以得到固定的128位的值），这样加密解密就会快很多。 十五、怎么证明CA机构的公钥是可信的？ 让我们回想一下数字证书到底是干啥的？没错，为了证明某公钥是可信的，即“该公钥是否对应该网站/机构等”，那这个CA机构的公钥是不是也可以用数字证书来证明？没错，操作系统、浏览器本身会预装一些它们信任的根证书，如果其中有该CA机构的根证书，那就可以拿到它对应的可信公钥了。 实际上证书之间的认证也可以不止一层，可以A信任B，B信任C，以此类推，我们把它叫做信任链或数字证书链，也就是一连串的数字证书，由根证书为起点，透过层层信任，使终端实体证书的持有者可以获得转授的信任，以证明身份。 另外，不知你们是否遇到过网站访问不了、提示要安装证书的情况？这里安装的就是根证书。说明浏览器不认给这个网站颁发证书的机构，那么没有该机构的根证书，你就得手动下载安装（风险自己承担XD）。安装该机构的根证书后，你就有了它的公钥，就可以用它验证服务器发来的证书是否可信了。 也就是说，公钥是从证书中获取的。证书是网站从机构那边申请来的，证书+签名传给浏览器。只要校验通过，那么公钥必然没有被篡改过，并且一定是这个网站传来的，那么解决了我们最核心的问题：确定公钥是我们指定的网站传来的。 既然公钥是正确的，那么小红就会用正确的公钥对随机字符串加密，中间不会出现篡改。 十六、HTTPS必须在每次请求中都要先在SSL/TLS层进行握手传输密钥吗？ 这也是我当时的困惑之一，显然每次请求都经历一次密钥传输过程非常耗时，那怎么达到只传输一次呢？用session就行。 服务器会为每个浏览器（或客户端软件）维护一个session ID，在TSL握手阶段传给浏览器，浏览器生成好密钥传给服务器后，服务器会把该密钥存到相应的session ID下，之后浏览器每次请求都会携带session ID，服务器会根据session ID找到相应的密钥并进行解密加密操作，这样就不必要每次重新制作、传输密钥了！ 十七、HTTPS原理 下面再来看看HTTPS原理就特别简单啦！ HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：可以理解为HTTP+SSL/TLS， 即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL，用于安全的 HTTP 数据传输。 1234HTTPSSL/TLSTCPIP 我们只要知道，在SSL层里面可以完成校验和密钥的传输。 理解了上面，这个图也就没啥好解释的了。 整理自：https://zhuanlan.zhihu.com/p/43789231]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP基础知识提炼]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F6.HTTP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%8F%90%E7%82%BC%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第六篇文章。这里简单记录一些关于HTTP的基本概念，比较基础。下面的内容是对《图解HTTP》提炼的再提炼，主要原因是很多重要的东西前面已经详细说过了，还有一些东西知道即可，用到再去查，作为一个后端攻城狮，也没有必要了解那么琐碎。 HTTP是不保存状态的协议 HTTP是无状态协议。自身不对请求和响应之间通信状态进行保存（即不做持久化处理）。 HTTP之所以设计得如此简单，是为了更快地处理大量事物，确保协议的可伸缩性。 HTTP/1.1 随时无状态协议，但可通过 Cookie 技术保存状态。 告知服务器意图的HTTP方法 GET：获取资源 POST：传输实体主体 PUT：传输文件 HEAD：获得报文首部，与GET方法一样，只是不返回报文主体内容。用于确认URI的有效性及资源更新的日期时间等。 DELETE：删除文件，与PUT相反（响应返回204 No Content）。 OPTIONS：询问支持的方法，查询针对请求URI指定的资源支持的方法（Allow:GET、POST、HEAD、OPTIONS）。 TRACE：追踪路径 CONNECT：要求用隧道协议连接代理（主要使用SSL（Secure Sockets Layer，安全套接层）和TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输）。 URI、URL 官方解释都是什么乱起八糟的东西。各种博客也是跟着抄，这两者到底是什么关系和意义？ 统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来。 对应于实际例子就是：每个人都有身份证，这个身份证号码就对应这个人。比如张三的身份证号码为123456，那么我只要知道123456就可以找到这个人。 那什么是URL呢？从名字看是：统一资源定位器。 如果做类比，URL就是：动物住址协议://地球/中国/浙江省/杭州市/西湖区/某大学/14号宿舍楼/525号寝/张三.人 我们通过这个详细的地址也可以找到张三这个人。 那么他们俩到底是什么关系呢？ URI是以某种规则唯一地标识资源，手段不限，比如身份证号。当然了，地址可以唯一标识，那么也属于URI的一种手段。所以说URL是URI的子集。 回到Web上，假设所有的Html文档都有唯一的编号，记作html:xxxxx，xxxxx是一串数字，即Html文档的身份证号码，这个能唯一标识一个Html文档，那么这个号码就是一个URI。 而URL则通过描述是哪个主机上哪个路径上的文件来唯一确定一个资源，也就是定位的方式来实现的URI。 对于现在网址我更倾向于叫它URL，毕竟它提供了资源的位置信息，如果有一天网址通过号码来标识变成了http://741236985.html，那感觉叫成URI更为合适。 HTTP请求报文 返回结果的HTTP状态码 状态码的职责是当客户端向服务器端发送请求时，描述返回的请求结果。 状态码如200 OK，以3为数字和原因短语组成。 数字中的第一位定义了响应类别，后两位无分类。响应类别有以下五种： 类别 原因短语 1XX Informational(信息性状态码) 2XX Success（成功状态码） 3XX Redirection（重定向状态码） 4XX Client Error（客户端错误状态码） 5XX Server Error（服务器错误状态码） ⭐2XX 成功 200 OK：请求被正常处理 204 No Content：一般在只需从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。 206 Partial Content：客户端进行范围请求 ⭐3XX 重定向 301 Moved Permanently：永久重定向。表示请求的资源已被分配了新的URI，以后应使用资源现在所指的URI。 也就是说，如果已经把资源对应的URI保存为书签了，这时应该按Location首部字段提示的URI重新保存。 302 Found：临时性重定向。表示请求的资源已被分配了新的URI，希望用户（本次）能使用新的URI访问。 和301 Moved Permanently状态码相似，但302状态码代表的资源不是被永久移动，只是临时性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。比如，用户把URI保存成书签，但不会像301状态码出现时那样去更新书签，而是仍旧保留返回302状态码的页面对应的URI（在Chrome中，还是会保存为重定向后的URI，不解）。 303 See Other：表示由于请求对应的资源存在着另一个URI，应使用GET方法定向获取请求的资源。这与302类似，但303明确表示客户端应当采用GET方法获取资源。 304 Not Modified：该状态码表示客户端发送附带条件的请求（指采用GET方法的请求报文中包含If-Match,If-Modified-Since，If-None-March，If-Range，If-Unmodified-Since中任一首部。）时，服务器端允许请求访问资源，但因发生请求为满足条件的情况后，直接返回304（服务器端资源未改变，可直接使用客户端未过期的缓存）。304状态码返回时，不包含任何响应的主体部分。 304虽被划分在3XX类别，但是和重定向没有关系。 307 Temporary Redirect：临时重定向。与302有相同含义。307遵守浏览器标准，不会从POST变成GET。 ⭐4XX 客户端错误 4XX的响应结果表明客户端是发生错误的原因所在。 400 Bad Request：表示请求报文中存在语法错误。 401 Unauthorized：表示发送的请求需要有通过HTTP认证（BASIC认证、DIGEST认证）的认证信息。 403 Forbidden：表明对请求资源的访问被服务器拒绝了。服务器端可在实体的主体部分对原因进行描述（可选） 404 Not Found：表明服务器上无法找到请求的资源。除此之外，也可以在服务器端拒绝请求且不想说明理由时时用。 ⭐5XX 服务器错误 5XX的响应结果表明服务器本身发生错误。 500 Interval Server Error：表明服务器端在执行请求时发生了错误。也有可能是Web应用存在的bug或某些临时的故障。 503 Service Unavailable：表明服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。如果事先得知解除以上状况需要的时间，最好写入Retry-After首部字段再返回给客户端。 HTTP的瓶颈 一条连接上只可发送一个请求（前面讲到，持久化可保持TCP连接状态，但仍完成一次请求/响应后才能进行下一次请求/响应，而管线化方式可让一个TCP连接并行发送多个请求。） 请求只能从客户端开始。客户端不可以接收除响应以外的指令 请求/响应首部未经压缩就发送。首部信息越多延迟越大 发送冗长(重复)的首部。每次互相发送相同的首部造成的浪费较多 SPDY以会话层的形式加入，控制对数据的流动，但还是采用HTTP建立通信连接。因此，可照常使用HTTP的GET和POST等方法、Cookie以及HTTP报文等。 使用 SPDY后，HTTP协议额外获得以下功能。 多路复用流：通过单一的TCP连接，可以无限制处理多个HTTP请求。所有请求的处理都在一条TCP连接上完成，因此TCP的处理效率得到提高。 赋予请求优先级：SPDY不仅可以无限制地并发处理请求，还可以给请求逐个分配优先级顺序。这样主要是为了在发送多个请求时，解决因带宽低而导致响应变慢的问题。 压缩HTTP首部：压缩HTTP请求和响应的首部。 推送功能：支持服务器主动向客户端推送数据的功能。 服务器提示功能：服务器可以主动提示客户端请求所需的资源。由于在客户端发现资源之前就可以获知资源的存在，因此在资源已缓存等情况下，可以避免发送不必要的请求。 WebSocket 利用Ajax和Comet技术进行通信可以提升Web的浏览速度。但问题在于通信若使用HTTP协议，就无法彻底解决瓶颈问题。 WebSocket技术主要是为了解决Ajax和Comet里XMLHttpRequst附带的缺陷所引起的问题。 一旦Web服务器与客户端之间建立起WebSocket协议的通信连接，之后所有的通信都依靠这个专用协议进行。通信过程中可互相发送JSON、XML、HTML或图片等任意格式的数据。 WebSocket的主要特点： 推送功能：支持由服务器向客户端推送数据。 减少通信量：和HTTP相比，不但每次连接时的总开销减少，而且由于WebSocket的首部信息很小，通信量也相应较少。 为了实现WebSocket通信，在HTTP连接建立之后，需要完成一次“握手”的步骤。 握手·请求：为了实现WebSocket通信，需要用到HTTP的Upgrade首部字段，告知服务器通信协议发生改变，以达到握手的目的。 握手·响应：对于之前的请求，返回状态码101 Switching Protocols 的响应。 成功握手确立WebSocket连接后，通信时不再使用HTTP的数据帧，而采用WebSocket独立的数据帧。 由于是建立在HTTP基础上的协议，因此连接的发起方仍是客户端，而一旦确立WebSocket通信连接，不论服务器端还是客户端，任意一方都可直接向对方发送报文。 整理自：https://github.com/JChehe/blog/blob/master/posts/《图解HTTP》读书笔记.md]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手和四次挥手]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F5.TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第五篇文章。面试讲到TCP，那么基本都会问三次握手和四次挥手的过程，以及比如对于握手，为什么是三次，而不是两次或者四次，本章详细探讨其中的门道。 1.复习 首先针对http协议，我们有必要复习一下最重要的东西。 HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 在HTTP 1.0以0.9版本中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。 通常的做法是即使不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道 客户端“在线”。 若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 2.SOCKET原理 2.1 套接字（socket）概念 初次接触这个名词：“套接字”，说实话，心里是蒙蔽的，这是啥玩意，但是可以去搜索一下什么是套接管： 我们可以看出来，两个管子可能直接连的话连不起来，那么可以通过中间一个东西连接起来。 那么，现在就好理解了，两个程序要通信，需要知道对方的一些信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 它是什么呢？它是网络通信过程中端点的抽象表示，这个抽象里面就包含了网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 那为什么一定要用它呢？ 在同一台计算机上，TCP协议与UDP协议可以同时使用相同的port而互不干扰。 操作系统根据套接字地址，可以决定应该将数据送达特定的进程或线程。这就像是电话系统中，以电话号码加上分机号码，来决定通话对象一般。 因为我们电脑上可能会跑很多的应用程序，TCP协议端口需要为这些同时运行的程序提供并发服务，或者说，传输层需要为应用层的多个进程提供通信服务，每个进程起一个TCP连接，那么这多个TCP连接可能是通过同一个 TCP协议端口传输数据。 如何区别哪个进程对应哪个TCP连接呢？ 许多计算机操作系统为应用程序与TCP／IP协议交互提供了套接字(Socket)接口。应 用层可以和传输层通过Socket接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。 2.2 建立socket连接 建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。 套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。 客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发 给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 2.3 SOCKET连接与TCP连接 创建Socket连接时，可以指定使用的传输层协议，Socket可以支持不同的传输层协议（TCP或UDP），当使用TCP协议进行连接时，该Socket连接就是一个TCP连接。 3.TCP基本字段 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 针对协议中的字段，我们只需要了解一下：ACK、SYN、序号这三个部分。 ACK : 确认 TCP协议规定，只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 SYN ： 在连接建立时用来同步序号。 当SYN=1而ACK=0时，表明这是一个连接请求报文。 对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此, SYN置1就表示这是一个连接请求或连接接受报文。 FIN 即终结的意思， 用来释放一个连接。 当 FIN = 1 时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接。 4.三次握手(重要，细读) 首先，TCP作为一种可靠传输控制协议，其核心思想：既要保证数据可靠传输，又要提高传输的效率，而用三次恰恰可以满足以上两方面的需求！ 然后，要明确TCP连接握手，握的是啥？ 答案：通信双方数据原点的序列号！ 我们在上面一篇文章知道，消息的完整是靠给每个消息包搞一个编号，依次地ACK确认。确认机制是累计的，意味着 X 序列号之前(不包括 X) 包都是被确认接收到的。 TCP可靠传输的精髓：TCP连接的一方A，由操作系统动态随机选取一个32位长的序列号（Initial Sequence Number）。 假设A的初始序列号为1000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，1001，1002，1003…，并把自己的初始序列号ISN告诉B。 让B有一个思想准备，什么样编号的数据是合法的，什么编号是非法的，比如编号900就是非法的，同时B还可以对A每一个编号的字节数据进行确认。 如果A收到B确认编号为2001，则意味着字节编号为1001-2000，共1000个字节已经安全到达。 同理B也是类似的操作，假设B的初始序列号ISN为2000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，2001，2002，2003…，并把自己的初始序列号ISN告诉A，以便A可以确认B发送的每一个字节。如果B收到A确认编号为4001，则意味着字节编号为2001-4000，共2000个字节已经安全到达。 好了，在理解了握手的本质之后，下面就可以总结上面图的握手过程了。 对于A与B的握手过程，可以总结为： A 发送同步信号SYN + A's Initial sequence number（丢失会A会重传） B 确认收到A的同步信号，并记录 A's ISN 到本地，命名 B's ACK sequence number B发送同步信号SYN + B's Initial sequence number （丢失B会周期性超时重传，直到收到A的确认） A确认收到B的同步信号，并记录 B's ISN 到本地，命名 A's ACK sequence number 很显然2和3 这两个步骤可以合并，只需要三次握手，可以提高连接的速度与效率。 这里就会引出一个问题，两次不行吗？ A 发送同步信号SYN + A’s Initial sequence number B发送同步信号SYN + B’s Initial sequence number + B’s ACK sequence number 这里有一个问题，A与B就A的初始序列号达成了一致，但是B无法知道A是否已经接收到自己的同步信号，如果这个同步信号丢失了，A和B就B的初始序列号将无法达成一致。 所以A必须再给B一个确认，以确认A已经接收到B的同步信号。 如果A发给B的确认丢了，该如何？ A会超时重传这个ACK吗？不会！TCP不会为没有数据的ACK超时重传。 那该如何是好？B如果没有收到A的ACK，会超时重传自己的SYN同步信号，一直到收到A的ACK为止。 写到这里，其实我们已经明白了，握手其实就是各自确认对方的序列号。因为后面的数据编号就会以此为基础，从而保证后续数据的可靠性。 谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 但是有的人指出，其实这只是表象，或者说并不是三次握手的设计初衷，我表示认同，这个防止已失效的连接请求报文段应该只是附加的一些好处，而不应该是解释为什么是三次握手的原因。 TCP初始阶段为什么是三次握手，原因总结如下： 为了实现可靠数据传输， TCP 协议的通信双方， 都必须维护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。 三次握手的过程即是通信双方相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤，如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认。 5.四次挥手 挥手过程的大概说明： 当主机A发出FIN报文段时，只是表示主机A已经没有数据要发送了，主机A告诉主机B，它的数据已经全部发送完毕了； 但是，这个时候主机A还是可以接受来自主机B的数据； 当主机B返回ACK报文段时，表示它已经知道主机A没有数据发送了，但是主机B还是可以发送数据到主机A的； 当主机B也发送了FIN报文段时，这个时候就表示主机B也没有数据要发送了，就会告诉主机A，我也没有数据要发送了； 主机A告诉主机B知道了，主机B收到这个确认之后就立马关闭自己。 主机A等待2MSL之后也关闭了自己。 说明完了四次挥手的过程，下面着重解释一下为什么主机A要等待2MSL之后才关闭自己。 针对最后一条消息，即主机A发送ack后，主机B接收到此消息，即认为双方达成了同步：双方都知道连接可以释放了，此时主机B可以安全地释放此TCP连接所占用的内存资源、端口号。 所以被动关闭的主机B无需任何wait time，直接释放资源。 但是主机A并不知道主机B是否接到自己的ACK，主机A是这么想的： 如果主机B没有收到自己的ACK，主机B会超时重传FiN，那么主机A再次接到重传的FIN，会再次发送ACK 如果主机B有收到自己的ACK，也不会再发任何消息，包括ACK 无论是情况1还是2，主机A都需要等待，要取这两种情况等待时间的最大值，以应对最坏的情况发生，这个最坏情况是： 主机B没有收到主机A的ACK，那么超时之后主机B会重传FIN，也就是说，要浪费一个主机A发出ACK的最大存活时间(MSL)+FIN消息的最大存活时间(MSL) 不可能时间再多了，这个已经针对最糟糕的状况。 等待2MSL时间，主机A就可以放心地释放TCP占用的资源、端口号，此时可以使用该端口号连接任何服务器。 在等待的时间内，主机B可以重试多次，因为2MSL时间为240秒，超时重传只有0.5秒，1秒，2秒，16秒。 当主机B重试次数达到上限，主机B会reset连接。 那么为什么是2MSL我们已经了解了，但是为什么要等这个时间呢？ 如果不等，释放的端口可能会重连刚断开的服务器端口，这样依然存活在网络里的老的TCP报文可能与新TCP连接报文冲突，造成数据冲突，为避免此种情况，需要耐心等待网络老的TCP连接的活跃报文全部死翘翘，2MSL时间可以满足这个需求。 整理自： https://www.zhihu.com/question/24853633 https://www.zhihu.com/question/67013338 https://github.com/jawil/blog/issues/14 https://www.jianshu.com/p/9968b16b607e]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议入门]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F4.TCP%E5%8D%8F%E8%AE%AE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第四篇文章。首先要明确：信道本身不可靠（丢包、重复包、出错、乱序），不安全。所以引出了七层或五层模型来保证。因此，任何一个东西的提出都是为了解决某个问题的。学习计算机，从他的历史出发，理解为什么会有不断低迭代，因为是为了解决某个痛点问题。比如HTTP的发展，为什么在HTTP1.0基础上还要提出HTTP1.1，为什么还要提出HTTP2.0，我们学习了他的发展历史之后就会明白了。同样，下面再说一说为什么要有TCP协议，TCP到底解决了什么问题。 一、回顾 首先简单回顾一下。 1.1 物理层 物理层是相当于物理连接，将0101以电信号的形式传输出去。 1.2 数据链路层 数据链路层，有一个叫做以太网协议，规定了电子信号是如何组成数据包的，这个协议的头里面，包含了自身的网卡信息，还有目的地的网卡信息（一般我们可以知道对方的IP，IP可以通过DNS解析到，然后根据ARP协议将IP转换为MAC地址）。那么，如果在同一个局域网内，我们就可以通过广播的方式找到对应MAC地址的主机。—即以太网协议解决了子网内部的点对点通信。 1.3 网络层 但是呢，以太网协议不能解决多个局域网通信，每个局域网之间不是互通的，那么以太网这种广播的方式不可用，就算可用，网络那么大，通过广播进行找，是一个可怕的场景。那么，IP协议可以连接多个局域网，简单来说，IP 协议定义了一套自己的地址规则，称为 IP 地址。物理器件，比如说路由器，就是基于IP协议，里面保存一套地址指路牌，想去哪个局域网，可以通过这个牌子来找，然后逐步路由到目标局域网，最后就可以找到那台主机了。IP层就是对应了网络层。 1.4 传输层 那么，此时解决了多个局域网路由问题，也解决了局域网内寻址问题，即我这台主机已经可以找到那台主机了，下面还有什么事情需要做呢？显然，找到主机还不行啊，比如我用微信发一条消息，我发到你主机了，但是你主机上的微信不知道这条消息发给他了，这里说的就是端口，信息要发到这个端口上，监听这个端口的程序才会收到消息。 1.5 应用层 OK，最上层的应用层，就是最贴近用户的，他的一系列协议只是为了让两台主机会互相都理解而已。 二、问题和解决 2.1 存在的问题 在明白了计算机网络为什么要这几层模型之后，我们再回到一开始，如何保证安全、可靠、完整地传输信息呢？ 很显然，上面提到的，只是保证信息能找到对方主机和端口，但是这个信息中途被拦截了、甚至被篡改了、信息延迟了（几分钟或者几个小时，或者几个世纪）、网络不通或者挂了，信息自己可不会告诉你他挂了或者要迟到一会，如果没有一个协议来保障可靠性，那么我这条消息发出去，能不能到、能不能及时到、能不能完整到、能不能不被篡改到等这些问题将会造成灾难，网络传输也就没有了意义。 计算机的前辈们，为我们提供了一系列的措施来尽可能保证信息能正确送达。 2.2 数据校验 首先在数据链路层，可以通过各种校验，比如奇偶校验等手段来判断数据包传的是否正确。 2.3 数据可靠性 好了，解决了数据是否正确之后，但是还不能保证线路是可靠的，加入某个包没发出去或者发错了，应该有一个出错重传机制，保证信息传输的可靠性。这就引出了TCP协议。 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 下面来看看TCP是如何解决丢包、重复包、出错、乱序等不可靠的一些问题的。 三、滑动窗口协议的提出 这就引出了TCP中最终的一个东西：滑动窗口协议。 3.1 朴素的方法来确保可靠性 先从简单的角度出发，自己想一下，如何保证不丢包、不乱序。 按照顺序，发送一个确认一个，显然，吞吐量非常低。那么，一次性发几个包，然后一起确认呢？ 3.2 改进方案 那么就引出第二个问题，我一次性发几个包合适呢？就这引出了滑动窗口。 四、数据包编号和重传机制 4.1 数据包编号 在说明滑动窗口原理之前，必须要说一下TCP数据包的编号SEQ。 我们知道，由于以太网数据包大小限制，所以每个包承载的数据是有限的，如果发一个很大的包，必然是要拆分的。 发送的时候，TCP 协议为每个包编号（sequence number，简称 SEQ），以便接收的一方按照顺序还原。万一发生丢包，也可以知道丢失的是哪一个包。 第一个包的编号是一个随机数。为了便于理解，这里就把它称为1号包。假定这个包的负载长度是100字节，那么可以推算出下一个包的编号应该是101。这就是说，每个数据包都可以得到两个编号：自身的编号，以及下一个包的编号。接收方由此知道，应该按照什么顺序将它们还原成原始文件。 4.2 数据重传机制 TCP协议就是根据这些编号来重新还原文件的。并且接收端保证顺序性返回ACK确认，比如有两个包发过去，为1号和2号，2号接收成功，但是发现1号包还没接收到，所以2号的ACK是不会发回去的，这个时候，如果在重传时间内收到1号了，那么就把这两个包的ACK都返回回去，如果超时了，就重传1号包。知道1号包接收成功，后续的才会返回ACK。 具体是如何做到的呢？ 前面说过，每一个数据包都带有下一个数据包的编号。如果下一个数据包没有收到，那么 ACK 的编号就不会发生变化。 举例来说，现在收到了4号包，但是没有收到5号包。ACK 就会记录，期待收到5号包。过了一段时间，5号包收到了，那么下一轮 ACK 会更新编号。如果5号包还是没收到，但是收到了6号包或7号包，那么 ACK 里面的编号不会变化，总是显示5号包。这会导致大量重复内容的 ACK。 如果发送方发现收到三个连续的重复 ACK，或者超时了还没有收到任何 ACK，就会确认丢包，即5号包遗失了，从而再次发送这个包。通过这种机制，TCP 保证了不会有数据包丢失。 （图片说明：Host B 没有收到100号数据包，会连续发出相同的 ACK，触发 Host A 重发100号数据包。） 五、慢启动 下面再来说说慢启动。 服务器发送数据包，当然越快越好，最好一次性全发出去。但是，发得太快，就有可能丢包。带宽小、路由器过热、缓存溢出等许多因素都会导致丢包。线路不好的话，发得越快，丢得越多。 最理想的状态是，在线路允许的情况下，达到最高速率。但是我们怎么知道，对方线路的理想速率是多少呢？答案就是慢慢试。 TCP 协议为了做到效率与可靠性的统一，设计了一个慢启动（slow start）机制。开始的时候，发送得较慢，然后根据丢包的情况，调整速率：如果不丢包，就加快发送速度；如果丢包，就降低发送速度。 Linux 内核里面设定了（常量TCP_INIT_CWND），刚开始通信的时候，发送方一次性发送10个数据包，即&quot;发送窗口&quot;的大小为10。然后停下来，等待接收方的确认，再继续发送。 默认情况下，接收方每收到两个 TCP 数据包，就要发送一个确认消息。&quot;确认&quot;的英语是 acknowledgement，所以这个确认消息就简称 ACK。 ACK 携带两个信息： 期待要收到下一个数据包的编号 接收方的接收窗口的剩余容量 发送方有了这两个信息，再加上自己已经发出的数据包的最新编号，就会推测出接收方大概的接收速度，从而降低或增加发送速率。这被称为&quot;发送窗口&quot;，这个窗口的大小是可变的。 我们可以知道，发送发和接收方都维护了一个缓冲区，可以理解为窗口。根据接收速度可以调整发送速度，逐渐达到这条线路最高的传输速率。 ok，下面就可以研究一下滑动窗口了。 六、滑动窗口原理 正常情况下： （如图，123表示已经正常发送并且收到了ACK确认。4567属于已发送但是还没有收到ACK。8910表示待发送。这个窗口当前长度为7.正常情况下，4号包收到ACK，那么窗口就会右移一格。） 但是往往会出现一些问题，比如5号包迟迟收不到ACK，在接收端可能是没有收到5号包，但是可能会收到6号包甚至是7、8号包，那么此时只能等待5号包，如果5号包顺利到达了，那么就把5678号包的ACK都发给发送端，那么发送端滑动窗口向右右移四格。如果迟迟收不到5号包，只能重传。 以上就是关于TCP中比较重要的出错重传、编号、慢启动以及滑动窗口。这些保证了数据传输的可靠性。 整理自：http://www.ruanyifeng.com/blog/2017/06/tcp-protocol.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http的前世今生]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F3.http%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第三篇文章。作为一个后端攻城狮，每天打交道最多的就是HTTP协议，在如今火热的微服务实现方案中，除了阿里的dubbo，就是spring cloud，而spring cloud目前适用的服务间通信方式也就是基于HTTP 的 restful接口来实现。并且作为浏览器上用的最多的协议，无论是前端、后端还是测试都应该去熟悉它，软件的发展是循序渐进的，每次的迭代升级都是为了解决上一版本的痛点，HTTP协议的发展也是如此，本章着重讲解HTTP的前世今生，让我们更加了解HTTP。 HTTP 是基于 TCP/IP 协议的应用层协议。它不涉及数据包（packet）传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。 一、HTTP/0.9 1.1 简介 这是第一个定稿的HTTP协议。 内容非常简单，只有一个命令GET 没有HEADER等描述数据的信息 服务器发送完毕，就关闭TCP连接（一个HTTP请求在一个TCP连接中完成） 1.2 请求格式 比如发起一个GET请求： GET /index.html 上面命令表示，TCP 连接（connection）建立后，客户端向服务器请求（request）网页index.html。 1.3 响应格式 协议规定，服务器只能回应HTML格式的字符串，不能回应别的格式。 123&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 服务器发送完毕，就关闭TCP连接。 二、HTTP/1.0 2.1 简介 跟现在比较普遍适用的1.1版本已经相差不多。 增加很多命令，比如POST、HEAD等命令 增加status code 和 header 多字符集支持、多部分发送、权限、缓存等 首先，任何格式的内容都可以发送。这使得互联网不仅可以传输文字，还能传输图像、视频、二进制文件。这为互联网的大发展奠定了基础。 其次，除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。 再次，HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。 其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 2.2 请求格式 下面是一个1.0版的HTTP请求的例子。 123GET / HTTP/1.0User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: */* 可以看到，这个格式与0.9版有很大变化。 第一行是请求命令，必须在尾部添加协议版本（HTTP/1.0）。后面就是多行头信息，描述客户端的情况。 客户端请求的时候，可以使用Accept字段声明自己可以接受哪些数据格式。上面代码中，客户端声明自己可以接受任何格式的数据。 2.3 响应格式 服务器的回应如下： 12345678910HTTP/1.0 200 OK Content-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 回应的格式是&quot;头信息 + 一个空行（\r\n） + 数据&quot;。其中，第一行是&quot;协议版本 + 状态码（status code） + 状态描述&quot;。 2.4 Content-Type 字段 关于字符的编码，1.0版规定，头信息必须是 ASCII 码，后面的数据可以是任何格式。因此，服务器回应的时候，必须告诉客户端，数据是什么格式，这就是Content-Type字段的作用。 2.5 缺点 每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。 为了解决这个问题，有些浏览器在请求时，用了一个非标准的Connection字段。 Connection: keep-alive 这个字段要求服务器不要关闭TCP连接，以便其他请求复用。服务器同样回应这个字段。 Connection: keep-alive 一个可以复用的TCP连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段，不同实现的行为可能不一致，因此不是根本的解决办法。 三、HTTP/1.1 3.1 持久连接和管道机制 持久连接（以前的版本中，一个HTTP请求就创建一个TCP连接，请求返回之后就关闭TCP连接，然而建立一次TCP连接的过程是比较耗时的，效率会比较低，现在建立一个TCP连接后，后面的HTTP请求都可以复用这个TCP连接，即允许了在同一个连接里面发送多个请求，会提高效率） pipeline（解决了同一个TCP连接中客户端可以发送多个HTTP请求，但是对于服务端来说，对于进来的请求要按照顺序进行内容的返回，如果前一个请求处理时间长，而后一个请求处理时间端，即便后面一个请求已经处理完毕了，也要等待前一个请求处理完毕返回他才可以返回结果，这种串行的方式比较慢） 在1.1版本以前，每次HTTP请求，都会重新建立一次TCP连接，服务器响应后，就立刻关闭。众所周知，建立TCP连接的新建成本很高，因为需要三次握手，并且有着慢启动的特性导致发送速度较慢。而1.1版本添加的持久连接功能可以让一次TCP连接中发送多条HTTP请求，值得一提的是默认是，控制持久连接的Connection字段默认值是keep-alive，也就是说是默认打开持久连接，如果想要关闭，只需将该字段的值改为close。 Connection: close 而管道化则赋予了客户端在一个TCP连接中连续发送多个请求的能力，而不需要等到前一个请求响应，这大大提高了效率。值得一提的是，虽然客户端可以连续发送多个请求，但是服务器返回依然是按照发送的顺序返回。（强调的是request不需要等待上一个request的response，其实发送的request还是有顺序的，服务端按照这个顺序接收，依次返回响应） HTTP/1.1允许多个http请求通过一个套接字同时被输出 ，而不用等待相应的响应。然后请求者就会等待各自的响应，这些响应是按照之前请求的顺序依次到达。（me：所有请求保持一个FIFO的队列，一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出；同时，服务器端返回这些请求的响应时也是按照FIFO的顺序）。管道化的表现可以大大提高页面加载的速度，尤其是在高延迟连接中。 3.2 Content-Length 字段 一个TCP连接现在可以传送多个回应，势必就要有一种机制，区分数据包是属于哪一个回应的。这就是Content-length字段的作用，声明本次回应的数据长度。 Content-Length: 3495 上面代码告诉浏览器，本次回应的长度是3495个字节，后面的字节就属于下一个回应了。 在1.0版中，Content-Length字段不是必需的，因为浏览器发现服务器关闭了TCP连接，就表明收到的数据包已经全了。 3.3 分块传输编码 对于一些很耗时的动态操作来说，这意味着，服务器要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用&quot;流模式&quot;（stream）取代&quot;缓存模式&quot;（buffer）。 因此，1.1版规定可以不使用Content-Length字段，而使用&quot;分块传输编码&quot;（chunked transfer encoding）。只要请求或回应的头信息有Transfer-Encoding字段，就表明回应将由数量未定的数据块组成。 Transfer-Encoding: chunked 每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后是一个大小为0的块，就表示本次回应的数据发送完了。下面是一个例子。 1234567891011121314151617HTTP/1.1 200 OKContent-Type: text/plainTransfer-Encoding: chunked25This is the data in the first chunk1Cand this is the second one3con8sequence0 3.3 其他功能 1.1版还新增了许多动词方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。 另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名。 Host: www.example.com 有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础。 3.4 缺点 虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为&quot;队头堵塞&quot;（Head-of-line blocking）。 四、SPDY 协议 2009年，谷歌公开了自行研发的 SPDY 协议，主要解决 HTTP/1.1 效率不高的问题。 这个协议在Chrome浏览器上证明可行以后，就被当作 HTTP/2 的基础，主要特性都在 HTTP/2 之中得到继承。 五、HTTP/2 5.1 二进制协议 HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。 HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为&quot;帧&quot;（frame）：头信息帧和数据帧。 二进制协议的一个好处是，可以定义额外的帧。HTTP/2 定义了近十种帧，为将来的高级应用打好了基础。如果使用文本实现这种功能，解析数据将会变得非常麻烦，二进制解析则方便得多。 5.2 多工 HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了&quot;队头堵塞&quot;。 举例来说，在一个TCP连接里面，服务器同时收到了A请求和B请求，于是先回应A请求，结果发现处理过程非常耗时，于是就发送A请求已经处理好的部分， 接着回应B请求，完成后，再发送A请求剩下的部分。 这样双向的、实时的通信，就叫做多工（Multiplexing）。 5.3 数据流 因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。 5.4 头信息压缩 HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如Cookie和User Agent，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。 HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息使用gzip或compress压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。 5.5 服务器推送 HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。 常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析HTML源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。 整理自：http://www.ruanyifeng.com/blog/2016/08/http.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从上到下看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F2.%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第二篇文章。经过上一篇文章的详细介绍，我们了解了一个数据是如何从物理层一步一步到达应用层的，那么本章从上而下的角度来看看一条请求时如何从浏览器传递到服务器并且返回的。 这个过程看的就是用户从浏览器输入一条url之后，是如何发送过去的。 1.上一篇文章的小结 我们已经知道，网络通信就是交换数据包。电脑A向电脑B发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样： 发送这个包，需要知道两个地址： 对方的MAC地址 对方的IP地址 有了这两个地址，数据包才能准确送到接收者手中。但是，前面说过，MAC地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的MAC地址，必须通过网关（gateway）转发。 上图中，1号电脑要向4号电脑发送一个数据包。它先判断4号电脑是否在同一个子网络，结果发现不是（后文介绍判断方法），于是就把这个数据包发到网关A。网关A通过路由协议，发现4号电脑位于子网络B，又把数据包发给网关B，网关B再转发到4号电脑。 1号电脑把数据包发到网关A，必须知道网关A的MAC地址。 发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的MAC地址。 有了一台新电脑之后，要想上网，一种方式是自己配置静态IP： 很多人都没有进行过这个配置，因为一般情况下我们根本不需要这样。但是有的时候也会用到，比如我在电信实习的时候，他们每一个网口旁边都贴着这四个参数，你联网必须要适用他提供的一系列地址才行。其实经过上面的学习，我们已经知道，通信的时候，需要知道对方的IP（ARP知道对方的MAC地址）、子网掩码（确定所在的子网）、默认网关（不在一个子网，要通过网关取转发、路由）、DNS服务器（解析域名为IP地址）。 但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用&quot;动态IP地址上网&quot;。 2.DHCP协议 所谓&quot;动态IP地址&quot;，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。 这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做&quot;DHCP服务器&quot;。新的计算机加入网络，必须向&quot;DHCP服务器&quot;发送一个&quot;DHCP请求&quot;数据包，申请IP地址和相关的网络参数。 前面说过，如果两台计算机在同一个子网络，必须知道对方的MAC地址和IP地址，才能发送数据包。但是，新加入的计算机不知道DHCP服务器的两个地址，怎么发送数据包呢？ DHCP协议做了一些巧妙的规定。 首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的： （1）最前面的&quot;以太网标头&quot;，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 （2）后面的&quot;IP标头&quot;，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。 （3）最后的&quot;UDP标头&quot;，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。 因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。 当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道&quot;这个包是发给我的&quot;，而其他计算机就可以丢弃这个包。 接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个&quot;DHCP响应&quot;数据包。 这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。 动态拿到最核心的四个参数：自己的IP地址、子网掩码、网关地址、DNS服务器，就可以联网了。 3.访问google的过程 我们假定，经过上一节的步骤，用户设置好了自己的网络参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。 这意味着，浏览器要向Google发送一个网页请求的数据包。 3.1 DNS协议 我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。 DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。 然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 3.2 子网掩码 接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。 已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。 3.3 应用层协议 浏览网页用的是HTTP协议，它的整个数据包构造是这样的： HTTP部分的内容，类似于下面这样： 123456789 GET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) ...... Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: ... ... 我们假定这个部分的长度为4960字节，它会被嵌在TCP数据包之中。 3.4 TCP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 3.5 IP协议 然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。 IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 3.6 以太网协议 最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 3.7 服务器响应 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。 根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的&quot;HTTP请求&quot;，接着做出&quot;HTTP响应&quot;，再用TCP协议发回来。 本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 整理自：http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从下到上看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F1.%E4%BB%8E%E4%B8%8B%E5%88%B0%E4%B8%8A%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第一篇文章。要想了解HTTP协议，必然要从最基本的计算机网络知识开始入手。本篇文章从下到上具体介绍五层经典模型，极速入门计算机网络。 经典五层模型 下面我们先来了解一下各层做的事情！ 1.物理层 电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。 这就叫做&quot;物理层&quot;，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 2.数据链路层 单纯的0和1没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？ 这就是&quot;链接层&quot;的功能，它在&quot;实体层&quot;的上方，确定了0和1的分组方式。 2.1 以太网协议 早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做&quot;以太网&quot;（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做&quot;帧&quot;（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。 &quot;标头&quot;包含数据包的一些说明项，比如发送者、接受者、数据类型等等；&quot;数据&quot;则是数据包的具体内容。 &quot;标头&quot;的长度，固定为18字节。&quot;数据&quot;的长度，最短为46字节，最长为1500字节。因此，整个&quot;帧&quot;最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。 2.2 MAC地址 上面提到，以太网数据包的&quot;标头&quot;，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？ 以太网规定，连入网络的所有设备，都必须具有&quot;网卡&quot;接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。 每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。 前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 2.3 广播 定义地址只是第一步，后面还有更多的步骤。 首先，一块网卡怎么会知道另一块网卡的MAC地址？ 回答是有一种ARP协议，可以解决这个问题。下面介绍ARP。 其次，就算有了MAC地址，系统怎样才能把数据包准确送到接收方？ 回答是以太网采用了一种很&quot;原始&quot;的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 上图中，1号计算机向2号计算机发送一个数据包，同一个子网络的3号、4号、5号计算机都会收到这个包。它们读取这个包的&quot;标头&quot;，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做&quot;广播&quot;（broadcasting）。 有了数据包的定义、网卡的MAC地址、广播的发送方式，&quot;链接层&quot;就可以在多台计算机之间传送数据了。 3.网络层 以太网协议，依靠MAC地址发送数据。理论上，单单依靠MAC地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。 但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一&quot;包&quot;，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。 互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。 因此，必须找到一种方法，能够区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用&quot;路由&quot;方式发送。（&quot;路由&quot;的意思，就是指如何向不同的子网络分发数据包，这是一个很大的主题，本文不涉及。）遗憾的是，MAC地址本身无法做到这一点。它只与厂商有关，与所处网络无关。 这就导致了&quot;网络层&quot;的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做&quot;网络地址&quot;，简称&quot;网址&quot;。 于是，&quot;网络层&quot;出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。 3.1 IP协议 规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。 目前，广泛采用的是IP协议第四版，简称IPv4。这个版本规定，网络地址由32个二进制位组成。 习惯上，我们用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。 互联网上的每一台计算机，都会分配到一个IP地址。这个地址分成两个部分，前一部分代表网络，后一部分代表主机。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。处于同一个子网络的电脑，它们IP地址的网络部分必定是相同的，也就是说172.16.254.2应该与172.16.254.1处在同一个子网络。 但是，问题在于单单从IP地址，我们无法判断网络部分。还是以172.16.254.1为例，它的网络部分，到底是前24位，还是前16位，甚至前28位，从IP地址上是看不出来的。 那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数&quot;子网掩码&quot;（subnet mask）。 所谓&quot;子网掩码&quot;，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.254.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。 知道&quot;子网掩码&quot;，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 比如，已知IP地址172.16.254.1和172.16.254.233的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，结果都是172.16.254.0，因此它们在同一个子网络。 总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。 3.2 IP数据包 根据IP协议发送的数据，就叫做IP数据包。不难想象，其中必定包括IP地址信息。 但是前面说过，以太网数据包只包含MAC地址，并没有IP地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？ 回答是不需要，我们可以把IP数据包直接放进以太网数据包的&quot;数据&quot;部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。 具体来说，IP数据包也分为&quot;标头&quot;和&quot;数据&quot;两个部分。&quot;标头&quot;部分主要包括版本、长度、IP地址等信息，&quot;数据&quot;部分则是IP数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。 IP数据包的&quot;标头&quot;部分的长度为20个字节，整个数据包的总长度最大为65,535字节。因此，理论上，一个IP数据包的&quot;数据&quot;部分，最长为65,515字节。前面说过，以太网数据包的&quot;数据&quot;部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。 3.3 ARP协议 关于&quot;网络层&quot;，还有最后一点需要说明。 因为IP数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的MAC地址，另一个是对方的IP地址。通常情况下，对方的IP地址是已知的，但是我们不知道它的MAC地址。 所以，我们需要一种机制，能够从IP地址得到MAC地址。 这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的&quot;网关&quot;（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个&quot;广播&quot;地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 4. 传输层 有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。 接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？ 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做&quot;端口&quot;（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 &quot;端口&quot;是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。 &quot;传输层&quot;的功能，就是建立&quot;端口到端口&quot;的通信。相比之下，“网络层&quot;的功能是建立&quot;主机到主机&quot;的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做&quot;套接字”（socket）。有了它，就可以进行网络应用程序开发了。 4.1 UDP协议 现在，我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。 UDP数据包，也是由&quot;标头&quot;和&quot;数据&quot;两部分组成。 &quot;标头&quot;部分主要定义了发出端口和接收端口，&quot;数据&quot;部分就是具体的内容。然后，把整个UDP数据包放入IP数据包的&quot;数据&quot;部分，而前面说过，IP数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样： UDP数据包非常简单，&quot;标头&quot;部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 4.2 TCP协议 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 为了解决这个问题，提高网络可靠性，TCP协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的UDP协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 因此，TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。 TCP数据包和UDP数据包一样，都是内嵌在IP数据包的&quot;数据&quot;部分。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 关于TCP细节以后再探讨。 5. 应用层 应用程序收到&quot;传输层&quot;的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。 &quot;应用层&quot;的作用，就是规定应用程序的数据格式。 举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了&quot;应用层&quot;。 这是最高的一层，直接面对用户。它的数据就放在TCP数据包的&quot;数据&quot;部分。因此，现在的以太网的数据包就变成下面这样。 *注：UDP头为8个字节，TCP头为20个字节 整理于：http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
</search>
