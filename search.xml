<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数值计算精度丢失问题]]></title>
    <url>%2F2019%2F01%2F31%2Fmiscellany%2F14%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%B2%BE%E5%BA%A6%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[无论在什么业务中，钱是非常重要的东西，对账的时候一定要对的上，不能这边少一分那边多一分。对于数值的计算，尤其是小数，double和double都是禁止使用的。 阿里强制要求存放小数时使用 decimal，禁止使用 float 和 double。 说明：float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。 处理方式可以为：mysql 可以用 decimal ，如果你是用 java， 在商业计算中我们要用 java.math.BigDecimal，注意：如果需要精确计算，非要用String来够造BigDecimal不可！ 那么到底是什么情况？ 一个例子说明 废话不多说，上图： 问题原因 无论是我们本文提到的double，还是float，都是浮点数。 在计算机科学中，浮点（英语：floating point，缩写为FP）是一种对于实数的近似值数值表现法，由一个有效数字（即尾数）加上幂数来表示，通常是乘以某个基数的整数次指数得到。以这种表示法表示的数值，称为浮点数（floating-point number）。 其实我觉得很好理解，我们之前说过，计算机计算加减乘除啊，都是用的加法器，实质都是二进制的加法处理。那么这里就有一个二进制表示的问题。试想，4，2，8之流都是2的幂次方，可以完美用二进制表示，计算当然不会出现问题。对于0，1，3，5之类也都可以用二进制来表示出来，所以，正数肯定是没问题的。 但是对于小数呢？1、0.5、0.25那都是可以转换成二进制的小数，如十进制的0.1，就无法用二进制准确的表示出来。因此只能使用近似值的方式表达。 如果我们尝试着把10进制的0.1转化成二进制，会怎么转呢？ 在十进制中，0.1如何计算出来的呢？ 0.1 = 1 ÷ 10 那么二进制中也是同理： 1 ÷ 1010 我们回到小学的课堂，来列竖式吧： 123456789101112131415 0.000110011... ------------------1010 ) 1 0000 1010 ------ 1100 1010 ---- 10000 1010 ----- 1100 1010 ---- 10 很显然，除不尽，除出了一个无限循环小数：二进制的 0.0001100110011… 那么，如何在计算机中表示这个无限不循环的小数呢？只能考虑按照不同的精度保理不同的位数。 我们知道float是单精度的，double是双精度的。不同的精度，其实就是保留的有效数字位数不同，保留的位数越多，精度越高。 所以，浮点数在Java中是无法精确表示的，因为大部分浮点数转换成二进制是一个无限不循环的小数，只能通过保留精度的方式进行近似表示。 问题的解决 String 构造方法是完全可预知的：写入 newBigDecimal(&quot;0.1&quot;) 将创建一个 BigDecimal，它正好等于预期的 0.1。因此，比较而言，通常建议优先使用String构造方法。 使用BigDecimal(String val)！ 123456789101112131415161718192021222324252627//加法public static BigDecimal add(double v1, double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.add(b2);&#125;//减法public static BigDecimal sub(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.subtract(b2);&#125;//乘法public static BigDecimal mul(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.multiply(b2);&#125;//除法public static BigDecimal div(double v1,double v2)&#123; BigDecimal b1 = new BigDecimal(Double.toString(v1)); BigDecimal b2 = new BigDecimal(Double.toString(v2)); return b1.divide(b2,2,BigDecimal.ROUND_HALF_UP);//四舍五入,保留2位小数,应对除不尽的情况&#125; 那么，上面的精度丢失问题就迎刃而解了。但是除不尽怎么办？比如10.0除以这里的3.0，保留小数点后三位有效数字： 那么，每个用户得到的都是3.333元，三个用户加起来是得不到10块钱的。 对于除法，始终会产生除不尽的情况怎么办？有个词叫轧差 什么意思呢？举个简单例子。假如现在需要把10元分成3分，如果是10除以3这么除，会发现为3.33333无穷尽的3。这些数字完全无法在程序或数据库中进行精确的存储。 简单理解就是，当除不尽或需去除小数点的时候，前面的n-1笔（这里n=3）做四舍五入。最后一笔做兜底（总金额减去前面n-1笔之和）。这样保证总金额的不会丢失。 比如10块钱，三个用户分，前面两个用户只能各分到3。333块钱，最后一个用户分到3.334块钱。保证总额不变。 至于原理，有一点点数学化，以后再作探讨吧。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis主从复制]]></title>
    <url>%2F2019%2F01%2F31%2Fredis%2FRedis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[介绍redis主从复制功能实现原理。作为高可用的基础，了解一下其中的门道是有必要的。 1.单机有什么问题 机器故障 容量瓶颈 QPS瓶颈 2. 主从复制的作用 数据副本 扩展读性能，slave专门用来读 一个master可以有多个slave，一个salve只能有一个master 3. 两种实现方式 方式一：slaveof命令 slaveof masterIp masterPort slaveof no one(不会清除原来同步的数据，而是新的数据不会再同步给他) 方式二：配置 修改某一行的配置：slaveof ip port 从节点只做读操作：slave-read-only yes 对比 命令的优点：不需要重启 命令的缺点：不便于管理 配置的优点：统一配置 配置的缺点：需要重启 一个场景，假如6380是6379的一个从节点，然后将6380执行salveof no one，然后插入一些新的数据；再重新变成6379的从节点，那么里面的新数据会被清除掉。 查看run_id redis-cli -p 6379 info server | grep run 4. 全量复制 全量复制开销 bgsave时间 rdb网络传输时间 从节点清空数据的时间 从节点加载RDB的时间 可能的AOF重写时间 存在的问题 时间开销比较大 如果master和slave之间网络扰动甚至断开，那么master此间更新的数据对于slave是不知道的，最简单的方法就是再进行一次全量复制，但是显然，消耗太大了。 5. 部分复制 6. 开发与运维的问题 读写分离 master只做写操作，slave来做读操作，来分摊流量。但是会有一些问题： 复制数据延迟 读到过期数据 从节点故障 主从配置不一致 例如maxmemory不一致：丢失数据 数据结构优化参数：内存不一致 规避全量复制 第一次全量复制：不可避免—小主节点(maxmemroy不要太大)或者在低峰时进行操作 节点run_id不匹配（主节点重启，那么master的run_id会发生变化，slave发现其run_id变化，会进行全量复制）；我们可以用故障转移，例如哨兵或集群来避免全量复制。 复制积压缓冲区不足(网络中断，部分复制无法满足)，可以增大复制缓冲区配置size，网络增强 规避复制风暴 概念：主节点宕机造成大量的全量复制 单主节点复制风暴：主节点重启，多从节点复制；解决：更换复制拓扑 单机器复制风暴：机器宕机后（该机器全是Mater），大量全量复制。解决：master分散多机器。 说到底，还是需要有一种高可用的实现方式，在master出现故障之后，如何自动实现从slave晋升为master继续使用.而不是一直死守着原来老的master不放，因为老的master啥时候恢复不知道，恢复了可能会造成复制风暴，既然从节点本来是一直与master节点保持尽量的同步的，那么为什么不将数据最新的从节点升级为主节点呢？下一章继续来分析。]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis持久化]]></title>
    <url>%2F2019%2F01%2F31%2Fredis%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[redis处理数据都是在内存中进行，所以速度特别快，同样，它也可以支持持久化，这里注意，并不是说redis要来充当mysql那种角色，其实更多的是为了在崩溃的时候快速恢复以及主从复制这样的功能。redis的持久化主要有两种方式，一种是RDB，一种是AOF，对于他们的原理和区别都是比较重要的面试考察点，需要掌握。 1. 什么是持久化 redis所有数据保持在内存中，对数据的更新将异步地保存到磁盘中。 2. 持久化的方式 快照—mysql dump或者redis rdb 写日志—mysql binlog或者hbase glog或者redis aof 3. RDB 什么是RDB 触发机制三种主要方式 save(同步持久化，会造成redis主线程的阻塞，不推荐使用) save是同步的，当保存的数据量很大时，可能造成redis的阻塞，即客户端访问redis被阻塞。 他的文件策略是：如果存在老的RDB文件，则新的替换老的。复杂度为O(n)。 bgsave(异步，fork一个子进程来进行持久化，不会造成主线程的阻塞) 一般情况下，fork是比较快的，但是也可以会慢，这时会阻塞redis。只要fork不慢，客户端不会被阻塞。 他的文件策略和复杂度与save是一样的。 save和bgsave两者对比： 自动 redis的自动保存的默认配置是： 配置 seconds changes save 900 1 save 300 10 save 60 10000 就是说，在60秒内改变了10000条数据，就自动保存；在300秒内有10条改变才自动保存；900秒内有1一条改变就保存。 RDB总结 RDB是Redis内存到硬盘的快照，用于持久化。 save通常会阻塞redis。 bgsave不会阻塞redis，但是会fork新进程。 save自动配置满足任一就会被执行。 有些触发机制不容忽视。 4. AOF RDB问题 全量数据存入磁盘 O(n)数据的备份，很耗时间；对于bgsave来说，fork()是一个很消耗内存的操作；将数据全写到硬盘，必然对硬盘IO占用很大。 宕机丢失数据多 还有一点是：某个时间点宕机，那么在某个时间段的数据就丢失了。 AOF原理 将对redis的操作追加到aof文件中。当redis宕机之后，使用aof恢复所有的操作继而实现数据的恢复。 AOF三种策略 always everysec redis出现故障，有可能丢失一秒的数据。redis默认方式。 no 三种策略的比较 AOF重写 好处是：减少硬盘占用、减少数据丢失 下面是AOF的bgrewirteaof的过程： 注意：这里的重写并不是上面演示的，将原来的aof文件进行重写，而是根据redis现在的内存数据进行一次回溯。 aof重写流程 也就是说，子进程在执行 AOF 重写时，主进程需要执行以下三个工作： 1.处理命令请求； 2.将写命令追加到现有的 AOF 文件中； 3.将写命令追加到 AOF 重写缓存中。 如此可以保证： 现有的AOF功能继续执行，即使 AOF 重写期间发生停机，也不会有任何数据丢失； 所有对数据库进行修改的命令都会被记录到 AOF 重写缓存中。 当子进程完成对 AOF 文件重写之后，它会向父进程发送一个完成信号，父进程接到该完成信号之后，会调用一个信号处理函数，该函数完成以下工作：(阻塞) 将 AOF 重写缓存中的内容全部写入到新的 AOF 文件中；(现有 AOF 文件、新的 AOF 文件和数据库三者的状态就完全一致了) 对新的 AOF 文件进行改名，覆盖原有的 AOF 文件。(执行完毕后，程序就完成了新旧两个 AOF 文件的替换) 当这个信号处理函数执行完毕之后，主进程就可以继续像往常一样接收命令请求了。在整个 AOF 后台重写过程中，只有最后的“主进程写入命令到AOF缓存”和“对新的 AOF 文件进行改名，覆盖原有的 AOF 文件”这两个步骤会造成主进程阻塞，在其他时候， AOF 后台重写都不会对主进程造成阻塞，这将 AOF 重写对性能造成的影响降到最低。 小结： AOF 重写的目的是轻量地保存数据库状态，整个重写过程基本上不影响 Redis 主进程处理命令请求； AOF在redis宕机的时候最多丢失一秒的数据，比RDB要好一点，并且可读性高，基本上能看得懂 AOF 重写其实是一个有歧义的名字，实际上重写工作是针对数据库的当前值来进行的，重写过程中不会读写、也不适用原来的 AOF 文件； AOF 可以由用户手动触发，也可以由服务器自动触发。 5. 持久化的取舍和选择 RDB和AOF对比 可以看出，世界上没有完美的东西，只有合适的东西。AOF同样存在一些问题：AOF文件的体积通常要大于RDB文件的体积、且恢复速度慢。 RDB最佳策略 “关”：建议关闭，但是后面主从复制功能是需要他的，因为需要主节点执行dbsave，然后将rdb文件传给从节点。所以说，关不是永久关。 “集中管理”：虽然RDB很重，但是对于数据备份是很重要的，按照小时或者天集中地进行备份比较好，因为他的文件很小，利于传输。 “主从，从开”：有时候从节点打开这个功能是比较好的，但是备份太频繁，取决于实际的场景。 AOF最佳策略 “开”：建议打开，如果仅仅是作为一个普通缓存，对于数据要求不是很高，这次数据丢了，下次可以从数据库取(数据库压力不是很大)，这种情况就建议关闭，因为AOF还是有性能开销的。 “everysec” Redis4 Redis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。 RDB和AOF共存的情况下如何恢复数据： 优点： 混合持久化结合了RDB持久化 和 AOF 持久化的优点, 由于绝大部分都是RDB格式，加载速度快，同时结合AOF，增量的数据以AOF方式保存了，数据更少的丢失。 缺点： 兼容性差，一旦开启了混合持久化，在4.0之前版本都不识别该aof文件，同时由于前部分是RDB格式，阅读性较差 策略是： 6. 总结 http://www.ywnds.com/?p=4876]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入Nginx原理]]></title>
    <url>%2F2019%2F01%2F30%2Fmiscellany%2F13%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Nginx%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Nginx是一个高性能的HTTP和反向代理服务器，及电子邮件（IMAP/POP3）代理服务器，同时也是一个非常高效的反向代理、负载平衡中间件。是非常常用的web server.我们需要理解它的原理，才能达到游刃有余的程度。 本篇文章需要对Nginx有基本的使用以及对IO复用模型有一定的了解。文章比较长。 1.正向代理和反向代理 正向代理的工作原理就像一个跳板，比如：我访问不了google.com，但是我能访问一个代理服务器A，A能访问google.com，于是我先连上代理服务器A，告诉他我需要google.com的内容，A就去取回来，然后返回给我。从网站的角度，只在代理服务器来取内容的时候有一次记录，有时候并不知道是用户的请求，也隐藏了用户的资料，这取决于代理告不告诉网站。 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 简单来说： 正向代理是不知道客户端是谁，代理是一个跳板，所有客户端通过这个跳板来访问到对应的内容。 反向代理是不知道服务端是谁，用户的请求被转发到内部的某台服务器去处理。 2.基本的工作流程 用户通过域名发出访问Web服务器的请求，该域名被DNS服务器解析为反向代理服务器的IP地址； 反向代理服务器接受用户的请求； 反向代理服务器在本地缓存中查找请求的内容，找到后直接把内容发送给用户； 如果本地缓存里没有用户所请求的信息内容，反向代理服务器会代替用户向源服务器请求同样的信息内容，并把信息内容发给用户，如果信息内容是缓存的还会把它保存到缓存中。 3.优点 保护了真实的web服务器，保证了web服务器的资源安全 节约了有限的IP地址资源 减少WEB服务器压力，提高响应速度(缓存功能) 请求的统一控制，包括设置权限、过滤规则等 实现负载均衡 区分动态和静态可缓存内容 … 4.使用场景 Nginx作为Http代理、反向代理 Nginx作为负载均衡器 Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 Nginx作为Web缓存 5.Nginx的Master-Worker模式 启动Nginx后，其实就是在80端口启动了Socket服务进行监听，如图所示，Nginx涉及Master进程和Worker进程。 6.Master进程的作用是？ 读取并验证配置文件nginx.conf；管理worker进程； 接收来自外界的信号 向各worker进程发送信号 监控worker进程的运行状态，当worker进程退出后(异常情况下)，会自动重新启动新的worker进程 7.Worker进程的作用是？ 每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 思考：Nginx如何做到热部署？ 所谓热部署，就是配置文件nginx.conf修改后，不需要stop Nginx，不需要中断请求，就能让配置文件生效！（nginx -s reload 重新加载/nginx -t检查配置/nginx -s stop） 通过上文我们已经知道worker进程负责处理具体的请求，那么如果想达到热部署的效果，可以想象： 方案一： 修改配置文件nginx.conf后，主进程master负责推送给woker进程更新配置信息，woker进程收到信息后，更新进程内部的线程信息。（有点volatile的味道） 方案二： 修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx采用的就是方案二来达到热部署的！ 思考：Nginx如何做到高并发下的高效处理？ 上文已经提及Nginx的worker进程个数与CPU绑定、worker进程内部包含一个线程高效回环处理请求，这的确有助于效率，但这是不够的。 作为专业的程序员，我们可以开一下脑洞：BIO/NIO/AIO、异步/同步、阻塞/非阻塞… 要同时处理那么多的请求，要知道，有的请求需要发生IO，可能需要很长时间，如果等着它，就会拖慢worker的处理速度。 Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。 思考：Nginx挂了怎么办？ Nginx既然作为入口网关，很重要，如果出现单点问题，显然是不可接受的。 答案是：Keepalived+Nginx实现高可用。 Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用。（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合） Keepalived+Nginx实现高可用的思路： 第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP） 第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,从而实现Nginx故障切换） 6.nginx.conf 第一：location可以进行正则匹配，应该注意正则的几种形式以及优先级。（这里不展开） 第二：Nginx能够提高速度的其中一个特性就是：动静分离，就是把静态资源放到Nginx上，由Nginx管理，动态请求转发给后端。 第三：我们可以在Nginx下把静态资源、日志文件归属到不同域名下（也即是目录），这样方便管理维护。 第四：Nginx可以进行IP访问控制，有些电商平台，就可以在Nginx这一层，做一下处理，内置一个黑名单模块，那么就不必等请求通过Nginx达到后端在进行拦截，而是直接在Nginx这一层就处理掉。 除了可以映射静态资源，上面已经说了，可以作为一个代理服务器来使用。 所谓反向代理，很简单，其实就是在location这一段配置中的root替换成proxy_pass即可。root说明是静态资源，可以由Nginx进行返回；而proxy_pass说明是动态请求，需要进行转发，比如代理到Tomcat上。 反向代理，上面已经说了，过程是透明的，比如说request -&gt; Nginx -&gt; Tomcat，那么对于Tomcat而言，请求的IP地址就是Nginx的地址，而非真实的request地址，这一点需要注意。不过好在Nginx不仅仅可以反向代理请求，还可以由用户自定义设置HTTP HEADER。 负载均衡【upstream】 上面的反向代理中，我们通过proxy_pass来指定Tomcat的地址，很显然我们只能指定一台Tomcat地址，那么我们如果想指定多台来达到负载均衡呢？ 第一，通过upstream来定义一组Tomcat，并指定负载策略（IPHASH、加权论调、最少连接），健康检查策略（Nginx可以监控这一组Tomcat的状态）等。 第二，将proxy_pass替换成upstream指定的值即可。 负载均衡可能带来的问题？ 负载均衡所带来的明显的问题是，一个请求，可以到A server，也可以到B server，这完全不受我们的控制，当然这也不是什么问题，只是我们得注意的是：用户状态的保存问题，如Session会话信息，不能在保存到服务器上。 7.惊群现象 定义：惊群效应就是当一个fd的事件被触发时，所有等待这个fd的线程或进程都被唤醒。 Nginx的IO通常使用epoll，epoll函数使用了I/O复用模型。与I/O阻塞模型比较，I/O复用模型的优势在于可以同时等待多个（而不只是一个）套接字描述符就绪。Nginx的epoll工作流程如下： master进程先建好需要listen的socket后，然后再fork出多个woker进程，这样每个work进程都可以去accept这个socket 当一个client连接到来时，所有accept的work进程都会受到通知，但只有一个进程可以accept成功，其它的则会accept失败，Nginx提供了一把共享锁accept_mutex来保证同一时刻只有一个work进程在accept连接，从而解决惊群问题 当一个worker进程accept这个连接后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完成的请求就结束了 8.Nginx架构及工作流程 Nginx真正处理请求业务的是Worker之下的线程。worker进程中有一个ngx_worker_process_cycle()函数，执行无限循环，不断处理收到的来自客户端的请求，并进行处理，直到整个Nginx服务被停止。 当一个 worker 进程在 accept() 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，一个完整的请求。一个请求，完全由 worker 进程来处理，而且只能在一个 worker 进程中处理。 这样做带来的好处： 节省锁带来的开销。每个 worker 进程都是独立的进程，不共享资源，不需要加锁。同时在编程以及问题查上时，也会方便很多。 独立进程，减少风险。采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快重新启动新的 worker 进程。当然，worker 进程的也能发生意外退出。 9.nginx为什么高性能 因为nginx是多进程单线程的代表，多进程模型每个进程/线程只能处理一路IO，那么 Nginx是如何处理多路IO呢？ 如果不使用 IO 多路复用，那么在一个进程中，同时只能处理一个请求，比如执行 accept()，如果没有连接过来，那么程序会阻塞在这里，直到有一个连接过来，才能继续向下执行。 而多路复用，允许我们只在事件发生时才将控制返回给程序，而其他时候内核都挂起进程，随时待命。 核心：Nginx采用的 IO多路复用模型epoll epoll通过在Linux内核中申请一个简易的文件系统（文件系统一般用什么数据结构实现？B+树），其工作流程分为三部分： 调用 int epoll_create(int size)建立一个epoll对象，内核会创建一个eventpoll结构体，用于存放通过epoll_ctl()向epoll对象中添加进来的事件，这些事件都会挂载在红黑树中。 调用 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event) 在 epoll 对象中为 fd 注册事件，所有添加到epoll中的事件都会与设备驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个sockfd的回调方法，将sockfd添加到eventpoll 中的双链表 调用 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) 来等待事件的发生，timeout 为 -1 时，该调用会阻塞直到有事件发生 这样，注册好事件之后，只要有 fd 上事件发生，epoll_wait() 就能检测到并返回给用户，用户就能”非阻塞“地进行 I/O 了。 epoll() 中内核则维护一个链表，epoll_wait 直接检查链表是不是空就知道是否有文件描述符准备好了。（epoll 与 select 相比最大的优点是不会随着 sockfd 数目增长而降低效率，使用 select() 时，内核采用轮训的方法来查看是否有fd 准备好，其中的保存 sockfd 的是类似数组的数据结构 fd_set，key 为 fd，value 为 0 或者 1。） 能达到这种效果，是因为在内核实现中 epoll 是根据每个 sockfd 上面的与设备驱动程序建立起来的回调函数实现的。那么，某个 sockfd 上的事件发生时，与它对应的回调函数就会被调用，来把这个 sockfd 加入链表，其他处于“空闲的”状态的则不会。在这点上，epoll 实现了一个”伪”AIO。但是如果绝大部分的 I/O 都是“活跃的”，每个 socket 使用率很高的话，epoll效率不一定比 select 高（可能是要维护队列复杂）。 可以看出，因为一个进程里只有一个线程，所以一个进程同时只能做一件事，但是可以通过不断地切换来“同时”处理多个请求。 例子：Nginx 会注册一个事件：“如果来自一个新客户端的连接请求到来了，再通知我”，此后只有连接请求到来，服务器才会执行 accept() 来接收请求。又比如向上游服务器（比如 PHP-FPM）转发请求，并等待请求返回时，这个处理的 worker 不会在这阻塞，它会在发送完请求后，注册一个事件：“如果缓冲区接收到数据了，告诉我一声，我再将它读进来”，于是进程就空闲下来等待事件发生。 这样，基于 多进程+epoll， Nginx 便能实现高并发。 10.几种负载均衡的算法介绍 轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 weight 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 ip_hash 每个请求按访问ip的hash结果分配，这样每个访客固定访问同一个后端服务器，可以解决session的问题。但是不能解决宕机问题。 前三种是nginx自带的，直接在配置文件中配置即可使用。 fair（第三方） 按后端服务器的相应时间来分配请求，相应时间短的优先分配。 url_hash（第三方） 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 11.基于不同层次的负载均衡 七层就是基于URL等应用层信息的负载均衡； 同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。 换句话说: 二层负载均衡会通过一个虚拟MAC地址接受请求，然后再分配到真是的MAC地址； 三层负载均衡会通过一个虚拟IP地址接收请求，然后再分配到真实的IP地址； 四层通过虚拟的URL或主机名接收请求，然后再分配到真是的服务器。 所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。 七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。 负载均衡器通常称为四层交换机或七层交换机。四层交换机主要分析IP层及TCP/UDP层，实现四层流量负载均衡。七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息。 负载均衡设备也常被称为&quot;四到七层交换机&quot;，那么四层和七层两者到底区别在哪里？ 第一，技术原理上的区别。 所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 第二，应用场景的需求。 七层应用负载的好处，是使得整个网络更&quot;智能化&quot;。例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。 另外一个常常被提到功能就是安全性。 12.总结 理解正向代理和反向代理的概念 nginx的优点和使用场景 master和work两种进程的作用 如何热部署 Nginx单点故障的预防 映射静态文件、反向代理跳转到后端服务器处理的写法 惊群现象 Nginx 采用的是多进程（单线程） &amp; 多路IO复用模型(底层依靠epoll实现) 几种负载均衡的算法 四层的负载均衡和七层的负载均衡]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis为什么快]]></title>
    <url>%2F2019%2F01%2F30%2Fredis%2FRedis%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第四篇文章，本文主要攻克面试题-Redis为什么这么快。这就涉及Redis的线程模型啦。 完全基于内存 Redis是纯内存数据库，相对于读写磁盘，读写内存的速度就不是几倍几十倍了，一般，hash查找可以达到每秒百万次的数量级。 多路复用IO “多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗）。 Redis为什么是单线程的？ 因为CPU不是Redis的瓶颈。Redis的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 为什么 Redis 中要使用 I/O 多路复用这种技术呢？ 首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的。 假设你是一个老师，让30个学生解答一道题目，然后检查学生做的是否正确，你有下面几个选择： 第一种选择：按顺序逐个检查，先检查A，然后是B，之后是C、D。。。这中间如果有一个学生卡主，全班都会被耽误。这种模式就好比，你用循环挨个处理socket，根本不具有并发能力。 第二种选择：你创建30个分身，每个分身检查一个学生的答案是否正确。 这种类似于为每一个用户创建一个进程或者线程处理连接。 第三种选择，你站在讲台上等，谁解答完谁举手。这时C、D举手，表示他们解答问题完毕，你下去依次检查C、D的答案，然后继续回到讲台上等。此时E、A又举手，然后去处理E和A。。。 第三种就是IO复用模型，Linux下的select、poll和epoll就是干这个的。将用户socket对应的fd注册进epoll，然后epoll帮你监听哪些socket上有消息到达，这样就避免了大量的无用操作。此时的socket应该采用非阻塞模式。这样，整个过程只在调用select、poll、epoll这些调用的时候才会阻塞，收发客户消息是不会阻塞的，整个进程或者线程就被充分利用起来，这就是事件驱动，所谓的reactor模式。 所以，I/O多路复用的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。 这里还涉及一个名词：fd文件描述符。 Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行I/O操作的系统调用都会通过文件描述符。 redis的线程模型？ Redis 服务采用 Reactor 的方式来实现文件事件处理器。 文件事件处理器使用 I/O 多路复用模块同时监听多个 FD，当 accept、read、write 和 close 文件事件产生时，文件事件处理器就会回调 FD 绑定的事件处理器。 虽然整个文件事件处理器是在单线程上运行的，但是通过 I/O 多路复用模块的引入，实现了同时对多个 FD 读写的监控，提高了网络通信模型的性能，同时也可以保证整个 Redis 服务实现的简单。 上面简单理解就是：多个网络连接并发读写redis的时候，先将对应的fd注册到epoll上，I/O多路复用模块会监听这些网络请求的情况，一旦有一个网络连接产生了accept、read、write 和 close 文件事件，I/O多路复用模块就会向文件事件分派器传送那些产生了事件的网络连接。 当然了，上面的文件事件可能会并发产生，这时的策略是，将所有产生事件的套接字（对应上面的网络连接）都入队到一个队列里面， 然后通过这个队列， 以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字： 当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字。再看看下图，与上图使一样的： 文件事件分派器接收 I/O 多路复用程序传来的套接字， 并根据套接字产生的事件的类型， 调用相应的事件处理器。 服务器会为执行不同任务的套接字关联不同的事件处理器， 这些处理器是一个个函数， 它们定义了某个事件发生时， 服务器应该执行的动作。 整个模块使 Redis 能以单进程运行的同时服务成千上万个文件描述符，避免了由于多进程应用的引入导致代码实现复杂度的提升，减少了出错的可能性，单线程还减少线程切换和调度，实现更加简单 最后总结一下，为什么redis比较快大概思路通俗的说就是：Redis是纯内存数据库，读取快，瓶颈在于IO上，如果使用阻塞式IO，因为是单线程的缘故，就会停止等待。所以采用IO多路复用监听文件描述符的状态，将对redis的开关读写换成事件，加入队列进行相应的事件处理，吞吐量比较大。 IO复用模型的选择 因为 Redis 需要在多个平台上运行，同时为了最大化执行的效率与性能，所以会根据编译平台的不同选择不同的 I/O 多路复用函数作为子模块，提供给上层统一的接口； 因为 select 函数是作为 POSIX 标准中的系统调用，在不同版本的操作系统上都会实现，所以将其作为保底方案： Redis 会优先选择时间复杂度为 O(1) 的 I/O 多路复用函数作为底层实现，包括 Solaries 10 中的 evport、Linux 中的 epoll 和 macOS/FreeBSD 中的 kqueue，上述的这些函数都使用了内核内部的结构，并且能够服务几十万的文件描述符。 但是如果当前编译环境没有上述函数，就会选择 select 作为备选方案，由于其在使用时会扫描全部监听的描述符，所以其时间复杂度较差 O(n)，并且只能同时服务 1024 个文件描述符，所以一般并不会以 select 作为第一方案使用。 reids在linux下的安装 Redis对于Linux是官方支持的，安装起来也非常地简单，直接编译源码然后进行安装即可。 这里以centos为例，大概说一下步骤： 下载redis编译工具:yum install gcc和yum install g++ 解压redis.tar.gz文件，进去之后进行编译:make 然后安装：make install PREFIX=/usr/local/redis 安装成功之后进入/usr/local/redis/bin下启动redis ./redis-server]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis其他的功能介绍]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2FRedis%E5%85%B6%E4%BB%96%E7%9A%84%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第三篇文章，本文主要介绍redis一些其他的功能。遇到某些场景的时候可以想到redis是不是可以实现。 一、慢查询日志 1.1 什么是慢查询日志 慢查询日志帮助开发和运维人员定位系统存在的慢操作。慢查询日志就是系统在命令执行前后计算每条命令的执行时间，当超过预设阀值，就将这条命令的相关信息（慢查询ID，发生时间戳，耗时，命令的详细信息）记录下来。 1.2 redis一条命令简单的生命周期 慢查询只会出现在【3.执行命令】这个阶段，即慢查询只记录命令执行时间，并不包括命令排队时间和网络传输时间。 1.3 慢查询配置参数 慢查询的预设阀值 slowlog-log-slower-than slowlog-log-slower-than参数就是预设阀值，单位是微秒,默认值是10000，如果一条命令的执行时间超过10000微妙(10毫秒)，那么它将被记录在慢查询日志中。 如果slowlog-log-slower-than的值是0，则会记录所有命令。 如果slowlog-log-slower-than的值小于0，则任何命令都不会记录日志。 redis的操作一般是微妙级，slowlog-log-slower-than不要设置太大，一般设置为1毫秒。支持动态设置。 慢查询日志的长度slowlog-max-len slowlog-max-len只是说明了慢查询日志最多存储多少条。 Redis使用一个列表来存储慢查询日志，showlog-max-len就是列表的最大长度。 当慢查询日志已经到达列表的最大长度时，又有慢查询日志要进入列表，则最早插入列表的日志将会被移出列表，新日志被插入列表的末尾。 默认是128，但是slowlog-max-len不要设置太小，可以设置为1000以上. 慢查询日志是一个先进先出队列，慢查询较多的情况下，可能会丢失部分慢查询命令，可以定期执行slow get命令将慢查询日志持久化到其他存储中。然后制作可视化界面查询。 二、pipeline 2.1 为什么会出现Pipeline 用普通的get和set，如果同时需要执行大量的命令，那就是等待上一条命令应答后再执行，这中间不仅仅多了RTT（Round Time Trip），而且还频繁的调用系统IO，发送网络请求。 对于多条命令不是有mget和mset吗？确实对于一批的get和set可以用mget和mset，但是它的问题在于如果我们需要同时传输get和hget呢？此时pipeline(流水线)就出现了。 所以流水线解决的问题是N条命令网络通信的减少。 为什么说网络耗费时间大呢？这里给出一个极端的例子。 pipeline与原生M操作的对比。 原生M操作是一个原子操作。 pipeline非原子命令。 当某个命令的执行需要依赖前一个命令的返回结果时，无法使用pipeline。 12mset a “a1” b “b” c “c1” mget a b c mget和mset命令也是为了减少网络连接和传输时间所设置的，其本质和pipeline的应用区别不大，但是在特定场景下只能用pipeline实现，例如： 12get aset b ‘1’ pipeline适合执行这种连续，且无相关性的命令。 2.2 一个demo 搭建一个quickstart的maven工程。过程略。 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 直接再单元测试中进行测试：普通的直接hset 10000条数据： 1234567891011@Testpublic void test1()&#123; Jedis jedis = new Jedis("127.0.0.1",6379); long before = System.currentTimeMillis(); for(int i=0;i&lt;10000;i++)&#123; jedis.hset("hashkey"+i,"filed"+i,"value"+i); &#125; long after = System.currentTimeMillis(); System.out.println("一共耗时: "+(after-before)+"ms");&#125; 运行结果： 一共耗时: 1526ms 但是用pipeline后： 123456789101112131415@Testpublic void test2()&#123; Jedis jedis = new Jedis("127.0.0.1",6379); long before = System.currentTimeMillis(); //分为10次批量发送 for(int i=0;i&lt;10;i++)&#123; Pipeline pipeline = jedis.pipelined(); for(int j=1000*i;j&lt;(i+1)*1000;j++)&#123; pipeline.hset("hashkey:"+j,"field:"+j,"value:"+j); &#125; pipeline.syncAndReturnAll(); &#125; long after = System.currentTimeMillis(); System.out.println("使用pipeline一共耗时: "+(after-before)+"ms");&#125; 运行结果：使用pipeline一共耗时: 139ms 可以预见，对于更多的传输次数，pipeline的优势将越来越明显。但是pipeline每次只能作用在一个redis节点上。 三、发布订阅 3.1 角色 发布者----频道----订阅者 3.2 模型 注意，新订阅的，是不能收到之前的消息的。 订阅者1：subscribe mytopic 订阅者2：subscribe mytopic 订阅者3：subscribe mytopic 发布者：publish mytopic “hello” 缺点是不能保证消息可达，所以还是用专业的消息队列传达比较保障。 与发布订阅模型很类似的是消息队列模型。 只有一个是可以收到消息的。 四、bitMap 4.1 位图是什么 就是通过一个bit位来表示某个元素对应的值或者状态,其中的key就是对应元素本身。我们知道8个bit可以组成一个Byte，所以bitmap本身会极大的节省储存空间。 Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32长度的位数组。 比如我们执行 set hello big 那么这个big其实是这个形态： 执行getbit hello 0 得到0； 执行getbit hello 1 得到1 setbit hello 7 1，那么再get hello 将得到cig 4.2 位图有什么用呢？ 位图除了getbit和setbit之外，还有bitcount key [start end]，就是获取执行范围内的1的个数。 bitop作用是做多个Bitmap的and,or,not,xor操作。 以一个场景为例：日活跃用户 每次用户登录时会执行一次redis.setbit(daily_active_users, user_id, 1) 因为日活跃用户每天都变化，所以需要每天创建一个新的bitmap。我们简单地把日期（年月日）添加到key后面，以后就可以根据年月日这个key找到某天活跃用户。实现了这个功能。 第二个场景：用户签到情况 将那天所代表的网站的上线日作为offset参数， 比如,如果今天是网站上线的第100天,而用户$uid=10001在今天阅览过网站, 那么执行命令SETBIT peter 100 1. 如果明天$uid=10001也继续阅览网站,那么执行命令SETBIT peter 101 1 ,以此类推. 仔细想想，用位图，一天签到一次只要占一个bit，8天才占一个字节。那么一年这个用户签到占的数据是365/8=45.625个字节.如果不用位图实现，保存一条记录将远远大于一个比特吧，那么当用户量很大的时候，差距将会特别大。 五、hyperLogLog 基于HyperLogLog算法：极小空间完成独立数量统计。本质还是字符串。 pfadd key element [element...]:向hyperloglog添加元素 pfcount key [key...]:计算hyperloglog的独立总数 pfmerge destkey sourcekey [sourcekey...]:合并多个hyperloglog api例子 为什么要用hyperLogLog呢 我们上面例子可以看到，他的功能类似于去重，统计出所有不一样元素的个数。 他的优点是：占用内存极小。 缺点也有： 他可能会出错，错误率为0.81%，看你是否能够容忍错误了 不能拿到单条数据 六、geo 存储经纬度、计算两地距离、范围计算等。 提到LBS(Location Based Service)，基于位置的服务。我立即想起Mongodb的GEO实现地理坐标查询等功能，具体介绍为地理位置附近查询的GEOHASH解决方案。 mongodb最大的特点是灵活，因为其数据是以json的格式存储，所以字段随时可以增加或减少；Redis的特点是快，适合单一的，简单的，大量数据的存储；HBase我没有做深入研究，它的特点是大，适合做离线缓存。在处理社交这种关系复杂的数据存储时，依然还是需要用mysql这种关系型数据库，nosql并不能完全替代。 七、总结 首先是慢查询日志，可以定时地持久化，并且用一个可视化页面进行监测。 pipeline解决的是对没有相互依赖的操作的批量执行，减少网络传输和IO时间。但是呢，需要注意一般只能往一个节点放数据，面对集群的时候，就需要采取一些策略了。mset、mget，目前只支持具有相同slot值的key执行批量操作。后文再讲。 可以实现发布订阅模型以及消息队列，但是消息是无状态的，不能保证消息一定送达，所以需要用专业的MQ来实现。 位图，可以实现极小的空间完成对大量用户信息的统计。 地理坐标服务]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构和操作]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2FRedis%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第二篇文章，本文主要介绍redis如何启动，以及基本的键命令和五种基本数据类型的操作。部分图片可能看不清楚，可以拖到新窗口打开。 一、启动方式 我的环境是windows，那么直接进入redis的解压目录中，分别执行redis-server.exe和redis-cli.exe两个可执行的程序。也可以通过cmd启动： 不要直接用crtl+C关闭server，在linux下，直接停掉server的话，会导致数据的丢失。正确的做法是在客户端执行 redis-cli.exe shutdown 还可以指定端口启动：./redis-server.exe --port 6380 那么对应客户端连接也要指定相应 的端口才能连接。关闭服务端也要指定相应的端口才行： -h指定远程redis的ip 通过配置文件启动,可以在下面这个文件中指定端口号： 结合配置文件启动: 还可以设置密码： 那么客户端连接就必须要密码验证了： 二、命令 1、基础命令 info:查看系统信息 select (0-15)，redis一共有16个工作区间，一般默认从0开始，到15. flushdb：清空当前选择的空间 flushall：清空所有 dbsize：当前空间里面key-value键值对的数目 save：人工实现redis的持久化 quit：退出 2、键命令 del key成功返回1，失败返回0. exits key ttl和expire type key 查看key的类型 randomkey: rename oldkey newkey 如果是重命名为已经存在的key呢？ renamenx: 三、redis数据结构 1、String字符串 setex&amp;psetex getrange&amp;getset mset&amp;mget&amp;strlen setnx&amp;msetnx 数值操作 2、hash 3、list 4、set 5、sorted set]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初步认识Redis]]></title>
    <url>%2F2019%2F01%2F29%2Fredis%2F%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86Redis%2F</url>
    <content type="text"><![CDATA[本文为redis学习笔记的第一篇文章，主要从感性层面来认识一下这个开发中的宠儿，无论是什么应用，只要有点用户量的，不上redis是不可能的。作为当今最优秀的缓存中间件，没有理由不去深入了解它！ 一、redis是什么 redis很快，官方宣称QPS(每秒查询率)达10万。 Redis是一个开源的使用ANSI C语言编写、支持网络、单进程单线程、可基于内存亦可持久化、一个高性能的key-value数据库。 简而言之，就是一个缓存数据库，基于内存，也可以持久化，速度贼快，几乎所有互联网公司都在使用。 有的初学者可能看到数据库这个字眼，就把他归类于mysql之类，其实不是，mysql是一种关系型数据库，是存在磁盘中的。核心的数据是一定要落地到mysql之类的数据库中的。redis其实使用最多的功能是缓存，既然能存东西，那么必然也有数据库的功能，但是有可能会造成数据的缺失。所以，数据一定是要落入数据库才保险，redis可以作为缓存，缓存热点数据或者只读数据，提高性能并保护数据库。 二、为什么要用redis 好了，我们已经知道它是一个高性能的缓存中间件。那么必然一大功能是作为缓存使用。那为什么要用缓存呢？直接从数据库查不就行了码？ 在实际的业务场景中，用户量一上来，数据库是吃不消的。数据库是性能的一大瓶颈，如果不采取措施，用户的操作将卡在数据库处理这一块，最终可能导致不可用。 那么，此时，加入缓存，比如商城首页有很多很多内容，这些内容不可能经常变化，至少也要两三天吧？所以，可以将这些数据放到redis中，用户进商城之后，数据直接从redis中获取即可。速度极快，提高了用户的体验。 既然是缓存，那么必定会存在数据不一致的情况，所以缓存最适合于读多写少的情况，当然啦，要修改缓存肯定是可以的，但是要注意热点key的问题，比如微博最火的一片新闻，此时有几百万人再看，你却要修改一下，肯定是要注意点什么东西才行的，后续的文章会讲到如何处理热点key修改的问题。 三、Redis与其他key-value存储有什么不同 这里先简单说说，后面会有文章详细比价一下。 多样的数据结构和原子性操作 Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。redis中的单个命令都是原子性的，什么是原子性，就是该命令不可分割。 运行于内存+持久化于磁盘 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。另一个优点是， 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。 同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 注：我们要知道，对于磁盘的顺序访问速度是远大于随机访问的。这种在硬盘上顺序追加效率很高。 四、redis特点 速度快： 基于内存,这是快的最主要原因。 持久化： 可以同步或异步保存到磁盘中 多种数据结构： 除了五种基本数据类型，还支持位图、HyperLogLog，GEO等 支持多种编程语言客户端： java，python，ruby，Lua… 功能丰富： 可以实现发布-订阅，支持事务、Lua脚本 简单： 不依赖与外部库、单线程模型 主从复制： 主服务器同步数据到从服务器，是高可用的基础 高可用、分布式： 高可用：redis-Sentinel(v2.8版本)；分布式：redis-cluster(v3.0版本) 五、redis典型应用场景 缓存系统：这个就不多说了，redis作为高速缓存是其主要存在价值。 计数器：因为是原子操作incr+单线程，作为计数器永远不会出错 消息队列系统：数据结构list可以实现这种生产者-消费者模式的消息队列。 排行榜：有序集合sorted set就可以实现 社交网络：redis与社交网络就是一家，非常方便用set就能实现诸如共同好友这些功能。 六、redis优势 缓存管理：可以在必要时将无效的旧数据从内存中删除，为新数据腾出新的空间 提供更大的灵活性：redis支持多种类型，并且采用key-value 的形式存储，key和value的大小限制都是512Mb,与编码无关，所以数据安全。但是memcached限制key最大为250字节，value为1MB，况且只支持String类型。 redis提供主从复制：实现高可用的cache系统，支持集群中多个服务器之间的数据同步。 数据持久化：redis可以通过两种方式将数据进行持久化，一定程度上规避缓存中的数据不稳定的问题，也可以在重启服务器时最快的恢复缓存中所需的数据，提高了效率的同时减轻了主数据库系统的开销。 与传统的Memcached相比，优势还是很大的，两者的具体对比我会在后续的文章中详细说明。这里注意存在即合理，Memcached也有不可替代的适用场景： 存储一些粒度比较小的静态数据，比如一些html片段，Memcached便是我们更好的选择。相对于redis而言，Memcached的元数据metadata更小些，所以相对来讲对于数据存储管理的性能更高，额外开销更小。 Memcached的特点：Memcached唯一支持的数据类型是String,所以更适合存储只读数据，因为字符串并不会因为额外的处理造成额外的开销。毕竟Memcached每次更新一个对象时，都需要重复执行下面的操作：获取整个字符串-&gt;反序列化为对象-&gt;修改其中的值-&gt;再次序列化该对象-&gt;在缓存中将整个字符串替换为新字符串。这样一来，更新存储数据就会有更高的消耗，可能就不是我们的最佳选择了。 七、总结 只要记住redis三个关键字：快、持久化、高可用和分布式]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地理位置附近查询的GEOHASH解决方案]]></title>
    <url>%2F2019%2F01%2F29%2Fmiscellany%2F12%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%E9%99%84%E8%BF%91%E6%9F%A5%E8%AF%A2%E7%9A%84GEOHASH%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[地理位置附近查询的GEOHASH解决方案 1.需求场景 现今互联网确实从方方面面影响我们的生活。现在我们可以足不出户就能买到我们心仪的衣服，找到附近的美食。当我们点开一个外卖的app就能看到自己附近的餐厅，那我们有没有想过这是怎么实现的呢？ 2.尝试解决 首先我们能想到的就是把所有餐厅的经纬度存下来 然后当用户选择附近餐厅时 我们先获取用户的经纬度，然后到数据库中查出所有的经纬度，依次计算它们和用户间的距离。 最后根据用户输入的距离范围过滤出合适的餐厅，并根据距离做一个升序排列。 这样貌似能查出附近的餐厅，但是餐厅的数量这么多，直接全查出来内存也要爆掉，即使分批处理计算量也十分大。这样用户等待的时间就会特别长。那有什么办法能减少我们的计算量呢？ 其实很简单，我们应该只计算用户关心的那一片数据，而不是计算所有的。例如用户在北京，那完全没必要计算海南，黑龙江，新疆，浙江等其它地区的数据。如果我们能快速定位到北京甚至某个区，那么我们的计算量将大大减少。我们发现这其实就是索引的功能，但是MySQL对这种二维的地理位置的索引支持并不友好（mongodb有直接的地理位置索引），它对一维的像字符串这样的支持很好。那如果我们的数据在MySQL中，有没有什么方法能将我们的二维坐标转换为一种可比较的字符串呢？这就是我们今天要介绍的geohash算法。 3.基本思想 geohash简单来说就是将一个地理坐标转换为一个可比较的字符串的算法。不过生成的字符串表示的是一个矩形的范围，并不是一个点。 比如西二旗地铁附近这一片矩形区域就可以用wx4eyu82这个字符串表示，并且越靠前的编码表示额范围越大，比如中国绝大部分地区可以用w这个字母表示的矩形区域内。像wx4eyu82表示的区域一定在wx4e表示的区域范围内。利用这些特性我们就可以实现附近餐厅的功能了，比如我们希望查看西二旗地铁附近的餐厅就可以这样查询：select * from table where geohash like 'wx4eyu82%'; 这样就可以利用索引，快速查询出相关餐厅的信息了。并且我们还可以用wx4eyu82为key，餐厅信息为value做缓存。 通过上面的介绍我们知道了GeoHash就是一种将经纬度转换成字符串的方法，并且使得在大部分情况下，字符串前缀匹配越多的距离越近. 4.GeoHash算法的步骤 首先我们将经度和纬度都单独转换为一个二进制编码 得到经度和纬度的二进制编码后，我们按照奇数位放纬度，偶数为放经度的规则（我们这里奇数偶数下标是从0开始）将它们合成一个二进制编码 最后我们需要将这个二进制编码转换为base32编码 举例 地球纬度区间是[-90,90]， 北海公园的纬度是39.928167，可以通过下面算法对纬度39.928167进行逼近编码: 区间[-90,90]进行二分为[-90,0),[0,90]，称为左右区间，可以确定39.928167属于右区间[0,90]，给标记为1； 接着将区间[0,90]进行二分为 [0,45),[45,90]，可以确定39.928167属于左区间 [0,45)，给标记为0； 递归上述过程39.928167总是属于某个区间[a,b]。随着每次迭代区间[a,b]总在缩小，并越来越逼近39.928167； 如果给定的纬度x（39.928167）属于左区间，则记录0，如果属于右区间则记录1，这样随着算法的进行会产生一个序列1011100，序列的长度跟给定的区间划分次数有关。 通过上述计算，纬度产生的编码为10111 00011，经度产生的编码为11010 01011。偶数位放经度，奇数位放纬度，把2串编码组合生成新串：11100 11101 00100 01111。 最后使用用0-9、b-z（去掉a, i, l, o）这32个字母进行base32编码，首先将11100 11101 00100 01111转成十进制，对应着28、29、4、15，十进制对应的编码就是wx4g。 5.缺陷-geohash的边界问题 比如红色的点是我们的位置，绿色的两个点分别是附近的两个餐馆，但是在查询的时候会发现距离较远餐馆的GeoHash编码与我们一样（因为在同一个GeoHash区域块上），而较近餐馆的GeoHash编码与我们不一致。 目前比较通行的做法就是我们不仅获取当前我们所在的矩形区域，还获取周围8个矩形块中的点。那么怎样定位周围8个点呢？关键就是需要获取周围8个点的经纬度，那我们已经知道自己的经纬度，只需要用自己的经纬度减去最小划分单位的经纬度就行。因为我们知道经纬度的范围,又知道需要划分的次数，所以很容易就能计算出最小划分单位的经纬度。 6.几种实现geohash方案的对比 6.1支持二维索引的存储数据库：mongodb mongoDB支持二维空间索引,使用空间索引,mongoDB支持一种特殊查询,如某地图网站上可以查找离你最近的咖啡厅,银行等信息。这个使用mongoDB的空间索引结合特殊的查询方法很容易实现。 API直接支持，很方便 支持按照距离排序，并支持分页。支持多条件筛选。 可满足实时性需求。 资源占用大，数据量达到百万级请流量在10w左右查询速度明显下降。 6.2升级Mysql至5.7，支持Geohash MySQL 5.7.5 增加了对GeoHash的支持，提供了一系列geohash的函数，但是其实Mysql并没有提供类似mogodb类型near这样的函数，仅仅提供了一些经纬度转hash、hash取经纬度的一些函数。 优点:函数直接调用，生成目标hash、根据hash获取经纬度。 缺点：不支持范围查询函数，需要自行处理周边8点的问题，需要补充geo的算法 6.3Redis Commands: Geography Edition GEO 特性在 Redis 3.2 版发布， 这个功能可以将用户给定的地理位置信息储存起来， 并对这些信息进行操作，GEO通过如下命令来完成GEO需求. 命令 描述 geoadd 添加一个或多个经纬度地理位置 georadius 获取指定范围内的对象，也可以增加参数withdistance直接算出距离，也可以增加参数descending/ascending 进行距离排序 georadiusbymember 通过指定的对象，获取其周边对象 geoencode 转换为geohash，52-bit，同时返回该区域最小角的geohash,最大角的geohash，及中心点 geodecode 同上逆操作 优点:效率高，API丰富 缺点：3.2版本是否稳定？ 面试的时候，问到geohash算法以及技术选型大概也能说一说了… 本文章借鉴很多优秀文章，七拼八凑而出。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[补码的前世今生]]></title>
    <url>%2F2019%2F01%2F29%2Fjava-basic%2F%E8%A1%A5%E7%A0%81%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[计算机如何来保存负数呢？其实只要达到这样的目的：正数负数都有一个唯一标识即可，但是，正如人类用+1和-1来表示可以提高效率一样，也得有一个比较适当的适合我们的计算机识别的一个方式。下面来详细讲解。 问题的由来 下面为了表示方便，先假设存储一个整型数字用4个bit。 举例来说，+2在计算机中表示为二进制的0010，那么-2怎么表示呢？ 很容易想到，可以将一个二进制位（bit）专门规定为符号位，它等于0时就表示正数，等于1时就表示负数。比如，在8位机中，规定每个字节的最高位为符号位。那么，+2就是0010，而-8则是1010。 更多的例子如下： 这就是直接用原码的方式来存储，虽然说这种方式理论上是可行的，毕竟每个数我都唯一标识了。 但是这种方式存在问题，我们希望+1和-1相加为0，但是通过这个方式算出来的是：0001+1001=1010 (-2)。也就是说，按照正常一步头的方式得不到我们想要的结果。 为了解决了“正负相加等于0”的问题，人们发明了反码。 反码 “反码”表示方式是用来处理负数的，符号位置不变，其余位置相反。 此时，我们再来算一下+1和-1相加，变成了0001+1110=1111，刚好反码表示方式中，1111象征-0。 此时，好像是解决了这个问题，但是我们发现，0这个时候有了两种表达：0000和1111。 即在用反码表示的情况下，0竟然可以用两个值来表示，这显然不好吧。毕竟+0和-0就是同一个玩意啊。 这个时候补码闪亮登场。 补码 很简单，在刚才反码的基础上加1。 此时，我们这里假定整形只有4位。那么-0表示为1111+1=10000，显然溢出了，就需要丢弃最高位，变成0000. 此时，神奇地发现，达到了统一，+0和-0都是用0000来表示了。 此时，也满足正负数相加为0的条件。比如+2为0010，-2为1110.此时两者相加为：0010+1110=(0)0000，丢掉最高位就是0000 那么对于普通情况，比如7+(-4)呢?即0111+1100=0011，就是3。OK，大功告成。 补码怎么求 上面已经说的很详细啦，比如-4，就是在4(0100)的基础上取反(1011)再加一(1100). 上面也解释了为什么要用补码。即保证了对称的正负数相加为0并且0只有一种表示方式。 还有一个重要的点就是，我们注意到，7-4其实我们都是转换成7+(-4)，也就是说，在计算机中，减法都是用加法的逻辑实现的。 即：一套加法的电路实现加减法。此外，乘法和除法其实都是加法这套电路实现的。 补码的本质 这里假设存储一个整型用8个bit。 要将正数转成对应的负数，其实只要用0减去这个数就可以了。比如，-8其实就是0-8。 则8的二进制是00001000，-8就可以用下面的式子求出： 123 ００００００００－００００１０００－－－－－－－－－ 因为00000000（被减数）小于0000100（减数），所以不够减。请回忆一下小学算术，如果被减数的某一位小于减数，我们怎么办？很简单，问上一位借1就可以了。 所以，0000000也问上一位借了1，也就是说，被减数其实是100000000，算式也就改写成： 1234１００００００００－００００１０００－－－－－－－－－ １１１１１０００ 进一步观察，可以发现100000000 = 11111111 + 1，所以上面的式子可以拆成两个： 1234567 １１１１１１１１－００００１０００－－－－－－－－－ １１１１０１１１＋０００００００１－－－－－－－－－ １１１１１０００ 通过这一步，我们就从数学上知道了为什么补码是取反加一了。 你看，求任何一个负数，都是0-正数，那么就用借位的思想来，则变成100000000。 100000000则可以分解为11111111+00000001。 此时求负数的过程就就变成11111111-X+1 而先用11111111来减这个正数，这个结果就是对正数取反。 此时再加上另外一个1. 这与我们求补码的过程是一样的，这也解释了为什么要这样求补码。 证明(可不看) 将上面的特例抽象一下，用统一表达式来证明一下。 我们要证明的是，X-Y或X+(-Y)可以用X加上Y的补码完成。 Y的补码等于(11111111-Y)+1。所以，X加上Y补码，就等于： 1X + (11111111-Y) + 1 我们假定这个算式的结果等于Z，即 1Z = X + (11111111-Y) + 1 接下来，分成两种情况讨论。 第一种情况，如果X小于Y，那么Z是一个负数。 由Y的补码等于(11111111-Y)+1，标记为F=(11111111-Y)+1,那么如何根据F逆向求Y呢？ 1Y=1111111-(F-1) OK,因为此时Z是一个负数，那么Z进行补码的逆运算就可以求出它的绝对值，即正数。再加一个符号，两者相等。 1Z = -[11111111-(Z-1)] = -[11111111-(X + (11111111-Y) + 1-1)] = X - Y 第二种情况，如果X大于Y 这意味着Z肯定大于11111111，但是我们规定了这是8位机，最高的第9位是溢出位，必须被舍去，这相当于减去100000000。所以， 1Z = Z - 100000000 = X + (11111111-Y) + 1 - 100000000 = X - Y 这就证明了，在正常的加法规则下，可以利用2的补码得到正数与负数相加的正确结果。换言之，计算机只要部署加法电路和补码电路，就可以完成所有整数的加法。 本文整理自： 关于2的补码 知乎第一条评论]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot使用logback实现日志按天滚动]]></title>
    <url>%2F2019%2F01%2F28%2Fmiscellany%2F11SpringBoot%E4%BD%BF%E7%94%A8logback%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E6%8C%89%E5%A4%A9%E6%BB%9A%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[日志是任何一个系统都必备的东西，日志的重要程度丝毫不亚于代码。而springboot中经常使用的是logback，那么今天我们就来学习一下在springboot下如何配置logback日志。理解了这里的配置，对于任何的日志都是一样的。 需求 日志按天滚动分割 info和error日志输出到不同文件 为什么使用Logback Logback是Log4j的升级版，作者为同一个人，作者不想再去改Log4j，所以写了Logback 使用日志框架的最佳实践是选择一款日志门面+一款日志实现，这里选择Slf4j+Logback,Slf4j作者也是Logback的作者 SpringBoot从1.4版本开始，内置的日志框架就是Logback Logback在SpringBoot中配置方式一 可以直接在applicatin.properties或者application.yml中配置 以在application.yml中配置为例 123456logging: pattern: console: "%d - %msg%n" file: /var/log/tomcat/sell.log level: com.imooc.LoggerTest: debug 可以发现，这种配置方式简单，但能实现的功能也很局限，只能 定制输出格式 输出文件的路径 指定某个包下的日志级别 如果需要完成我们的需求，这就得用第二种配置了 Logback在SpringBoot中配置方式二 在resource目录下新建logback-spring.xml, 内容如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;configuration&gt; &lt;!--打印到控制台的格式--&gt; &lt;appender name="consoleLog" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt; &lt;pattern&gt; %d - %msg%n &lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!--除了error级别的日志文件保存格式以及滚动策略--&gt; &lt;appender name="fileInfoLog" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--过滤器，将error级别过滤掉--&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;DENY&lt;/onMatch&gt; &lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt; %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;!--滚动策略--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--路径--&gt; &lt;fileNamePattern&gt;/var/log/tomcat/sell/info.%d.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;!--error级别日志文件保存格式以及滚动策略--&gt; &lt;appender name="fileErrorLog" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--只让error级别的日志进来--&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt; %msg%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;!--滚动策略--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--路径--&gt; &lt;fileNamePattern&gt;/var/log/tomcat/sell/error.%d.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;/appender&gt; &lt;root level="info"&gt; &lt;appender-ref ref="consoleLog" /&gt; &lt;appender-ref ref="fileInfoLog" /&gt; &lt;appender-ref ref="fileErrorLog" /&gt; &lt;/root&gt;&lt;/configuration&gt; 每一个appender你可以理解为一个日志处理策略。 第一个appender的name=&quot;consoleLog&quot;, 名字是自己随意取的，取这个名字，表示这个策略用于控制台的日志。 我们重点看第二个和第三个appender,因为要把info和error日志输入到不同文件，所以我们分别建了两个appender。 rollingPolicy是滚动策略，这里我们设置按时间滚动 filter是日志的过滤方式，我们在fileInfoLog里做了如下过滤 123&lt;level&gt;ERROR&lt;/level&gt;&lt;onMatch&gt;DENY&lt;/onMatch&gt;&lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; 上述代码翻译之后：拦截ERROR级别的日志。如果匹配到了，则禁用处理。如果不匹配，则接受，开始处理日志。 那有的同学要问了，不能这样写吗 1&lt;level&gt;INFO&lt;/level&gt; 这样不是只拦截INFO日志了吗？ 不对！ 这就得说一下日志级别了 DEBUG -&gt;INFO -&gt; WARN -&gt;ERROR 如果你设置的日志级别是INFO，那么是会拦截ERROR日志的哦。也就是说，如果直接写info，那么大于等于info级别的日志都会写进去，违背了我们的需求。 整理自： http://www.imooc.com/article/19005]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql面试高频理论知识]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2Fmysql%E9%9D%A2%E8%AF%95%E9%AB%98%E9%A2%91%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[整理一些面试题，简单看看。 目录 数据库三范式 事务 mysql数据库默认最大连接数 分页 触发器 存储过程 用jdbc怎么调用存储过程？ 对jdbc的理解 写一个简单的jdbc的程序。写一个访问oracle数据的jdbc程序 JDBC中的PreparedStatement相比Statement的好处 数据库连接池作用 选择合适的存储引擎 数据库优化-索引 数据库优化-分表 数据库优化-读写分离 数据库优化-缓存 数据库优化-sql语句优化的技巧 jdbc批量插入几百万数据怎么实现 聚簇索引和非聚簇索引 sql注入问题 mysql悲观锁和乐观锁 1. 数据库三范式 1.1 范式是什么 范式就是规范，要满足第二范式必须先满足第一范式，要满足第三范式，必须要先满足第二范式。 1NF(第一范式)：列数据不可分割，即一列不能有多个值 2NF(第二范式)：主键(每一行都有唯一标识) 3NF(第三范式)：外键(表中不包含已在其他表中包含的非主关键信息) 1.2 反三范式 反三范式：有时为了效率，可以设置重复或者推导出的字段，例如：订单总价格订单项的单价，这个订单总价虽然可以由订单项计算出来，但是当订单数目庞大时，效率比较低，所以订单的总价这个字段是必要的。 2. 事务 2.1 含义 事务时并发控制的单位，是用户定义的一个操作序列，要么都做，要么都不做，是不可分割的工作单位。 2.2 事务的四个特征(ACID特性) 原子性：表示事务内操作不可分割 一致性：要么成功，要么失败，若后面失败，前面则回滚 隔离性：一个事务开始了，不被其他事务干扰 持久性：事务开始了，就不能突然终止 3. mysql数据库默认最大连接数 3.1 为什么需要最大连接数 特定服务器上的数据库只能支持一定数目同时连接，这时需要我们设置最大连接数（最多同时服务多少连接）。在数据库安装时会有一个默认的最大连接数。 my.ini中max_connections=100 4. 分页 4.1 为什么需要分页？ 在很多数据时，不可能完全显示数据。进行分段显示. 4.2 mysql如何分页 12String sql = "select * from students order by id limit " + pageSize*(pageNumber-1) + "," + pageSize; 4.3 oracle分页 是使用了三层嵌套查询。 1234String sql = &quot;select * from &quot; + (select *,rownum rid from (select * from students order by postime desc) where rid&lt;=&quot; + pagesize*pagenumber + &quot;) as t&quot; + &quot;where t&gt;&quot; + pageSize*(pageNumber-1); 5. 触发器 略。 6. 存储过程 6.1 数据库存储过程具有如下优点： 1、存储过程只在创建时进行编译，以后每次执行存储过程都不需再重新编译，而一般 SQL 语句每执行一次就编译一次，因此使用存储过程可以大大提高数据库执行速度。 2、通常，复杂的业务逻辑需要多条 SQL 语句。这些语句要分别地从客户机发送到服务器，当客户机和服务器之间的操作很多时，将产生大量的网络传输。如果将这些操作放在一个存储过程中，那么客户机和服务器之间的网络传输就会大大减少，降低了网络负载。 3、存储过程创建一次便可以重复使用，从而可以减少数据库开发人员的工作量。 4、安全性高，存储过程可以屏蔽对底层数据库对象的直接访问，使用 EXECUTE 权限调用存储过程，无需拥有访问底层数据库对象的显式权限。 6.2 定义存储过程: 12345678create procedure insert_Student (_name varchar(50),_age int ,out _id int)begin insert into student value(null,_name,_age); select max(stuId) into _id from student;end;call insert_Student('wfz',23,@id);select @id; 7. 用jdbc怎么调用存储过程？ 贾琏欲执事 加载驱动 获取连接 设置参数 执行 释放连接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.sql.CallableStatement;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.Types;public class JdbcTest &#123; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stub Connection cn = null; CallableStatement cstmt = null; try &#123; //这里最好不要这么干，因为驱动名写死在程序中了 Class.forName("com.mysql.jdbc.Driver"); //实际项目中，这里应用DataSource数据，如果用框架， //这个数据源不需要我们编码创建，我们只需Datasource ds = context.lookup() //cn = ds.getConnection(); cn = DriverManager.getConnection("jdbc:mysql:///test","root","root"); cstmt = cn.prepareCall("&#123;call insert_Student(?,?,?)&#125;"); cstmt.registerOutParameter(3,Types.INTEGER); cstmt.setString(1, "wangwu"); cstmt.setInt(2, 25); cstmt.execute(); //get第几个，不同的数据库不一样，建议不写 System.out.println(cstmt.getString(3)); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; /*try&#123;cstmt.close();&#125;catch(Exception e)&#123;&#125; try&#123;cn.close();&#125;catch(Exception e)&#123;&#125;*/ try &#123; if(cstmt != null) cstmt.close(); if(cn != null) cn.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 8. 对jdbc的理解 Java database connection java数据库连接.数据库管理系统(mysql oracle等)是很多，每个数据库管理系统支持的命令是不一样的。 Java只定义接口，让数据库厂商自己实现接口，对于我们者而言。只需要导入对应厂商开发的实现即可。然后以接口方式进行调用.(mysql + mysql驱动（实现）+jdbc) 9. 写一个简单的jdbc的程序。写一个访问oracle数据的jdbc程序 贾琏欲执事 加载驱动(com.mysql.jdbc.Driver,oracle.jdbc.driver.OracleDriver) 取连接(DriverManager.getConnection(url,usernam,passord)) 设置参数 Statement PreparedStatement cstmt.setXXX(index, value); 执行 executeQuery executeUpdate 释放连接(是否连接要从小到大，必须放到finnaly) 10. JDBC中的PreparedStatement相比Statement的好处 大多数我们都使用PreparedStatement代替Statement 1：PreparedStatement是预编译的，比Statement速度快 2：代码的可读性和可维护性 虽然用PreparedStatement来代替Statement会使代码多出几行,但这样的代码无论从可读性还是可维护性上来说.都比直接用Statement的代码高很多档次： 123456789stmt.executeUpdate("insert into tb_name (col1,col2,col2,col4) values('"+var1+"','"+var2+"',"+var3+",'"+var4+"')"); perstmt = con.prepareStatement("insert into tb_name (col1,col2,col2,col4) values (?,?,?,?)");perstmt.setString(1,var1);perstmt.setString(2,var2);perstmt.setString(3,var3);perstmt.setString(4,var4);perstmt.executeUpdate(); 3：安全性 PreparedStatement可以防止SQL注入攻击，而Statement却不能。 比如说： String sql = “select * from tb_name where name= '”+varname+&quot;’ and passwd=’&quot;+varpasswd+&quot;’&quot;; 如果我们把[' or '1' = '1]作为varpasswd传入进来.用户名随意,看看会成为什么? select * from tb_name = ‘随意’ and passwd = ‘’ or ‘1’ = ‘1’; 因为'1'='1'肯定成立，所以可以任何通过验证。 更有甚者：把[';drop table tb_name;]作为varpasswd传入进来,则： select * from tb_name = ‘随意’ and passwd = ‘’;drop table tb_name; 有些数据库是不会让你成功的，但也有很多数据库就可以使这些语句得到执行。 而如果你使用预编译语句你传入的任何内容就不会和原来的语句发生任何匹配的关系，只要全使用预编译语句你就用不着对传入的数据做任何过虑。而如果使用普通的statement,有可能要对drop等做费尽心机的判断和过虑。 11. 数据库连接池作用 1、限定数据库的个数，不会导致由于数据库连接过多导致系统运行缓慢或崩溃 2、数据库连接不需要每次都去创建或销毁，节约了资源 3、数据库连接不需要每次都去创建，响应时间更快。 12. 选择合适的存储引擎 在开发中，我们经常使用的存储引擎 myisam / innodb/ memory MyISAM存储引擎 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. 比如 bbs 中的 发帖表，回复表. INNODB存储引擎: 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表. Memory 存储 我们数据变化频繁，不需要入库，同时又频繁的查询和修改，我们考虑使用memory, 速度极快. MyISAM 和 INNODB的区别(主要) 事务安全 myisam不支持事务而innodb支持 查询和添加速度 myisam不用支持事务就不用考虑同步锁，查找和添加和添加的速度快 支持全文索引 myisam支持innodb不支持 锁机制 myisam支持表锁而innodb支持行锁(事务) 外键 MyISAM 不支持外键， INNODB支持外键. (通常不设置外键，通常是在程序中保证数据的一致) 下面是数据库的优化手段，但是只是表面，需要以后再好好探究 在项目自验项目转测试之前，在启动mysql数据库时开启慢查询，并且把执行慢的语句写到日志中，在运行一定时间后。通过查看日志找到慢查询语句。 1234567891011121314151617181920212223242526272829show variables like '%slow%'; #查看MySQL慢查询是否开启set global slow_query_log=ON; #开启MySQL慢查询功能show variables like "long_query_time"; #查看MySQL慢查询时间设置，默认10秒set global long_query_time=5; #修改为记录5秒内的查询select sleep(6); #测试MySQL慢查询show variables like "%slow%"; #查看MySQL慢查询日志路径show global status like '%slow%'; #查看MySQL慢查询状态或者vi /etc/my.cnf #编辑，在[mysqld]段添加以下代码slow-query-log = on #开启MySQL慢查询功能slow_query_log_file = /var/run/mysqld/mysqld-slow.log #设置MySQL慢查询日志路径long_query_time = 5 #修改为记录5秒内的查询，默认不设置此参数为记录10秒内的查询log-queries-not-using-indexes = on #记录未使用索引的查询:wq! #保存退出service mysqld restart #重启MySQL服务 13. 数据库优化-索引 13.1 索引的概念 索引（Index）是帮助DBMS高效获取数据的数据结构。 13.2 索引有哪些 分类：普通索引/唯一索引/主键索引/全文索引 普通索引:允许重复的值出现 唯一索引:除了不能有重复的记录外，其它和普通索引一样(用户名、用户身份证、email,tel) 主键索引：是随着设定主键而创建的，也就是把某个列设为主键的时候，数据库就会給改列创建索引。这就是主键索引.唯一且没有null值 全文索引:用来对表中的文本域(char，varchar，text)进行索引， 全文索引针对MyIsam explain select * from articles where match(title,body) against(‘database’);【会使用全文索引】 13.3 使用索引的注意事项 索引弊端 占用磁盘空间。 对dml(插入、修改、删除)操作有影响，变慢。 使用场景： 肯定在where条件经常使用,如果不做查询就没有意义 该字段的内容不是唯一的几个值(sex) 字段内容不是频繁变化. 注意事项 对于创建的多列索引（复合索引），不是使用的第一部分就不会使用索引。 123alter table dept add index my_ind (dname,loc); // dname 左边的列,loc就是右边的列explain select * from dept where dname='aaa'\G 会使用到索引explain select * from dept where loc='aaa'\G 就不会使用到索引 对于使用like的查询，查询如果是%aaa不会使用到索引而aaa%会使用到索引。 12explain select * from dept where dname like '%aaa'\G不能使用索引explain select * from dept where dname like 'aaa%'\G使用索引. 所以在like查询时，‘关键字’的最前面不能使用% 或者 _这样的字符，如果一定要前面有变化的值，则考虑使用 全文索引-&gt;sphinx. 索引列排序 MySQL查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来。否则不使用索引。 123expain select * from dept where dname=’111’;expain select * from dept where dname=111;（数值自动转字符串）expain select * from dept where dname=qqq;报错 也就是，如果列是字符串类型，无论是不是字符串数字就一定要用 ‘’ 把它包括起来. 如果mysql估计使用全表扫描要比使用索引快，则不使用索引。 表里面只有一条记录 索引不会包含有NULL值的列 只要列中包含有NULL值都将不会被包含在MySQL索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 使用短索引 对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 不要在列上进行运算，不使用NOT IN和&lt;&gt;操作，不支持正则表达式。 14. 数据库优化-分表 分表分为水平(按行)分表和垂直(按列)分表 水平分表情形： 根据经验，Mysql表数据一般达到百万级别，查询效率会很低，容易造成表锁，甚至堆积很多连接，直接挂掉；水平分表能够很大程度较少这些压力。 垂直分表情形： 如果一张表中某个字段值非常多(长文本、二进制等)，而且只有在很少的情况下会查询。这时候就可以把字段多个单独放到一个表，通过外键关联起来。考试详情，一般我们只关注分数，不关注详情。 水平分表策略： 1.按时间分表 这种分表方式有一定的局限性，当数据有较强的实效性，如微博发送记录、微信消息记录等，这种数据很少有用户会查询几个月前的数据，如需要就可以按月分表。 2.按区间范围分表 一般在有严格的自增id需求上，如按照user_id水平分表： 123table_1 user_id从1~100w table_2 user_id从101~200w table_3 user_id从201~300w 3.hash分表 通过一个原始目标的ID或者名称通过一定的hash算法计算出数据存储表的表名，然后访问相应的表。 15. 数据库优化-读写分离 一台数据库支持的最大并发连接数是有限的，如果用户并发访问太多。一台服务器满足不要要求是就可以集群处理。Mysql的集群处理技术最常用的就是读写分离。 主从同步 数据库最终会把数据持久化到磁盘，如果集群必须确保每个数据库服务器的数据是一直的。能改变数据库数据的操作都往主数据库去写，而其他的数据库从主数据库上同步数据。 读写分离 使用负载均衡来实现写的操作都往主数据去，而读的操作往从服务器去。 16. 数据库优化-缓存 什么是缓存 在持久层(dao)和数据库(db)之间添加一个缓存层，如果用户访问的数据已经缓存起来时，在用户访问时直接从缓存中获取，不用访问数据库。而缓存是在操作内存级，访问速度快。 作用 减少数据库服务器压力，减少访问时间。 Java中常用的缓存有 hibernate的二级缓存。该缓存不能完成分布式缓存。 可以使用redis(memcahe等)来作为中央缓存。对缓存的数据进行集中处理 17. 数据库优化-sql语句优化的技巧 DDL优化 通过禁用索引来提供导入数据性能，这个操作主要针对现有数据库的表追加数据 123456//去除键alter table test3 DISABLE keys;//批量插入数据insert into test3 ***//恢复键alter table test3 ENABLE keys; 关闭唯一校验 12set unique_checks=0 关闭set unique_checks=1 开启 修改事务提交方式(导入)（变多次提交为一次） 123set autocommit=0 关闭//批量插入set autocommit=1 开启 DML优化（变多次提交为一次） 12345insert into test values(1,2);insert into test values(1,3);insert into test values(1,4);//合并多条为一条insert into test values(1,2),(1,3),(1,4) DQL优化 Order by优化 多用索引排序 普通结果排序（非索引排序）Filesort group by优化 使用order by null,取消默认排序 等等等等… 18. jdbc批量插入几百万数据怎么实现 1、变多次提交为一次 2、使用批量操作 3、像这样的批量插入操作能不使用代码操作就不使用，可以使用存储过程来实现 mysql优化手段介绍到这里。 19. 聚簇索引和非聚簇索引 索引分为聚簇索引和非聚簇索引。 “聚簇索引” 以一本英文课本为例，要找第8课，直接翻书，若先翻到第5课，则往后翻，再翻到第10课，则又往前翻。这本书本身就是一个索引，即“聚簇索引”。 “非聚簇索引” 如果要找&quot;fire”这个单词，会翻到书后面的附录，这个附录是按字母排序的，找到F字母那一块，再找到&quot;fire”，对应的会是它在第几课。这个附录，为“非聚簇索引”。 由此可见，聚簇索引，索引的顺序就是数据存放的顺序，所以，很容易理解，一张数据表只能有一个聚簇索引。 聚簇索引要比非聚簇索引查询效率高很多，特别是范围查询的时候。所以，至于聚簇索引到底应该为主键，还是其他字段，这个可以再讨论。 1、MYSQL的索引 mysql中，不同的存储引擎对索引的实现方式不同，大致说下MyISAM和InnoDB两种存储引擎。 MyISAM存储引擎的索引实现 MyISAM的B+Tree的叶子节点上的data，并不是数据本身，而是数据存放的地址。主索引和辅助索引没啥区别，只是主索引中的key一定得是唯一的。这里的索引都是非聚簇索引。 MyISAM还采用压缩机制存储索引，比如，第一个索引为“her”，第二个索引为“here”，那么第二个索引会被存储为“3,e”，这样的缺点是同一个节点中的索引只能采用顺序查找。 InnoDB存储引擎的索引实现 InnoDB 的数据文件本身就是索引文件，B+Tree的叶子节点上的data就是数据本身，key为主键，这是聚簇索引。非聚簇索引，叶子节点上的data是主键 (所以聚簇索引的key，不能过长)。为什么存放的主键，而不是记录所在地址呢，理由相当简单，因为记录所在地址并不能保证一定不会变，但主键可以保证。 至于为什么主键通常建议使用自增id呢？ 2.聚簇索引 聚簇索引的数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。如果主键不是自增id，那么可以想 象，它会干些什么，不断地调整数据的物理地址、分页，当然也有其他一些措施来减少这些操作，但却无法彻底避免。但，如果是自增的，那就简单了，它只需要一 页一页地写，索引结构相对紧凑，磁盘碎片少，效率也高。 聚簇索引不但在检索上可以大大滴提高效率，在数据读取上也一样。比如：需要查询f~t的所有单词。 一个使用MyISAM的主索引，一个使用InnoDB的聚簇索引。两种索引的B+Tree检索时间一样，但读取时却有了差异。 因为MyISAM的主索引并非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。 不过，如果涉及到大数据量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的。 鉴于聚簇索引的范围查询效率，很多人认为使用主键作为聚簇索引太多浪费，毕竟几乎不会使用主键进行范围查询。但若再考虑到聚簇索引的存储，就不好定论了。 20. sql注入问题 20.1 什么是sql注入 sql注入大家都不陌生，是一种常见的攻击方式，攻击者在界面的表单信息或url上输入一些奇怪的sql片段，例如“or ‘1’=’1’”这样的语句，有可能入侵参数校验不足的应用程序。所以在我们的应用中需要做一些工作，来防备这样的攻击方式。在一些安全性很高的应用中，比如银行软件，经常使用将sql语句全部替换为存储过程这样的方式，来防止sql注入，这当然是一种很安全的方式，但我们平时开发中，可能不需要这种死板的方式。 20.2 PrepareStatement解决SQL注入的问题 在使用JDBC的过程中，可以使用PrepareStatement进行预处理，预处理的优势就是预防绝大多数的SQL注入；而且针对多次操作数据库的情况，可以极大的提高访问数据库的效率。 那为什么它这样处理就能预防SQL注入提高安全性呢？其实是因为SQL语句在程序运行前已经进行了预编译。在程序运行时第一次操作数据库之前，SQL语句已经被数据库分析，编译和优化，对应的执行计划也会缓存下来并允许数据库以参数化的形式进行查询。当运行时动态地把参数传给PreprareStatement时，即使参数里有敏感字符如 or ‘1=1’，数据库也会作为一个参数一个字段的属性值来处理而不会作为一个SQL指令。如此，就起到了SQL注入的作用了！ 20.3 MyBatis如何防止sql注入 mybatis框架作为一款半自动化的持久层框架，其sql语句都要我们自己来手动编写，这个时候当然需要防止sql注入。其实Mybatis的sql是一个具有“输入+输出”功能，类似于函数的结构，如下： 12345&lt;select id=“getBlogById“ resultType=“Blog“ parameterType=”int”&gt; select id,title,author,content from blog where id=#&#123;id&#125; &lt;/select&gt; 这里，parameterType标示了输入的参数类型，resultType标示了输出的参数类型。回应上文，如果我们想防止sql注入，理所当然地要在输入参数上下功夫。上面代码中“#{id}”即输入参数在sql中拼接的部分，传入参数后，打印出执行的sql语句，会看到sql是这样的： select id,title,author,content from blog where id = ? 不管输入什么参数，打印出的sql都是这样的。这是因为mybatis启用了预编译功能，在sql执行前，会先将上面的sql发送给数据库进行编译，执行时，直接使用编译好的sql，替换占位符“？”就可以了。因为sql注入只能对编译过程起作用，所以这样的方式就很好地避免了sql注入的问题。 mybatis是如何做到sql预编译的呢？其实在框架底层，是jdbc中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的sql语句。这种“准备好”的方式不仅能提高安全性，而且在多次执行一个sql时，能够提高效率，原因是sql已编译好，再次执行时无需再编译。 补充 12345&lt;select id=“orderBlog“ resultType=“Blog“ parameterType=”map”&gt; select id,title,author,content from blog order by $&#123;orderParam&#125; &lt;/select&gt; 仔细观察，内联参数的格式由“#{xxx}”变为了${xxx}。如果我们给参数“orderParam”赋值为”id”,将sql打印出来，是这样的： select id,title,author,content from blog order by id 显然，这样是无法阻止sql注入的。在mybatis中，”${xxx}”这样格式的参数会直接参与sql编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式，所以，这样的参数需要我们在代码中手工进行处理来防止注入。 21. mysql悲观锁和乐观锁 21.1 悲观锁 悲观锁（Pessimistic Lock），顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 Java synchronized 就属于悲观锁的一种实现，每次线程要修改数据时都先获得锁，保证同一时刻只有一个线程能操作数据，其他线程则会被block。 21.2 乐观锁 乐观锁（Optimistic Lock），顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。乐观锁适用于读多写少的应用场景，这样可以提高吞吐量。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 乐观锁一般来说有以下2种方式： 使用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 version 字段来实现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 使用时间戳（timestamp）。乐观锁定的第二种实现方式和第一种差不多，同样是在需要乐观锁控制的table中增加一个字段，名称无所谓，字段类型使用时间戳（timestamp）, 和上面的version类似，也是在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK，否则就是版本冲突。 Java JUC中的atomic包就是乐观锁的一种实现，AtomicInteger 通过CAS（Compare And Set）操作实现线程安全的自增。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂查询训练]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2F%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[对一些场景进行sql的一些训练。 1、查询语文成绩比数学成绩好的学生的学号 1234567891011121、拿到语文科目的所有分数id和分数：SELECT Sid,score from score WHERE Cid=12、拿到数学科目的所有分数id和分数：SELECT Sid,score FROM score WHERE Cid=23、找到同一个学生对应的语文分数比数学分数高的人：SELECT a.Sid from (SELECT Sid,score from score WHERE Cid=1) a, (SELECT Sid,score FROM score WHERE Cid=2) bWHERE a.score&gt;b.score and a.Sid=b.Sid 2、查询平均成绩大于60分的同学的学号和平均成绩 123456789SELECT sc.Sid, AVG(sc.score)FROM score scGROUP BY sc.SidHAVING AVG(sc.score) &gt; 60 3、查询所有同学的学号、姓名、选课数、总成绩 12345678910SELECT st.Sid AS '学号', st.Sname AS '学生姓名', COUNT(sc.Cid) AS '选课数', SUM(sc.score) AS '总分'FROM student stJOIN score sc ON st.Sid = sc.SidGROUP BY st.Sid 4、查询姓“王”的老师的个数 123456SELECT count(te.Tname)FROM teacher teWHERE te.Tname LIKE '王%' 5、查询没学过“王二”老师课的同学的学号、姓名 123456789101112131415161718192021#1、找出所有学生对应的学号和老师SELECT st.Sid,te.Tname from student stJOIN score sc ON st.Sid = sc.SidJOIN course co on sc.Cid = co.CidJOIN teacher te ON co.Tid = te.Tid#2、找到所有老师是王五的学生id和姓名SELECT st.Sid,st.Sname,te.Tname from student stJOIN score sc ON st.Sid = sc.SidJOIN course co on sc.Cid = co.CidJOIN teacher te ON co.Tid = te.TidWHERE te.Tname='王五'#3、最后将王五的全部剔除，就是没有学王五课的学生信息SELECT st.Sid,st.Sname from student st WHERE st.Sid NOT IN(SELECT st.Sid from student stJOIN score sc ON st.Sid = sc.SidJOIN course co on sc.Cid = co.CidJOIN teacher te ON co.Tid = te.TidWHERE te.Tname='王五') 6、查询学过“语文”并且也学过“数学”课程的同学的学号、姓名 1234567891011121314#1、找到学过"语文"的学生SELECT st.Sid from student st JOIN score sc ON st.Sid = sc.Sid AND sc.Cid = 1#2、在学过语文的学生中再找出学数学的学生SELECT st.Sid,st.Sname from student st JOIN score sc ON st.Sid = sc.Sid AND sc.Cid = 2AND st.Sid IN(SELECT st.Sid from student st JOIN score sc ON st.Sid = sc.Sid AND sc.Cid = 1) 7、查询课程编号“数学”的成绩比课程编号“语文”课程低的所有同学的学号、姓名 1234567891011121314151617#1、先找到课程为语文的所有人的分数SELECT * from score sc WHERE sc.Cid=1#2、先找到课程为数学的所有人的分数SELECT * from score sc WHERE sc.Cid=2#3、找到语文比数学成绩好的学生的成绩SELECT * FROM (SELECT * from score sc WHERE sc.Cid=1) sc1,(SELECT * from score sc WHERE sc.Cid=2) sc2WHERE sc1.score&gt;sc2.score AND sc1.Sid=sc2.Sid#4、将学生信息再拿出来SELECT st.Sid,st.Sname from student st WHERE st.Sid IN(SELECT sc1.Sid FROM (SELECT * from score sc WHERE sc.Cid=1) sc1,(SELECT * from score sc WHERE sc.Cid=2) sc2WHERE sc1.score&gt;sc2.score AND sc1.Sid=sc2.Sid) 8、查询所有课程成绩小于60分的同学的学号、姓名 123SELECT DISTINCT st.Sid,st.Sname from student stJOIN score sc on st.Sid = sc.SidWHERE sc.score&lt;60 9、查询没有学全所有课的同学的学号、姓名 12345678910#1、统计出所有学生学习的课程数SELECT COUNT(sc.Cid) from student stJOIN score sc on st.Sid=sc.SidGROUP BY sc.Sid#2、每个学生学习的课程数小于总课程数，说明没有学全SELECT st.Sid,st.Sname from student stJOIN score sc on st.Sid=sc.SidGROUP BY sc.SidHAVING COUNT(sc.Cid)&lt;(SELECT count(*) FROM course) 10、 查询至少有一门课与学号为“1”的同学所学相同的同学的学号和姓名 12345678#1、找出学号为1的学生所学的课程idSELECT sc.Cid from student st,score sc WHERE st.Sid=sc.Sid AND st.Sid=1#2、从查询结果来看是选择了语文（1）和数学（2）SELECT DISTINCT st.Sid,st.Sname from student st,score sc WHERE st.Sid=sc.SidAND sc.Cid IN( SELECT sc.Cid from student st,score sc WHERE st.Sid=sc.Sid AND st.Sid=1) 11、查询和“1”号的同学学习的课程完全相同的其他同学学号和姓名 12345678910111213141516171819202122232425#1、找到2号同学所有学的课程SELECT sc.Cid from student stJOIN score sc on st.Sid=sc.SidWHERE st.Sid=2#2、找到有跟他学的不一样的学生SELECT DISTINCT st.Sid,st.Sname from student st,score sc WHERE st.Sid=sc.SidAND sc.Cid not in ( SELECT sc.Cid from student st JOIN score sc on st.Sid=sc.Sid WHERE st.Sid=1)#3、再找出一样的学生，并且将他自己剔除SELECT DISTINCT st.Sid,st.Sname from student st,score sc WHERE st.Sid=sc.SidAND st.Sid not in( SELECT DISTINCT st.Sid from student st,score sc WHERE st.Sid=sc.Sid AND sc.Cid not in ( SELECT sc.Cid from student st JOIN score sc on st.Sid=sc.Sid WHERE st.Sid=1 ))AND st.Sid!=1 12、按平均成绩从高到低显示所有学生的三门的课程成绩,按如下形式显示： 学生ID,语文,数学,英语,有效课程数,有效平均分 123456789101112131415161718#1、根据学生id分组，按照学生的平均成绩排序SELECT *FROM student stJOIN score sc ON st.Sid=sc.SidGROUP BY st.SidORDER BY AVG(sc.score)#2、按照要求显示SELECT st.Sid AS '学生id',(SELECT score from score WHERE Cid=1 and st.Sid=Sid) AS '语文',(SELECT score from score WHERE Cid=2 and st.Sid=Sid) AS '数学',(SELECT score from score WHERE Cid=3 and st.Sid=Sid) AS '英语',COUNT(*) AS '有效课程数',AVG(sc.score) AS '平均分'FROM student stJOIN score sc ON st.Sid=sc.SidGROUP BY st.SidORDER BY AVG(sc.score) 13、查询各科成绩最高和最低的分：以如下形式显示：课程ID，最高分，最低分 12345SELECT sc.Cid,MAX(sc.score) AS '最高分',MIN(sc.score) AS '最低分' from student stJOIN score sc on st.Sid=sc.SidGROUP BY sc.Cid 14、检索“语文”课程分数小于60，按分数降序排列的同学学号 1234SELECT st.Sid,st.Sname from student stJOIN score sc ON st.Sid=sc.SidWHERE sc.score&lt;60 AND sc.Cid=1ORDER BY sc.score DESC 15、查询两门以上不及格课程的同学的学号及其平均成绩 12345678910111213141516#1、先查询出挂科超过两门的学生idSELECT sc.Sid from score sc WHERE sc.score&lt;60 GROUP BY sc.Sid HAVING count(*)&gt;2#2、再相应地查出学生的信息SELECT st.Sid,AVG(sc.score) from student stJOIN score sc on st.Sid=sc.SidWHERE st.Sid in( SELECT sc.Sid from score sc WHERE sc.score&lt;60 GROUP BY sc.Sid HAVING count(*)&gt;2)GROUP BY st.Sid 16、查询没学过“王五”老师讲授的任一门课程的学生姓名 123456789101112131415#1、查出报名王五老师课程的学生idSELECT st.Sid from student stJOIN score sc on st.Sid=sc.SidJOIN course co on sc.Cid=co.CidJOIN teacher te on co.Tid=te.TidWHERE te.Tname="王五"#3、找出没有上王五老师课的学生是哪些SELECT * from student st WHERE st.Sid not in(SELECT st.Sid from student stJOIN score sc on st.Sid=sc.SidJOIN course co on sc.Cid=co.CidJOIN teacher te on co.Tid=te.TidWHERE te.Tname="王五") 17、查询选修全部课程的学生 12345678910#1、查出每个学生选修的课程数SELECT st.Sid,st.Sname,count(sc.Cid) from student stJOIN score sc on st.Sid=sc.SidGROUP BY sc.Sid#2、再找出数量等于全部课程的学生SELECT st.Sid,st.Sname,count(sc.Cid) from student stJOIN score sc on st.Sid=sc.SidGROUP BY sc.SidHAVING count(sc.Cid)=(SELECT count(*) FROM course) 18、查询全部学生都选修的课程的课程号和课程名 12345#1、在分数表中找出每种课程的选修人数SELECT sc.Cid,count(sc.Cid) from score sc GROUP BY sc.Cid#2、人数等于7(8号学生无任何参与)说明所有人都选了SELECT * from course co where co.Cid=(SELECT sc.Cid from score sc GROUP BY sc.Cid HAVING count(sc.Cid) = 7) 19、检索至少选修两门课程的学生学号 12345678910#1、先选出每个学生选秀的课程SELECT count(sc.Cid) from student stJOIN score sc on sc.Sid=st.Sidgroup by sc.Sid#2、再选出选秀课程超过2门的学生SELECT st.Sid,st.Sname from student stJOIN score sc on sc.Sid=st.Sidgroup by sc.SidHAVING count(sc.Cid)&gt;=2 20、统计每门课程的学生选修人数 1234#1、统计出每门选秀的学生数SELECT count(sc.Sid) from student stJOIN score sc on sc.Sid=st.SidGROUP BY sc.Cid]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL必知必会知识点提炼]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2FSQL%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[这是对《mysql必知必会》的知识提炼。 一、基础 模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。 主键的值不允许修改，也不允许复用（不能使用已经删除的主键值赋给新数据行的主键）。 SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。 SQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。 SQL 支持以下三种注释： 12345# 注释SELECT *FROM mytable; -- 注释/* 注释1 注释2 */ 二、创建表 123456CREATE TABLE mytable ( id INT NOT NULL AUTO_INCREMENT, col1 INT NOT NULL DEFAULT 1, col2 VARCHAR(45) NULL, col3 DATE NULL, PRIMARY KEY (`id`)); 三、修改表 添加列 12ALTER TABLE mytableADD col CHAR(20); 删除列 12ALTER TABLE mytableDROP COLUMN col; 删除表 1DROP TABLE mytable; 四、插入 普通插入 12INSERT INTO mytable(col1, col2)VALUES(val1, val2); 插入检索出来的数据 123INSERT INTO mytable1(col1, col2)SELECT col1, col2FROM mytable2; 将一个表的内容插入到一个新表 12CREATE TABLE newtable ASSELECT * FROM mytable; 五、更新 123UPDATE mytableSET col = valWHERE id = 1; 六、删除 12DELETE FROM mytableWHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。 1TRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。 七、查询 DISTINCT 相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。 12SELECT DISTINCT col1, col2FROM mytable; LIMIT 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。 返回前 5 行： 123SELECT *FROM mytableLIMIT 5; 123SELECT *FROM mytableLIMIT 0, 5; 返回第 3 ~ 5 行： 123SELECT *FROM mytableLIMIT 2, 3; 八、排序 ASC ：升序（默认） DESC ：降序 可以按多个列进行排序，并且为每个列指定不同的排序方式： 123SELECT *FROM mytableORDER BY col1 DESC, col2 ASC; 九、过滤 不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。 123SELECT *FROM mytableWHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符 操作符 说明 = 等于 &lt; 小于 &gt; 大于 &lt;&gt; != 不等于 &lt;= !&gt; 小于等于 &gt;= !&lt; 大于等于 BETWEEN 在两个值之间 IS NULL 为 NULL 值 应该注意到，NULL 与 0、空字符串都不同。 AND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。 IN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。 NOT 操作符用于否定一个条件。 十、通配符 通配符也是用在过滤语句中，但它只能用于文本字段。 % 匹配 &gt;=0 个任意字符； \_ 匹配 ==1 个任意字符； [ ] 可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。 使用 Like 来进行通配符匹配。 123SELECT *FROM mytableWHERE col LIKE '[^AB]%'; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。 十一、计算字段 在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。 计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。 12SELECT col1 * col2 AS aliasFROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。 12SELECT CONCAT(TRIM(col1), '(', TRIM(col2), ')') AS concat_colFROM mytable; 十二、函数 汇总 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。 使用 DISTINCT 可以让汇总函数值汇总不同的值。 12SELECT AVG(DISTINCT col1) AS avg_colFROM mytable; 文本处理 函数 说明 LEFT() 左边的字符 RIGHT() 右边的字符 LOWER() 转换为小写字符 UPPER() 转换为大写字符 LTRIM() 去除左边的空格 RTRIM() 去除右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。 123SELECT *FROM mytableWHERE SOUNDEX(col1) = SOUNDEX('apple') 日期和时间处理 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 1mysql&gt; SELECT NOW(); 12018-4-14 20:25:11 数值处理 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 十三、分组 分组就是把具有相同的数据值的行放在同一组中。 可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。 指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。 123SELECT col, COUNT(*) AS numFROM mytableGROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。 1234SELECT col, COUNT(*) AS numFROM mytableGROUP BY colORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。 12345SELECT col, COUNT(*) AS numFROM mytableWHERE col &gt; 2GROUP BY colHAVING num &gt;= 2; 分组规定： GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。 十四、子查询 子查询中只能返回一个字段的数据。 可以将子查询的结果作为 WHRER 语句的过滤条件： 1234SELECT *FROM mytable1WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次： 123456SELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_numFROM CustomersORDER BY cust_name; 十五、连接 连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。 连接可以替换子查询，并且比子查询的效率一般会更快。 可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。 内连接 内连接又称等值连接，使用 INNER JOIN 关键字。 123SELECT A.value, B.valueFROM tablea AS A INNER JOIN tableb AS BON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。 123SELECT A.value, B.valueFROM tablea AS A, tableb AS BWHERE A.key = B.key; 在没有条件语句的情况下返回笛卡尔积。 自连接 自连接可以看成内连接的一种，只是连接的表是自身而已。 一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。 子查询版本 123456SELECT nameFROM employeeWHERE department = ( SELECT department FROM employee WHERE name = "Jim"); 自连接版本 1234SELECT e1.nameFROM employee AS e1 INNER JOIN employee AS e2ON e1.department = e2.department AND e2.name = "Jim"; 自然连接 自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。 内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。 12SELECT A.value, B.valueFROM tablea AS A NATURAL JOIN tableb AS B; 外连接 外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。 检索所有顾客的订单信息，包括还没有订单信息的顾客。 123SELECT Customers.cust_id, Orders.order_numFROM Customers LEFT OUTER JOIN OrdersON Customers.cust_id = Orders.cust_id; customers 表： cust_id cust_name 1 a 2 b 3 c orders 表： order_id cust_id 1 1 2 1 3 3 4 3 结果： cust_id cust_name order_id 1 a 1 1 a 2 3 c 3 3 c 4 2 b Null 十六、组合查询 使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。 每个查询必须包含相同的列、表达式和聚集函数。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 1234567SELECT colFROM mytableWHERE col = 1UNIONSELECT colFROM mytableWHERE col =2; 十七、视图 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。 对视图的操作和对普通表的操作一样。 视图具有如下好处： 简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 1234CREATE VIEW myview ASSELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_colFROM mytableWHERE col5 = val; 十八、存储过程 存储过程可以看成是对一系列 SQL 操作的批处理； 使用存储过程的好处： 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。 包含 in、out 和 inout 三种参数。 给变量赋值都需要用 select into 语句。 每次只能给一个变量赋值，不支持集合的操作。 123456789101112delimiter //create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end //delimiter ; 12call myprocedure(@ret);select @ret; 十九、游标 在存储过程中使用游标可以对一个结果集进行移动遍历。 游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。 使用游标的四个步骤： 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标； 1234567891011121314151617181920delimiter //create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; # 定义了一个 continue handler，当 sqlstate '02000' 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate '02000' set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器 触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。 触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。 INSERT 触发器包含一个名为 NEW 的虚拟表。 1234CREATE TRIGGER mytrigger AFTER INSERT ON mytableFOR EACH ROW SELECT NEW.col into @result;SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。 UPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改地，而 OLD 是只读的。 MySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。 二十一、事务处理 基本术语： 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。 MySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。 通过设置 autocommit 为 0 可以取消自动提交，直到 autocommit 被设置为 1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。 如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。 1234567START TRANSACTION// ...SAVEPOINT delete1// ...ROLLBACK TO delete1// ...COMMIT 二十二、字符集 基本术语： 字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。 除了给表指定字符集和校对外，也可以给列指定： 123CREATE TABLE mytable(col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci )DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对： 123SELECT *FROM mytableORDER BY col COLLATE latin1_general_ci; 二十三、权限管理 MySQL 的账户信息保存在 mysql 这个数据库中。 12USE mysql;SELECT user FROM user; 创建账户 1CREATE USER myuser IDENTIFIED BY 'mypassword'; 新创建的账户没有任何权限。 修改账户名 1RENAME myuser TO newuser; 删除账户 1DROP USER myuser; 查看权限 1SHOW GRANTS FOR myuser; 授予权限 1GRANT SELECT, INSERT ON mydatabase.* TO myuser; 账户用 username@host 的形式定义，username@% 使用的是默认主机名。 删除权限 1REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; GRANT 和 REVOKE 可在几个层次上控制访问权限： 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.\*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 更改密码 必须使用 Password() 函数 1SET PASSWROD FOR myuser = Password('new_password'); 转自： SQL]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务核心问题]]></title>
    <url>%2F2019%2F01%2F27%2Fmysql%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文从事务引出了Mysql InnoDB RR隔离级别下是如何防止幻读的。 1. 数据库事务的四大特性 最重要的是ACID特性。 原子性(Atomicity)：事务要么都成功要么都失败 一致性(Consistency)：关系型数据库有很多约束，事务前后都要满足这些约束(不仅仅是数据库物理约束，还包括内部逻辑上的一些假设) 隔离性(Isolation)：两个事务互相独立，不能互相干扰 持久性(Durability)：事务执行成功之后结果可以持久化，永久存储下来(redo日志) 对于一致性，可能解释比较抽象，他的实际含义是：数据库的数据应满足完整性约束。拿转账业务来说，假设用户A和用户B一共有2000块钱，那么他们之间无论如何转账，总共的钱应该都是2000. 2. 事务并发访问引起的问题 更新丢失-mysql所有事务隔离级别在数据库层面均可避免 取款事务 存款事务 开始事务 开始事务 查询余额为100元 无 无 查询余额为100元 无 存入20，余额变为120元 无 提交事务 取出10元，余额改为90元 无 回滚事务，余额恢复为100元 更新丢失 脏读问题-一个事务读到另一个事务未提交的数据 不可重复读-事务A多次读取数据，未提交数据，此时事务B提交新的数据，导致A多次读取数据期间数据不一致，不满足隔离性 幻读-事务A受到另一个事务插入新的一行或者删除一行的影响，导致幻觉 不可重复读的重点是修改: 同样的条件的select, 你读取过的数据, 再次读取出来发现值不一样了 幻读的重点在于新增或者删除: 同样的条件的select, 第1次和第2次读出来的记录数不一样 具体可以自己设置不同的隔离级别进行演示。 3. 事务的隔离级别 Read uncommitted：读到其他事务未commit的值 Read committed：解决了脏读问题，但是会读到其他事务commit的值，读两次可能会读到两个值，所以又叫不可重复读 Repeatable Read：解决了不可重复读问题，可重复读，别人commit对我没有影响，但是对于别的事务插入操作，可能会产生幻读 Serializable：串行化，当发生两个事务同时提交，结果只可能有一个，相当于串行执行后的某个结果 级别越来越高，安全性也越来越高，但是但是性能越来越低。说明一下，出现幻读只是针对这种Repeatable Read隔离级别，但是InnoDB已经不存在幻读问题了，如何解决的呢？主要是用next-key锁来解决，下文会讲到。 4. 当前读和快照读 4.1 当前读 读取的都是当前数据的最新版本，并且在读的时候对其加锁，不允许其他事务进行修改操作。 select ... lock in share mode（共享锁）以及 select ... for update、update、delete、insert（排他锁）这些操作都是当前读。 为什么将 插入/更新/删除 操作，都归为当前读？可以看看下面这个 更新 操作，在数据库中的执行流程： 从图中，可以看到，一个Update操作的具体流程。当Update SQL被发给MySQL后，MySQL Server会根据where条件，发出current read 读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回，并加锁 (current read)。 待MySQL Server收到这条加锁的记录之后，会再发起一个Update请求，更新这条记录。 一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此，Update操作内部，就包含了一个当前读。 4.2 快照读 不加锁的非阻塞读，简单的select（前提是事务级别不是serializable，因为在serializable级别下都是串行读，普通的select也会退化为当前读即select ... lock in share mode） 快照读的实现是基于多版本并发控制（MVCC）实现，旨在提高性能。有可能读到的不是数据的最新版本。（创建快照的时机决定了读到的数据的版本，如果事务A先快照读，事务B修改，那么事务A再快照读就还是更新前的版本，事务A的当前读会读到最新的数据；而当事务B先更新，事务A再快照读，就会读到数据最新版本了） 4.3 MVCC MVCC在MySQL的InnoDB中的实现 在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下： SELECT时，读取创建版本号&lt;=当前事务版本号，删除版本号为空或&gt;当前事务版本号。 INSERT时，保存当前事务版本号为行的创建版本号 DELETE时，保存当前事务版本号为行的删除版本号 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 通过MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。 说白了，就是乐观锁的一种实现。免去了加锁解锁的过程，对于读多写少的场景特别适用。 5. RC，RR级别下的InnoDB非阻塞读（快照读）如何实现 通过数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID这三个字段 DB_TRX_ID，最后一次修改本行事务的ID DB_ROLL_PTR，即回滚指针,与undo日志配合 DB_ROW_ID，随着新行插入而单调递增的行号（innoDB中如果既没有主键索引也没有唯一索引的时候，就会自动生成一个隐藏主键，就是这个玩意） 这三个字段结合undo日志，这个日志里面记录的都是老版本的数据，这样，快照读就可以读出适合的一个版本的数据出来。在数据库中，日志是非常重要的东西，可以说其重要性是大于数据本身的，因为数据丢失可以通过日志找回来，但是日志丢失了，那么以后数据库出现崩溃等就麻烦了。 6. 日志 数据库数据存放的文件称为data file；日志文件称为log file；数据库数据是有缓存的，如果没有缓存，每次都写或者读物理disk，那性能就太低下了。数据库数据的缓存称为data buffer，日志（redo）缓存称为log buffer；既然数据库数据有缓存，就很难保证缓存数据（脏数据）与磁盘数据的一致性。比如某次数据库操作： 1update driver_info set driver_status = 2 where driver_id = 10001; 更新driver_status字段的数据会存放在缓存中，等待存储引擎将driver_status刷新data_file，并返回给业务方更新成功。如果此时数据库宕机，缓存中的数据就丢失了，业务方却以为更新成功了，数据不一致，也没有持久化存储。 上面的问题就可以通过事务的ACID特性来保证。 12345BEGIN trans；update driver_info set driver_status = 2 where driver_id = 10001;COMMIT; 这样执行后，更新要么成功，要么失败。业务方的返回和数据库data file中的数据保持一致。要保证这样的特性这就不得不说存储引擎innodb的redo和undo日志。 6.1 undo是啥 undo日志用于存放数据修改被修改前的值，假设修改 tba 表中 id=2的行数据，把Name=‘B’ 修改为Name = ‘B2’ ，那么undo日志就会用来存放Name='B’的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 对数据的变更操作，主要来自 INSERT UPDATE DELETE，而UNDO LOG中分为两种类型，一种是 INSERT_UNDO（INSERT操作，事务提交后可以立即丢弃），记录插入的唯一键值；一种是 UPDATE_UNDO（包含UPDATE及DELETE操作），记录修改的唯一键值以及old column记录。 6.2 redo是啥 存储引擎也会为redo undo日志开辟内存缓存空间，log buffer。磁盘上的日志文件称为log file，是顺序追加的，性能非常高，注：磁盘的顺序写性能比内存的写性能差不了多少。 redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。例如某一事务的事务序号为T1，其对数据X进行修改，设X的原值是5，修改后的值为15，那么Undo日志为&lt;T1, X, 5&gt;，Redo日志为&lt;T1, X, 15&gt;。 梳理下事务执行的各个阶段： 写undo日志到log buffer； 执行事务，并写redo日志到log buffer； 如果innodb_flush_log_at_trx_commit=1，则将redo日志写到log file，并刷新落盘。 提交事务。 那redo日志是写进去了，但是数据呢？ 在数据库的世界里，数据从来都不重要，日志才是最重要的，有了日志就有了一切。 因为data buffer中的数据会在合适的时间 由存储引擎写入到data file，如果在写入之前，数据库宕机了，根据落盘的redo日志，完全可以将事务更改的数据恢复。好了，看出日志的重要性了吧。先持久化日志的策略叫做Write Ahead Log，即预写日志。 6.3 Undo + Redo事务的简化过程 假设有A、B两个数据，值分别为1,2，开始一个事务，事务的操作内容为：把1修改为3，2修改为4，那么实际的记录如下（简化）： 事务开始. 记录A=1到undo log buffer. 修改A=3. 记录A=3到redo log buffer. 记录B=2到undo log buffer. 修改B=4. 记录B=4到redo log buffer. 将redo log写入磁盘。 事务提交 我们可以看到，2，4，5，7，8都是新增操作，但是2，4，5，7都是缓冲到buffer区，只有8是磁盘IO操作。为了保证Redo Log有较好的IO性能，设计一般有以下特点： 尽量保持Redo Log存储在一段连续的空间上。因此在系统第一次启动时就会将日志文件的空间完全分配。 以顺序追加的方式记录Redo Log,通过顺序IO来改善性能。 批量写入日志。日志并不是直接写入文件，而是先写入redo log buffer.当需要将日志刷新到磁盘时 (如事务提交),将许多日志一起写入磁盘. 并发的事务共享Redo Log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起， 以减少日志占用的空间。例如,Redo Log中的记录内容可能是这样的： 记录1: &lt;trx1, insert …&gt; 记录2: &lt;trx2, update …&gt; 记录3: &lt;trx1, delete …&gt; 记录4: &lt;trx3, update …&gt; 记录5: &lt;trx2, insert …&gt; 因为上一条的原因,当一个事务将Redo Log写入磁盘时，也会将其他未提交的事务的日志写入磁盘 Redo Log上只进行顺序追加的操作，当一个事务需要回滚时，它的Redo Log记录也不会从Redo Log中删除掉。 6.4 回滚 前面说到未提交的事务和回滚了的事务也会记录Redo Log，因此在进行恢复时,这些事务要进行特殊的的处理。有2种不同的恢复策略： 进行恢复时，只重做已经提交了的事务。 进行恢复时，重做所有事务包括未提交的事务和回滚了的事务。然后通过Undo Log回滚那些未提交的事务。 MySQL数据库InnoDB存储引擎使用了第二个策略。 InnoDB可重复读隔离级别下如何避免幻读 表象原因:快照读（非阻塞读）–伪MVCC 内在原因：next-key锁（行锁+gap锁） 6.5 next-key锁 在 RR 级别下，如果查询条件能使用上唯一索引，或者是一个唯一的查询条件，那么仅加行锁，如果是一个范围查询，那么就会给这个范围加上 gap 锁或者 next-key锁 (行锁+gap锁)。 那么gap锁啥时候出现呢？ 使用主键索引或者唯一索引时： 如果where条件全部命中，则不会用Gap锁，只会加记录锁 如果where条件部分命中或者全不命中，则会加Gap锁 在走非唯一索引或者不走索引的当前读中，也会出现Gap锁。对于不走索引的情况，那么就会锁住整张表。 总结一下：只有对唯一索引+全部命中才不会加gap锁。 具体来个例子说明间隙锁如何工作。 7. 例子-走唯一索引 7.1 准备工作 有这样一个表test，其中name为主键，id为唯一键。 123456CREATE TABLE `test` ( `name` varchar(11) primary key, `id` int, unique KEY `id` (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into test(name,id) values ("f",1), ("h",2), ("b",3), ("a",5), ("c",6),("d",9); name id f 1 h 2 b 3 a 5 c 6 d 9 首先验证一下使用主键索引或者唯一索引时会怎么样。 7.2 第一种情况：唯一索引+命中所有数据 session1执行 1delete from test where id = 3; session2执行： 1insert into test(name,id) values("swg",4); 此时由于id是唯一索引，并且是命中的，所以只是对这一行加排他锁，而没有加gap锁，所以session2是可以正常执行的，不能被阻塞。 7.3 第一种情况：唯一索引+不命中数据 session1执行 1delete from test where id = 7; session2执行： 1insert into test(name,id) values("swg",8); 此时session2会阻塞住，证明id=7周围加了gap锁。gap锁的范围遵从左开右闭的原则，这里就是(6,7）以及(7,9)都会被锁住。加上record锁组成next-key锁，所以next-key锁的范围是(6,7]以及(7,9]这个范围。 7.4 第三种情况：唯一索引+不命中所有数据 session1执行 1select * from test where id in (5,7,9) lock in share mode; 这里是一个范围，5和9都是存在的，但是7不存在，即部分数据不存在。 session2执行： 1234567insert into test(name,id) values("swg",4);&lt;!--可以--&gt;insert into test(name,id) values("swg",7);&lt;!--不可以--&gt;insert into test(name,id) values("swg",8);&lt;!--不可以--&gt;insert into test(name,id) values("swg",10);&lt;!--可以--&gt; 那么对于(5,9]的范围内就阻塞住了，那么部分命中就是部分加gap锁。 7.5 第四种情况：唯一索引+命中所有数据 session1执行 1select * from test where id in (5,6,9) lock in share mode; 这里全部命中，那么 session2执行： 123insert into test(name,id) values("swg",7);&lt;!--可以--&gt;insert into test(name,id) values("swg",8);&lt;!--可以--&gt; 这个时候就不会加gap锁了。 8. 例子-不走唯一索引或者不走索引 下面来看看不走非唯一索引的当前读是什么情况。 此时表的数据为： name id h 2 c 6 b 9 d 9 f 11 a 15 把id上的唯一索引换成了普通索引。 8.1 第五种情况：非唯一索引 session1执行 1delete from test where id = 9; session2执行： 1insert into test(name,id) values("swg",9); 此时session2是会被block住的。gap的范围是(6,9]以及(9,11]. 12345insert into test(name,id) values("swg",5);&lt;!--可以--&gt;insert into test(name,id) values("swg",7);&lt;!--不可以--&gt;insert into test(name,id) values("swg",12);&lt;!--可以--&gt; 上面的原理都是一样的，即只要是6和11之间的数，不包含临界值的时候，无论插入什么数据，都是会阻塞的。 但是关于临界值6和11，这里就比较特殊了，因为需要加上主键的值才能进行精准的判断。 123insert into test(name,id) values(&quot;bb&quot;,6);&lt;!--可以--&gt;insert into test(name,id) values(&quot;dd&quot;,6);&lt;!--不可以--&gt; 这是什么原因呢？ 我们将数据画成图： 这里的gap区间可能是(负无穷，2],(2,6],(6,9],(9,11],(11,15],(15,正无穷) 我们可以看到，id为6的行，对应的name为c(不要忘记name是主键，主键按照顺序排序)，那么主键中就是按照字母表的顺序进行排列的（ASCII码），如果插入的name小于c，那么就不在gap的范围内(c,)，就可以插入，但是dd在gap的范围内,所以就会阻塞住。 8.2 第五种情况：不走索引 这个时候，所有的间隙都会加上间隙锁，那么就是锁表了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锁模块]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E9%94%81%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[本章对行锁表所、共享锁排他锁进行详细说明。这是数据库锁的核心知识。 MySQL中几种重要的锁概念 共享锁（S）和 排他锁（X） InnoDB 实现了标准的行级锁，包括两种：共享锁（简称 s 锁）、排它锁（简称 x 锁） 共享锁允许持锁事务读取一行 排它锁允许持锁事务更新或者删除一行 如果事务 T1 持有行 r 的 s 锁，那么另一个事务 T2 请求 r 的锁时，会做如下处理： T2 请求 s 锁立即被允许，结果 T1 T2 都持有 r 行的 s 锁 T2 请求 x 锁不能被立即允许 如果 T1 持有 r 的 x 锁，那么 T2 请求 r 的 x、s 锁都不能被立即允许，T2 必须等待T1释放 x 锁才行。 意向锁 innodb的意向锁主要用户多粒度的锁并存的情况。比如事务A要在一个表上加S锁，如果表中的一行已被事务B加了X锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了“意向锁”的概念。 举个例子，如果表中记录1亿，事务A把其中有几条记录上了行锁了，这时事务B需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务B先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。 说白了意向锁的主要作用是处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁” 意向排它锁（简称 IX 锁）表明一个事务意图在某个表中设置某些行的 x 锁 意向共享锁（简称 IS 锁）表明一个事务意图在某个表中设置某些行的 s 锁 例如， SELECT ... LOCK IN SHARE MODE 设置一个 IS 锁, SELECT ... FOR UPDATE 设置一个 IX 锁。 意向锁的原则如下： 一个事务必须先持有该表上的 IS 或者更强的锁才能持有该表中某行的 S 锁 一个事务必须先持有该表上的 IX 锁才能持有该表中某行的 X 锁 next-key锁 InnoDB有三种行锁的算法： Record Lock：单个行记录上的锁。分为S Lock和X Lock Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。 Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。 在默认情况下，mysql的事务隔离级别是可重复读，并且innodb_locks_unsafe_for_binlog参数为0，这时默认采用next-key locks。所谓Next-Key Locks，就是Record lock和gap lock的结合，即除了锁住记录本身，还要再锁住索引之间的间隙。 例子：假设一个索引包含值 10,11,13和20，索引上可能的NK 锁包括如下几个区间（注意开闭区间） 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) Innodb使用NK 锁来进行索引搜索和扫描，阻止了幻读。 间隙锁在Innodb中是被“十足的抑制”的，也就是说，他们只阻止其他事务插入到间隙中，他们不阻止其他事物在同一个间隙上获得间隙锁。 下篇文章会详细介绍一下。 MyISAM和InnoDB关于锁方面的区别 结论： MyISAM默认使用的是表级锁，不支持行级锁 InnoDB默认使用的是行级锁，也支持表级锁 所谓表级锁，就是锁住整张表。开销小，加锁快；不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 MyISAM在执行select的时候会产生一个表共享读锁，当进行更新等操作的时候会产生表独占写锁（排他锁）。所以： myISAM表的读操作，不会阻塞其他用户对同一个表的读请求，但会阻塞对同一个表的写请求。 myISAM表的写操作，会阻塞其他用户对同一个表的读和写操作。 myISAM表的读、写操作之间、以及写操作之间是串行的。 这里的读是共享锁，也可以将其变为排他锁，语法是select … for update 上面说完了MyISAM的表锁，下面要说说InnoDB啦。InnoDB支持行级锁。 所谓行级锁，就是锁住一行数据。开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发性也最高。 SELECT … LOCK IN SHARE MODE 和 SELECT … FOR UPDATE 如果你在查询数据，然后在同一个事务里插入或者修改相关的数据，常规的 select 语句不会提供足够的保护。其他的事务可以修改或者删除你正在查询的行。InnoDB 支持两种可以提供安全机制的读取锁： SELECT ... LOCK IN SHARE MODE SELECT ... FOR UPDATE SELECT … LOCK IN SHARE MODE 在读取的行上设置一个共享锁，其他的session可以读这些行，但在你的事务提交之前不可以修改它们。如果这些行里有被其他的还没有提交的事务修改，你的查询会等到那个事务结束之后使用最新的值。 索引搜索遇到的记录，SELECT … FOR UPDATE 会锁住行及任何关联的索引条目，和你对那些行执行 update 语句相同。其他的事务会被阻塞在比如执行 update 操作，获取共享锁，或从某些事务隔离级别读取数据等操作。 使用 SELECT FOR UPDATE 为 update 操作锁定行，只适用于 autocommit 被禁用（当使用 START TRANSACTION 开始事务或者设置 autocommit 为0时）。如果 autocommit 已启用，符合规范的行不会被锁定。 以上是对官方文档的翻译解读。 SELECT … LOCK IN SHARE MODE ：共享锁(S锁, share locks)。其他事务可以读取数据，但不能对该数据进行修改，直到所有的共享锁被释放。 如果事务对某行数据加上共享锁之后，可进行读写操作；其他事务可以对该数据加共享锁，但不能加排他锁，且只能读数据，不能修改数据。 SELECT … FOR UPDATE：排他锁(X锁, exclusive locks)。如果事务对数据加上排他锁之后，则其他事务不能对该数据加任何的锁。获取排他锁的事务既能读取数据，也能修改数据。 注：普通 select 语句默认不加锁，而CUD操作默认加排他锁。 当前事务获取共享锁后，可以读写，其他事务是否可以进行读写操作和获取共享锁：可以读，可以获取共享锁，不可以写 两个事务同时获取共享锁后，是否可以进行update操作：不可以 当前事务获取排他锁后，其他事务是否可以进行读写操作和获取共享锁：其他事务可以读，不可以获取共享锁，不可以写 是否可对一条数据加多个排他锁：不可以 行锁和索引的关系：查询字段未加索引（主键索引、普通索引等）时，使用表锁 注：InnoDB行级锁基于索引实现。 未加索引时，两种行锁情况为（使用表锁）： 事务1获取某行数据共享锁，其他事务可以获取不同行数据的共享锁，不可以获取不同行数据的排他锁 事务1获取某行数据排他锁，其他事务不可以获取不同行数据的共享锁、排他锁 加索引后，两种行锁为（使用行锁）： 事务1获取某行数据共享锁，其他事务可以获取不同行数据的排他锁 事务1获取某行数据排他锁，其他事务可以获取不同行数据的共享锁、排他锁 索引数据重复率太高会导致全表扫描：当表中索引字段数据重复率太高，则MySQL可能会忽略索引，进行全表扫描，此时使用表锁。可使用 force index 强制使用索引。 总结（很重要） MyISAM默认使用的是表级锁，不支持行级锁 执行select的时候会产生一个表共享读锁 当进行更新等操作的时候会产生表独占写锁（排他锁） 读不会阻塞其他session的读以及获取表共享读锁 写会阻塞其他session读和写操作 写与读之间是串行的 InnoDB默认使用的是行级锁，也支持表级锁 InnoDB 支持两种可以提供安全机制的读取锁：SELECT … LOCK IN SHARE MODE以及SELECT … FOR UPDATE SELECT … LOCK IN SHARE MODE 在读取的行上设置一个共享锁 SELECT … FOR UPDATE：排他锁 一个session对某一行上共享锁，其他的session可以读这行，也可以获取共享锁，但是不允许写，更不允许获取写锁。对于其他行，可以读写其他行数据也可以上读写锁。 一个session对某一行上排他锁，其他的session则不能加任何锁，包括共享锁。允许读这一行，但是不能写。允许对其他行数据进行读写以及上读写锁。 InnoDB中行级锁基于索引实现，所以在不加索引的时候，这两者上的其实都是表锁；加上索引之后，使用行锁。 以上的内容都是从博客：https://blog.csdn.net/u012099869/article/details/52778728 中整理而来，具体的实验也在他的博客中进行了详细的展示。 MyISAM适合场景 频繁执行全表count语句(MyISAM已经用一个表保存了行数) 对数据进行增删改的频率不高，查询非常频繁 没有事务 InnoDB适合场景 数据增删改查都相当频繁 可靠性要求比较高，要求支持事务]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于索引失效和联合索引]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E5%85%B3%E4%BA%8E%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E5%92%8C%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[理解最左前缀原则，可以帮助我们避免索引失效。 索引失效 查询条件包含or 当or左右查询字段只有一个是索引，该索引失效，explain执行计划key=null；只有当or左右查询字段均为索引时，才会生效； 组合索引，不是使用第一列索引，索引失效 如果select * from key1=1 and key2= 2;建立组合索引（key1，key2）; select * from key1 = 1;组合索引有效； select * from key1 = 1 and key2= 2;组合索引有效； select * from key2 = 2;组合索引失效；不符合最左前缀原则 like 以%开头 使用like模糊查询，当%在前缀时，索引失效； 如何列类型是字符串，where时一定用引号括起来，否则索引失效 当全表扫描速度比索引速度快时，mysql会使用全表扫描，此时索引失效 最左前缀原则 建立以下sql： 123456789CREATE TABLE IF NOT EXISTS `test_index`( `id` int(4) NOT NULL AUTO_INCREMENT, `a` int(4) NOT NULL DEFAULT '0', `b` int(4) NOT NULL DEFAULT '0', `c` int(4) NOT NULL DEFAULT '0', `data` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `union_index` (`a`,`b`,`c`))ENGINE=InnoDB ROW_FORMAT=DYNAMIC DEFAULT CHARSET=binary; 测试的mysql版本是 5.7. 首先以列a作为条件查询数据，我们看到 type: ref 表示引用查找, key_len: 4 表示索引长度为4，也就是利用上了索引来进行查找: 123456789101112131415explain select data from test_index where a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.01 sec) 然后以列b作为条件查询数据，可以看到type: ALL表示全表查找, key_len: NULL 表示没有索引，也就说明如果只使用b作为查询条件，不能利用索引来加快查找速度. 123456789101112131415explain select data from test_index where b = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 接着以列c作为条件查询数据，可以看到type: ALL表示全表查找, key_len: NULL 表示没有索引，情况与用b作为条件一样，只使用c作为查询条件也不能利用索引来加快查找速度 123456789101112131415explain select data from test_index where c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 现在来测一下使用a、b作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 8 表示索引长度为8，也就是说我们利用上了a、b联合索引来进行查找 123456789101112131415explain select data from test_index where a = 1 and b = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 紧接着来测一下使用a、c作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 4 表示索引长度为4，这就奇怪了，按照最左原则来说，a、c上是不会建立索引的，为什么会有索引长度呢？其实与a、b上的索引一比较我们就能发现，a、c上的索引长度只有4，而且单独的c上是没有索引的，所以4字节长度的索引只能是a上的，也就是说这种情况我们只使用了a列上的索引来进行查找 123456789101112131415explain select data from test_index where a = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 为了进一步验证上面的想法，这一次测一下使用b、c作为条件的情况，我们看到 type: ALL 表示全表查找, key_len: NULL 表示没有索引可以使用，按照最左原则来说，b列上没有索引，c列上也没有索引，同时b、c的上也不存在联合索引，所以使用b、c作为查询条件时无法利用联合索引 123456789101112131415explain select data from test_index where b = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 1.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 测试完两个条件的情况，接下来测试一下使用a、b、c作为条件的情况，我们看到 type: ref 表示引用查找, key_len: 12 表示索引长度为12，这完全符合联合索引的最左原则，同时使用3个条件查询可以利用联合索引 123456789101112131415explain select data from test_index where a = 1 and b = 1 and c = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 12 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 下面这种情况也能利用a、b上的联合索引，索引长度为8 123456789101112131415explain select data from test_index where b = 1 and a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 再来试试这种情况，按照最左原则，c上没有建立索引，a上有索引，c、a没有建立联合索引，所以只能使用a上的索引进行查找，结果索引长度只有4，验证了我们的想法，联合查询条件使用索引时满足“交换律” 123456789101112131415explain select data from test_index where c = 1 and a = 1*************************** 1. row *************************** id: 1select_type: SIMPLE table: test_indexpartitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 联合索引总结 联合索引的最左原则就是建立索引KEY union_index (a,b,c)时，等于建立了(a)、(a,b)、(a,b,c)三个索引，从形式上看就是索引向左侧聚集，所以叫做最左原则，因此最常用的条件应该放到联合索引的组左侧。 **对于&quot;=&quot;和&quot;in&quot;可以乱序。**利用联合索引加速查询时，联合查询条件符合“交换律”，也就是where a = 1 and b = 1 等价于 where b = 1 and a = 1。这归功于mysql查询优化器，mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。 mysql会一直向右匹配直到遇到范围查询(&lt;,&gt;,between,like)就停止匹配。比如a=3 and b=4 and c&gt;5 and d=6，如果建立(a,b,c,d)顺序的索引，d是用不到索引的。如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 对于最左匹配原则的理解 mysql索引最左匹配原则的理解?–沈杰的回答 其实我觉得只要理解一点就是，只要有最左边的索引元素，那么这个索引结构一定是按照最左索引元素排序的，后序的索引元素也是依赖于最左元素之后才有可能变得有意义。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL调优]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2FMySQL%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[本文介绍最基本的sql调优手段。 根据慢日志定位慢查询sql 12&lt;!--这里是用模糊查询查出关于查询的一些配置项--&gt;show variables like '%query%' 我们关注slow_query_log：OFF，表示慢查询处于关闭状态。关注long_query_time：超出这个时间就是慢查询，记录到slow_query_log_file文件中。 1show status like '%slow_queries%' 这一句作用是统计慢查询的数量。 如何打开慢查询呢？ 1234&lt;!--打开慢查询--&gt;set global slow_query_log = on;&lt;!--慢查询的标准是1秒--&gt;set global long_query_time = 1; 注意要重启一下客户端。或者在配置文件中设置，重启服务端就永久保留了。 explain分析慢日志 上一步时打开慢查询日志。下面要进行分析。 1explain select ... 对这个命令进行分析。有两个关键字段： type：表示mysql找到数据行的方式，下面的顺序是由快到慢： system&gt;const&gt; eq_ref&gt;ref&gt;fulltext&gt;ref_or_null&gt;index_merge&gt;unique_subquery&gt;index_subquery&gt;range&gt;index&gt;all 其中index和all为全表扫描。说明需要优化。 extra： using_filesort：表示MySQL会对结果使用一个外部索引排序，而不是从表里按索引次序读到相关内容。可能在内存或者磁盘上进行排序。MqSQL中无法利用索引完成的排序操作称为“文件排序” using temporary：表示MySQL在对查询结果排序时使用临时表。常见于排序order by 和分组查询 group by。 当extra中出现以上两项意味着MYSQL根本不能使用索引，效率会受到重大影响，应尽可能对此进行优化。 修改sql或者尽量让sql走索引 上一步分析完之后，就要采取一定的措施来修正。 如果是没有加索引，可以对其加上索引。extra就会变成using index，表示走了索引。 索引是越多越好吗 数据量小的表不需要建立索引，建立会增加额外的索引开销 数据变更需要维护索引，因此更多的索引意味着更多的维护成本 更多的索引意味着也需要更多的空间 可以理解为，一个几页的宣传手册 对于几页的宣传手册我们还需要建立一个目录吗？ 变更这个小的宣传手册里面的章节还要修改目录不是更烦吗？ 一个宣传手册内容就两页，结果你的目录就占了一页，这合理吗？]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引全面解读]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2FMySQL%E7%B4%A2%E5%BC%95%E5%85%A8%E9%9D%A2%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[彻底理解MySQL的存储引擎数据结构以及聚集索引。 1.为什么需要索引 如果不用索引，那么最简单的方案是：将数据全部或者分批次地加载到内存，因为数据是以页的形式存储的，所以我们可以轮询这些页，找出有没有我们需要的数据。 在大数据量的情况下，显然是会非常慢的。因为它要进行全表的扫描。 而索引的灵感来源于字典，我们知道，新华字典前面有按照拼音或者按偏旁部首排序的一个列表页，我们可以快速地根据这个索引目录迅速定位到某几页，然后我们到这个某几页找一下就可以找到了。不需要全表扫描。 2.索引的数据结构 2.1 二叉查找树上阵 二叉查找树的特点： 二分搜索树本质上是一棵二叉树。不需要是一棵完全二叉树。 每个节点的键值大于左孩子 每个节点的键值小于右孩子 以左右孩子为根的子树仍为二分搜索树 二叉查找树有点很明显，我们查询一个数据只需要O(logn)的时间。但是它存在一个致命问题： 我们有时会删除增加数据，搞的不好，会把他恶化成一个链表。 但是有的同学说，我们可以利用红黑树之类的数据结构来维持住平衡二叉树的特性，这样不就好了吗？ 这里还存在另一个问题，就是IO。我们知道从磁盘查询数据，影响性能的关键点是iO的次数，然后这种一个节点只有两个孩子，在海量数据里，IO的次数还是太多，影响性能。 我们这个时候就知道了方向，我们想找一个数据结构，它既包含了二叉树的优点，还要是平衡的树，还能使树的高度变矮，并且每个节点存储更多的数据。 2.2 BTree上阵 B数又叫平衡多路查找树。M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉 有几个特点： 根节点至少包含两个孩子 排序方式：所有节点关键字是按递增次序排列，并遵循左小右大原则 子节点数：树中每个节点最多包含m个孩子(m&gt;=2)；除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子 关键字数：枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于m-1个（分别比孩子数少一个） 所有叶子节点都位于同一层，即所有叶子节点高度都一样 我们结合这个图来理解。 可以看到，我们这是一个3路B树，根节点有3个孩子，有2个关键字。根据规则，子节点数最多为m个即3个，最少为ceil(1.5)个即2个；关键字数最多为m-1个即2个，最少为于ceil(1.5)-1个即1个. 那么我们进行插入的时候，关键字这里最多为2个，所以大于2就要进行拆分。 如何拆分呢？拿个例子来： 定义一个5阶树（平衡5路查找树;），现在我们要把3、8、31、11、23、29、50、28 这些数字构建出一个5阶树出来; 那么关键字最多为4个，超过4个就拆分。 先插入 3、8、31、11 再插入23、29 再插入50、28 大概就是这样的流程。总之要维护一个从左到右逐渐增大的一个特性，并且必须是平衡的。(大概忽略里面可能存在的一些小错误，理解其中意思即可) 对于删除也是如此，要满足以上的特性才行，这里就不再赘述了。 对B树总结一下： B树相对于平衡二叉树的不同是，每个节点包含的关键字增多了，特别是在B树应用到数据库中的时候，数据库充分利用了磁盘块的原理（磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来）把节点大小限制和充分使用在磁盘快大小范围；把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度; 2.3 B+树 是B树的变体，其定义基本上与B树是差不多的。除了： 非叶子节点的子树指针与关键字个数相同 非叶子节点仅用来索引，数据都保存在叶子节点中 所有叶子节点均有一个链指针指向下一个叶子节点 这就是B+树相对于B树的改进的几个点。 由于数据存在叶子节点，优点是非叶子节点保存的关键字更多了，树的高度就会更矮。 2.4 总结 B+树更适合用来做存储索引： B+树的磁盘读写代价更低（因为内部不存放数据，一次性读取的关键字更多，IO次数降低） B+树的查询效率更加稳定（任何关键字的查找都要到叶子节点，导致每个查询都差不多） B+树有利于对数据库扫描（遍历叶子节点就可以直接扫描整个表，这个适合做范围查询） 2.5 Hash索引也可以考虑一下 Hash结构可以一次性地定位到响应位置。如果遇到碰撞的情况，只需要遍历链表即可。那么性能这么高，为什么我们不用Hash索引呢？ 它也有缺点： 只能做等值操作，不能使用范围查询 hash索引不是按照索引值顺序存储，无法使用于排序。 不能利用部分索引键查询（比如组合索引，hash索引是对这几个索引一起hash计算的，而我们用组合索引中的部分索引时就无法用了） 不能避免表扫描（会出现hashs冲突，必然要扫描里面具体的数据才行） 遇到大量hash值相等的时候性能不一定比B树高(同上) 3. 聚集索引 3.1 什么是聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 3.2 MyISAM和InnoDB索引实现 第一个不同是：InnoDB的数据文件本身就是索引文件。而MyISAM的索引和数据是分开的。 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址，是没有任何顺序而言的，所以MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。 InnoDB也使用B+Tree作为索引结构。InnoDB的数据文件本身就是索引文件，即 InnoDB 表是基于聚簇索引建立的。 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址（这一点可以通过在data目录下查看数据库文件验证。Innodb每一个数据库只有一个数据文件，而Myisam则有三个（数据文件、索引文件、表结构文件））。 而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。聚集索引的确定规则为： 若一个主键被定义，该主键则作为聚集索引 若没有主键被定义，该表的第一个唯一非空索引作为聚集索引 若上述都找不到，innodb内部会生成一个隐藏主键(聚集索引) 非主键索引存储相关键位和其对应的主键值，包含两次查找 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。 换句话说，InnoDB的所有辅助索引都引用主键作为data域。 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择 4. 总结 本文首先介绍的是为什么要索引。这个问题很简单。 然后介绍了几种数据结构：二叉搜索树、二叉平衡树、B树以及B+树。一步一步引出为什么最终是B+树。面试的时候就看解决：为什么不能用二叉搜索树、为什么不用红黑树、B树和B+树各自的数据结构特点、B+树的优点 最后介绍了聚集索引，因为这是MyISAM与InnoDB索引结构最大的不同。 之前还是不能太准确理解聚集索引，这两种存储引擎都是以B+树数据结构建立索引结构的，但是InnoDB本身这个B+树就作为了索引文件，即索引与数据是放在一起的，所以逻辑上这样排的数据，它物理上也是这么排。 而MyISAM的索引结构(B+树)与数据是分离的，虽然B+树可能是按照主键有序地组织，但是表的数据在另一个地方是随机放的，找数据是根据地址来找即可，所以这种结构就不是聚集的。 理解了这个，下面就非常好理解了，InnoDB这个B+树，我们知道，叶子节点的核心数据就是主键。所以是按照主键递增的方式进行排列。这样子，无论是按照主键排序还是范围搜索，都会非常地快。 那么如果是非主键索引的辅助索引呢？InnoDB只能通过两次查询来实现了，首先第一步是根据这个辅助索引找到存放在叶子节点中的主键值，然后根据主键再去主键索引中去查找对应的数据。 而MyISAM索引，主键索引和辅助索引就区别不大了。都是单独一个索引结构，然后根据最后叶子节点中的该条数据的地址去找。 上面说的按照主键排列，就是这里所谓的聚集索引啦。当然了，如果没有指定主键，会按照上面所说的规则去构建聚集索引。 那么，面试的时候，就可以应对InnoDB与MyISAM索引结构的各自的实现和不同点啦。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引入门]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[索引是数据库中提高性能的一大利器。本篇入门索引的基本知识。 1. 什么是索引 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2. 为什么要用索引 索引主要就是为了提高查询速度用的。 3. 索引的一些缺点 第一，创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 第二，索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 第三，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4. 哪些字段适合用索引 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。（这同一） 5. 不应该创建索引的的这些列具有下列特点 第一，对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 第二，对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 第三，对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 第四，当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 6. 索引的分类 B-Tree 索引， Hash 索引， Fulltext 索引和R-Tree 索引 最主要关心的是B-Tree 索引。下面再提一下聚集索引，因为这是innodb最主要的组织方式。 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 下一节会详细讲到InnoDB和MyISAM的索引实现方式，他们最大的区别就是InnoDB是聚集索引，而MyISAM不是。 7. 局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 8.B树/B+树 想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是B tree或者 B+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然， 有的数据库也使用哈希桶作用索引的数据结构 ， 然而， 主流的RDBMS都是把平衡树当做数据表默认的索引数据结构的。 我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个加了主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。 如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。 假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。 这里不讲解平衡树的运行细节， 但是从上图能看出，树一共有三层， 从根节点至叶节点只需要经过三次查找就能得到结果。如下图 这一节先对索引入个门，关于B+树以及聚集索引下篇文章来具体分析。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一个关系型数据库]]></title>
    <url>%2F2019%2F01%2F26%2Fmysql%2F%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[这是一个比较空的面试题，这里说一下如何回答。 如何设计一个关系型数据库 我们考虑开发一个数据库最重要的模块是什么。首先数据存储是其核心功能。因此会有一个存储模块来存储数据。介质主要是硬盘。 可是，光有存储是不行的。我们需要有以下程序模块对数据进行组织。 存储管理 我们需要对数据的格式和文件的分隔进行统一的管理，通过逻辑的形式来组合和表示出来。 我们知道程序处理，需要将数据先加载到内存中去，不可能直接在硬盘上进行处理。 我们通过io读取磁盘数据，磁盘的io是非常耗时的，所以硬盘以页的形式存储数据，根据局部性原理，往往用户要查询的数据周围的数据也会被查询到，所以取数据都是以页为单位查取多个数据，提高效率。 缓存机制 也就是上面提到的，一次IO不会只取用户所需要的一点数据，所以会涉及到缓存，缓存可能会不够放，那就涉及一些缓存淘汰的算法，比如比较常用的是LRU算法。 SQL解析 将SQL进行编译执行。如何提高SQL解析效率呢？可能也用缓存，缓存好SQL解析后的结果，下次再执行一样的SQL就可以免去解析的过程。 日志管理 要记录SQL操作，方便主从同步、灾难恢复等。这里要了解一下binlog. 权限划分 就是权限。 容灾机制 要对异常情况做好准备，比如数据库挂了怎么办。 索引管理 优化数据库执行效率。 锁模块 使得数据库支持并发操作。 总结 了解了上面的内容，我们就可以对这个问题做一个简单的总结性回答了，如何设计关系型数据库呢？首先数据库有一个存储的功能，使得它能存储在比如机械硬盘或者固态硬盘上面。其次，我们需要一个存储管理模块来映射程序逻辑与物理地址，实现存储管理。还需要缓存机制，对一些数据进行缓存提高效率，并且缓存不能太大，必须配备缓存淘汰机制；然后需要一个SQL解析模块，来解析SQL；然后需要日志管理来提供主从赋值、主从同步等功能；还需要一个权限划分模块，来提供给多用户使用场景；还需要容灾机制面对异常情况；最后，为了提高数据查询效率需要有索引管理模块；为了支持并发操作需要有锁模块。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[delete和truncate以及drop区别]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fdelete%E5%92%8Ctruncate%E4%BB%A5%E5%8F%8Adrop%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[这个题目我自己也被问过，这里简单整理一下。 先来个总结： drop直接删掉表； truncate删除的是表中的数据，再插入数据时自增长的数据id又重新从1开始； delete删除表中数据，可以在后面添加where字句。 日志是否记录 DELETE语句执行删除操作的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。 TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 是否可以回滚 delete 这个操作会被放到 rollback segment 中,事务提交后才生效。truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment 中，不能回滚. 所以在没有备份情况下，谨慎使用 drop 与 truncate。 表和索引占的空间 当表被 TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小。 而 DELETE 操作不会减少表或索引所占用的空间。 drop语句将表所占用的空间全释放掉。 TRUNCATE 和 DELETE 只删除数据，而 DROP 则删除整个表（结构和数据） 所以从干净程度，drop &gt; truncate &gt; delete ok，差不多了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最基础知识小结]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%2Fmysql%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文介绍关于数据库的最最最最基本的一些语法知识，如果这些都不熟悉，建议多多练习，因为后续的文章会比较深入原理。 一、DDL语句 1、创建数据库：create database dbname; 2、删除数据库：drop database dbname; 3、创建表：create table tname; 4、删除表：drop table tname; 5、修改表：略，懒得看 二、DML语句 插入： 1insert into table(字段1，字段2，...) values (value1,value2,...) , (value3,value4,..) 更新： 1update table set 字段=value where ... 删除： 1delete from table where ... 这里要注意下delete和truncate以及drop三者的区别，下篇文章详解。 单表查询： 1select 字段 from table 连表查询方式1： 1select 别名1.字段,别名2.字段 from table1 别名1,table2 别名2 where ... 连表查询方式2： 1select 别名1.字段,别名2.字段 from table1 别名1 join table2 别名2 on ... 这是全连接，这里就要了解一下笛卡儿积，简单来说，最后行数是左边表的函数乘以右边表的行数。详细的可以自行google. 查询不重复的记录： 1select distinct 字段 from table ... 排序：默认是升序 1select 字段 from table where ... order by ... asc/desc limit：主要用于分页 1select * from table where ... order by ... asc/desc limit 起始偏移位置，显示条数 聚合： 1select count(*)/avg(..)/sum(...)/max(...)/min(...) from table group by ... having .... 注意这里的having和where的区别：where是对表结果进行筛选，having 是对查询结果进行筛选，与group by 合用 左连接和右连接 左连接意思就是左表中的记录都在，右表没有匹配项就以null显示。记录数等于左表的行数。 右连接与之同理，尽量转为左连接做。 子查询： 1select * from table where ... in (select ....) 所谓子查询就是根据另一个select的结果再进行筛选，常用的是in,not in,=,!=,exits,not exits union 主要用于两张表中的数据的合并： 1select 字段 from table1 union all select 字段 from table2 要想不重复用union]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常见的面试题]]></title>
    <url>%2F2019%2F01%2F25%2Fnetwork%2F8.%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[对这一块整理一些常见的面试题。 1.TCP三次握手、四次挥手 这部分略。前面已经说的很详细，包括握手为什么不是两次、为什么不是四次，为什么挥手要等2MSL的时间。 2.常见的HTTP状态码及其含义 200 OK：正常返回信息 400 Bad Reqest：客户端请求有语法错误，不能被服务器所理解 401 Unauthorized：请求未经授权，这个状态码必须与WWW-Authenticate报头域一起使用 403 Forbidden：服务器收到请求，但是拒绝提供服务 404 Not Found：请求资源不存在 500 Internal Server Error：服务器发生不可预期的错误 503 Server Unavilable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常 3.Get请求和Post请求的区别 Http报文层面：GET将请求信息放在URL，POST则放在报文体中 数据库层面：GET符合幂等性和安全性(查询不会改变数据库)，POST不符合 其他层面：GET可以被缓存、被存储为书签，而POST不行 4.Cookie和Session的区别 对于session，字面上理解是会话，可以理解为用户与服务端一对一的交互。是一个比较抽象的概念。 但是我们常说的session其实是这里抽象概念的一种实现方式罢了，我觉得没有必要咬文嚼字，下面直接从面试角度来分析一下。 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。 这个Session是保存在服务端的，有一个唯一标识，这个唯一标识对应一个用户。在服务端保存Session的方法很多，内存、数据库、文件都有。 服务端解决了用户标识问题，但是服务端怎么知道此时操作浏览器的用户是谁呢？ 这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。 实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端(放在响应头中返回)，需要在 Cookie 里面记录一个Session ID，以后每次请求(请求头)把这个会话ID发送到服务器，我就知道你是谁了。 有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 总结： Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中； Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 下面说一下很常见的一种写法。比如在单体应用中，我此时登陆你的网站了，你可以将我的信息保存在session中： 12User currentUserInfo = userService.getUserByUsernameAndPasswd(username,password);session.setAttribute("currentUser",currentUserInfo); 下次，我就可以在我们之间的会话中随时获取我的个人信息： 1User currentUser = session.getAttribute("currentUser"); 其实这些就是利用存放在Cookie中的JSESSIONID来实现的。 5.HTTP和HRTTPS的关系 来说一下SSL(Security Sockets Layer，安全套接层) 为网络通信提供安全及数据完整性的一种安全协议 是操作系统对外的API，SSL3.0之后更名为TLS 采用身份验证和数据加密保证网络通信的安全和数据的完整性 HTTPS数据传输流程： 浏览器将支持的加密算法信息发送给服务器 服务器选择一套浏览器支持的加密算法，以证书的形式回发浏览器 浏览器验证证书合法性，并结合证书公钥加密信息发给服务器 服务器使用私钥解密信息，验证哈希，加密响应消息回发浏览器 浏览器解密响应消息，并对消息进行验证，之后进行加密交互数据 这个也就不赘述了，下面直接说说区别。 HTTPS需要到CA申请证书，HTTP不需要 HTTPS密文传输，HTTP明文传输 连接方式不同，HTTPS默认使用443端口，HTTP使用80端口 HTTPS=HTTP+加密+认证+完整性保护，更安全 但是仍然存在一定的风险： 浏览器默认填充http://，请求需要进行跳转，有被劫持的风险 可以使用HSTS(HTTP Strict Transport Security)优化（这个还不未主流，面试问的少） Socket简介 我们知道，进程与进程直接的通信最基本的要求是：可以唯一确定进程。 在本地进程通信中，可以用PID来唯一标识一个进程。 但是PID只在本地唯一，网络中PID冲突的几率还是存在的。 我们知道，到IP层就可以唯一定位到一台主机了，TCP层(tcp协议+端口号)可以唯一定位一台主机中的一个进程。 这样，我们可以通过ip地址+协议+端口号可以唯一标识一台主机的一个进程。这样就可以通过socket进行网络通信了。 socket是对TCP/IP协议的抽象，是操作系统对外开放的接口。 socket起源于unix，而unix是遵从一切皆文件的哲学。Socket是一种基于从打开、读/写、关闭的模式实现的。客户端和服务器各自维护一个文件，在连接建立后，可以供对方读取或者读取对方内容。 socket相关题目 编写一个网络程序，有客户端和服务端，客户端向服务端发送一个字符串，服务器收到字符串之后打印到命令行上，然后向客户端返回该字符串的长度，最后，客户端输出服务端返回的该字符串的长度，分别用TCP和UDP两种方式去实现。 代码地址：https://github.com/sunweiguo/TcpAndUdp/]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础核心-理解类、对象、面向对象编程、面向接口编程]]></title>
    <url>%2F2019%2F01%2F24%2Fjava-basic%2FJAVA%E5%9F%BA%E7%A1%80%E6%A0%B8%E5%BF%83-%E7%90%86%E8%A7%A3%E7%B1%BB%E3%80%81%E5%AF%B9%E8%B1%A1%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E3%80%81%E9%9D%A2%E5%90%91%E6%8E%A5%E5%8F%A3%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是类，什么是对象，什么是面向对象编程，什么是面向接口编程。学习面向对象思想的语言，比如java，第一关可能就是要理解这些概念。下面就来好好琢磨一下。 类和对象的概念 首先总结一下：类是一个模板，对象就是用这个模板创造出来的东西。 比如，男孩，他就是一个模板，男的就行，那么对象是什么呢？就是具体某个男孩，比如男孩BOB，男孩fourColor. 请看下面一张图： 男孩女孩是比较抽象的概念，是模板，左边一排就是其具体的一些对象。你看长的都不一样，有的黑，有的白，有的高，有的矮，国家地区也不一样。但是他们都属于男孩或者女孩。 那么同理，人就是一个类，男孩女孩就是人的子类，因为人可能不仅包括男孩女孩，还包括第三性别这个类。 这里还引出了JAVA特性中的继承。继承简单理解就是父类有的东西(访问级别不能是private)的，那都是你的。比如你老爸的房子，就是属于你的，你出入自由。 人还可以分为胖人和瘦人这个子类。所以只要是抽象的模板，就是一个类。 对象：对象是类的一个实例（对象不是找个女朋友），有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。 类：类是一个模板，它描述一类对象的行为和状态。 下面就拿狗这个类来说事。狗是动物这个类的子类。 Java中创建类 构造器方法说明 需要创造一个类对象出来的时候，要用到这个类的构造器方法，那么啥是构造器方法呢？构造器方法就是创造类时的初始化方法，和类同名的方法，你可以在里面写自己的代码 1234567891011121314151617181920//模版class 类名称 &#123; 访问权限 构造方法名称()&#123; &#125;&#125;//例子public class Dog&#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125;&#125; 一个相对比较完整的类 1234567891011121314151617181920212223242526272829//模版class 类名称 &#123; //构造器方法 //声明成员变量---这个变量属于这个类 //声明成员方法 //在方法里面定义的变量是局部变量，区别于成员变量&#125;//例子public class Dog &#123; //构造方法一 public Dog()&#123; System.out.println("nothing to do..."); &#125; //构造方法二 public Dog(String name)&#123; //这里就可以给每条new出来的对象(狗)初始化一个名字 System.out.println("hi,my name is "+name); &#125; //狗的颜色--成员属性 public String color;//一般是private，赋值用set方法，取值用get方法，这里只是演示 //狗的行为，它会叫---成员方法 private void say()&#123; int i = 0;//局部变量 System.out.println("我会叫：汪汪汪~"); &#125;&#125; 创建对象 语法： 类名 对象名 = new 类名() ; 举例： 12Dog fourcolor ; // 先声明一个 Dog 类的对象 fourcolorfourcolor = new Dog(&quot;fourcolor&quot;) ; // 用 new 关键字实例化 Dog 的对象 fourcolor,此时调用构造方法二 通过Dog这个类可以创造出fourcolor对象.下面我才能操作这个对象： 1234//让它的颜色为黑色fourcolor.color = &quot;black&quot;;//让它叫fourcolor.say(); 面向对象 在理解了什么是类，什么是对象，就可以来说说面向对象到底是什么了。 先来说说面向过程，大家都学习过C语言。C语言就是典型的面向过程的语言。 举个例子：要把大象装进冰箱里，这件事，面向过程的程序员是这样思考的： 把冰箱门儿打开。 把大象装进去。 把冰箱门儿关上。 上面的每一件事都用一个函数来实现。抽象为下面三个函数： openTheDoor()； pushElephant()； closeTheDoor()； 这样不挺好的吗？为什么不用面向过程的这种思维来编程呢，还要搞出什么面向对象来。 需求又来啦： 「我要把大象装微波炉里」 「我要把狮子也装冰箱里」 「我要把大象装冰箱，但是门别关，敞着就行」 这个时候，面向过程的程序员就悲剧了，来一个需求我就写一个函数，我还能下班吗？ 面向对象从另一个角度来解决这个问题。它抛弃了函数，把「对象」作为程序的基本单元。 面向对象的世界里，到处都是对象。即：万物皆对象。 比如人这个类，每个具体的人(对象)都要有这样的属性：身高、体重、年龄。每个人都有这样的行为：吃饭、睡觉、上厕所。 那么，这些通用的属性+方法可以构建一个模板：人这个类。因为每个具体的人（对象）都需要这些基本的东西。当然了，每个人具体什么身高、什么体重、吃什么都是不一样的，所以每个对象一般都是不一样的。但是模板是一样的。 那么，回到刚才的需求，面向对象是如何思考这件事的呢？ 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 就是说，我不用亲自管开门的细节，我只要叫他开门即可。 我们创建的对象，应该是刚刚好能做完它能做的事情，不多做，不少做。多做了容易耦合，各种功能杂糅在一个对象里。比如我有一个对象叫「汽车」，可以「行驶」，可以「载人」，现在的需求是要实现「载人飞行」，就不能重用这个对象，必须新定义一个对象「飞机」来做。如果你给「汽车」插上了翅膀，赋予了它「飞行」的能力，那么新来的同学面对你的代码就会莫名其妙，无从下手。 但是不禁要问：怎么实现这种下达命令就可以自动去执行的效果呢？或者说，我怎么知道它有这个功能啊！ 面向接口编程 现在我们把「数据」和「行为」都封装到了对象里，相当于对象成了一个黑匣子，那我们怎么知道对象具有什么样的能力呢？这个问题的关键就是接口。 因为无论是把大象装进洗衣机还是冰箱，都要求洗衣机或者冰箱有开门和关门的功能。这个时候，我们就可以抽象出来一个接口：【自动门】。这个接口里面定义两个能力：【开门】和【关门】。 让洗衣机、冰箱、微波炉这些带门的东西全部实现【自动门】接口。 这个时候，每个具体的实现可能略有不同，比如冰箱开门是往外拽，但是洗衣机开门可能是往上翻盖子。 此时，我有一个需求，把大象放进冰箱。我一看，冰箱实现了【自动门】这个接口，里面有【开门】和【关门】两个方法，ok，我知道冰箱是可以开门和关门了，那就好办了。我直接下达命令即可。还是跟上面一样的步骤. 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，需要将狮子也装冰箱里。那还是一样： 向冰箱下达「开门」的命令。 向狮子下达「进冰箱」的命令。 向冰箱下达「关门」的命令。 此时，我要把大象装冰箱，但是门别关，敞着就行，那就： 向冰箱下达「开门」的命令。 向大象下达「进冰箱」的命令。 是不是很方便？冰箱也可以换，我可以换成任何东西，只要实现了这个接口，这些东西就都有这些能力，那我才不管里面到底怎么实现的呢，直接下达【开门】【关门】命令即可。 这也引入了JAVA特性中另一个特性：封装。外界不知道里面实现细节，只需要知道它的功能和入参即可。 这就是面向过程和面向对象编程的区别，也顺带地理解了什么是面向接口编程。这是学习JAVA最基础也是最核心的点。 整理自： https://tryenough.com/java05 http://www.woshipm.com/pmd/294180.html]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串核心一网打尽]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%B8%E5%BF%83%E4%B8%80%E7%BD%91%E6%89%93%E5%B0%BD%2F</url>
    <content type="text"><![CDATA[对字符串中最核心的点：对象创建和动态加入常量池这些点进行深入分析。 比如有两个面试题： Q1：String s = new String(&quot;abc&quot;); 定义了几个对象。 Q2：如何理解String的intern方法？ A1：对于通过 new 产生的对象，会先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象，然后在堆中创建一个常量池中此 “abc” 对象的拷贝对象。所以答案是：一个或两个。如果常量池中原来没有 ”abc”, 就是两个。如果原来的常量池中存在“abc”时，就是一个。 A2：当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； 字面量和运行时常量池 JVM为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化。为了减少在JVM中创建的字符串的数量，字符串类维护了一个字符串常量池。 在JVM运行时区域的方法区中，有一块区域是运行时常量池，主要用来存储编译期生成的各种字面量和符号引用。 了解过JVM就会知道，在java代码被javac编译之后，文件结构中是包含一部分Constant pool的。比如以下代码： 123public static void main(String[] args) &#123; String s = "abc";&#125; 经过编译后，常量池内容如下： 1234567891011Constant pool: #1 = Methodref #4.#20 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #21 // abc #3 = Class #22 // StringDemo #4 = Class #23 // java/lang/Object ... #16 = Utf8 s .. #21 = Utf8 abc #22 = Utf8 StringDemo #23 = Utf8 java/lang/Object 上面的Class文件中的常量池中，比较重要的几个内容： 123#16 = Utf8 s#21 = Utf8 abc#22 = Utf8 StringDemo 上面几个常量中，s就是前面提到的符号引用，而abc就是前面提到的字面量。而Class文件中的常量池部分的内容，会在运行期被运行时常量池加载进去。 new String创建了几个对象 下面，我们可以来分析下String s = new String(&quot;abc&quot;);创建对象情况了。 这段代码中，我们可以知道的是，在编译期，符号引用s和字面量abc会被加入到Class文件的常量池中。由于是new的方式，在类加载期间，先去常量池检查有没有 “abc”，如果没有，先在常量池创建一个 “abc” 对象。 在运行期间，在堆中创建一个常量池中此 “abc” 对象的拷贝对象。 运行时常量池的动态扩展 编译期生成的各种字面量和符号引用是运行时常量池中比较重要的一部分来源，但是并不是全部。那么还有一种情况，可以在运行期像运行时常量池中增加常量。那就是String的intern方法。 当一个String实例调用intern()方法时，JVM会查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用； intern()有两个作用，第一个是将字符串字面量放入常量池（如果池没有的话），第二个是返回这个常量的引用。 一个例子： 123456789String s1 = "hello world";String s2 = new String("hello world");System.out.println("s==s1:"+(s==s1));String s3 = new String("hello world").intern();System.out.println("s==s2:"+(s==s2)); 运行结果是： 12s1==s2:falses2==s3:true 你可以简单的理解为String s1 = &quot;hello world&quot;;和String s3 = new String(&quot;hello world&quot;).intern();做的事情是一样的（但实际有些区别，这里暂不展开）。都是定义一个字符串对象，然后将其字符串字面量保存在常量池中，并把这个字面量的引用返回给定义好的对象引用。 对于String s3 = new String(&quot;hello world&quot;).intern();，在不调intern情况，s3指向的是JVM在堆中创建的那个对象的引用的（如s2）。但是当执行了intern方法时，s3将指向字符串常量池中的那个字符串常量。 由于s1和s3都是字符串常量池中的字面量的引用，所以s1==s3。但是，s2的引用是堆中的对象，所以s2!=s1。 intern的正确用法 不知道，你有没有发现，在String s3 = new String(&quot;abc&quot;).intern();中，其实intern是多余的？ 因为就算不用intern，“abc&quot;作为一个字面量也会被加载到Class文件的常量池”&quot;，进而加入到运行时常量池中，为啥还要多此一举呢？到底什么场景下才会用到intern呢? 在解释这个之前，我们先来看下以下代码： 1234String s1 = "hello";String s2 = "world";String s3 = s1 + s2;String s4 = "hello" + "world"; 在经过反编译后，得到代码如下： 1234String s1 = "hello";String s2 = "world";String s3 = (new StringBuilder()).append(s1).append(s2).toString();String s4 = "helloworld"; 这就是阿里巴巴文档里为什么规定循环拼接字符串不准使用&quot;+&quot;而必须使用StringBuilder，因为反编译出的字节码文件显示每次循环都会 new 出一个 StringBuilder 对象，然后进行append 操作，最后通过 toString 方法返回 String 对象，造成内存资源浪费。 不恰当的方式形如： 1234String str = "start";for (int i = 0; i &lt; 100; i++) &#123; str = str + "hello";&#125; 好了，言归正传，可以发现，同样是字符串拼接，s3和s4在经过编译器编译后的实现方式并不一样。s3被转化成StringBuilder及append，而s4被直接拼接成新的字符串。 如果你感兴趣，你还能发现，String s4 = s1 + s2; 经过编译之后，常量池中是有两个字符串常量的分别是 hello、world（其实hello和world是String s1 = &quot;hello&quot;;和String s2 = &quot;world&quot;;定义出来的），拼接结果helloworld并不在常量池中。 如果代码只有String s4 = &quot;hello&quot; + &quot;world&quot;;，那么常量池中将只有helloworld而没有hello和 world。 究其原因，是因为常量池要保存的是已确定的字面量值。也就是说，对于字符串的拼接，纯字面量和字面量的拼接，会把拼接结果作为常量保存到字符串。 如果在字符串拼接中，有一个参数是非字面量，而是一个变量的话，整个拼接操作会被编译成StringBuilder.append，这种情况编译器是无法知道其确定值的。只有在运行期才能确定。 那么，有了这个特性了，intern就有用武之地了。那就是很多时候，我们在程序中用到的字符串是只有在运行期才能确定的，在编译期是无法确定的，那么也就没办法在编译期被加入到常量池中。 这时候，对于那种可能经常使用的字符串，使用intern进行定义，每次JVM运行到这段代码的时候，就会直接把常量池中该字面值的引用返回，这样就可以减少大量字符串对象的创建了。 总结 第一种情况： 12String str1 = "abc"; System.out.println(str1 == "abc"); 栈中开辟一块空间存放引用str1； String池中开辟一块空间，存放String常量&quot;abc&quot;； 引用str1指向池中String常量&quot;abc&quot;； str1所指代的地址即常量&quot;abc&quot;所在地址，输出为true 第二种情况： 12String str2 = new String("abc"); System.out.println(str2 == "abc"); 栈中开辟一块空间存放引用str2； 堆中开辟一块空间存放一个新建的String对象&quot;abc&quot;； 引用str2指向堆中的新建的String对象&quot;abc&quot;； str2所指代的对象地址为堆中地址，而常量&quot;abc&quot;地址在池中，输出为false； 第三、四种情况 1234567891011121314//（3）String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//str1 和 str2 是字符串常量，所以在编译期就确定了。//str3 中有个 str1 是引用，所以不会在编译期确定。//又因为String是 final 类型的，所以在 str1 + "b" 的时候实际上是创建了一个新的对象，在把新对象的引用传给str3。//（4）final String str1 = "a"；String str2 = "b"；String str3 = str1 + "b"；//这里和(3)的不同就是给 str1 加上了一个final，这样str1就变成了一个常量。//这样 str3 就可以在编译期中就确定了 这里的细节在上面已经详细说明了。 第五种情况 1234String str1 = "ab"；String str2 = new String("ab");System.out.println(str1== str2);//falseSystem.out.println(str2.intern() == str1);//true 整理自： 我终于搞清楚了和String有关的那点事儿 https://www.jianshu.com/p/2624036c9daa]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java字符串]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2Fjava%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[关于java字符串不可变特性的深入理解。 什么是不可变对象？ 众所周知， 在Java中， String类是不可变的。那么到底什么是不可变的对象呢？ 可以这样认为：如果一个对象，在它创建完成之后，不能再改变它的状态，那么这个对象就是不可变的。不能改变状态的意思是，不能改变对象内的成员变量，包括基本数据类型的值不能改变，引用类型的变量不能指向其他的对象，引用类型指向的对象的状态也不能改变。 区分对象和对象的引用 对于Java初学者， 对于String是不可变对象总是存有疑惑。看下面代码： 12345String s = "ABCabc"; System.out.println("s = " + s); s = "123456"; System.out.println("s = " + s); 打印结果: 12s = ABCabcs = 123456 首先创建一个 String 对象 s ，然后让 s 的值为 ABCabc ， 然后又让 s 的值为 123456 。 从打印结果可以看出，s 的值确实改变了。那么怎么还说 String 对象是不可变的呢？ 其实这里存在一个误区：s只是一个String对象的引用，并不是对象本身。 对象在内存中是一块内存区，成员变量越多，这块内存区占的空间越大。 引用只是一个4字节的数据，里面存放了它所指向的对象的地址，通过这个地址可以访问对象。 也就是说，s 只是一个引用，它指向了一个具体的对象，当 s=“123456”; 这句代码执行过之后，又创建了一个新的对象“123456”， 而引用s重新指向了这个新的对象，原来的对象“ABCabc”还在内存中存在，并没有改变。内存结构如下图所示： 为什么String对象是不可变的？ 要理解 String 的不可变性，首先看一下 String 类中都有哪些成员变量。 在JDK1.6中，String 的成员变量有以下几个： 1234567891011121314public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 在JDK1.7和1.8中，String 类做了一些改动，主要是改变了substring方法执行时的行为，这和本文的主题不相关。JDK1.7中 String 类的主要成员变量就剩下了两个： 1234567public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 由以上的代码可以看出， 在Java中 String 类其实就是对字符数组的封装。 JDK6中， value是String封装的数组，offset是String在这个value数组中的起始位置，count是String所占的字符的个数。 在JDK7中，只有一个value变量，也就是value中的所有字符都是属于String这个对象的。这个改变不影响本文的讨论。 除此之外还有一个hash成员变量，是该 String 对象的哈希值的缓存，这个成员变量也和本文的讨论无关。在Java中，数组也是对象。 所以value也只是一个引用，它指向一个真正的数组对象。其实执行了 1String s = “ABCabc"; 这句代码之后，真正的内存布局应该是这样的： value，offset和count这三个变量都是private的，并且没有提供setValue， setOffset和setCount等公共方法来修改这些值，所以在String类的外部无法修改String。也就是说一旦初始化就不能修改， 并且在String类的外部不能访问这三个成员。 此外，value，offset和count这三个变量都是final的， 也就是说在 String 类内部，一旦这三个值初始化了， 也不能被改变。所以可以认为 String 对象是不可变的了。 那么在 String 中，明明存在一些方法，调用他们可以得到改变后的值。这些方法包括substring， replace， replaceAll， toLowerCase等。例如如下代码： 1234String a = "ABCabc"; System.out.println("a = " + a); //ABCabca = a.replace('A', 'a'); System.out.println("a = " + a); //aBCabc 那么a的值看似改变了，其实也是同样的误区。再次说明， a只是一个引用， 不是真正的字符串对象，在调用a.replace('A', 'a')时， 方法内部创建了一个新的String对象，并把这个心的对象重新赋给了引用a。String中replace方法的源码可以说明问题： 1234567891011121314151617181920212223242526public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(buf, true);//new出了新的String对象 &#125; &#125; return this;&#125; String对象真的不可变吗？ 从上文可知String的成员变量是private final 的，也就是初始化之后不可改变。那么在这几个成员中， value比较特殊，因为他是一个引用变量，而不是真正的对象。 value是final修饰的，也就是说final不能再指向其他数组对象，那么我能改变value指向的数组吗？ 比如将数组中的某个位置上的字符变为下划线“_”。 至少在我们自己写的普通代码中不能够做到，因为我们根本不能够访问到这个value引用，更不能通过这个引用去修改数组。 那么用什么方式可以访问私有成员呢？ 没错，用反射， 可以反射出String对象中的value属性， 进而改变通过获得的value引用改变数组的结构。下面是实例代码： 123456789101112131415161718192021public static void testReflection() throws Exception &#123; //创建字符串"Hello World"， 并赋给引用s String s = "Hello World"; System.out.println("s = " + s); //Hello World //获取String类中的value字段 Field valueFieldOfString = String.class.getDeclaredField("value"); //改变value属性的访问权限 valueFieldOfString.setAccessible(true); //获取s对象上的value属性的值 char[] value = (char[]) valueFieldOfString.get(s); //改变value所引用的数组中的第5个字符 value[5] = '_'; System.out.println("s = " + s); //Hello_World &#125; 在这个过程中，s始终引用的同一个 String 对象，但是再反射前后，这个 String 对象发生了变化， 也就是说，通过反射是可以修改所谓的“不可变”对象的。但是一般我们不这么做。 这个反射的实例还可以说明一个问题：如果一个对象，他组合的其他对象的状态是可以改变的，那么这个对象很可能不是不可变对象。例如一个Car对象，它组合了一个Wheel对象，虽然这个Wheel对象声明成了private final 的，但是这个Wheel对象内部的状态可以改变， 那么就不能很好的保证Car对象不可变。 参考： https://blog.csdn.net/zhangjg_blog/article/details/18319521]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer拆箱和装箱]]></title>
    <url>%2F2019%2F01%2F23%2Fjava-basic%2FInteger%E6%8B%86%E7%AE%B1%E5%92%8C%E8%A3%85%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[由于笔试经常遇到，所以这里整理一下。将Integer这一块一网打尽。 拆箱和装箱 这里以面试笔试经常出现的Integer类型为例，请看下面的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; /*第一组*/ Integer i = new Integer(128); Integer i2 = 128; System.out.println(i == i2); Integer i3 = new Integer(127); Integer i4 = 127; System.out.println(i3 == i4); /*第二组*/ Integer i5 = 128; Integer i6 = 128; System.out.println(i5 == i6); Integer i7 = 127; Integer i8 = 127; System.out.println(i7 == i8); /*第三组*/ Integer i9 = new Integer(128); int i10 = 128; System.out.println(i9 == i10); Integer i11 = new Integer(127); int i12 = 127; System.out.println(i11== i12); /*第四组*/ Integer i13 = new Integer(128); Integer i14 = Integer.valueOf(128); System.out.println(i13 == i14); Integer i15 = new Integer(127); Integer i16 = Integer.valueOf(127); System.out.println(i13 == i14); /*第五组*/ Integer i17 = Integer.valueOf(128); Integer i18 = 128; System.out.println(i17 == i18); Integer i19 = Integer.valueOf(127); Integer i20 = 127; System.out.println(i19 == i20);&#125; 执行结果为： 1234567891011121314falsefalsefalsetruetruetruefalsefalsefalsetrue 翻开源码(jdk8)，我们可以看到一个私有的静态类，叫做整形缓存。顾名思义，就是缓存某些整型值，我们可以看到，它默认将-127-128之间数字封装成对象，放进一个常量池中，以后定义类似于Integer a = 1里面的a就可以直接从这个常量池中取对象即可，不需要重新new 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 对于Integer.valueOf(): 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 我们可以看到，也是先看看是不是在-127到128之间的范围，是的话就从 cache 中取出相应的 Integer 对象即可。 第一组中 第一种情况，i 是创建的一个Integer的对象，取值是128。i2 是进行自动装箱的实例，因为这里超出了-128到127的范围，所以是创建了新的Integer对象。由于==比较的是地址，所以两者必然不一样。 第二种情况就不一样了，i4是不需要自己new，而是可以直接从缓存中取，但是i3是new出来的，地址还是不一样。 第二组中 第一种情况是都超出范围了，所以都要自己分别去new，所以不一样 第二种情况是在范围内，都去缓存中取，实际上都指向同一个对象，所以一样 第三组中 i10和i12都是int型，i9和i11与它们比较的时候都要自动拆箱，所以比较的是数值，所以都一样 第四组中 与第一组原理一样 四五组中 与第二组原理一样 所以啊，new Integer()是每次都直接new对象出来，而Integer.valueOf()可能会用到缓存，所以后者效率高一点。 总结 int 和 Integer 在进行比较的时候， Integer 会进行拆箱，转为 int 值与int` 进行比较。 Integer 与 Integer 比较的时候，由于直接赋值的时候会进行自动的装箱，那么这里就需要注意两个问题，一个是 -128&lt;= x&lt;=127 的整数，将会直接缓存在 IntegerCache 中，那么当赋值在这个区间的时候，不会创建新的 Integer 对象，而是从缓存中获取已经创建好的 Integer 对象。二：当大于这个范围的时候，直接 new Integer 来创建 Integer 对象。 new Integer(1) 和 Integer a = 1 不同，前者会创建对象，存储在堆中，而后者因为在-128到127的范围内，不会创建新的对象，而是从 IntegerCache 中获取的。那么 Integer a = 128, 大于该范围的话才会直接通过 new Integer(128)创建对象，进行装箱。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务解决方案思考]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F10%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[在分布式系统中，最头疼的就是分布式事务问题，处理起来一定要小心翼翼。由于没有此方面实战，本文就从理论上看看比较好的分布式事务处理方案。 什么是分布式事务 众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？ 比如用户下单过程。当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 方案1：基于可靠消息服务的分布式事务 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程中，如果任务A处理失败，那么需要进入回滚流程: 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 注意，这个方案需要消息队列具有事务消息的能力，阿里的RocketMQ可以实现这个目标。其他的MQ还不行。 方案2：最大努力通知（定期校对） 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。 如果这两步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第一种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。还有其他的一些解决思路，这里就暂时只描述这些。后续再学习。 参考自：https://juejin.im/post/5aa3c7736fb9a028bb189bca]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[库存扣减问题]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F09%E5%BA%93%E5%AD%98%E6%89%A3%E5%87%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[库存扣减问题一直是秒杀中最关键的一个点。如果把控不好，扣成负数，那可就麻烦了，如么如何保证不会出现超卖还能保证性能呢？ 一、扣减库存问题分析 在提交订单的时候，要扣减库存，对于sql，是这么写的： 1update t_stcok set stock = stock-2 where sku_id = 1 首先这条sql存在超卖问题，很有可能会减成负数。可能会改成如下： 1update t_stcok set stock = stock-2 where sku_id = 1 and stock &gt; 2 这样好像解决了超卖问题。但是引入了新的问题。由于库存牵涉进货、补货等系统，所以是个独立的服务。 并且，比如我是通过MQ去通知库存进行扣减库存，但是由于网络抖动，请求扣减库存没有结果，这个时候可能需要进行重试。重试之后，可能成功了，这个时候，有可能这两次都成功了。那么，一个用户买一样东西，但是库存扣了两遍。这就是幂等。如果不做幂等处理，重试会出现上述这种致命问题。 那么如何做到幂等呢？ 实际上就是追求数据一致性。那么就可以考虑锁来保证，比如我这里用乐观锁来实现： 123select stock,version from t_stock;if(stock &gt; 用户购买数量) update t_stcok set stock = stock-2 where sku_id = 1 and version = last_version 但是，一旦出现并发，那么可能这个用户是执行update失败的，所以还需要去重试(guava retry或者spring retry都可以优雅地实现重试)，直到成功或者库存已经不足。 那么，在少量并发的情况下，可以考虑乐观锁，要不然会大量失败，此时需要用悲观锁： 12select * from t_stock for update;下面执行update操作。。。 在一个事务内，第一句为select for update，那么这一行数据就会被本线程锁住，整个事务执行完才能允许其他线程进来。 存在的问题：一个线程锁住这行数据，那么其他线程都要等待，效率很低。 那么，如何保证数据一致性，还可以提高效率呢？ 对于扣减库存，往往是先在redis中进行扣减库存。redis是单线程，是高速串行执行，不存在并发问题。 如果是单机redis，可以在同一个事务中保证一次性执行: 12345watch stockmultiif stock &gt; count stock = stock - count;exec 但是不能在集群中用（分布在不同节点上时），所以用watch不通用。 redis都是原子操作，比如自增:incrby，用这个就可以判断库存是否够。就是所谓的redis预减库存。 但是在实际中，库存表里有两个字段：库存和锁定库存。 锁定库存是表示多少用户真正下单了，但是还没有支付。锁定库存+库存=总库存，等用户真正支付之后，就可以将锁定库存减掉。那么，此时，redis中需要存库存和锁定库存这两个值，上面单一的原子操作就不行了。 解决方案：redis+lua 为什么要用lua呢？可以用lua将一系列操作封装起来执行，输入自己的参数即可。lua脚本在redis中执行是串行的、原子性的。 OK，下面就实战一波：根据skuId查询缓存中的库存值。 二、查询库存（设置库存） 首先，我们要明确一点，redis中的库存初始值是由后台的系统人工提前配置好的，在进行商品销售时（用户下单时），直接从redis中先进行库存的扣减。 这里呢，我们没有进行初始化，而是在程序中进行判断：如果redis已经有了这个库存值，就将他查询出来返回；否则，就去数据库查询，然后对redis进行初始化。 这里的一个问题是：如果存在并发问题，但是我们初始化两个值（库存值和库存锁定值），这里采用lua脚本，在lua脚本中完成初始化，并且对于两个用户同时进行初始化库存的问题，可以在lua中进行判断,因为redis是单线程，lua也是单线程，不用担心会同时初始化两次。 下面首先写一个接口，根据skuid查询库存(库存和锁定库存)： 123456789101112@RequestMapping("/query/&#123;skuId&#125;")public ApiResult&lt;Stock&gt; queryStock(@PathVariable long skuId)&#123; ApiResult&lt;Stock&gt; result = new ApiResult(Constants.RESP_STATUS_OK,"库存查询成功"); Stock stock = new Stock(); stock.setSkuId(skuId); int stockCount = stockService.queryStock(skuId); stock.setStock(stockCount); result.setData(stock); return result;&#125; service层： 123456789101112131415161718192021222324252627282930@Overridepublic int queryStock(long skuId) &#123; //先查redis Stock stock ; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+skuId; String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+skuId; //只需要查询一个即可，比如我这里只查询库存就行 Object stockObj = redisTemplate.opsForValue().get(stockKey); Integer stockInRedis = null ; if(stockObj!=null)&#123; stockInRedis = Integer.valueOf(stockObj.toString()); &#125; //没有，那么我就需要将数据库中的数据初始化到redis中 if(stockInRedis==null)&#123; //去数据库查询 然后对redis进行初始化 stock = stockMapper.selectBySkuId(skuId); //两个key和两个库存值通过lua脚本塞到redis中 //这里如果发生两个用户并发初始化redis，脚本中会进行判断，如果已经初始化了，脚本就会停止执行 // 设置库存不应该在这配置，应该是后台管理系统进行设置，所以正常情况下，这里redis中应该是必然存在的 //如果是在后台配置，就没有必要这么复杂了 redisUtils.skuStockInit(stockKey,stockLockKey,stock.getStock().toString(),stock.getLockStock().toString()); &#125;else&#123; return stockInRedis;//缓存中有就直接返回 &#125; //缓存结果可能会返回设置不成功，所以还是返回数据库查询结果 return stock.getStock();&#125; 那么这个工具类为： 123456789101112131415161718192021222324252627282930313233/** * 查看redis是否已经初始化好库存初始值，没有就初始化 */public static final String STOCK_CACHE_LUA = "local stock = KEYS[1] " + "local stock_lock = KEYS[2] " + "local stock_val = tonumber(ARGV[1]) " + "local stock_lock_val = tonumber(ARGV[2]) " + "local is_exists = redis.call(\"EXISTS\", stock) " + "if is_exists == 1 then " + " return 0 " + "else " + " redis.call(\"SET\", stock, stock_val) " + " redis.call(\"SET\", stock_lock, stock_lock_val) " + " return 1 " + "end"; /** * @Description 缓存sku库存 以及锁定库存 */public boolean skuStockInit(String stockKey,String stockLockKey,String stock,String stockLock)&#123; //用jedis去执行lua脚本 输入的参数要注意顺序 都是写死的 第一组是key，第二组是stock Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_CACHE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stock, stockLock))); &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 对于lua脚本进行稍微的解释一下： 12345678910111213141516171819202122//第一组数据是key数组；第二组数据是args数组，是与key数组对应的值，就是库存//我们这里第一组为[stockKey,stockLockKey],就是存在redis中的名字，这里是在service层中定义好了//第二组为[50,0]，这个值就是可以从数据库表t_stock中查询出来的//因为执行这段lua脚本的话，说明redis中没有缓存的数据，所以需要先查询数据库，然后将缓存设置好//lua中定义变量用locallocal stock = KEYS[1]local stock_lock = KEYS[2]local stock_val = tonumber(ARGV[1])local stock_lock_val = tonumber(ARGV[2])//再查询一遍缓存是否存在，防止两个线程同时进来设置缓存//存在就不用设置缓存了，否则就设置缓存local is_exists = redis.call("EXISTS", stock)if is_exists == 1 then return 0else redis.call("SET", stock, stock_val) redis.call("SET", stock_lock, stock_lock_val) return 1end 那么，启动工程mama-buy-stock：假如我去查询skuId=1的商品： 第一次库存不存在，那么就会去查询数据库： 我们再来看看redis中的数据： 三、扣减库存 下面来看看扣减库存是如何实现的。因为提交订单后，往往是不止一件商品的，往往购物车内有很多件商品，同时过来，假设有五件商品，但是其中只有一件暂时没有库存了，那么我还是希望其他的四件商品能够卖出去，只是没有库存的商品就不算钱了。所以扣减库存用一个map来装，即Map&lt;skuId,count&gt; controller层： 1234567@RequestMapping("/reduce")public ApiResult&lt;Map&lt;Long,Integer&gt;&gt; reduceStock(@RequestBody List&lt;StockReduce&gt; stockReduceList)&#123; ApiResult result = new ApiResult(Constants.RESP_STATUS_OK,"库存扣减成功"); Map&lt;Long,Integer&gt; resultMap = stockService.reduceStock(stockReduceList); result.setData(resultMap); return result;&#125; service层： 1234567891011121314151617181920212223242526272829303132333435@Override@Transactionalpublic Map&lt;Long, Integer&gt; reduceStock(List&lt;StockReduce&gt; stockReduceList) &#123; Map&lt;Long, Integer&gt; resultMap = new HashMap&lt;&gt;(); //遍历去减redis中库存，增加锁定库存 stockReduceList.stream().forEach(param -&gt; &#123; String stockKey = Constants.CACHE_PRODUCT_STOCK+":"+param.getSkuId(); String stockLockKey = Constants.CACHE_PRODUCT_STOCK_LOCK+":"+param.getSkuId(); Object result = redisUtils.reduceStock(stockKey, stockLockKey, param.getReduceCount().toString(),//incrby一个负数，就是减 String.valueOf(Math.abs(param.getReduceCount())));//incrby一个正数，就是加 if(result instanceof Long)&#123; //库存不存在或者不足 扣减失败 sku下单失败 记录下来 resultMap.put(param.getSkuId(),-1); &#125;else if (result instanceof List)&#123; //扣减成功 记录扣减流水 List resultList = ((List) result); int stockAftChange = ((Long)resultList.get(0)).intValue(); int stockLockAftChange = ((Long) resultList.get(1)).intValue(); StockFlow stockFlow = new StockFlow(); stockFlow.setOrderNo(param.getOrderNo()); stockFlow.setSkuId(param.getSkuId()); stockFlow.setLockStockAfter(stockLockAftChange); stockFlow.setLockStockBefore(stockLockAftChange+param.getReduceCount()); stockFlow.setLockStockChange(Math.abs(param.getReduceCount())); stockFlow.setStockAfter(stockAftChange); stockFlow.setStockBefore(stockAftChange+Math.abs(param.getReduceCount())); stockFlow.setStockChange(param.getReduceCount()); stockFlowMapper.insertSelective(stockFlow); resultMap.put(param.getSkuId(),1); &#125; &#125;); return resultMap;&#125; 对于redis的操作，基本与上一致： 12345678910111213141516171819202122232425262728293031323334/** * @Description 扣减库存lua脚本 * @Return 0 key不存在 错误 -1 库存不足 返回list 扣减成功 */public static final String STOCK_REDUCE_LUA= "local stock = KEYS[1]\n" + "local stock_lock = KEYS[2]\n" + "local stock_change = tonumber(ARGV[1])\n" + "local stock_lock_change = tonumber(ARGV[2])\n" + "local is_exists = redis.call(\"EXISTS\", stock)\n" + "if is_exists == 1 then\n" + " local stockAftChange = redis.call(\"INCRBY\", stock,stock_change)\n" + " if(stockAftChange&lt;0) then\n" + " redis.call(\"DECRBY\", stock,stock_change)\n" + " return -1\n" + " else \n" + " local stockLockAftChange = redis.call(\"INCRBY\", stock_lock,stock_lock_change)\n" + " return &#123;stockAftChange,stockLockAftChange&#125;\n" + " end " + "else \n" + " return 0\n" + "end"; public Object reduceStock(String stockKey,String stockLockKey,String stockChange,String stockLockChange)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(STOCK_REDUCE_LUA, Collections.unmodifiableList(Arrays.asList(stockKey,stockLockKey)) ,Collections.unmodifiableList(Arrays.asList(stockChange, stockLockChange))); &#125;); return result;&#125; 此时，一旦数据库发生异常，那么就会回滚，但是redis中是无法回滚的。这个问题不用担心，因为数据库发生异常是及其严重的问题，是很少会发生的，一旦发生，只需要去这个流水的表中去查看情况，然后去执行脚本去初始化这个redis即可。所以是可以补救的。 但是接口的幂等性还没有做。重复尝试调用这个接口（通常是发生在MQ的失败重传机制，客户端的连续点击一般是可以避免的），可能会重复减redis库存并且重复地去插入流水记录。这个问题该如何解决呢？ 四、redis分布式锁来实现幂等性 主流的方案，比如有用一张表来控制，比如以这个orderID为唯一主键，一旦插入成功，就可以根据这个唯一主键的存在与否判断是否为重复请求（也就是说，这里的扣减库存和插入去重表放在一个事务里，去重表中有一个字段为orderId，全局唯一不重复，用唯一索引进行约束，那么插入的时候判断这个去重表是否可以插入成功，如果不成功，那么数据库操作全部回滚）。 可以用redis分布式锁给这个订单上锁。以订单id为锁，不会影响其他线程来扣减库存，所以不影响性能。 针对这个订单，第一次肯定是可以去扣减库存的，但是第二次再接收到这个请求，那么就要返回已经成功了，不要再重复扣减。 对于reduceStock()这个方法最前面增加锁： 123456789//防止扣减库存时MQ正常重试时的不幂等//以订单ID 加个缓存锁 防止程序短时间重试 重复扣减库存 不用解锁 自己超时Long orderNo = stockReduceList.get(0).getOrderNo();boolean lockResult = redisUtils.distributeLock(Constants.ORDER_RETRY_LOCK+orderNo.toString(),orderNo.toString(),300000);if(!lockResult)&#123; //锁定失败 重复提交 返回一个空map return Collections.EMPTY_MAP;&#125;... 123456789101112131415161718192021222324252627282930313233343536373839404142private static final String LOCK_SUCCESS = "OK";private static final String SET_IF_NOT_EXIST = "NX";private static final String SET_WITH_EXPIRE_TIME = "PX";private static final Long EXCUTE_SUCCESS = 1L;/**lua脚本 在redis中 lua脚本执行是串行的 原子的 */private static final String UNLOCK_LUA= "if redis.call('get', KEYS[1]) == ARGV[1] then " + " return redis.call('del', KEYS[1]) " + "else " + " return 0 " + "end";/** * @Description 获取分布式锁 * @Return boolean */public boolean distributeLock(String lockKey, String requestId, int expireTime)&#123; String result = redisTemplate.execute((RedisCallback&lt;String&gt;) redisConnection -&gt; &#123; JedisCommands commands = (JedisCommands)redisConnection.getNativeConnection(); return commands.set(lockKey,requestId,SET_IF_NOT_EXIST,SET_WITH_EXPIRE_TIME,expireTime);//一条命令实现setnx和setexpire这些操作，原子性 &#125;); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125;/** * @Description 释放分布式锁 * @Return boolean */public boolean releaseDistributelock(String lockKey, String requestId)&#123; Object result = redisTemplate.execute((RedisCallback&lt;Object&gt;) redisConnection -&gt; &#123; Jedis jedis = (Jedis)redisConnection.getNativeConnection(); return jedis.eval(UNLOCK_LUA, Collections.singletonList(lockKey), Collections.singletonList(requestId));//lua脚本中原子性实现：get查询和delete删除这两个操作 &#125;); if (EXCUTE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 注意，这里不需要我们主动去释放分布式锁，只要设置一个大于重试时间的过期时间即可。让它自己删除。 注意redis在集群下做分布式锁，最好要用Redission。这里如果用于集群，如何lua脚本在一个事务里同时操作多个key的时候，如果要保证这个事务生效，就需要保证这几个key都要在同一个节点上。但是，比如我们这里的两个key： 12public static final String CACHE_PRODUCT_STOCK = "product:stock";public static final String CACHE_PRODUCT_STOCK_LOCK = "product:stock:lock"; 因为我们这里要同时对库存和锁定库存这两个key进行操作，需要放在一个事务内执行，不处理的话，一旦他们不在一个节点，那么事务就不会生效，解决方案： 12public static final String CACHE_PRODUCT_STOCK = "&#123;product:stock&#125;";public static final String CACHE_PRODUCT_STOCK_LOCK = "&#123;product:stock&#125;:lock"; 如果加上花括号，那么在进行计算hash值的时候，他们两就会是一样的，会被投放到同一个slot中，自然就保证了在同一个节点上。 五、测试一下]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK平台搭建]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F08ELK%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[因为要完成产品的全文搜索这个功能，所以需要准备一下ES的环境。本节安装ELK。 ELK由Elasticsearch、Logstash和Kibana三部分组件组成。 前言 Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 简单来说，他是个全文搜索引擎，可以快速地储存、搜索和分析海量数据。 Logstash是一个完全开源的工具，它可以把分散的、多样化的日志日志，或者是其他数据源的数据信息进行收集、分析、处理，并将其存储供以后使用。 Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。 你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。 你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。 Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。 一、安装ES 1.1 首先是安装JDK： 12345cd /opt/wget --no-cookies --no-check-certificate --header &quot;Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie&quot; &quot;http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz&quot;tar xzf jdk-8u141-linux-x64.tar.gz 1.2 添加环境变量： 123456789101112vim /etc/profileJAVA_HOME=/opt/jdk1.8.0_141JAVA_JRE=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATHsource /etc/profilejava -version 1.3 下载6.2.4版本： 123456wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gztar -xzvf elasticsearch-6.2.4.tar.gztar -zxvf elasticsearch-6.2.4.tar.gzmv elasticsearch-6.2.4 elasticsearch 1.4 配置sysctl.conf 1234567891011#修改sysctl配置vim /etc/sysctl.conf #添加如下配置vm.max_map_count=262144 #让配置生效sysctl -p #查看配置的数目sysctl -a|grep vm.max_map_count 1.5 elasticsearch从5.0版本之后不允许root账户启动 1234567891011121314151617181920#添加用户adduser dev #设定密码passwd dev #添加权限chown -R dev /opt/elasticsearch #切换用户su dev #查看当前用户who am i #启动./elasticsearch/bin/elasticsearch #后台启动./elasticsearch/bin/elasticsearch -d 1.6 配置limits.conf 123456789101112131415vim /etc/security/limits.conf 把* soft nofile 65535* hard nofile 65535 改为* soft nofile 65536* hard nofile 65536 #切换用户su dev #查看配置是否生效ulimit -Hn 1.7 配置所有用户访问 1vim /opt/elasticsearch/config/elasticsearch.yml 1.8 添加一下内容 1network.host: 0.0.0.0 1.9 重启 12ps -ef | grep elastickill -9 xxxx 1.10 测试： 1curl http://localhost:9200/ 显示： 123456789101112131415&#123; &quot;name&quot; : &quot;MmiaBfA&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;zjX-q5PDRLyrWMy5TiBDkw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.2.4&quot;, &quot;build_hash&quot; : &quot;ccec39f&quot;, &quot;build_date&quot; : &quot;2018-04-12T20:37:28.497551Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.2.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 就说明成功了。 二、安装Kibana 6.2.4 1234567wget https://artifacts.elastic.co/downloads/kibana/kibana-6.2.4-linux-x86_64.tar.gztar -zxvf kibana-6.2.4-linux-x86_64.tar.gzmv kibana-6.2.4-linux-x86_64 kibanavim /opt/kibana/config/kibana.yml 2.1 添加以下内容： 123server.port: 5601server.host: &quot;0.0.0.0&quot;elasticsearch.url: &quot;http://127.0.0.1:9200&quot; 2.2 切换到bin目录下，启动即可。 12345#不能关闭终端./kibana #可关闭终端nohup ./kibana &amp; 2.3 开放防火墙和安全组对应的这个端口 浏览器访问：http://106.14.163.235:5601 看到一个控制台页面就成功啦。 2.4 关闭这个进程 1234567891011121314ps -ef|grep kibana ps -ef|grep 5601 都找不到 尝试 使用 fuser -n tcp 5601 kill -9 端口 启动即可 ./kibana或者去这个目录下的.out日志中可以看到看到它占用的pid 三、logstash 1234567891011# 下载wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.4.tar.gz# 解压tar -zxvf logstash-6.2.4.tar.gz# 重命名mv logstash-6.2.4.tar.gz logstash# 进入cd logstash 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 新建一个配置文件 我这里是mysqltones.confinput &#123; stdin &#123; &#125; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/mama-buy-trade&quot; jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;root&quot; jdbc_driver_library =&gt; &quot;/opt/logstash/mysql-connector-java-5.1.46-bin.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; # mysql文件, 也可以直接写SQL语句在此处，如下： statement =&gt; &quot;SELECT * from t_product&quot; # statement_filepath =&gt; &quot;/opt/logstash/conf/jdbc.sql&quot; # 这里类似crontab,可以定制定时操作，比如每10分钟执行一次同步(分 时 天 月 年) schedule =&gt; &quot;*/10 * * * *&quot; type =&gt; &quot;jdbc&quot; # 是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中 record_last_run =&gt; &quot;true&quot; # 是否需要记录某个column 的值,如果record_last_run为真,可以自定义我们需要 track 的 column 名称，此时该参数就要为 true. 否则默认 track 的是 timestamp 的值. use_column_value =&gt; &quot;true&quot; # 如果 use_column_value 为真,需配置此参数. track 的数据库 column 名,该 column 必须是递增的. 一般是mysql主键 tracking_column =&gt; &quot;id&quot; last_run_metadata_path =&gt; &quot;/opt/logstash/conf/last_id&quot; # 是否清除 last_run_metadata_path 的记录,如果为真那么每次都相当于从头开始查询所有的数据库记录 clean_run =&gt; &quot;false&quot; # 是否将 字段(column) 名称转小写 lowercase_column_names =&gt; &quot;false&quot; &#125;&#125;# 此处我不做过滤处理filter &#123;&#125;output &#123; # 输出到elasticsearch的配置 elasticsearch &#123; hosts =&gt; [&quot;127.0.0.1:9200&quot;] index =&gt; &quot;jdbc&quot; # 将&quot;_id&quot;的值设为mysql的autoid字段 document_id =&gt; &quot;%&#123;id&#125;&quot; template_overwrite =&gt; true &#125; # 这里输出调试，正式运行时可以注释掉 stdout &#123; codec =&gt; json_lines &#125;&#125; 12# 启动./bin/logstash -f ./mysqltones.conf 看到这个就说明成功了： 安装mysql数据库 这一步要在执行logstash之前搞定，我的是阿里云centos7.3版本，mysql版本是5.7，安装过程如下： 123456789101112131415161718192021222324252627282930313233343536373839# 下载MySQL源安装包: wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm# 安装MySQL源：yum localinstall mysql57-community-release-el7-8.noarch.rpm # 检查MySQL源安装情况： yum repolist enabled | grep &quot;mysql.*-community.*&quot;# 安装MySQL: yum install mysql-community-server# 启动MySQL: systemctl start mysqld# 查看MySQL状态: systemctl status mysqld# 设置开机启动MySQL：systemctl enable mysqld systemctl daemon-reload# 查找并修改MySQL默认密码（注意密码要符合规范，否则会失败）：grep &apos;temporary password&apos; /var/log/mysqld.log mysql -uroot -p alter user root@localhost identified by &apos;你的新密码&apos;;# 远程连接测试添加远程账户：GRANT ALL PRIVILEGES ON *.* TO &apos;用户&apos;@&apos;%&apos; IDENTIFIED BY &apos;密码&apos; WITH GRANT OPTION;# 立即生效：flush privileges;# 退出MySQL：exit# 最后远程将数据给导入数据库 安装分词器 ik_max_word是分词比较细腻的一款，我们就用它来做分词，首先需要安装一下： 12345678# 直接安装./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.4/elasticsearch-analysis-ik-6.2.4.zip # 重新启动ESps -ef | grep elastickill -9 xxxxsu dev./bin/elasticsearch -d 对这个分词器在kibana中进行测试： 下面结合数据库模拟一下：]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Curator]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F07Curator%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[从技术角度出发，注册一个网站，再高并发的时候，有可能出现用户名重复这样的问题（虽然一般情况下不会出现这种问题），如何解决呢？ 从数据库角度，对于单表，我可以用select .. for update悲观锁实现，或者用version这种乐观锁的思想。 更好的方法是将这个字段添加唯一索引，用数据库来保证不会重复。一旦插入重复，那么就会抛出异常，程序就可以捕获到。 但是，假如我们这里分表了，以上都是针对单表，第一种方案是锁表，不行，设置唯一索引是没有用。怎么办呢？ 解决方案：用ZK做一个分布式锁。 首先准备一个ZK客户端，用的是Curator来连接我们的ZK： 1234567891011121314151617181920@Componentpublic class ZkClient &#123; @Autowired private Parameters parameters; @Bean public CuratorFramework getZkClient()&#123; CuratorFrameworkFactory.Builder builder= CuratorFrameworkFactory.builder() .connectString(parameters.getZkHost()) .connectionTimeoutMs(3000) .retryPolicy(new RetryNTimes(5, 10)); CuratorFramework framework = builder.build(); framework.start(); return framework; &#125;&#125; 注册用一个分布式锁来控制： 1234567891011121314151617181920212223242526272829303132333435@Overridepublic void registerUser(User user) throws Exception &#123; InterProcessLock lock = null; try&#123; lock = new InterProcessMutex(zkClient, Constants.USER_REGISTER_DISTRIBUTE_LOCK_PATH); boolean retry = true; do&#123; if (lock.acquire(3000, TimeUnit.MILLISECONDS))&#123; //查询重复用户 User repeatedUser = userMapper.selectByEmail(user.getEmail()); if(repeatedUser!=null)&#123; throw new MamaBuyException("用户邮箱重复"); &#125; user.setPassword(passwordEncoder.encode(user.getPassword())); user.setNickname("码码购用户"+user.getEmail()); userMapper.insertSelective(user); //跳出循环 retry = false; &#125; //可以适当休息一会...也可以设置重复次数，不要无限循环 &#125;while (retry); &#125;catch (Exception e)&#123; log.error("用户注册异常",e); throw e; &#125;finally &#123; if(lock != null)&#123; try &#123; lock.release(); log.info(user.getEmail()+Thread.currentThread().getName()+"释放锁"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 思路非常简单，就是先尝试上锁，即acquire，但是有可能失败，所以这里用一个超时时间，即3000ms之内上不了锁就失败，进入下一次循环。最后释放锁即可。 ok，这里要来说说ZK实现分布式锁了。这里用了开源客户端Curator，他对于实现分布式锁进行了封装，但是，我还是想了解一下它的实现原理： 每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。 也就是说，最小的那个节点就是Leader，进来判断是不是为那个节点，是的话就可以获取到锁，反之不行。 为什么不能通过大家一起创建节点，如果谁成功了就算获取到了锁。 多个client创建一个同名的节点，如果节点谁创建成功那么表示获取到了锁，创建失败表示没有获取到锁。 答：使用临时顺序节点可以保证获得锁的公平性，及谁先来谁就先得到锁，这种方式是随机获取锁，会造成无序和饥饿。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Session]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F06Spring%20Session%2F</url>
    <content type="text"><![CDATA[在单体应用中，我们经常用http session去管理用户信息，但是到了分布式环境下，显然是不行的，因为session对于不同的机器是隔离的，而http本身是无状态的，那么就无法判断出用户在哪一个服务器上登陆的。这个时候就需要有一个独立的地方存储用户session。spring session可以做到无代码侵入的方式实现分布式session存储。 在spring boot开发中，我们先注册相应bean并且打开相应的注解： 1234567891011121314151617181920212223242526272829@Configuration@EnableRedisHttpSession //(maxInactiveIntervalInSeconds = 604800)//session超时public class HttpSessionConfig &#123; @Autowired private Parameters parameters; @Bean public HttpSessionStrategy httpSessionStrategy()&#123; return new HeaderHttpSessionStrategy(); &#125; @Bean public JedisConnectionFactory connectionFactory()&#123; JedisConnectionFactory connectionFactory = new JedisConnectionFactory(); String redisHost = parameters.getRedisNode().split(":")[0]; int redisPort = Integer.valueOf(parameters.getRedisNode().split(":")[1]); connectionFactory.setTimeout(2000); connectionFactory.setHostName(redisHost); connectionFactory.setPort(redisPort);// connectionFactory.setPassword(parameters.getRedisAuth()); return connectionFactory; &#125;&#125; ok，这样子其实就配置好了，一开始我也云里雾里的，这是啥玩意？ 其实官网的文档中讲的是最准确的。所以还是官网看看吧！ ok，来spring session的官网(https://spring.io/projects/spring-session)来看看把，我们来看看1.3.4GA版本的文档(https://docs.spring.io/spring-session/docs/1.3.4.RELEASE/reference/html5/#httpsession-rest). spring session可以存在很多介质中，比如我们的数据源，比如redis，甚至是mongodb等。但是我们常用的是存在redis中，结合redis的过期机制来做。 所以其实我们只要关心如何跟redis整合，以及restful接口。 我们可以看到一开始文档就告诉我们要配置一下HttpSessionStrategy和存储介质。从HttpSessionStrategy语义就能大致看出配置的是它的策略，是基于header的策略。这个是什么意思，下面会提到。 那么我们就来看看文档吧！ 好了，我们知道了它的基本原理，下面来看看是如何在restful接口中实现用户session的管理的： 也就是说要想在restful接口应用中用这种方式，直接告诉spring session:return new HeaderHttpSessionStrategy();即可。进入源码我们就会知道，它默认给这个header里面放置的一条类似于token的名字是private String headerName = &quot;x-auth-token&quot;;。 那么在用户登陆成功之后，到底存到是什么呢，先来看看响应数据的header里面是什么： 这一串数字正好可以跟redis中对应上，我们可以先来redis中看看到底在里面存储了啥玩意： 我们已经看到了想要看到的一串字符串，这里解释一下redis中存储的东西： spring:session是默认的Redis HttpSession前缀（redis中，我们常用’:’作为分割符） 每一个session都会有三个相关的key，第一个key(spring:session:sessions:37...)最为重要，它是一个HASH数据结构，将内存中的session信息序列化到了redis中。如本项目中用户信息,还有一些meta信息，如创建时间，最后访问时间等。 另外两个key，一个是spring:session:expiration，还有一个是spring:session:sessions:expires，前者是一个SET类型，后者是一个STRING类型，可能会有读者发出这样的疑问，redis自身就有过期时间的设置方式TTL，为什么要额外添加两个key来维持session过期的特性呢？redis清除过期key的行为是一个异步行为且是一个低优先级的行为，用文档中的原话来说便是，可能会导致session不被清除。于是引入了专门的expiresKey，来专门负责session的清除，包括我们自己在使用redis时也需要关注这一点。 这样子，就可以用独立的redis来存储用户的信息，通过前端传来的header里面的token，就可以到redis拿出当前登陆用户的信息了。 OK，在解决了spring session的问题之后，下面就可以来实现登陆啦： controller: 1234567891011121314@RequestMapping("login")public ApiResult login(@RequestBody @Valid User user, HttpSession session)&#123; ApiResult&lt;UserElement&gt; result = new ApiResult&lt;&gt;(Constants.RESP_STATUS_OK,"登录成功"); UserElement ue= userService.login(user); if(ue != null)&#123; if(session.getAttribute(Constants.REQUEST_USER_SESSION) == null)&#123; session.setAttribute(Constants.REQUEST_USER_SESSION,ue); &#125; result.setData(ue); &#125; return result;&#125; 就跟以前一样，将session直接存进去就可以了。 12345678910111213141516171819202122@Overridepublic UserElement login(User user) &#123; UserElement ue = null; User userExist = userMapper.selectByEmail(user.getEmail()); if(userExist != null)&#123; //对密码与数据库密码进行校验 boolean result = passwordEncoder.matches(user.getPassword(),userExist.getPassword()); if(!result)&#123; throw new MamaBuyException("密码错误"); &#125;else&#123; //校验全部通过，登陆通过 ue = new UserElement(); ue.setUserId(userExist.getId()); ue.setEmail(userExist.getEmail()); ue.setNickname(userExist.getNickname()); ue.setUuid(userExist.getUuid()); &#125; &#125;else &#123; throw new MamaBuyException("用户不存在"); &#125; return ue;&#125;]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式ID生成策略]]></title>
    <url>%2F2019%2F01%2F23%2Fmiscellany%2F05%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[分布式环境下如何保证ID的不重复呢？一般我们可能会想到用UUID来实现嘛。但是UUID一般可以获取当前时间的毫秒数再加点随机数，但是在高并发下仍然可能重复。最重要的是，如果我要用这种UUID来生成分表的唯一ID的话，重复不谈，这种随机的字符串对于我们的innodb存储引擎的插入效率是很低的。所以我们生成的ID如果作为主键，最好有两种特性：分布式唯一和有序。 唯一性就不用说了，有序保证了对索引字段的插入的高效性。我们来具体看看ShardingJDBC的分布式ID生成策略是如何保证。 snowflake算法 sharding-jdbc的分布式ID采用twitter开源的snowflake算法，不需要依赖任何第三方组件，这样其扩展性和维护性得到最大的简化；但是snowflake算法的缺陷（强依赖时间，如果时钟回拨，就会生成重复的ID）。 雪花算法是由Twitter公布的分布式主键生成算法，它能够保证不同进程主键的不重复性，以及相同进程主键的有序性。 在同一个进程中，它首先是通过时间位保证不重复，如果时间相同则是通过序列位保证。 同时由于时间位是单调递增的，且各个服务器如果大体做了时间同步，那么生成的主键在分布式环境可以认为是总体有序的，这就保证了对索引字段的插入的高效性。例如MySQL的Innodb存储引擎的主键。 使用雪花算法生成的主键，二进制表示形式包含4部分，从高位到低位分表为：1bit符号位、41bit时间戳位、10bit工作进程位以及12bit序列号位。 雪花算法主键的详细结构见下图。 符号位(1bit) 预留的符号位，恒为零。 时间戳位(41bit) 41位的时间戳可以容纳的毫秒数是2的41次幂，一年所使用的毫秒数是：365 * 24 * 60 * 60 * 1000。通过计算可知： 1Math.pow(2, 41) / (365 * 24 * 60 * 60 * 1000L); 结果约等于69.73年。ShardingSphere的雪花算法的时间纪元从2016年11月1日零点开始，可以使用到2086年，相信能满足绝大部分系统的要求。 工作进程位(10bit) 该标志在Java进程内是唯一的，如果是分布式应用部署应保证每个工作进程的id是不同的。该值默认为0，可通过调用静态方法DefaultKeyGenerator.setWorkerId()设置。 序列号位(12bit) 该序列是用来在同一个毫秒内生成不同的ID。如果在这个毫秒内生成的数量超过4096(2的12次幂)，那么生成器会等待到下个毫秒继续生成。 时钟回拨 服务器时钟回拨会导致产生重复序列，因此默认分布式主键生成器提供了一个最大容忍的时钟回拨毫秒数。 如果时钟回拨的时间超过最大容忍的毫秒数阈值，则程序报错；如果在可容忍的范围内，默认分布式主键生成器会等待时钟同步到最后一次主键生成的时间后再继续工作。 最大容忍的时钟回拨毫秒数的默认值为0，可通过调用静态方法DefaultKeyGenerator.setMaxTolerateTimeDifferenceMilliseconds()设置。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springMVC全局异常+spring包扫描包隔离+spring事务传播]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F04springMVC%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%2Bspring%E5%8C%85%E6%89%AB%E6%8F%8F%E5%8C%85%E9%9A%94%E7%A6%BB%2Bspring%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[在开发中，springMVC全局异常+spring包扫描包隔离+spring事务传播这三个不可能不会遇到。下面来好好说说他们吧。 1、全局异常引入原因 假设在我们的login.do的controller方法中第一行增加一句： 1int i = 1/0; 重新启动服务器进行用户登录操作，那么就会抛出异常： 123java.lang.ArithmeticException: / by zero com.swg.controller.portal.UserController.login(UserController.java:37) ...其他的堆栈信息 这些信息会直接显示在网页上，如果是关于数据库的错误，同样，会详细地将数据库中的字段都显示在页面上，这对于我们的项目来说是存在很大的安全隐患的。这个时候，需要用全局异常来处理，如果发生异常，我们就对其进行拦截，并且在页面上显示我们给出的提示信息。 对于SpringBoot，一般全局异常是: 1234567891011121314151617@ControllerAdvice@ResponseBody@Slf4jpublic class ExceptionHandlerAdvice &#123; @ExceptionHandler(Exception.class) public ServerResponse handleException(Exception e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(Constants.RESP_STATUS_INTERNAL_ERROR,"系统异常，请稍后再试"); &#125; @ExceptionHandler(SnailmallException.class) public ServerResponse handleException(SnailmallException e)&#123; log.error(e.getMessage(),e); return ServerResponse.createByErrorCodeMessage(e.getExceptionStatus(),e.getMessage()); &#125;&#125; 2、引入全局异常 12345678910111213@Slf4j@Componentpublic class ExceptionResolver implements HandlerExceptionResolver&#123; @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) &#123; log.error("exception:&#123;&#125;",httpServletRequest.getRequestURI(),e); ModelAndView mv = new ModelAndView(new MappingJacksonJsonView()); mv.addObject("status",ResponseEnum.ERROR.getCode()); mv.addObject("msg","接口异常，详情请查看日志中的异常信息"); mv.addObject("data",e.toString()); return mv; &#125;&#125; 那么，再执行登陆操作之后，就不会在页面上直接显示异常信息了。有效地屏蔽了关键信息。 3、spring和springmvc配置文件的优化 3.1 包隔离优化 在编写全局异常之前，先进行了包隔离和优化，一期中的扫描包的写法是： 12345&lt;!--spring:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt;&lt;!--springmvc:--&gt;&lt;context:component-scan base-package="com.swg" annotation-config="true"/&gt; 即spring和springmvc扫描包下面的所有的bean和controller.优化后的代码配置： 1234567891011#spring&lt;context:component-scan base-package="com.swg" annotation-config="true"&gt;&lt;!--将controller的扫描排除掉--&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt;#springmvc&lt;context:component-scan base-package="com.swg.controller" annotation-config="true" use-default-filters="false"&gt;&lt;!--添加白名单，只扫描controller，总之要将service给排除掉即可--&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt; 这样做的原因是：Spring和SpringMVC是有父子容器关系的，而且正是因为这个才往往会出现包扫描的问题。 针对包扫描只要记住以下几点即可： spring是父容器，springmvc是子容器，子容器可以访问父容器的bean,父容器不能访问子容器的bean。 只有顶级容器（spring）才有加强的事务能力，而springmvc容器的service是没有的。 如果springmvc不配置包扫描的话，页面404. 3.2 事务的传播机制 针对事务，不得不展开说明spring事务的几种传播机制了。在 spring 的 TransactionDefinition 接口中一共定义了七种事务传播属性： PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择（默认）。 PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 4、补充 Spring默认情况下，会对运行期例外(RunTimeException)，即uncheck异常，进行事务回滚。如果遇到checked异常就不回滚。如何改变默认规则： 让checked例外也回滚：在整个方法前加上 @Transactional(rollbackFor=Exception.class) 让unchecked例外不回滚： @Transactional(notRollbackFor=RunTimeException.class) 不需要事务管理的(只查询的)方法：@Transactional(propagation=Propagation.NOT_SUPPORTED) 5、那么什么是嵌套事务呢？ 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫做save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点在于那个save point，看以下几个问题： 问题1：如果子事务回滚，会发生什么？ 父事务会回到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。 问题2：如果父事务回滚，会发生什么？ 父事务回滚，子事务也会跟着回滚，为什么呢？因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理/ 问题3：父事务先提交，然后子事务再提交；还是子事务先提交，然后父事务再提交呢？ 答案是第二种情况，子事务是父事务的一部分，由父事务同意提交。 6、spring配置文件的一些理解： 容器 在Spring整体框架的核心概念中，容器是核心思想，就是用来管理Bean的整个生命周期的，而在一个项目中，容器不一定只有一个，Spring中可以包括多个容器，而且容器有上下层关系，目前最常见的一种场景就是在一个项目中引入Spring和SpringMVC这两个框架，那么它其实就是两个容器，Spring是父容器，SpringMVC是其子容器，并且在Spring父容器中注册的Bean对于SpringMVC容器中是可见的，而在SpringMVC容器中注册的Bean对于Spring父容器中是不可见的，也就是子容器可以看见父容器中的注册的Bean，反之就不行。 1&lt;context:component-scan base-package="com.springmvc.test" /&gt; 我们可以使用统一的如下注解配置来对Bean进行批量注册，而不需要再给每个Bean单独使用xml的方式进行配置。 从Spring提供的参考手册中我们得知该配置的功能是扫描配置的base-package包下的所有使用了@Component注解的类，并且将它们自动注册到容器中，同时也扫描@Controller，@Service，@Respository这三个注解，因为他们是继承自@Component。 1&lt;context:annotation-config/&gt; 其实有了上面的配置，这个是可以省略掉的，因为上面的配置会默认打开以下配置。以下配置会默认声明了@Required、@Autowired、 @PostConstruct、@PersistenceContext、@Resource、@PreDestroy等注解。 1&lt;mvc:annotation-driven /&gt; 这个是SpringMVC必须要配置的，因为它声明了@RequestMapping、@RequestBody、@ResponseBody等。并且，该配置默认加载很多的参数绑定方法，比如json转换解析器等。 7、总结 在实际工程中会包括很多配置，我们按照官方推荐根据不同的业务模块来划分不同容器中注册不同类型的Bean：Spring父容器负责所有其他非@Controller注解的Bean的注册，而SpringMVC只负责@Controller注解的Bean的注册，使得他们各负其责、明确边界。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis实现分布式锁]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F03redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[为了讲解redis分布式锁，我将引入一个场景：定时关单。因为往往订单服务是一个集群，那么定时器会同时触发这些集群去取消订单，显然是浪费机器资源的，所以目的是：只让其中一台机器去执行取消订单即可。这里可以用分布式锁来实现。 项目是从练手项目中截取出来的，框架是基于SSM的XML形式构成，所以下面还涉及一点XMl对于定时器spring schedule的配置内容。 1、引入目标 定时自动对超过两个小时还未支付的订单对其进行取消，并且重置库存。 2、配置 首先是spring配置文件引入spring-schedule 123456xmlns:task="http://www.springframework.org/schema/task"...http://www.springframework.org/schema/taskhttp://www.springframework.org/schema/task/spring-task.xsd...&lt;task:annotation-driven/&gt; 补充：针对applicationContext-datasource.xml中的dataSource读取配置文件的信息无法展现的问题，在spring的配置文件中增加一条配置： 1&lt;context:property-placeholder location="classpath:datasource.properties"/&gt; 3、定时调度代码 此代码的主要功能是：定时调用取消订单服务。 1234567891011121314@Component@Slf4jpublic class CloseOrderTask &#123; @Autowired private OrderService orderService; @Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍的时候执行 public void closeOrderTaskV1()&#123; log.info("关闭订单定时任务启动"); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); log.info("关闭订单定时任务结束"); &#125;&#125; @Component一定要加，否则spring扫描不到。 close.order.task.time.hour 也是配置在snailmall.properties中的，这里配置的是默认的2，即两个小时，下订单超过两个小时仍然不支付，就取消该订单。 对于orderService里面的具体方法： 这里是关单的具体逻辑，细节是行锁。这段代码只要知道他是具体关单的逻辑即可，不需要仔细了解代码。 1234567891011121314151617181920212223242526@Overridepublic void closeOrder(int hour) &#123; Date closeDateTime = DateUtils.addHours(new Date(),-hour); //找到状态为未支付并且下单时间是早于当前检测时间的两个小时的时间,就将其置为取消 //SELECT &lt;include refid="Base_Column_List"/&gt; from mmall_order WHERE status = #&#123;status&#125; &lt;![CDATA[ and create_time &lt;= #&#123;date&#125; ]]&gt; order by create_time desc List&lt;Order&gt; orderList = orderMapper.selectOrderStatusByCreateTime(Const.OrderStatusEnum.NO_PAY.getCode(),DateTimeUtil.dateToStr(closeDateTime)); for(Order order:orderList)&#123; List&lt;OrderItem&gt; orderItemList = orderItemMapper.getByOrderNo(order.getOrderNo()); for(OrderItem orderItem:orderItemList)&#123; //一定要用主键where条件，防止锁表。同时必须是支持MySQL的InnoDB. Integer stock = productMapper.selectStockByProductId(orderItem.getProductId()); if(stock == null)&#123; continue; &#125; //更新产品库存 Product product = new Product(); product.setId(orderItem.getProductId()); product.setStock(stock+orderItem.getQuantity()); productMapper.updateByPrimaryKeySelective(product); &#125; //关闭order //UPDATE mmall_order set status = 0 where id = #&#123;id&#125; orderMapper.closeOrderByOrderId(order.getId()); log.info("关闭订单OrderNo:&#123;&#125;",order.getOrderNo()); &#125;&#125; 这样，debug启动项目，一分钟后就会自动执行closeOrderTaskV1方法了。找一个未支付的订单，进行相应测试。 4、存在的问题 经过实验发现，同时部署两台tomcat服务器，执行定时任务的时候是两台都同时执行的，显然不符合我们集群的目标，我们只需要在同一时间只有一台服务器执行这个定时任务即可。那么解决方案就是引入redis分布式锁。 redis实现分布式锁，核心命令式setnx命令。所以阅读下面，您需要对redis分布式锁的基本实现原理必须要先有一定的认识才行。 5、第一种方案 第一步：setnx进去，如果成功，说明塞入redis成功，抢占到锁 第二步：抢到锁之后，先设置一下过期时间，即后面如果执行不到delete，也会将这个锁自动释放掉，防止死锁 第三步：关闭订单，删除redis锁 存在的问题：如果因为tomcat关闭或tomcat进程在执行closeOrder()方法的时候，即还没来得及设置锁的过期时间的时候，这个时候会造成死锁。需要改进。 123456789101112131415161718192021222324252627//第一个版本，在突然关闭tomcat的时候有可能出现死锁@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV2()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; log.info("关闭订单定时任务结束");&#125;private void closeOrder(String lockName) &#123; //给锁一个过期时间，如果因为某个原因导致下面的锁没有被删除，造成死锁 RedisShardPoolUtil.expire(lockName,50); log.info("获取&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); int hour = Integer.parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); //关闭订单之后就立即删除这个锁 RedisShardPoolUtil.del(lockName); log.info("释放&#123;&#125;，ThreadName:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,Thread.currentThread().getName()); System.out.println("=============================================");&#125; 6、改进 图看不清，可以重新打开一个窗口看。具体的逻辑代码： 12345678910111213141516171819202122232425262728@Scheduled(cron = "0 */1 * * * ?")//每隔一分钟执行，一分钟的整数倍public void closeOrderTaskV3()&#123; log.info("关闭订单定时任务启动"); //设置锁，value是用当前时间+timeout进行设置的 long timeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout")); Long setnxResult = RedisShardPoolUtil.setnx(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() ==1)&#123; //说明被当前的tomcat进程抢到锁，下面就可以关闭订单 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; //在没有拿到锁的情况下，也要进行相应的判断，确保不死锁 String lockValueStr = RedisShardPoolUtil.get(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); //如果判断锁是存在的并且现在已经超时了，那么我们这个进程就有机会去占有这把锁 if(lockValueStr != null &amp;&amp; System.currentTimeMillis() &gt; Long.parseLong(lockValueStr))&#123; //当前进程进行get set操作，拿到老的key，再塞进新的超时时间 String getSetResult = RedisShardPoolUtil.getset(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK,String.valueOf(System.currentTimeMillis()+timeout)); //如果拿到的是空的，说明老的锁已经释放，那么当前进程有权占有这把锁进行操作； //如果拿到的不是空的，说明老的锁仍然占有，并且这次getset拿到的key与上面查询get得到的key一样的话，说明没有被其他进程刷新，那么本进程还是有权占有这把锁进行操作 if(getSetResult == null || (getSetResult != null &amp;&amp; StringUtils.equals(lockValueStr,getSetResult)))&#123; closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125;else &#123; log.info("没有获取分布式锁：&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125; log.info("关闭订单定时任务结束");&#125; 这样两次的防死锁措施，不仅可以防止死锁，还可以提高效率。 7、扩展 mysql四种事务隔离机制 read uncommitted:读取未提交内容 两个线程，其中一个线程执行了更新操作，但是没有提交，另一个线程在事务内就会读到该线程未提交的数据。 read committed:读取提交内容（不可重复读） 针对第一种情况，一个线程在一个事务内不会读取另一个线程未提交的数据了。但是，读到了另一个线程更新后提交的数据，也就是说重复读表的时候，数据会不一致。显然这种情况也是不合理的，所以叫不可重复读。 repeatable read:可重复读（默认） 可重复读，显然解决2中的问题，即一个线程在一个事务内不会再读取到另一个线程提交的数据，保证了该线程在这个事务内的数据的一致性。 对于某些情况，这种方案会出现幻影读，他对于更新操作是没有任何问题的了，但是对于插入操作，有可能在一个事务内读到新插入的数据（但是MySQL中用多版本并发控制机制解决了这个问题），所以默认使用的就是这个机制，没有任何问题。 serializable:序列化 略。 存储引擎 MySQL默认使用的是InnoDB，支持事务。还有例如MyISAM,这种存储引擎不支持事务，只支持只读操作，在用到数据的修改的地方，一般都是用默认的InnoDB存储引擎。 索引的一个注意点 一般类型为normal和unique，用btree实现，对于联合索引(字段1和字段2)，在执行查询的时候，例如 1select * from xxx where 字段1="xxx" ... 是可以利用到索引的高性能查询的，但是如果是 1select * from xxx where 字段2="xxx" ... 效率跟普通的查询时一样的，因为用索引进行查询，最左边的那个字段必须要有，否则无效。 扩展的内容知识顺便提一下，在数据库这一块，会详细介绍一下。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson实现Redis分布式锁原理]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F02Redisson%E5%AE%9E%E7%8E%B0Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[我们可以自己来实现一个redis分布式锁，但是如何用Redisson优雅地实现呢？本文探讨一下它的原理。 用Redisson来实现分布式锁异常地简单，形如： 还支持redis单实例、redis哨兵、redis cluster、redis master-slave等各种部署架构，都可以给你完美实现。 加锁 原理图： 现在某个客户端要加锁。如果该客户端面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。紧接着，就会发送一段lua脚本到redis上，那段lua脚本如下所示： 为啥要用lua脚本呢？因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的原子性。 解释一下这段脚本的意思。 这里的KEYS[1]代表的是你加锁的那个key的名字。这个key就是我们常看到的： 1RLock lock = redisson.getLock("myLock"); 中的myLock，我就是对这个key进行加锁。 这里的ARGV[1]代表的就是锁key的默认生存时间，默认30秒。ARGV[2]代表的是加锁的客户端的ID:比如8743c9c0-0795-4907-87fd-6c719a6b4586:1 第一段if判断语句，就是相当于用exists myLock命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。如何加锁呢？很简单，用下面的命令：hset myLock。 执行完hest之后，设置了一个hash数据结构：8743c9c0-0795-4907-87fd-6c719a6b4586:1 1，这行命令执行后，会出现一个类似下面的数据结构： 紧接着会执行pexpire myLock 30000命令，设置myLock这个锁key的生存时间是30秒。好了，到此为止，ok，加锁完成了。 锁互斥 那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？很简单，第一个if判断会执行exists myLock，发现myLock这个锁key已经存在了。接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。 所以这个客户端2两个if都不能进入，只能执行最后的pttl myLock，返回值代表了myLock这个锁key的剩余生存时间。比如还剩15000毫秒的生存时间。此时客户端2会进入一个while循环，不停的尝试加锁。 watch dog自动延期机制 客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？ 简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，他是一个后台线程，会每隔10秒检查一下，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。 可重入加锁机制 看一下代码，相同的客户进来，会进入第二个if，会执行hincrby，即增1，那么这个hash结构就会变成： 释放锁 如果执行lock.unlock()，就可以释放分布式锁，此时的业务逻辑也是非常简单的。其实说白了，就是每次都对myLock数据结构中的那个加锁次数减1。如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用：del myLock命令，从redis里删除这个key。然后呢，另外的客户端2就可以尝试完成加锁了。 这就是所谓的分布式锁的开源Redisson框架的实现机制。 存在的问题 其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。 但是复制的这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。 假设客户端1在redis master上获得锁，然后主机宕机，redis slave成为新的redis master，但是还未同步到redis slave上，但是客户端1已经觉得自己获取到了锁。 此时，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，此时就会发生多个客户端完成对一个key的加锁。这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。 所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：在redis master实例宕机的时候，可能导致多个客户端同时完成加锁。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring事务的传播行为]]></title>
    <url>%2F2019%2F01%2F22%2Fmiscellany%2F01%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BC%A0%E6%92%AD%E8%A1%8C%E4%B8%BA%2F</url>
    <content type="text"><![CDATA[经常听到别人说事务传播行为，那到底什么是事务的传播行为呢？ 1.什么是事务？ 在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。 这里注意，其实事务就是数据库才能保证的，所以抛开数据库层面来谈事务本身就是不存在的，所以事务的概念就是数据库一系列操作的一个完整单元。 2.什么是ACID？ ACID是指数据库管理系统在写入或更新资料的过程中，为保证事务是正确可靠的，所必须具备的四个特性：原子性、一致性、隔离性、持久性。 Atomicity：一个事务中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 Isolation：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔 离分为不同级别，包括读未提交(Read uncommitted)、读提交(read committed)、可重复读(repeatable read)和串行化(Serializable)。 Durability：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 3.spring事务传播行为 在我们用SSM开发项目的时候，我们一般都是将事务设置在Service层 那么当我们调用Service层的一个方法的时候它能够保证我们的这个方法中执行的所有的对数据库的更新操作保持在一个事务中，在事务层里面调用的这些方法要么全部成功，要么全部失败。那么事务的传播特性也是从这里说起的。 如果你在你的`Service`层的这个方法中，除了调用了`Dao`层的方法之外，还调用了本类的其他的`Service`方法，那么在调用其他的`Service`方法的时候，这个事务是怎么规定的呢，我必须保证我在我方法里调用的这个方法与我本身的方法处在同一个事务中，否则如果保证事物的一致性。事务的传播特性就是解决这个问题的. 在Spring中有针对传播特性的多种配置我们大多数情况下只用其中的一种:PROPGATION_REQUIRED：这个配置项的意思是说当我调用service层的方法的时候开启一个事务(具体调用那一层的方法开始创建事务，要看你的aop的配置),那么在调用这个service层里面的其他的方法的时候,如果当前方法产生了事务就用当前方法产生的事务，否则就创建一个新的事务。这个工作使由Spring来帮助我们完成的。 默认情况下当发生`RuntimeException`的情况下，事务才会回滚，所以要注意一下：如果你在程序发生错误的情况下，有自己的异常处理机制定义自己的`Exception`，必须从`RuntimeException`类继承，这样事务才会回滚！ 4.事务隔离级别 1、Serializable：最严格的级别，事务串行执行，资源消耗最大； 2、REPEATABLE READ：保证了一个事务不会修改已经由另一个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 3、READ COMMITTED:大多数主流数据库的默认事务等级，保证了一个事务不会读到另一个并行事务已修改但未提交的数据，避免了“脏读取”。该级别适用于大多数系统。 4、Read Uncommitted：保证了读取过程中不会读取到非法数据。 5.总结 本文的重点是在于理解事务的传播行为这个概念，从事务的概念，到事务的ACID介绍，引出事务传播传播行为和隔离级别这两个概念加以理解。]]></content>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap]]></title>
    <url>%2F2019%2F01%2F22%2Fjava-collection%2F12.ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[对于并发场景下，推荐使用线程安全的 concurrentHashMap ，而不是 HashMap 或者是 HashTable .concurrentHashMap在JDK7和JDK8中的实现原理是不一样的。本文分别对其核心思想和方法进行阐述。 一、JDK7实现 ConcurrentHashMap 的内部细分了若干个小的 HashMap ，称之为段（ SEGMENT ）。 ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment ，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel ( Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 1.1 get方法 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值. 内部 HashEntry 类 ： 12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 1.2 put方法 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。 首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。 而 ConcurrentHashMap 不一样，它是在将数据插入之前检查是否需要扩容，之后再做插入操作。 1.3 size方法 每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。 但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。 在 JDK7 中，第一种方案他会使用不加锁的模式去尝试多次计算 ConcurrentHashMap 的 size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的。 第二种方案是如果第一种方案不符合，他就会给每个 Segment 加上锁，然后计算 ConcurrentHashMap 的 size 返回。其源码实现: 12345678910111213141516171819202122232425262728293031323334353637public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 其中 12// 锁之前重试次数static final int RETRIES_BEFORE_LOCK = 2; 二、JDK8实现 jdk8 中的 ConcurrentHashMap 数据结构和实现与 jdk7 还是有着明显的差异。 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 也将 jdk7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。 其中的 val next 都用了 volatile 修饰，保证了可见性。 2.1 put方法 重点来看看 put 函数： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。 如果都不满足(不需要初始化、Node不为空、不需要扩容)，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 2.2 get方法 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 都不满足那就按照链表的方式遍历获取值。 2.3 size方法 JDK8 实现相比 JDK7 简单很多，只有一种方案，我们直接看 size() 代码： 123456789101112131415161718192021public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; //获取baseCount值 long sum = baseCount; //遍历CounterCell数组全部加到baseCount上，它们的和就是size if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; 可能你会有所疑问，ConcurrentHashMap 中的 baseCount 属性不就是记录的所有键值对的总数吗？直接返回它不就行了吗？ 之所以没有这么做，是因为我们的 addCount 方法用于 CAS 更新 baseCount，但很有可能在高并发的情况下，更新失败，那么这些节点虽然已经被添加到哈希表中了，但是数量却没有被统计。 还好，addCount 方法在更新 baseCount 失败的时候，会调用 fullAddCount 将这些失败的结点包装成一个 CounterCell 对象，保存在 CounterCell 数组中。那么整张表实际的 size 其实是 baseCount 加上 CounterCell数组中元素的个数。 三、总结 并发情况下请使用concurrentHashMap 在jdk7中，用的是分段锁，默认是12段，那么并发量最多也就12. get不加锁，第一次hash定位到segment，第二次hash定位到元素，元素值是用volatile保证内存可见性 put需要加锁，hash定位到segment后，先检查是否需要扩容再插入。 size先使用不加锁的模式去尝试多次计算size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入；如果不一致，给每个 Segment 加上锁再依次去计算个数 在jdk8中，采用了 CAS + synchronized 来保证并发安全性 put的过程比较复杂，简单来说是：先计算hash定位到node—》判断是否初始化—》如果node为空则表示可以插入，用cas插入—》判断是否需要扩容—》如果不需要初始化、Node不为空、不需要扩容，则利用 synchronized 锁写入数据—》判断是否需要转换为红黑树 get就比较简单，直接根据hash定位到node，然后以链表或者红黑树的方式拿到 size方法就一种方案：baseCount+CounterCell[]中所有元素 整理自： https://crossoverjie.top/JCSprout/#/thread/ConcurrentHashMap?id=size-方法 https://www.jianshu.com/p/e99e3fcface4]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap死循环问题]]></title>
    <url>%2F2019%2F01%2F21%2Fjava-collection%2F11.HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JDK1.7或者更老的版本中在多线程情况下是会存在死循环问题，究其原因是put过程中的resize方法在调用transfer方法的时候导致的死锁。这次我们来看看到底是哪里出了问题！ 核心源码 在JDK8中，内部已经调整，解决了死循环问题，是如何解决的呢？将JDK7中头插入法改为末端插入。就是这么简单。关于这个，可以查看jdk8源码中的resize方法。 上面提到是由于put时出现问题，那么先来到put()中看看： 我们看到，put一个不存在的新元素，必然增加一个节点，我们进入这个增加节点的方法： 检查是否需要扩容，需要的话就resize: 下面就是对链表数据进行迁移： 核心的代码就是这么多，首先要强调一下：两个线程进来，是分别建立了两个独立的扩容后的数组，比如这里是两个长度为4的数组。老的数组为2个数就是唯一的。所以在第一步，线程2运行结束时，老的数组元素已经空了。 下面先演示一下正常的rehash过程。 正常情况 假设了我们的hash算法就是简单的用 key mod 一下数组(hash表)的大小 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。 接下来的三个步骤是Hash表 resize 成4，然后所有的 &lt;key,value&gt; 重新 rehash 的过程 注意到，在JDK7中，是按照头插入法依次插入的。所以7插到了3前面。 并发情况 1.初始情况 假设我们有两个线程。我用红色和浅蓝色标注了一下。 对于第一个线程先执行完这一行，然后挂起，此时 e 和 next 都附好值了： 而让线程二执行完成。于是我们有下面的这个样子： 因为Thread1的 e 指向了 key(3) ，而 next 指向了 key(7) ，其在 Thread2 rehash后，指向了 Thread2 重组后的链表。 2.Thread1被调度回来执行 先是执行 newTalbe[i] = e ：此时线程1的第三个位置就是指向元素3; 然后是 e = next，导致了 e 指向了 key(7) ; 而下一次循环的 next = e.next 导致了 next 指向了 key(3) ; 3.一切安好 线程一接着工作。把 key(7) 摘下来，放到 newTable[i] 的第一个，然后把 e 和 next 往下移。 4.环形链接出现 e.next = newTable[i] 导致 key(3).next 指向了 key(7) 注意：此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 自己的简单整理 这里还是比较绕的，理解的最好方式左边放源码，右边放图，中间用草稿纸画一画。 那么，这里我在对其过程尽可能地讲明白一点。我们先确定7和3会全部落到扩容后的下标为3的位置(3%4=3,7%4=3)。 规定线程1开辟的数组为 arr1 ，线程2开辟的数组为 arr2; 1. 初始状态 线程一： e -&gt; key3 , next -&gt; key7 线程二： 数组3号位置 arr2[3] -&gt; key7 -&gt; key3 注意此时 key7 指向 key3 . 我们要明确一下，发生死循环，是指在put操作完毕之后，最终生成的数组中有死循环引用才行，千万不要一开始看线程一种key3指向key7，然后线程二种key7指向key3就是死循环了。。。 2. 线程一继续执行 i = 3 e.next = key7,此时 e=key3 ,所以是 key3.next = key7（这是线程1的初始状态决定的） arr1[3] 指向 key3 e 为 key7 3.由于e不为空，所以还会循环： 上一步 e 为 key7，所以 next = key7.next ，到线程2中一看是 key3 ，所以 next = key3（线程2中key7.next就是key3） i = 3 e.next = key3------注意，这里就是Key7指向了key3,key7的next引用下面没有变过，所以这里做一下记录，即key7指向key3 newTable[3] = key7 e = key3 4.由于e不为空，所以还会循环： 上一步 e=key3 , next=null i=3 key3.next = key7，注意,由于key7已经指向了key3，此时key3又指向key7,发生死循环 newTable[3] = key3 e = null 5.e为null，跳出循环。 此时发现key3又指向了key7。发生死循环。 整理自:https://coolshell.cn/articles/9606.html/comment-page-2#comments]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux重要的一些命令]]></title>
    <url>%2F2019%2F01%2F21%2Flinux%E9%87%8D%E8%A6%81%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[很多命令之所以重要，是因为用它们会大幅提高工作效率。所以，熟悉它们是我们必须要做的一件事情。下面着重提取了find、grep、管道、wc、awk以及sed等几个命令来看看如何使用。 一、Linux体系结构 对这幅图进行详细说明一下。 体系结构主要分为用户态(用户上层活动)和内核态 内核：本质是一段管理计算机硬件设备的程序，这个程序直接管理硬件：包括CPU、内存空间、硬盘接口、网络接口等。所有的计算机操作都要通过内核来操作。 系统调用：内核的访问接口，是一种不能再简化的操作(可以认为系统调用已经是最小的原子操作，上层完成一个功能要依托于若干系统调用才能完成) 由于系统调用比较基础，要完成一个功能需要很多系统调用组合才能实现，对于程序员来说比较复杂。这个时候怎么办呢？我们可以调用公共函数库：系统调用的组合拳。简化程序员操作。 Shell也是一种特殊的应用程序，是一个命令解释器，可以编程。 Shell下通系统调用，上通各种应用，是上层和下层之间粘合的胶水，让不同程序可以偕同工作。 在没有图形界面之前，用户通过shell命令行或者可编程的shell脚本可以完成很多事情。 二、如何根据文件名检索文件 find 在指定目录下查找文件 语法 find path [options] params 2.1 精确查找文件 比如我在当前目录(可能在子目录下)下找一个文件叫做test.java： 1find -name "test.java" 这个指令就可以在本目录以及子目录下递归查找这个文件了。 实例：精确查询名字叫snailmall-api-gateway-8080.jar这个文件： 2.2 全局搜索 如果是全局查找，也很简单，无非是从根目录开始递归查找。 1find / -name "test.java" 实例：我对这台服务器全局查找文件名以snailmall开头的所有文件： 2.3 模糊查询 如果找以test打头的文件： 1find -name "test*" 即用 * 通配符就可以模糊查询到以 test 打头的文件。 实例，我的这台服务器上部署了几个关于商城的服务，这个目录下我放了jar包和相应的启动信息文件。我对其进行模糊查询： 2.4 忽略大小写 1find -iname "test*" 三、如何根据文件内的内容进行检索 3.1 grep命令 grep 查找文件里符合条件的字符串 语法：grep [options] pattern file 比如 test.java 中有一句话是： 1System.out.println("i love java"); 那么如何查找 test.java 中的 java 呢？ 1grep "java" test* 这句话意思就是查找以 test 打头的文件中的包含 java 字符串所在的行。 直接输入： 1grep "hello" 会等待用户输入文本。然后再对输入的内容进行检索。 3.2 管道操作符 | 可将指令连接起来，前一个指令的输出作为后一个指令的输入。 1find / | grep "test" 作用同： 1find / -name "test" 注意： 只有前一个指令正确才会再处理。 管道右边命令必须能接收标准输入流，否则传递过程中数据会被抛弃 3.3 grep结合管道 1grep 'xxx' hello.info 可以将 xxx 所在的行全部筛选出来，但是还是特别多，我比如关心这每一行中某个字段的信息，比如是 param[xx12]这种信息。如何实现筛选呢？ 1grep 'xxx' hello.info | grep -o 'param\[[0-9a-z]*\]' 这样就只把类似于 param[xx12] 这样的信息清晰地展现出来。 如何过滤掉不要的信息呢？可以用： 1grep -v 比如我们查询 tomcat 进程信息： 1ps -ef | grep tomcat 我们会发现，不仅 tomcat 的信息展现出来了，执行 grep 命令本身的进程信息也展示出来了。我们要将这个 grep 命令过滤掉，只展现 tomcat 进程信息，可以： 1ps -ef | grep tomcat | grep -v "grep" 这样就把 grep 进程信息过滤掉了。 四、如何对文件内容做统计 awk 一次读取一行文本，按输入分隔符进行切片，切成多个组成部分 将切片直接保存再内建的变量中，$1$2…($0表示行的全部) 支持对单个切片的判断，支持循环判断，默认分隔符为空格 语法：awk [options] ‘cmd’ file 有这样一个文件text1.txt： 123456789proto Recv-Q Send-Q Local Address Foreign Address statetcp 0 48 111.34.134.2:ssh 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.13.2:s0 124.213.2.12:12565 ESTABLISHEDtcp 0 48 localhost:mysql 124.213.2.12:12565 ESTABLISHEDudp 1 48 localhots:webcac 124.213.2.12:12565 ESTABLISHEDtcp 1 48 111.34.134.2:s1 124.213.2.12:12565 ESTABLISHEDudp 1 48 111.34.134.2:s2 124.213.2.12:12565 ESTABLISHEDudp 0 48 111.34.134.2:s3 124.213.2.12:12565 ESTABLISHED 列出切分出来的第一列和第二列： 1awk '&#123;print $1,$2&#125;' test1.txt 结果： 筛选出第一列为tcp和第二列为1的所在行，将这些行数据全部打印出来： 1awk '$1="tcp" &amp;&amp; $2==1&#123;print $0&#125;' test1.txt 结果： 打印带有表头的数据： 1awk '($1="tcp" &amp;&amp; $2==1) || NR==1 &#123;print $0&#125;' test1.txt 默认是以空格分隔，那么以逗号或者其他符号可以吗？答案当然是可以。对于这样的文件text2.txt： 12345adas,123wqe,54412321,dddfsdaasd,1235465547,fjigj 1awk -F "," '&#123;print $2&#125;' text2.txt 五、WC统计 有一个文件test2.txt，里面的内容是： 123swg123eh shwfshsfswg7 121 32n dswg17328 123swg1 2h1jhwjqbsjwqbsh ddddh wg ehdedhd dhsjh 六、sed命令 sed是一个很好的文件处理工具，本身是一个管道命令，主要是以行为单位进行处理，可以将数据行进行替换、删除、新增、选取等特定工作 sed [-n/e/f/r/i] ‘command’ 输入文本 常用选项： -n∶使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN的资料一般都会被列出到萤幕上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e∶直接在指令列模式上进行 sed 的动作编辑； -f∶直接将 sed 的动作写在一个档案内， -f filename 则可以执行 filename 内的sed 动作； -r∶sed 的动作支援的是延伸型正规表示法的语法。(预设是基础正规表示法语法) -i∶直接修改读取的档案内容，而不是由萤幕输出。 常用命令： a ∶新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ∶取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ∶删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ∶插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ∶列印，亦即将某个选择的资料印出。通常 p 会与参数 sed -n 一起运作～ s ∶取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 这里我就主要看一下批量替换这个功能。 如果只是给一个文件中的若干字符串批量替换，只需要： 1sed -i "s/oldstring/newstring/g" filename 如果是对某一路径下很多的文件批量替换： 1sed -i &quot;s/oldstring/newstring/g&quot; `grep oldstring -rl path` 其中，oldstring是待被替换的字符串，newstring是待替换oldstring的新字符串，grep操作主要是按照所给的路径查找oldstring，path是所替换文件的路径； -i选项是直接在文件中替换，不在终端输出； -r选项是所给的path中的目录递归查找； -l选项是输出所有匹配到oldstring的文件； 这里只是模拟一下，将目录下的所有文件进行批量修改：]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[站点文章汇总]]></title>
    <url>%2F2019%2F01%2F21%2F%E7%AB%99%E7%82%B9%E6%96%87%E7%AB%A0%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[☝️☝️☝️🔝🔝🔝本文为置顶文章，为了方便管理和查阅，在这里详细展示目录索引，看完索引，你就知道本站的大体内容啦！我相信一定会给小伙伴们一些收获！ 计算机网络🐭🐭🐭 这一部分主要是关于HTTP和TCP的必备知识。 《计算机网络相关系列》： 《从下到上看五层模型》 《从上到下看五层模型》 《HTTP的前世今生》 《TCP协议入门》 《TCP三次握手和四次挥手》 《HTTP基础知识提炼》 《一步一步理解HTTPS》 《一些常见的面试题》 JAVA容器🐱🐱🐱 这一部分是JAVA容器一系列文章，主要讲了常用JAVA容器的源码和一些特性，面试必问点。 《JAVA容器》： 《ArrayList/Vector》 《LinkedList》 《CopyOnWriteArrayList》 《HashCode/Equals》 《HashMap》 《HashSet》 《LinkedHashMap》 《HashMap和LinkedHashMap遍历机制》 《HashTable》 《LinkedHashSet》 《JDK7中HashMap死循环原因剖析》 《ConcurrentHashMap》 Linux&amp;操作系统🐶🐶🐶 一些必备的Liunx相关的知识点整理。 《Linux》： 《Linux面试重要命令》 《操作系统相关》： 《面试-进程与线程》 JAVA虚拟机相关🐹🐹🐹 主要是介绍JVM相关知识。轻松以应付面试。 JAVA核心基础知识🐺🐺🐺 主要是介绍比较核心的JAVA基础知识，属于JAVA基础进阶。 《Integer拆箱和装箱》 《String为什么不可变》 《java字符串核心一网打尽》 《JAVA基础核心-理解类、对象、面向对象编程、面向接口编程》 《补码的前世今生》 《数值计算精度丢失问题》 JAVA多线程🐸🐸🐸 多线程这一块比较棘手，且学且保重。 Redis🐯🐯🐯 系统学习redis的笔记整理。 《Redis》： 《初步认识Redis》 《Redis基本数据结构和操作》 《Redis其他的功能介绍》 《Redis为什么快》 《Redis持久化》 《Redis主从复制》 MySQL数据库🐨🐨🐨 作为必备技能，用法和原理都要会。 《MySQL》： 《mysql最基础知识小结》 《SQL必知必会知识点提炼》 《复杂查询训练》 《delete和truncate以及drop区别》 《如何设计一个关系型数据库》 《数据库索引入门》 《MySQL索引全面解读》 《MySQL调优》 《关于索引失效和联合索引》 《锁模块》 《数据库事务核心问题》 《mysql面试高频理论知识》 算法🐻🐻🐻 算法这一块也是面试痛点和难点，头发越来越少了呢！ Spring🐷🐷🐷 大厂必问啊啊啊啊，源码终究还是要读的~ Spring Cloud相关🐮🐮🐮 这一块就比较偏实践了。分布式。。。路漫漫。。。 Zookeeper🐗🐗🐗 作为当今分布式协调中心，核心的Paxos算法你不想了解一下吗？ 杂记🐵🐵🐵 在这个板块，不划分类别，文章尽可能地简短，也可谓之记忆碎片。 《技术短文杂记》： 《spring事务的传播行为》 《Redisson实现Redis分布式锁原理》 《redis实现分布式锁》 《springMVC全局异常+spring包扫描包隔离+spring事务传播》 《分布式ID生成策略》 《Spring Session》 《Curator》 《ELK平台搭建》 《库存扣减问题》 《分布式事务解决方案思考》 《SpringBoot使用logback实现日志按天滚动》 《地理位置附近查询的GEOHASH解决方案》 《深入探究Nginx原理》 实战作品🐰🐰🐰 记录一些实战作品，代码主要存放在github上。 《我的实战》： 《快乐蜗牛商城代码》 《码码购分布式电商实战代码》]]></content>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F10.LinkedHashSet%2F</url>
    <content type="text"><![CDATA[HashSet 和 LinkedHashSet 的关系类似于 HashMap 和 LinkedHashMap 的关系，即后者维护双向链表，实现迭代顺序可为插入顺序或是访问顺序。所以也就轻松加愉快快速了解一下即可。 从源码中可以看到其空的构造函数为： 123public LinkedHashSet() &#123; super(16, .75f, true);&#125; 这个super即父类是HashSet，从它的继承关系就可以显然看到： 123public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable 那么HashSet内部的数据结构就是一个 HashMap，其方法的内部几乎就是在调用 HashMap 的方法。 LinkedHashSet 首先我们需要知道的是它是一个 Set 的实现，所以它其中存的肯定不是键值对，而是值。此实现与 HashSet 的不同之处在于，LinkedHashSet 维护着一个运行于所有条目的双向循环链表。 这一切都与LinkedHashMap类似。 LinkedHashSet 内部有个属性 accessOrder 控制着遍历次序。默认情况下该值为 false ,即按插入排序访问。如果将该值设置为 true 的话，则按访问次序排序(即最近最少使用算法，最近最少使用的放在链表头部，最近访问的则在链表尾部)。 一、 示例 HashSet的遍历： 123456789101112public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new HashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125;&#125; 输出结果是： aaa ccc bbb eee LinkedHashSet的遍历： 1234567891011121314public static void main(String[] args) &#123; Set&lt;String&gt; linkedHashSet = new LinkedHashSet&lt;&gt;(); linkedHashSet.add("aaa"); linkedHashSet.add("eee"); linkedHashSet.add("ccc"); linkedHashSet.add("bbb"); linkedHashSet.add(null); Iterator&lt;String&gt; it = linkedHashSet.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; 输出结果是： aaa eee ccc bbb null 可以看到与输入顺序是一致的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashtable]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F9.HashTable%2F</url>
    <content type="text"><![CDATA[Hashtable 是个过时的集合类，不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换。但这并不是我们不去了解它的理由。最起码 Hashtable 和 HashMap 的面试题在面试中经常被问到。 一、前言 Hashtable和HashMap，从存储结构和实现来讲基本上都是相同的。 它和HashMap的最大的不同是它是线程安全的，另外它不允许key和value为null。 为了能在哈希表中成功地保存和取出对象，用作key的对象必须实现hashCode方法和equals方法。 二、fail-fast机制 iterator方法返回的迭代器是fail-fast的。如果在迭代器被创建后hashtable被结构型地修改了，除了迭代器自己的remove方法，迭代器会抛出一个ConcurrentModificationException异常。 因此，面对在并发的修改，迭代器干脆利落的失败，而不是冒险的继续。 关于这个的理解，其实在上一章讲LinkedHashMap中的第八点提到： 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 简单说，就是两个线程同时分别进行修改和遍历时，会抛出这个异常。 面试题：集合在遍历过程中是否可以删除元素，为什么迭代器就可以安全删除元素？ 集合在使用 for 循环迭代的过程中不允许使用，集合本身的 remove 方法删除元素，如果进行错误操作将会导致 ConcurrentModificationException 异常的发生 Iterator 可以删除访问的当前元素(current)，一旦删除的元素是Iterator 对象中 next 所正在引用的，在 Iterator 删除元素通过 修改 modCount 与 expectedModCount 的值，可以使下次在调用 remove 的方法时候两者仍然相同因此不会有异常产生。 迭代器的fail-fast机制并不能得到保证，它不能够保证一定出现该错误。一般来说，fail-fast会尽最大努力抛出ConcurrentModificationException异常。因此，为提高此类操作的正确性而编写一个依赖于此异常的程序是错误的做法，正确做法是：ConcurrentModificationException 应该仅用于检测 bug。 Hashtable是线程安全的。如果不需要线程安全的实现是不需要的，推荐使用HashMap代替Hashtable。如果需要线程安全的实现，推荐使用java.util.concurrent.ConcurrentHashMap代替Hashtable。 二、继承关系 123public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable&#123;&#125; extends Dictionary&lt;K,V&gt;：Dictionary类是一个抽象类，用来存储键/值对，作用和Map类相似。 implements Map&lt;K,V&gt;：实现了Map，实现了Map中声明的操作和default方法。 hashMap以及TreeMap的源码，都没有继承于这个类。不过当我看到注释中的解释也就明白了，其 Dictionary 源码注释是这样的：NOTE: This class is obsolete. New implementations should implement the Map interface, rather than extending this class. 该话指出 Dictionary 这个类过时了，新的实现类应该实现Map接口。 三、属性 1234567891011121314//哈希表private transient Entry&lt;?,?&gt;[] table;//记录哈希表中键值对的个数private transient int count;//扩容的阈值private int threshold;//负载因子private float loadFactor;//hashtable被结构型修改的次数。private transient int modCount = 0; HashTable并没有像HashMap那样定义了很多的常量，而是直接写死在了方法里。 Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。 四、构造函数 12345678/** * 使用默认初始化容量（11）和默认负载因子（0.75）来构造一个空的hashtable. * * 这里可以看到，Hashtable默认初始化容量为16，而HashMap的默认初始化容量为11。 */public Hashtable() &#123; this(11, 0.75f);&#125; 我们可以获取到这些信息：HashTable默认的初始化容量为11（与HashMap不同），负载因子默认为0.75（与HashMap相同）。而正因为默认初始化容量的不同，同时也没有对容量做调整的策略，所以可以先推断出，HashTable使用的哈希函数跟HashMap是不一样的（事实也确实如此）。 五、重要方法 5.1 get方法 12345678910111213public synchronized V get(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //通过哈希函数，计算出key对应的桶的位置 int index = (hash &amp; 0x7FFFFFFF) % tab.length; //遍历该桶的所有元素，寻找该key for (Entry&lt;?,?&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return (V)e.value; &#125; &#125; return null;&#125; 这里可以看到，Hashtable和HashMap确认key在数组中的索引的方法不同。 Hashtable通过index = (hash &amp; 0x7FFFFFFF) % tab.length;来确认 HashMap通过i = (n - 1) &amp; hash;来确认 跟HashMap相比，HashTable的get方法非常简单。我们首先可以看见get方法使用了synchronized来修饰，所以它能保证线程安全。并且它是通过链表的方式来处理冲突的。另外，我们还可以看见HashTable并没有像HashMap那样封装一个哈希函数，而是直接把哈希函数写在了方法中。而哈希函数也是比较简单的，它仅对哈希表的长度进行了取模。 5.2 put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public synchronized V put(K key, V value) &#123; // 确认value不为null if (value == null) &#123; throw new NullPointerException(); &#125; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //找到key在table中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") //获取key所在索引的entry Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，判断key是否已经存在 for(; entry != null ; entry = entry.next) &#123; //如果key已经存在 if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; //保存旧的value V old = entry.value; //替换value entry.value = value; //返回旧的value return old; &#125; &#125; //如果key在hashtable不是已经存在，就直接将键值对添加到table中，返回null addEntry(hash, key, value, index); return null;&#125;private void addEntry(int hash, K key, V value, int index) &#123; modCount++; Entry&lt;?,?&gt; tab[] = table; //哈希表的键值对个数达到了阈值，则进行扩容 if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; //把新节点插入桶中（头插法） tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++;&#125; 从代码中可以总结出Hashtable的put方法的总体思路： 确认value不为null。如果为null，则抛出异常 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key已经存在，替换value，返回旧的value 如果key在hashtable不是已经存在，就直接添加，否则直接将键值对添加到table中，返回null 在方法中可以看到，在遍历桶中元素时，是按照链表的方式遍历的。可以印证，HashMap的桶中可能为链表或者树。但Hashtable的桶中只可能是链表。 5.3 remove方法 12345678910111213141516171819202122232425public synchronized V remove(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); //计算key在hashtable中的索引 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)tab[index]; //遍历entry，如果entry中存在key为参数key的键值对，就删除键值对，并返回键值对的value for(Entry&lt;K,V&gt; prev = null ; e != null ; prev = e, e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; modCount++; if (prev != null) &#123; prev.next = e.next; &#125; else &#123; tab[index] = e.next; &#125; count--; V oldValue = e.value; e.value = null; return oldValue; &#125; &#125; //如果不存在key为参数key的键值对，返回value return null;&#125; 从代码中可以总结出Hashtable的remove方法的总体思路： 找到key在table中的索引，获取key所在位置的entry 遍历entry，判断key是否已经存在 如果key存在，删除key映射的键值对，返回旧的value 如果key在hashtable不存在，返回null 5.4 rehash方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 增加hashtable的容量，为了更有效地存放和找到它的entry。 * 当键值对的数量超过了临界值（capacity*load factor）这个方法自动调用 * 长度变为原来的2倍+1 * */@SuppressWarnings("unchecked")protected void rehash() &#123; //记录旧容量 int oldCapacity = table.length; //记录旧桶的数组 Entry&lt;?,?&gt;[] oldMap = table; // overflow-conscious code //新的容量为旧的容量的2倍+1 int newCapacity = (oldCapacity &lt;&lt; 1) + 1; //如果新的容量大于容量的最大值MAX_ARRAY_SIZE if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; //如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; //如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE newCapacity = MAX_ARRAY_SIZE; &#125; //创建新的数组，容量为新容量 Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; //结构性修改次数+1 modCount++; //计算扩容的临界值 threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; //将旧的数组中的键值对转移到新数组中 for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125;&#125; 看完代码，我们可以总结出rehash的总体思路为： 新建变量新的容量，值为旧的容量的2倍+1 如果新的容量大于容量的最大值MAX_ARRAY_SIZE 如果旧容量为MAX_ARRAY_SIZE，容量不变，中断方法的执行 如果旧容量不为MAX_ARRAY_SIZE，新容量变为MAX_ARRAY_SIZE 创建新的数组，容量为新容量 将旧的数组中的键值对转移到新数组中 这里可以看到，一般情况下，HashMap扩容后容量变为原来的两倍，而Hashtable扩容后容量变为原来的两倍加一。 HashTable的rehash方法相当于HashMap的resize方法。跟HashMap那种巧妙的rehash方式相比，HashTable的rehash过程需要对每个键值对都重新计算哈希值，而比起异或和与操作，取模是一个非常耗时的操作，所以这也是导致效率较低的原因之一。 六、遍历 可以使用与HashMap一样的遍历方式，但是由于历史原因，多了Enumeration的方式。 针对Enumeration，这里与iterator进行对比一下。 相同点 Iterator和Enumeration都可以对某些容器进行遍历。 Iterator和Enumeration都是接口。 不同点 Iterator有对容器进行修改的方法。而Enumeration只能遍历。 Iterator支持fail-fast，而Enumeration不支持。 Iterator比Enumeration覆盖范围广，基本所有容器中都有Iterator迭代器，而只有Vector、Hashtable有Enumeration。 Enumeration在JDK 1.0就已经存在了，而Iterator是JDK2.0新加的接口。 七、Hashtable与HashMap对比 HashTable的应用非常广泛，HashMap是新框架中用来代替HashTable的类，也就是说建议使用HashMap。 下面着重比较一下二者的区别： 1.继承不同 Hashtable是基于陈旧的Dictionary类的，HashMap是java1.2引进的Map接口的一个实现。 2.同步 Hashtable 中的方法是同步的，保证了Hashtable中的对象是线程安全的。 HashMap中的方法在缺省情况下是非同步的,HashMap中的对象并不是线程安全的。在多线程并发的环境下，可以直接使用Hashtable，但是要使用HashMap的话就要自己增加同步处理了。 3.效率 单线程中, HashMap的效率大于Hashtable。因为同步的要求会影响执行的效率，所以如果你不需要线程安全的集合，HashMap是Hashtable的轻量级实现，这样可以避免由于同步带来的不必要的性能开销，从而提高效率。 4.null值 Hashtable中，key和value都不允许出现null值，否则出现NullPointerException。 在HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键，而应该用containsKey()方法来判断。 5.遍历方式 Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable可以使用Enumeration的方式。 6.容量 Hashtable和HashMap它们两个内部实现方式的数组的初始大小和扩容的方式。 HashTable中hash数组默认大小是11，增加的方式是 old*2+1。 HashMap中hash数组的默认大小是16，而且一定是2的指数。 八、总结 无论什么时候有多个线程访问相同实例的可能时，就应该使用Hashtable，反之使用HashMap。非线程安全的数据结构能带来更好的性能。 如果在将来有一种可能—你需要按顺序获得键值对的方案时，HashMap是一个很好的选择，因为有HashMap的一个子类 LinkedHashMap。 所以如果你想可预测的按顺序迭代（默认按插入的顺序），你可以很方便用LinkedHashMap替换HashMap。反观要是使用的Hashtable就没那么简单了。 如果有多个线程访问HashMap，Collections.synchronizedMap（）可以代替，总的来说HashMap更灵活，或者直接用并发容器ConcurrentHashMap。 整理自： http://blog.csdn.net/panweiwei1994/article/details/77428710 http://blog.csdn.net/u013124587/article/details/52655042]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap和LinkedHashMap遍历机制]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F8.HashMap%E5%92%8CLinkedHashMap%E9%81%8D%E5%8E%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本篇单独讲一下HashMap和LinkedHashMap遍历方式。 一、对HashMap和LinkedHashMap遍历的几种方法 这里以HashMap为例，LinkedHashMap一样的方式。 12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println("key=" + next.getKey() + " value=" + next.getValue());&#125; 123456Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println("key=" + key + " value=" + map.get(key));&#125; 123map.forEach((key,value)-&gt;&#123; System.out.println("key=" + key + " value=" + value);&#125;); 强烈建议使用第一种 EntrySet 进行遍历。 第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。 我们知道，HashMap的输出顺序与元素的输入顺序无关，LinkedHashMap可以按照输入顺序输出，也可以根据读取元素的顺序输出。这一现象，已经在上一篇中展示出来了。 二、HashMap的遍历机制 HashMap 提供了两个遍历访问其内部元素Entry&lt;k,v&gt;的接口： Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()-------&gt;返回此映射所包含的映射关系的 Set 视图。 Set&lt;K&gt; keySet()--------&gt;返回此映射中所包含的键的 Set 视图。 实际上，第二个接口表示的Key的顺序，和第一个接口返回的Entry顺序是对应的，也就是说：这两种接口对HashMap的元素遍历的顺序相相同的。 那么，HashMap遍历内部Entry&lt;K,V&gt; 的顺序是什么呢？ 搞清楚这个问题，先要知道其内部结构是怎样的。 HashMap在存储Entry对象的时候，是根据Key的hash值判定存储到Entry[] table数组的哪一个索引值表示的链表上。 对HashMap遍历Entry对象的顺序和Entry对象的存储顺序之间没有任何关系。 HashMap散列图、Hashtable散列表是按“有利于随机查找的散列(hash)的顺序”。并非按输入顺序。遍历时只能全部输出，而没有顺序。甚至可以rehash()重新散列，来获得更利于随机存取的内部顺序。 所以对HashMap的遍历，由内部的机制决定的，这个机制是只考虑利于快速存取，不考虑输入等顺序。 三、LinkedHashMap 的遍历机制 LinkedHashMap 是HashMap的子类，它可以实现对容器内Entry的存储顺序和对Entry的遍历顺序保持一致。 为了实现这个功能，LinkedHashMap内部使用了一个Entry类型的双向链表，用这个双向链表记录Entry的存储顺序。当需要对该Map进行遍历的时候，实际上是遍历的是这个双向链表。 LinkedHashMap内部使用的LinkedHashMap.Entry类继承自Map.Entry类，在其基础上增加了LinkedHashMap.Entry类型的两个字段，用来引用该Entry在双向链表中的前面的Entry对象和后面的Entry对象。 它的内部会在Map.Entry类的基础上，增加两个Entry类型的引用：before，after。LinkedHashMap使用一个双向连表，将其内部所有的Entry串起来。 1234LinkedHashMap linkedHashMap = new LinkedHashMap(); linkedHashMap.put("name","louis"); linkedHashMap.put("age","24"); linkedHashMap.put("sex","male"); 对LinkedHashMap进行遍历的策略： 从 header.after 指向的Entry对象开始，然后一直沿着此链表遍历下去，直到某个entry.after == header 为止，完成遍历。 根据Entry&lt;K,V&gt;插入LinkedHashMap的顺序进行遍历的方式叫做：按插入顺序遍历。 另外，LinkedHashMap还支持一种遍历顺序，叫做：Get读取顺序。 如果LinkedHashMap的这个Get读取遍历顺序开启，那么，当我们在LinkedHashMap上调用get(key) 方法时，会导致内部key对应的Entry在双向链表中的位置移动到双向链表的最后。 四、遍历机制的总结 HashMap对元素的遍历顺序跟Entry插入的顺序无关，而LinkedHashMap对元素的遍历顺序可以跟Entry&lt;K,V&gt;插入的顺序保持一致：从双向。 当LinkedHashMap处于Get获取顺序遍历模式下，当执行get() 操作时，会将对应的Entry&lt;k,v&gt;移到遍历的最后位置。 LinkedHashMap处于按插入顺序遍历的模式下，如果新插入的&lt;key,value&gt; 对应的key已经存在，对应的Entry在遍历顺序中的位置并不会改变。 除了遍历顺序外，其他特性HashMap和LinkedHashMap基本相同。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F7.LinkedHashMap%2F</url>
    <content type="text"><![CDATA[大多数情况下，只要不涉及线程安全问题， Map 基本都可以使用 HashMap ，不过 HashMap 有一个问题，就是迭代 HashMap 的顺序并不是 HashMap 放置的顺序，也就是无序。 HashMap 的这一缺点往往会带来困扰，因为有些场景，我们期待一个有序的 Map。 篇幅有点长，但是在理解了HashMap之后就比较简单了。 这个时候，LinkedHashMap就闪亮登场了，它虽然增加了时间和空间上的开销，但是可以解决有排序需求的场景。 它的底层是继承于 HashMap 实现的，由一个双向循环链表所构成。 LinkedHashMap 的排序方式有两种： 根据写入顺序排序。 根据访问顺序排序。 其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能得到一个按照访问顺序排序的链表。 一、LinkedHashMap数据结构 LinkedHashMap是通过哈希表和双向循环链表实现的，它通过维护一个双向循环链表来保证对哈希表迭代时的有序性，而这个有序是指键值对插入的顺序。 我们可以看出，遍历所有元素只需要从header开始遍历即可，一直遍历到下一个元素是header结束。 另外，当向哈希表中重复插入某个键的时候，不会影响到原来的有序性。也就是说，假设你插入的键的顺序为1、2、3、4，后来再次插入2，迭代时的顺序还是1、2、3、4，而不会因为后来插入的2变成1、3、4、2。（但其实我们可以改变它的规则，使它变成1、3、4、2） LinkedHashMap的实现主要分两部分，一部分是哈希表，另外一部分是链表。哈希表部分继承了HashMap，拥有了HashMap那一套高效的操作，所以我们要看的就是LinkedHashMap中链表的部分，了解它是如何来维护有序性的。 二、demo示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public static void main(String[] args) &#123; /** * HashMap插入数据，遍历输出无序 */ System.out.println("----------HashMap插入数据--------"); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("apple", "a"); map.put("watermelon", "b"); map.put("banana", "c"); map.put("peach", "d"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap插入数据，遍历，默认以插入顺序为序 */ System.out.println("----------LinkedHashMap插入数据,按照插入顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap = new LinkedHashMap&lt;&gt;(); linkedHashMap.put("apple", "a"); linkedHashMap.put("watermelon", "b"); linkedHashMap.put("banana", "c"); linkedHashMap.put("peach", "d"); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = linkedHashMap.entrySet().iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; /** * LinkedHashMap插入数据，设置accessOrder=true实现使得其遍历顺序按照访问的顺序输出，这里先用get方法来演示 */ System.out.println("----------LinkedHashMap插入数据,accessOrder=true:按照访问顺序进行排序--------"); Map&lt;String, String&gt; linkedHashMap2 = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); linkedHashMap2.put("apple", "aa"); linkedHashMap2.put("watermelon", "bb"); linkedHashMap2.put("banana", "cc"); linkedHashMap2.put("peach", "dd"); linkedHashMap2.get("banana");//banana移动到了内部的链表末尾 linkedHashMap2.get("apple");//apple移动到了内部的链表末尾 Iterator iter2 = linkedHashMap2.entrySet().iterator(); while (iter2.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter2.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; /** * LinkedHashMap的put方法在accessOrder=true的情况下 */ System.out.println("-----------"); linkedHashMap2.put("watermelon", "bb");//watermelon移动到了内部的链表末尾 linkedHashMap2.put("stawbarrey", "ee");//末尾插入新元素stawbarrey linkedHashMap2.put(null, null);//插入新的节点 null Iterator iter3 = linkedHashMap2.entrySet().iterator(); while (iter3.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter3.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125; &#125; 输出结果是: 12345678910111213141516171819202122----------HashMap插入数据--------banana=capple=apeach=dwatermelon=b----------LinkedHashMap插入数据,按照插入顺序进行排序--------apple=awatermelon=bbanana=cpeach=d----------LinkedHashMap插入数据,按照访问顺序进行排序--------watermelon=bbpeach=ddbanana=cc//banana到了末尾apple=aa//apple到了末尾-----------peach=ddbanana=ccapple=aawatermelon=bb//watermelon到了链表末尾stawbarrey=ee//新插入的放在末尾null=null//新插入的放在末尾 三、属性 LinkedHashMap可以认为是HashMap+LinkedList，即它既使用HashMap操作数据结构，又使用LinkedList维护插入元素的先后顺序 3.1 继承关系 123public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; LinkedHashMap是HashMap的子类，自然LinkedHashMap也就继承了HashMap中所有非private的方法。所以它已经从 HashMap 那里继承了与哈希表相关的操作了，那么在LinkedHashMap中，它可以专注于链表实现的那部分，所以与链表实现相关的属性如下。 3.2 属性介绍 1234567891011121314151617//LinkedHashMap的链表节点继承了HashMap的节点，而且每个节点都包含了前指针和后指针，所以这里可以看出它是一个双向链表static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125;//头指针transient LinkedHashMap.Entry&lt;K,V&gt; head;//尾指针transient LinkedHashMap.Entry&lt;K,V&gt; tail;//默认为false。当为true时，表示链表中键值对的顺序与每个键的插入顺序一致，也就是说重复插入键，也会更新顺序//简单来说，为false时，就是上面所指的1、2、3、4的情况；为true时，就是1、3、4、2的情况final boolean accessOrder; 五、构造方法 1234public LinkedHashMap() &#123; super(); accessOrder = false;&#125; 其实就是调用的 HashMap 的构造方法: HashMap 实现： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap init();&#125; 可以看到里面有一个空的 init()，具体是由 LinkedHashMap 来实现的： 12345@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 其实也就是对 header 进行了初始化。 六、添加元素 LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法. newNode() 会在HashMap的putVal() 方法里被调用，putVal() 方法会在批量插入数据putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) 或者插入单个数据public V put(K key, V value)时被调用。 LinkedHashMap重写了newNode(),在每次构建新节点时，通过linkNodeLast(p);将新节点链接在内部双向链表的尾部。 12345678910111213141516171819//在构建新节点时，构建的是`LinkedHashMap.Entry` 不再是`Node`.Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125;//将新增的节点，连接在链表的尾部private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; //集合之前是空的 if (last == null) head = p; else &#123;//将新节点连接在链表的尾部 p.before = last; last.after = p; &#125;&#125; 以及HashMap专门预留给LinkedHashMap的afterNodeAccess() 、afterNodeInsertion() 、afterNodeRemoval() 方法。 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 如果你没有注意到注释的解释的话，你可能会很奇怪为什么会有三个空方法，而且有不少地方还调用过它们。其实这三个方法表示的是在访问、插入、删除某个节点之后，进行一些处理，它们在LinkedHashMap有各自的实现。LinkedHashMap正是通过重写这三个方法来保证链表的插入、删除的有序性。 123456789101112131415161718//回调函数，新节点插入之后回调,判断是否需要删除最老插入的节点。//如果实现LruCache会用到这个方法。void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; //LinkedHashMap 默认返回false 则不删除节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;//LinkedHashMap 默认返回false 则不删除节点。 //返回true 代表要删除最早的节点。//通常构建一个LruCache会在达到Cache的上限是返回trueprotected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; void afterNodeInsertion(boolean evict)以及boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) 是构建LruCache需要的回调，在这可以忽略它们。 七、删除元素 LinkedHashMap也没有重写remove() 方法，因为它的删除逻辑和HashMap并无区别。 但它重写了afterNodeRemoval() 这个回调方法。该方法会在Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) 方法中回调，removeNode() 会在所有涉及到删除节点的方法中被调用，上文分析过，是删除节点操作的真正执行者。 1234567891011121314151617//在删除节点e时，同步将e从双向链表上删除void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //待删除节点 p 的前置后置节点都置空 p.before = p.after = null; //如果前置节点是null，则现在的头结点应该是后置节点a if (b == null) head = a; else//否则将前置节点b的后置节点指向a b.after = a; //同理如果后置节点时null ，则尾节点应是b if (a == null) tail = b; else//否则更新后置节点a的前置节点为b a.before = b;&#125; 八、查询元素 LinkedHashMap重写了get()和getOrDefault() 方法： 12345678910111213141516public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125;public V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return defaultValue; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 对比HashMap中的实现,LinkedHashMap只是增加了在成员变量(构造函数时赋值)accessOrder为true的情况下，要去回调void afterNodeAccess(Node&lt;K,V&gt; e) 函数。 1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 在afterNodeAccess() 函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。 12345678910111213141516171819202122232425262728293031void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last;//原尾节点 //如果accessOrder 是true ，且原尾节点不等于e if (accessOrder &amp;&amp; (last = tail) != e) &#123; //节点e强转成双向链表节点p LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //p现在是尾节点， 后置节点一定是null p.after = null; //如果p的前置节点是null，则p以前是头结点，所以更新现在的头结点是p的后置节点a if (b == null) head = a; else//否则更新p的前直接点b的后置节点为 a b.after = a; //如果p的后置节点不是null，则更新后置节点a的前置节点为b if (a != null) a.before = b; else//如果原本p的后置节点是null，则p就是尾节点。 此时 更新last的引用为 p的前置节点b last = b; if (last == null) //原本尾节点是null 则，链表中就一个节点 head = p; else &#123;//否则 更新 当前节点p的前置节点为 原尾节点last， last的后置节点是p p.before = last; last.after = p; &#125; //尾节点的引用赋值成p tail = p; //修改modCount。 ++modCount; &#125;&#125; 图示(注意这个图，1和6也应该是连在一起的，因为是双向循环链表，所以视为一个小错误)： 说明：从图中可以看到，结点3链接到了尾结点后面。 值得注意的是，afterNodeAccess() 函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 九、判断元素是否存在 它重写了该方法，相比HashMap的实现，更为高效。 123456789public boolean containsValue(Object value) &#123; //遍历一遍链表，去比较有没有value相等的节点，并返回 for (LinkedHashMap.Entry&lt;K,V&gt; e = head; e != null; e = e.after) &#123; V v = e.value; if (v == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; return false;&#125; 对比HashMap，是用两个for循环遍历，相对低效。 12345678910111213public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false;&#125; 十、替换某个元素 1234567891011121314// 用dst替换srcprivate void transferLinks(LinkedHashMap.Entry&lt;K,V&gt; src, LinkedHashMap.Entry&lt;K,V&gt; dst) &#123; LinkedHashMap.Entry&lt;K,V&gt; b = dst.before = src.before; LinkedHashMap.Entry&lt;K,V&gt; a = dst.after = src.after; if (b == null) head = dst; else b.after = dst; if (a == null) tail = dst; else a.before = dst;&#125; 十二、总结 LinkedHashMap相对于HashMap的源码比，是很简单的。因为大树底下好乘凉。它继承了HashMap，仅重写了几个方法，以改变它迭代遍历时的顺序。这也是其与HashMap相比最大的不同。 在每次插入数据，或者访问、修改数据时，会增加节点、或调整链表的节点顺序。以决定迭代时输出的顺序。 accessOrder默认是false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。为true时，可以在这基础之上构建一个LruCache. LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法.在每次构建新节点时，将新节点链接在内部双向链表的尾部 accessOrder=true的模式下,在afterNodeAccess()函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。值得注意的是，afterNodeAccess()函数中，会修改modCount,因此当你正在accessOrder=true的模式下,迭代LinkedHashMap时，如果同时查询访问数据，也会导致fail-fast，因为迭代的顺序已经改变。 nextNode() 就是迭代器里的next()方法 。该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。 而双链表节点的顺序在LinkedHashMap的增、删、改、查时都会更新。以满足按照插入顺序输出，还是访问顺序输出。 它与HashMap比，还有一个小小的优化，重写了containsValue()方法，直接遍历内部链表去比对value值是否相等。 整理自： http://blog.csdn.net/zxt0601/article/details/77429150 http://wiki.jikexueyuan.com/project/java-collection/linkedhashmap.html http://blog.csdn.net/u013124587/article/details/52659741 http://www.cnblogs.com/leesf456/p/5248868.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashSet]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F6.HashSet%2F</url>
    <content type="text"><![CDATA[HashSet 是一个不允许存储重复元素的集合，它是基于 HashMap 实现的， HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成。所以只要理解了 HashMap，HashSet 就水到渠成了。 成员变量 首先了解下HashSet的成员变量: 1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 发现主要就两个变量: map ：用于存放最终数据的。 PRESENT ：是所有写入map的value值。 构造方法 1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 构造函数很简单，利用了HashMap初始化了map。 add 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 比较关键的就是这个 add() 方法。 可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会受到影响，这样就保证了 HashSet 中只能存放不重复的元素。 该方法如果添加的是在 HashSet 中不存在的，则返回 true；如果添加的元素已经存在，返回 false。其原因在于我们之前提到的关于 HashMap 的 put 方法。该方法在添加 key 不重复的键值对的时候，会返回 null。 总结 HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。 所以 HashMap 会出现的问题 HashSet 依然不能避免。 对于 HashSet 中保存的对象，请注意正确重写其 equals 和 hashCode 方法，以保证放入的对象的唯一性。这两个方法是比较重要的。]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F5.HashMap%2F</url>
    <content type="text"><![CDATA[HashMap基本是面试必问的点，因为这个数据结构用的太频繁了，jdk1.8中的优化也是比较巧妙。有必要去深入探讨一下。但是涉及的内容比较多，这里只先探讨jdk8中HashMap的实现，至于jdk7中HashMap的死循环问题、红黑树的原理等都不会在本篇文章扩展到。其他的文章将会再去探讨整理。 本篇文章较长，高能预警。 一、前言 之前的List，讲了ArrayList、LinkedList，最后讲到了CopyOnWriteArrayList，就前两者而言，反映的是两种思想： （1）ArrayList以数组形式实现，顺序插入、查找快，插入、删除较慢 （2）LinkedList以链表形式实现，顺序插入、查找较慢，插入、删除方便 那么是否有一种数据结构能够结合上面两种的优点呢？有，答案就是HashMap。 HashMap是一种非常常见、方便和有用的集合，是一种键值对（K-V）形式的存储结构，在有了HashCode的基础后，下面将还是用图示的方式解读HashMap的实现原理。 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 二、HashMap的结构 其中哈希表是一个数组，我们经常把数组中的每一个节点称为一个桶，哈希表中的每个节点都用来存储一个键值对。 在插入元素时，如果发生冲突（即多个键值对映射到同一个桶上）的话，就会通过链表的形式来解决冲突。 因为一个桶上可能存在多个键值对，所以在查找的时候，会先通过key的哈希值先定位到桶，再遍历桶上的所有键值对，找出key相等的键值对，从而来获取value。 如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数： 容量 负载因子 容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。 三、继承关系 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 说明： HashMap继承自AbstractMap，AbstractMap是Map接口的骨干实现，AbstractMap中实现了Map中最重要最常用和方法，这样HashMap继承AbstractMap就不需要实现Map的所有方法，让HashMap减少了大量的工作。 四、属性 123456789101112131415161718192021222324252627//默认的初始容量为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//最大的容量上限为2^30static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的负载因子为0.75static final float DEFAULT_LOAD_FACTOR = 0.75f;//变成树型结构的临界值为8static final int TREEIFY_THRESHOLD = 8;//恢复链式结构的临界值为6static final int UNTREEIFY_THRESHOLD = 6;/** * 哈希表的最小树形化容量 * 当哈希表中的容量大于这个值时，表中的桶才能进行树形化 * 否则桶内元素太多时会扩容，而不是树形化 * 为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD */static final int MIN_TREEIFY_CAPACITY = 64;//哈希表transient Node&lt;K,V&gt;[] table;//哈希表中键值对的个数transient int size;//哈希表被修改的次数transient int modCount;//它是通过capacity*load factor计算出来的，当size到达这个值时，就会进行扩容操作int threshold;//负载因子final float loadFactor; 4.1 几个属性的详细说明 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍（为什么是两倍下文会说明）。 默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子`Load factor`的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子`loadFactor`的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意size和table的长度length、容纳最大键值对数量threshold的区别。 而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，因为常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考 http://blog.csdn.net/liuqiyao_01/article/details/14475159 ，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。下文会说明。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，并且链表的长度超过64时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 这里着重提一下MIN_TREEIFY_CAPACITY字段，容易与TREEIFY_THRESHOLD打架，TREEIFY_THRESHOLD是指桶中元素达到8个，就将其本来的链表结构改为红黑树，提高查询的效率。MIN_TREEIFY_CAPACITY是指最小树化的哈希表元素个数，也就是说，小于这个值，就算你(数组)桶里的元素数量大于8了，还是要用链表存储，只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。注意扩容是数组的复制。 4.2 Node结构 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 例如程序执行下面代码： 1map.put("美团","小美"); 系统将调用&quot;美团&quot;这个key的hashCode()方法得到其hashCode值（该方法适用于每个Java对象）。 然后再通过Hash算法来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。 当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。 那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法(5.4节)和扩容机制(5.5节)。下文会讲到。 五、方法 5.1 get方法 123456789101112131415161718192021222324252627282930//get方法主要调用的是getNode方法，所以重点要看getNode方法的实现public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //如果哈希表不为空 &amp;&amp; key对应的桶上不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //是否直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //判断是否有后续节点 if ((e = first.next) != null) &#123; //如果当前的桶是采用红黑树处理冲突，则调用红黑树的get方法去获取节点 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //不是红黑树的话，那就是传统的链式结构了，通过循环的方法判断链中是否存在该key do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 实现步骤大致如下： 通过hash值获取该key映射到的桶。 桶上的key就是要查找的key，则直接命中。 桶上的key不是要查找的key，则查看后续节点： 如果后续节点是树节点，通过调用树的方法查找该key。 如果后续节点是链式节点，则通过循环遍历链查找该key。 5.2 put方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法的具体实现也是在putVal方法中，所以我们重点看下面的putVal方法public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果哈希表为空，则先创建一个哈希表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果当前桶没有碰撞冲突，则直接把键值对插入，完事 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果桶上节点的key与当前key重复，那你就是我要找的节点了 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果是采用红黑树的方式处理冲突，则通过红黑树的putTreeVal方法去插入这个键值对 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //否则就是传统的链式结构 else &#123; //采用循环遍历的方式，判断链中是否有重复的key for (int binCount = 0; ; ++binCount) &#123; //到了链尾还没找到重复的key，则说明HashMap没有包含该键 if ((e = p.next) == null) &#123; //创建一个新节点插入到尾部 p.next = newNode(hash, key, value, null); //如果链的长度大于TREEIFY_THRESHOLD这个临界值，则把链变为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //找到了重复的key if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //这里表示在上面的操作中找到了重复的键，所以这里把该键的值替换为新值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //判断是否需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 1234567891011121314151617181920final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; put方法比较复杂，实现步骤大致如下： 先通过hash值计算出key映射到哪个桶。 如果桶上没有碰撞冲突，则直接插入。 如果出现碰撞冲突了，则需要处理冲突： 如果该桶使用红黑树处理冲突，则调用红黑树的方法插入。 否则采用传统的链式方法插入。如果链的长度到达临界值，则把链转变为红黑树。 如果桶中存在重复的键，则为该键替换新值。 如果size大于阈值，则进行扩容。 5.3 remove方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//remove方法的具体实现在removeNode方法中，所以我们重点看下面的removeNode方法public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果当前key映射到的桶不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //如果桶上的节点就是要找的key，则直接命中 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //如果是以红黑树处理冲突，则构建一个树节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); //如果是以链式的方式处理冲突，则通过遍历链表来寻找节点 else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //比对找到的key的value跟要删除的是否匹配 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //通过调用红黑树的方法来删除节点 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //使用链表的操作来删除节点 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 5.4 hash方法(确定哈希桶数组索引位置) 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。 注意get方法和put方法源码中都需要先计算key映射到哪个桶上，然后才进行之后的操作，计算的主要代码如下： 1(n - 1) &amp; hash 上面代码中的n指的是哈希表的大小，hash指的是key的哈希值，hash是通过下面这个方法计算出来的，采用了二次哈希的方式，其中key的hashCode方法是一个native方法： 123456static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 总结就是：由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2^n 。这样用 2^n - 1 做位运算与取模效果一致，并且效率还要高出许多。这样回答了上文中：好的Hash算法到底是什么。 5.5 resize方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //计算扩容后的大小 if (oldCap &gt; 0) &#123; //如果当前容量超过最大容量，则无法进行扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //没超过最大值则扩为原来的两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //新的resize阈值 threshold = newThr; //创建新的哈希表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; //遍历旧哈希表的每个桶，重新计算桶里元素的新位置 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //如果桶上只有一个键值对，则直接插入 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果是通过红黑树来处理冲突的，则调用相关方法把树分离开 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果采用链式处理冲突 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //通过上面讲的方法来计算节点的新位置 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。 例如，原来的容量为32，那么应该拿hash跟31（0x11111）做与操作；在扩容扩到了64的容量之后，应该拿hash跟63（0x111111）做与操作。新容量跟原来相比只是多了一个bit位，假设原来的位置在23，那么当新增的那个bit位的计算结果为0时，那么该节点还是在23；相反，计算结果为1时，则该节点会被分配到23+31的桶上。 这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。回答了上文中好的扩容机制。 六、总结 HashMap的结构底层是一个数组，每个数组元素是一个桶，后面可能会连着一串因为碰撞而聚在一起的(key,value)节点，以链表的形式或者树的形式挂着 按照原来的拉链法来解决冲突，如果一个桶上的冲突很严重的话，是会导致哈希表的效率降低至O（n），而通过红黑树的方式，可以把效率改进至O（logn）。相比链式结构的节点，树型结构的节点会占用比较多的空间，所以这是一种以空间换时间的改进方式。 threshold是数组长度扩容的临界值 modCount字段主要用来记录HashMap内部结构发生变化的次数，这里结构变化必须是新的值塞进来或者某个值删除这种类型，而不是仅仅是覆盖 只有同时满足：表中数据容量已经扩容到MIN_TREEIFY_CAPACITY这个长度，并且桶里的数据个数达到8个的时候，才会将该桶里的结构进行树化。 好的hash算法：由于在计算中位运算比取模运算效率高的多，所以HashMap规定数组的长度为 2^n 。这样用 2^n - 1 与 hash 做位运算与取模效果一致，并且效率还要高出许多。 好的扩容机制：因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置。这样做的好处：正是因为这样巧妙的rehash方式，保证了rehash之后每个桶上的节点数必定小于等于原来桶上的节点数，即保证了rehash之后不会出现更严重的冲突。 还有就是要记住put的过程。 整理自： https://zhuanlan.zhihu.com/p/21673805 http://blog.csdn.net/u013124587/article/details/52649867]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashcode/Equals]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F4.hashcode%E5%92%8Cequals%2F</url>
    <content type="text"><![CDATA[hashcode涉及到集合HashMap等集合，此篇侧重于了解hashcode和equals方法的作用的原理。有助于下一篇HashMap的理解。 一、Hash是什么 Hash是散列的意思，就是把任意长度的输入，通过散列算法变换成固定长度的输出，该输出就是散列值。这个玩意还可以做加密。 不同关键字经过散列算法变换后可能得到同一个散列地址，这种现象称为碰撞。 如果两个Hash值不同（前提是同一Hash算法），那么这两个Hash值对应的原始输入必定不同 二、什么是hashcode HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的。 如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置。 如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写。为什么呢？下文会说。 三、HashCode有什么用 比方说Set里面已经有1000个元素了，那么第1001个元素进来的时候，最多可能调用1000次equals方法，如果equals方法写得复杂，对比的东西特别多，那么效率会大大降低。 使用HashCode就不一样了，比方说HashSet，底层是基于HashMap实现的，先通过HashCode取一个模，这样一下子就固定到某个位置了，如果这个位置上没有元素，那么就可以肯定HashSet中必定没有和新添加的元素equals的元素，就可以直接存放了，都不需要比较； 如果这个位置上有元素了，逐一比较，比较的时候先比较HashCode，HashCode都不同接下去都不用比了，肯定不一样，HashCode相等，再equals比较，没有相同的元素就存，有相同的元素就不存。 如果原来的Set里面有相同的元素，只要HashCode的生成方式定义得好（不重复），不管Set里面原来有多少元素，只需要执行一次的equals就可以了。这样一来，实际调用equals方法的次数大大降低，提高了效率。 当俩个对象的`hashCode`值相同的时候，`Hashset`会将对象保存在同一个位置，但是他们`equals`返回`false`，所以实际上这个位置采用链式结构来保存多个对象。 四、为什么重写Object的equals()方法尽量要重写Object的hashCode()方法 面临问题：若两个对象equals相等，但由于不在一个区间，因为hashCode的值在重写之前是对内存地址计算得出，所以根本没有机会进行比较，会被认为是不同的对象(这就是为什么还要重写hashcode方法了)。所以Java对于eqauls方法和hashCode方法是这样规定的： 1 如果两个对象相同(equals为true)，那么它们的hashCode值一定要相同。也告诉我们重写equals方法，一定要重写hashCode方法，也就是说hashCode值要和类中的成员变量挂上钩，对象相同–&gt;成员变量相同—-&gt;hashCode值一定相同。 2 如果两个对象的hashCode相同(只是映射到同一个位置而已)，它们并不一定相同，这里的对象相同指的是用eqauls方法比较。 简单来说，如果只重写equals方法而不重写hashcode方法，会导致重复元素的产生。具体通过下面的例子进行说明。 五、举例 6.1 Student类 很简单，定义了id和name两个字段，无参和有参构造函数，toString方法。 1234567891011121314151617181920public class Student &#123; private int id; private String name; get(),set()略... public Student()&#123;&#125; public Student(int id, String name) &#123; super(); this.id = id; this.name = name; &#125; @Override public String toString() &#123; return "Student [id=" + id + ", name=" + name + "]"; &#125;&#125; 6.2 main方法 1234567891011121314151617181920public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 执行结果： 1234set集合容量为: 3Student [id=1, name=hh]---1735600054Student [id=1, name=hh]---356573597Student [id=2, name=gg]---21685669 我们可以看到，只要是new的对象，他们的hashcode是不一样的。所以，就会认为他们是不一样的对象。所以，集合里面数量为3. 6.3 只重写equals()方法，而不重写HashCode()方法 输出： 1234set集合容量为: 3Student [id=2, name=gg]---2018699554Student [id=1, name=hh]---366712642Student [id=1, name=hh]---1829164700 结论：覆盖equals（Object obj）但不覆盖hashCode(),导致数据不唯一性。 在这里，其实我们可以看到，student1和student2其实是一个对象，但是由于都是new并且没有重写hashcode导致他们变成了两个不一样的对象。 分析： （1）当执行set.add(student1)时，集合为空，直接存入集合； （2）当执行set.add(student2)时，首先判断该对象（student2）的hashCode值所在的存储区域是否有相同的hashCode，因为没有覆盖hashCode方法，所以jdk使用默认Object的hashCode方法，返回内存地址转换后的整数，因为不同对象的地址值不同，所以这里不存在与student2相同hashCode值的对象，因此jdk默认不同hashCode值，equals一定返回false，所以直接存入集合。 （3）当执行set.add(student3)时,与2同理。 （4）当最后执行set.add(student1)时，因为student1已经存入集合，同一对象返回的hashCode值是一样的，继续判断equals是否返回true，因为是同一对象所以返回true。此时jdk认为该对象已经存在于集合中，所以舍弃。 6.4 只重写HashCode()方法，equals()方法直接返回false 1234set集合容量为: 3Student [id=1, name=hh]---4320Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 按照上面的分析，可能会觉得里面应该装4个，因为两次add的student1，虽然他们的hashcode一样，但是equals直接返回false，那么应该判定为两个不同的对象。但是结果确跟我们预想的不一样。 分析： 首先student1和student2的对象比较hashCode，因为重写了HashCode方法，所以hashcode相等,然后比较他们两的equals方法，因为equals方法始终返回false,所以student1和student2也是不相等的，所以student2也被放进了set 首先student1(student2)和student3的对象比较hashCode，不相等，所以student3放进set中 最后再看最后重复添加的student1,与第一个student1的hashCode是相等的，在比较equals方法，因为equals返回false,所以student1和student4不相等;同样，student2和student4也是不相等的;student3和student4的hashcode都不相等，所以肯定不相等的，所以最后一个重复的student1应该可以放到set集合中，那么结果应该是size:4,那为什么会是3呢？ 这时候我们就需要查看HashSet的源码了，下面是HashSet中的add方法的源码： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 这里我们可以看到其实HashSet是基于HashMap实现的，我们在点击HashMap的put方法，源码如下： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 首先是判断hashCode是否相等，不相等的话，直接跳过，相等的话，然后再来比较这两个对象是否相等或者这两个对象的equals方法，因为是进行的或操作，所以只要有一个成立即可，那这里我们就可以解释了，其实上面的那个集合的大小是3,因为最后的一个r1没有放进去，以为r1==r1返回true的，所以没有放进去了。所以集合的大小是3，如果我们将hashCode方法设置成始终返回false的话，这个集合就是4了。 6.5 同时重写 我的写法是： 12345678910111213141516171819@Overridepublic int hashCode() &#123; int result = 17; result = result * 31 + name.hashCode(); result = result * 31 + id; return result;&#125;@Overridepublic boolean equals(Object obj) &#123; if(obj == this) return true; if(!(obj instanceof Student)) return false; Student o = (Student)obj; return o.name.equals(name) &amp;&amp; o.id == id;&#125; 结果： 123set集合容量为: 2Student [id=2, name=gg]---118515Student [id=1, name=hh]---119506 达到我们预期的效果。 六、内存泄露 我们上面实验了重写equals和hashcode方法，执行main，执行结果是： 123set集合容量为: 2Student [id=1, name=hh]---4320Student [id=2, name=gg]---4319 将main方法改为： 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; Student student1 = new Student(1,"hh"); Student student2 = new Student(1,"hh"); Student student3 = new Student(2,"gg"); HashSet&lt;Student&gt; set = new HashSet&lt;Student&gt;(); set.add(student1); set.add(student2); set.add(student3); set.add(student1);//重复添加了student1 System.out.println("set集合容量为: "+set.size()); //------新增的开始------- student3.setId(11); set.remove(student3); System.out.println("set集合容量为: "+set.size()); //------新增的结束------- Iterator&lt;Student&gt; iterator = set.iterator(); while (iterator.hasNext()) &#123; Student student = iterator.next(); System.out.println(student+"---"+student.hashCode()); &#125; &#125; 运行结果是： 1234set集合容量为: 2set集合容量为: 2Student [id=1, name=hh]---4320Student [id=11, name=gg]---4598 我们调用了remove删除student3对象，以为删除了student3,但事实上并没有删除，这就叫做内存泄露，就是不用的对象但是他还在内存中。所以我们多次这样操作之后，内存就爆了。 原因： 在调用remove方法的时候，会先使用对象的hashCode值去找到这个对象，然后进行删除，这种问题就是因为我们在修改了对象student3的id属性的值，又因为RectObject对象的hashCode方法中有id值参与运算,所以student3对象的hashCode就发生改变了，所以remove方法中并没有找到student3了，所以删除失败。即student3的hashCode变了，但是他存储的位置没有更新，仍然在原来的位置上，所以当我们用他的新的hashCode去找肯定是找不到了。 总结： 上面的这个内存泄露告诉我一个信息：如果我们将对象的属性值参与了hashCode的运算中，在进行删除的时候，就不能对其属性值进行修改，否则会出现严重的问题。 七、总结 hashCode是为了提高在散列结构存储中查找的效率，在线性表中没有作用。 equals和hashCode需要同时覆盖。 若两个对象equals返回true，则hashCode有必要也返回相同的int数。 若两个对象equals返回false，则hashCode不一定返回不同的int数,但为不相等的对象生成不同hashCode值可以提高哈希表的性能。 若两个对象hashCode返回相同int数，则equals不一定返回true。 同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 整理自： http://blog.csdn.net/haobaworenle/article/details/53819838 http://www.cnblogs.com/xrq730/p/4842028.html http://blog.csdn.net/qq_21688757/article/details/53067814 http://blog.csdn.net/fyxxq/article/details/42066843]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CopyOnWriteArrayList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F3.CopyOnWriteArrayList%2F</url>
    <content type="text"><![CDATA[CopyOnWriteArrayList是ArrayList的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 CopyOnWriteArrayList是一个写时复制的容器，采用了读写分离的思想。通俗点来讲，在对容器进行写操作时，不直接修改当前容器，而是先对当前容器进行拷贝得到一个副本，然后对副本进行写操作，最后再将原容器的引用指向拷贝出来的副本。这样做的好处就是可以对容器进行并发读而不用进行加锁。 一、类的继承关系 12public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 含义不需要再赘述了。 二、类的属性 12345678910111213141516171819202122/** 用于在对数组产生写操作的方法加锁. */final transient ReentrantLock lock = new ReentrantLock();/** 底层的存储结构. */private transient volatile Object[] array;/** 反射机制. */private static final sun.misc.Unsafe UNSAFE;/** lock域的内存偏移量.是通过反射拿到的 */private static final long lockOffset;static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = CopyOnWriteArrayList.class; lockOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("lock")); &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 三、数组末尾添加一个元素 12345678910111213141516171819202122public boolean add(E e) &#123; // 可重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 元素数组 Object[] elements = getArray(); // 数组长度 int len = elements.length; // 复制数组 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将要添加的元素放到副本数组的末尾去 newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 基本原理很简单，就是对当前数组加锁，内部复制一个新数组，处理完毕，修改引用即可，达到最终一致的效果。 四、如果没有这个元素则添加 12345public boolean addIfAbsent(E e) &#123; Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false : addIfAbsent(e, snapshot);&#125; 该函数用于添加元素（如果数组中不存在，则添加；否则，不添加，直接返回）。如何可以保证多线程环境下不会重复添加元素？ 答案：通过快照数组和当前数组进行对比来确定是否一致，确保添加元素的线程安全 12345678910111213141516171819202122232425262728293031323334private boolean addIfAbsent(E e, Object[] snapshot) &#123; // 重入锁 final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 获取数组 Object[] current = getArray(); // 数组长度 int len = current.length; if (snapshot != current) &#123; // 快照不等于当前数组，对数组进行了修改 // 取较小者 int common = Math.min(snapshot.length, len); for (int i = 0; i &lt; common; i++) // 遍历 if (current[i] != snapshot[i] &amp;&amp; eq(e, current[i])) // 当前数组的元素与快照的元素不相等并且e与当前元素相等 // 表示在snapshot与current之间修改了数组，并且设置了数组某一元素为e，已经存在 // 返回false return false; if (indexOf(e, current, common, len) &gt;= 0) // 在当前数组中找到e元素 // 返回false return false; &#125; // 复制数组 Object[] newElements = Arrays.copyOf(current, len + 1); // 对数组len索引的元素赋值为e newElements[len] = e; // 设置数组 setArray(newElements); return true; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 该函数的流程如下： 获取锁，获取当前数组为current，current长度为len，判断数组之前的快照snapshot是否等于当前数组current，若不相等，则进入步骤2；否则，进入步骤3 不相等，表示在snapshot与current之间，对数组进行了修改，直接返回false结束; 说明当前数组等于快照数组，说明数组没有被改变。在当前数组中索引指定元素，若能够找到，说明已经存在此元素，直接返回false结束；否则进入4 说明没有当前要插入的元素，通过数组复制的方式添加到末尾 无论如何，都要释放锁 五、获取指定索引的元素 1234567public E get(int index) &#123; return get(getArray(), index);&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 通过写时复制的方式，CopyOnWriteArrayList 的 get 方法不用加锁也可以保证线程安全，所以 CopyOnWriteArrayList 并发读的效率是非常高的，它是直接通过数组下标获取元素的。 六、总结 简单而言要记住它的三个特点： CopyOnWriteArrayList 是一个并发的数组容器，它的底层实现是数组。 CopyOnWriteArrayList 采用写时复制的方式来保证线程安全。 通过写时复制的方式，可以高效的进行并发读，但是对于写操作，每次都要进行加锁以及拷贝副本，效率非常低，所以 CopyOnWriteArrayList 仅适合读多写少的场景。 Vector虽然是线程安全的，但是只是一种相对的线程安全而不是绝对的线程安全，它只能够保证增、删、改、查的单个操作一定是原子的，不会被打断，但是如果组合起来用，并不能保证线程安全性。 CopyOnWriteArrayList在并发下不会产生任何的线程安全问题，也就是绝对的线程安全 另外，有两点必须讲一下。 我认为CopyOnWriteArrayList这个并发组件，其实反映的是两个十分重要的分布式理念： （1）读写分离 我们读取CopyOnWriteArrayList的时候读取的是CopyOnWriteArrayList中的Object[] array，但是修改的时候，操作的是一个新的Object[] array，读和写操作的不是同一个对象，这就是读写分离。这种技术数据库用的非常多，在高并发下为了缓解数据库的压力，即使做了缓存也要对数据库做读写分离，读的时候使用读库，写的时候使用写库，然后读库、写库之间进行一定的同步，这样就避免同一个库上读、写的IO操作太多 （2）最终一致 对CopyOnWriteArrayList来说，线程1读取集合里面的数据，未必是最新的数据。因为线程2、线程3、线程4四个线程都修改了CopyOnWriteArrayList里面的数据，但是线程1拿到的还是最老的那个Object[] array，新添加进去的数据并没有，所以线程1读取的内容未必准确。不过这些数据虽然对于线程1是不一致的，但是对于之后的线程一定是一致的，它们拿到的Object[] array一定是三个线程都操作完毕之后的Object array[]，这就是最终一致。最终一致对于分布式系统也非常重要，它通过容忍一定时间的数据不一致，提升整个分布式系统的可用性与分区容错性。当然，最终一致并不是任何场景都适用的，像火车站售票这种系统用户对于数据的实时性要求非常非常高，就必须做成强一致性的。 最后总结一点，随着CopyOnWriteArrayList中元素的增加，CopyOnWriteArrayList的修改代价将越来越昂贵，因此，CopyOnWriteArrayList适用于读操作远多于修改操作的并发场景中。 感谢 http://www.cnblogs.com/xrq730/p/5020760.html http://blog.csdn.net/u013124587/article/details/52863533 https://www.cnblogs.com/leesf456/p/5547853.html]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F2.LinkedList%2F</url>
    <content type="text"><![CDATA[提到ArrayList，就会比较与LinkedList的区别。本文来看看LinkedList的核心原理。 如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。 一、LinkedList属性 123456//链表的节点个数.transient int size = 0;//Pointer to first node.transient Node&lt;E&gt; first;//Pointer to last node.transient Node&lt;E&gt; last; 二、Node的结构 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next;//后置指针 Node&lt;E&gt; prev;//前置指针 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 三、添加元素 3.1 LinkedList表头添加一个元素 当向表头插入一个节点时，很显然当前节点的前驱一定为 null，而后继结点是 first 指针指向的节点，当然还要修改 first 指针指向新的头节点。除此之外，原来的头节点变成了第二个节点，所以还要修改原来头节点的前驱指针，使它指向表头节点，源码的实现如下： 1234567891011121314151617public void addFirst(E e) &#123; linkFirst(e);&#125;private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); //新节点作为新的first节点 first = newNode; if (f == null) last = newNode;//初始就是个空LinkedList的话，last指向当前新节点 else f.prev = newNode;//初始值不为空，将其前置指针指向新节点 size++; modCount++;&#125; 3.2 LinkedList表尾添加一个元素 当向表尾插入一个节点时，很显然当前节点的后继一定为 null，而前驱结点是 last 指针指向的节点，然后还要修改 last 指针指向新的尾节点。此外，还要修改原来尾节点的后继指针，使它指向新的尾节点，源码的实现如下： 123456789101112131415161718public void addLast(E e) &#123; linkLast(e);&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; //新节点前置指针指向空，后置指针指向first节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); //新节点作为新的last节点 last = newNode; //如果原来有尾节点，则更新原来节点的后继指针，否则更新头指针 if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; 3.3 LinkedList在指定节点前添加一个元素 12345678910111213141516171819202122232425262728293031323334public void add(int index, E element) &#123; //判断数组是否越界 checkPositionIndex(index); if (index == size) linkLast(element);//直接插在最后一个 else linkBefore(element, node(index));//在index节点之前插入&#125;private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; //找到索引位置的前面一个元素pred final Node&lt;E&gt; pred = succ.prev; //新节点，前置指针指向pred,后置指针指向索引处元素 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); //修改索引出元素的前置指针为新节点 succ.prev = newNode; if (pred == null) first = newNode;//说明是插在表头 else pred.next = newNode;//说明是插在非表头位置，修改pred后置指针为新指针 size++; modCount++;&#125; 可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。 四、删除元素 删除操作与添加操作大同小异，例如删除指定节点的过程如下图所示，需要把当前节点的前驱节点的后继修改为当前节点的后继，以及当前节点的后继结点的前驱修改为当前节点的前驱。 就不赘述了。 五、获取元素 12345678910111213141516171819202122//获取指定索引对应的元素public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;//寻找元素的方向是根据index在表中的位置决定的Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123;//索引小于表长的一半，从表头开始往后找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123;//索引大于表长的一半，从表尾往前开始找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 上述代码，利用了双向链表的特性，如果index离链表头比较近，就从节点头部遍历。否则就从节点尾部开始遍历。使用空间（双向链表）来换取时间。 node()会以O(n/2)的性能去获取一个结点 如果索引值大于链表大小的一半，那么将从尾结点开始遍历 这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 总结 1、理论上无容量限制，只受虚拟机自身限制影响，所以没有扩容方法。 2、和ArrayList一样，LinkedList也是是未同步的，多线程并发读写时需要外部同步，如果不外部同步，那么可以使用Collections.synchronizedList方法对LinkedList的实例进行一次封装。 3、和ArrayList一样，LinkedList也对存储的元素无限制，允许null元素。 4、顺序插入速度ArrayList会比较快，因为ArrayList是基于数组实现的，数组是事先new好的，只要往指定位置塞一个数据就好了；LinkedList则不同，每次顺序插入的时候LinkedList将new一个对象出来，如果对象比较大，那么new的时间势必会长一点，再加上一些引用赋值的操作，所以顺序插入LinkedList必然慢于ArrayList 5、基于上一点，因为LinkedList里面不仅维护了待插入的元素，还维护了Entry的前置Entry和后继Entry，如果一个LinkedList中的Entry非常多，那么LinkedList将比ArrayList更耗费一些内存 6、数据遍历的速度，看最后一部分，这里就不细讲了，结论是：使用各自遍历效率最高的方式，ArrayList的遍历效率会比LinkedList的遍历效率高一些 7、有些说法认为LinkedList做插入和删除更快，这种说法其实是不准确的： LinkedList做插入、删除的时候，慢在寻址，快在只需要改变前后Entry的引用地址 ArrayList做插入、删除的时候，慢在数组元素的批量copy，快在寻址 所以，如果待插入、删除的元素是在数据结构的前半段尤其是非常靠前的位置的时候，LinkedList的效率将大大快过ArrayList，因为ArrayList将批量copy大量的元素；越往后，对于LinkedList来说，因为它是双向链表，所以在第2个元素后面插入一个数据和在倒数第2个元素后面插入一个元素在效率上基本没有差别，但是ArrayList由于要批量copy的元素越来越少，操作速度必然追上乃至超过LinkedList。 从这个分析看出，如果你十分确定你插入、删除的元素是在前半段，那么就使用LinkedList；如果你十分确定你删除、删除的元素在比较靠后的位置，那么可以考虑使用ArrayList。如果你不能确定你要做的插入、删除是在哪儿呢？那还是建议你使用LinkedList吧，因为一来LinkedList整体插入、删除的执行效率比较稳定，没有ArrayList这种越往后越快的情况；二来插入元素的时候，弄得不好ArrayList就要进行一次扩容，记住，ArrayList底层数组扩容是一个既消耗时间又消耗空间的操作. 8、ArrayList使用最普通的for循环遍历，LinkedList使用foreach循环比较快.注意到ArrayList是实现了RandomAccess接口而LinkedList则没有实现这个接口.关于RandomAccess这个接口的作用，看一下JDK API上的说法： 9、如果使用普通for循环遍历LinkedList，在大数据量的情况下，其遍历速度将慢得令人发指 整理自： 1、http://www.cnblogs.com/xrq730/p/5005347.html 2、http://blog.csdn.net/u013124587/article/details/52837848 3、http://blog.csdn.net/u011392897/article/details/57115818 4、http://blog.csdn.net/fighterandknight/article/details/61476335]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList/Vector]]></title>
    <url>%2F2019%2F01%2F20%2Fjava-collection%2F1.ArrayList%E5%92%8CVector%2F</url>
    <content type="text"><![CDATA[面试中，关于java的一些容器，ArrayList是最简单也是最常问的，尤其是里面的扩容机制。 ArrayList 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。 构造函数为： 123456789101112131415161718//用初始容量作为参数的构造方法public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //初始容量大于0，实例化数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //初始容量等于0，赋予空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125;//无参的构造方法public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 从构造方法中我们可以看见，默认情况下，elementData是一个大小为0的空数组，当我们指定了初始大小的时候，elementData的初始大小就变成了我们所指定的初始大小了。 ArrayList相当于动态数据，其中最重要的两个属性分别是: elementData 数组，以及 size 大小。 在调用 add() 方法的时候： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 首先进行扩容校验。 将插入的值放到尾部，并将 size + 1 。 如果是调用 add(index,e) 在指定位置添加的话： 12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 也是首先扩容校验。 接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。 其实扩容最终调用的代码: 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 也是一个数组复制的过程，ArrayList每次扩容都是扩1.5倍，然后调用Arrays类的copyOf方法，把元素重新拷贝到一个新的数组中去。 由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。 序列化 由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。 1transient Object[] elementData; 因此 ArrayList 自定义了序列化与反序列化： 1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。 从实现中可以看出 ArrayList 只序列化了被使用的数据。 Vector Vector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。 Vector比ArrayList多了一个属性： 1protected int capacityIncrement; 这个属性是在扩容的时候用到的，它表示每次扩容只扩capacityIncrement个空间就足够了。该属性可以通过构造方法给它赋值。先来看一下构造方法： 123456789101112131415public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 从构造方法中，我们可以看出Vector的默认大小也是10，而且它在初始化的时候就已经创建了数组了，这点跟ArrayList不一样。再来看一下grow方法： 12345678910private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从grow方法中我们可以发现，newCapacity默认情况下是两倍的oldCapacity，而当指定了capacityIncrement的值之后，newCapacity变成了oldCapacity+capacityIncrement。 以下是 add() 方法： 123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 以及指定位置插入数据: 12345678910111213public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt; " + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;]]></content>
      <tags>
        <tag>java容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年的最后一天，对商城项目的架构做个改造]]></title>
    <url>%2F2019%2F01%2F20%2F2018%E5%B9%B4%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%A4%A9%EF%BC%8C%E5%AF%B9%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%9E%B6%E6%9E%84%E5%81%9A%E4%B8%AA%E6%94%B9%E9%80%A0%2F</url>
    <content type="text"><![CDATA[一直以来都是学习慕课的实战视频，虽然也跟着做出了一些东西，但是思路都是别人提供好的，脱离了老师，我一直在问自己一个问题：能不能独立地按照自己的思路做出一些东西来？ 前方图片高能…更有几十兆gif演示动画，图片全部存放于七牛云上。 在去年，即2017年年底，我在慕课上学习了这两门课程： 第一期项目实现了比较简单的电商业务，整合SSM，并且部署到云端。 第二期实现了tomcat集群，配合redis实现分布式session，还有一些定时任务、redis分布式锁、maven环境隔离的一些东西，还涉及很多spring和springmvc的有用的机巧。 整体感觉是：一期实现业务，二期对于一期的提高不是太大，跟分布式无太大关系，仅仅实现了单点登陆和分布式session存储而已。 个人感觉下一期的课程应该是springCloud的分布式改造，进行服务拆分和治理。所以，在整合这两个课程的基础上用springCloud进行微服务治理。 项目详细描述 项目源码地址：https://github.com/sunweiguo/MMall 整体效果演示： 下面贴个小一点的gif: 部分页面截图： 经过一遍遍测试，商城还是存在一些无伤大雅的bug，但是主要还是锻炼自己的能力嘛！ 下订单的时候，报错：商品不存在或库存不足，是因为我模拟的秒杀，所以商品的库存要提前预置于redis中，后端管理系统的商品管理页面有预置库存的按钮。 新增商品的时候，对于上传图片，需要耐心等待一会，需要等待FTP服务器上传完毕，给一个返回信息(Map，是图片的文件名)才能真正显示（对于富文本中的图片上传，在上传之后需要等待一会，时间与小图上传差不多，否则直接保存不报错，但是前端看不到图，因为还没上传完毕,url还没回传回来） 普通注册的账号，没有管理员权限，所以不能登陆后台管理系统。 后来做了eureka集群，但是配置文件还是只指向其中一个eureka 在学习的视频中，一期只是实现业务功能，单体架构，一个tomcat。二期对其做了集群，并且解决了集群模式下session存储问题，实现了比较简单的单点登陆功能。架构如下： 对于上面的架构来说，只是做了一些集群进行优化，随着业务的发展，用户越来越多，用户服务等其他服务必然要拆分出来独立成为一个服务，这样做的好处是，一方面一个团队负责一个服务可以提高开发效率，另外，对于扩展性也是非常有利的，但是也是有缺点的，会带来很多的复杂性，尤其是引入了分布式事务，所以不能为了分布式而分布式，而是针对不同的业务场景而采用合适的架构。 微服务的实现，主要有两种，国内是阿里系的以dubbo+zookeeper为核心的一套服务治理和发现生态。另一个则是大名鼎鼎的spring cloud栈。 spring cloud并不是像spring是一个框架，他是解决微服务的一种方案，是由各种优秀开源组件共同配合而实现的微服务治理架构。下面的图是我构思的项目结构图： 最前面是Nginx，这里就作为一个静态资源映射和负载均衡，nginx中有几个配置文件，分别为www.oursnail.cn.conf，这个主要是对zuul网关地址做一个负载均衡，指向网关所在的服务器，并且找到前台页面所在位置对页面进行渲染。admin.oursnail.cn.conf，这个主要是配置后端以及后端的页面文件；img.oursnail.cn.conf是对图片服务器地址进行映射。 然后是zuul网关，这里主要是用来限流、鉴权以及路由转发。 再后面就是我们的应用服务器啦。对服务器进行了服务追踪(sleuth)，实现了动态刷新配置(spring cloud config+bus)等功能。以http restful的方式进行通信(openFeign),构建起以eureka为注册中心的分布式架构。 每个服务都是基于springboot打造，结合mybatis持久层操作的框架，完成基本的业务需求。springboot基于spring，特点是快速启动、内置tomcat以及无xml配置。将很多东西封装起来，引入pom就可以直接使用，比如springMVC就基本上引入starter-web即可。 由于资源的原因，只有三台最低配的服务器，所以本来想做的基于ES的全文检索服务没有做，也没有分库分表。 至于定时任务以及Hystrix服务熔断和降级，比较简单，就不做了。 项目的接口文档详见wiki：https://github.com/sunweiguo/spring-cloud-for-snailmall/wiki 项目的数据库表设计：snailmall.sql 下面详细介绍每个模块实现的大体思路（仅供参考，毕竟应届生，真实项目没做过）： 用户模块 关于用户模块，核心的功能是登陆。再核心是如何验证以及如何存储用户信息。这里采取的方案为： 对于用户注册，我这里就是用户名（昵称），那么如何保证不重复呢（高并发）？这里还是用了分布式锁来保证的。 对于未登陆章台下用户修改密码，逻辑为： 购物车模块 订单模块 针对这些问题，我想说一下我的思路。 对于幂等性，这里产生幂等性的主要原因在于MQ的重传机制，可能第一个消息久久没有发出去，然后重新发送一条，结果第一条消息突然又好了，那么就会重复发两跳，对于用户来说，只下一次单，但是服务器下了多次订单。网上解决这个问题的思路是创建一张表，如果是重复的订单号，就不可能创建多次了。还有一种可能方案是用分布式锁对该订单号锁住一段时间，由于只是锁住订单号，所以不影响性能，在这一段时间内是不可以再放同一个订单号的请求进来。 对于MQ消息不丢失，只能是订阅模式了。消息发出去之后，消费端给MQ回复一个接收到的信息，MQ本次消费成功，给订阅者一个回复。 对于全局唯一ID生成，这里用的是雪花算法，具体介绍可以看我的笔记 对于分布式事务，比较复杂，这里其实并没有真正处理，对于数据库扣减库存和数据库插入订单，他们在不同的数据库，廖师兄比较倾向的方式是： 这一切的基础还是需要有一个可靠的消息服务，确保消息要能送达。 针对redis预减库存存在的并发问题，这里的思路是用lua+redis，在预减之前判断库存是否够，这两个操作要在一个原子操作里面才行，lua恰好可以实现原子性、顺序性地操作。 支付模块 这里对接的是支付宝-扫码支付，用到是支付宝沙箱环境。支付的扫码支付详细流程在这里聊一聊哈。 商户前台将商品参数发送至商户后台，商户后台生成内部订单号并用于请求支付平台创建预下单，支付平台创建完预订单后将订单二维码信息返还给商户，此时用户即可扫取二维码进行付款操作。 内部订单号：这是相对于支付宝平台而言的，这个订单号是我们商城自己生成的，对于我们商城来说是内部订单号，但是对于支付宝来说是外部订单号。 将一系列的数据按照支付宝的要求发送给支付宝平台，包括商品信息，生成的验签sign，公钥；支付宝去将sign解密，进行商品的各种信息校验。校验通过，同步返回二维码串。 支付业务流程图： 在获取支付的二维码串之后，用工具包将其转换未二维码展示给用户扫码。 用户扫码后，会收到第一次支付宝的回调，展示要支付的金额，商品信息等。 用户输入密码成功后，正常情况会收到支付宝的第二次回调，即支付成功信息。 但是也可能会由于网络等原因，迟迟收不到支付宝的回调，这个时候就需要主动发起轮询去查看支付状态。 在支付成功之后，接收回调的接口要记得返回success告诉支付宝我已经收到你的回调了，不要再重复发给我了。接收回调的接口也要做好去除重复消息的逻辑。 这个流程是多么地简单而理所当然！ 对应于代码层面，其实就是两个接口，一个是用户点击去支付按钮，此时发起预下单，展示付款二维码，另一个是接收支付宝回调： 预下单： 接收支付宝支付状态回调： 项目进展 [x] 2018/12/31 完成了聚合工程的创建、Eureka服务注册中心、spring cloud config+gitHub+spring cloud bus（rabbitMQ）实现配置自动刷新–v1.0 [x] 2018/12/31 将Eureka注册中心(单机)和配置中心部署到服务器上，这比较固定，所以先部署上去，以后本地就直接用这两个即可，对配置进行了一点点修改 [ ] 2018/12/31 关于配置的自动刷新，用postman发送post请求是可以的，但是用github webhook不行，不知道是不是这个版本的问题 [x] 2018/12/31 用户模块的逻辑实现,首先增加了一些pom文件的支持，整合mybatis，测试数据库都通过，下面就可以真正去实现业务代码了 [x] 2019/1/1 完成用户注册、登陆、校验用户名邮箱有效性、查看登陆用户信息、根据用户名去拿到对应的问题、校验答案是否正确、重置密码这个几个接口，在注册这个接口，增加一个ZK分布式锁来防止在高并发场景下出现用户名或邮箱重复问题 [x] 2019/1/2 上午完成门户用户模块所有接口–v2.0 [x] 2019/1/2 下午完成品类管理模块，关于繁琐的获取用户并且鉴权工作，这里先放每个接口里面处理，后面放到网关中去实现–v3.0 [x] 2019/1/3 上午引入网关服务，将后台重复的权限校验统一放到网关中去做，并且加了限流，解决了一下跨域问题。–v4.0 [x] 2019/1/3 下午和晚上完成门户和后台的商品管理模块所有的接口功能，除了上传文件的两个接口没有测试以外，其他接口都进行了简单的测试，其中还用Feign去调用了品类服务接口–v5.0 [x] 2019/1/3 初步把购物车模块和模块引入，通过基础测试，后面在此基础上直接开发代码即可，明天下午看《大黄蜂》，晚上师门聚餐吃火锅，明天早上赶一赶吧，今天任务结束！ [x] 2019/1/4 整理了接口文档，并且画了一下购物车模块的流程图以及订单服务的流程图，针对订单服务中，记录了需要一些注意的问题，尽可能地完善，提高可用性和性能。并且完成购物车模块的controller层。 [x] 2019/1/5 完成购物车模块，并且进行了简单的测试，这里进行了两处改造，一个是判断了一下是否需要判断库存；另一个是商品信息从redis中取，取不出来则调用商品服务初始化值 [x] 2019/1/5 收货地址管理模块，这个模块就是个增删改查，没啥东西写，这里就不加缓存了。 [x] 2019/1/6 完成了后台订单管理模块并且进行了测试，调用收货地址服务时，发现收货地址服务无法读取到cookie，通过这个方法(https://blog.csdn.net/WYA1993/article/details/84304243) 暂时解决了问题 [x] 2019/1/6 预置所有商品库存到redis中；预置所有商品到redis中；大概确定好订单服务思路：预减库存（redis判断库存）—对userID增加分布式锁防止用户重复提交订单–MQ异步下订单 [x] 2019/1/6 新增全局唯一ID生成服务，雪花算法实现 [x] 2019/1/7 完善订单服务-这一块涉及跨库操作，并且不停地调用其他服务，脑子都快晕了，这里采取的策略是：用到购物车的时候，去调用购物车服务获取；产品详情从redis中获取。首先将商品以及商品库存全部缓存到redis中，然后用户下单，先从redis中判断库存，够则减，判断 和扣减放在lua脚本中原子执行，然后MQ异步出去生成订单（生成订单主表和订单详情表放在一个本地事务中），这两步操作成功之后，再用MQ去异步删除购物车。MQ消费不成功则重试。 对于扣减库存这一步，想法是用定时任务，定时与redis中进行同步。这里是模拟了秒杀场景，预减库存+MQ异步，提交订单–&gt;redis判断并且减库存–&gt;调用cart-service获取购物车–&gt;MQ异步(userId,shippingId)生成订单主表和详情表–&gt;上面都成功，则MQ异步(userId) 去清除购物车，库存用定时任务去同步(未做)，理想的做法是：MQ异步扣减库存，订单服务订阅扣减库存消息，一旦库存扣减成功，则进行订单生成。 [x] 2019/1/8 继续完善订单接口，完成支付服务，就直接放在订单服务里面了，因为与订单逻辑紧密，就放在一起了。 [x] 2019/1/8 使用了一下swagger，发现代码侵入比较强，每一个接口上面都要手动打上响应的注解 [x] 2019/1/8 关于hystrix熔断与降级，可以引入hystrix的依赖，用@HystrixCommand注解来控制超时时间、服务降级以及服务熔断。也可以直接再@FeignClient接口中指定服务降级的类，这里不演示了，因为设置比如超时时间，我还要重新测试，写起来很简单，测起来有点儿麻烦 [x] 2019/1/9 服务跟踪，服务端是直接用的线程的，只需要下载：wget -O zipkin.jar ‘https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec’，然后nohup java -jar zipkin.jar &gt; zipkin.server.out &amp; ，开放9411端口，打开浏览器http://ip:9411看到页面即可。 客户端只需要添加相应依赖和配置文件即可。用客户端测试，发现死活不出现我的请求，经过搜索，发现需要增加spring.zipkin.sender.type= web这个配置项才行. [x] 2019/1/10 初步把项目部署到服务器上，进行测试，bug多多，修改中… [x] 2019/1/10 改了一天的bug，其中网关的超时时间以及feign的超时时间都要改大一点，否则会超时报错。最终成功，花了三台服务器，部署了11个服务。后面把部署过程写一下。 [x] 2019/1/11 将注册中心做成集群，因为早上一起来，注册中心挂了？？？ [ ] 2019/1/11 docker部署(商城第四期的改造目标:容器化+容器编排)，本期改造结束。 [x] 2019/1/11 完善readme文档 [x] 2019/1/12 两次发现redis数据被莫名其妙清空，我确定不是缓存到期，为了安全起见，设置了redis的密码，明天看缓存数据还在不在。 [x] 2019/1/14 redis数据没有再丢失，修复用户更新信息的bug 项目启动 安装redis、zookeeper、mysql、jdk、nginx以及rabbitMQ。 对代码进行maven-package操作。打包成jar包。将其放到服务器上： 执行nohup java -jar snailmall-user-service.8081 &gt; user-service.out &amp;后台启动即可。 补充：针对配置刷新，修改了github信息，用postman请求http://xxxxx:8079/actuator/bus-refresh 触发更新。 本改造是基于快乐慕商城一期和快乐慕商城二期的基础上进行改造。所以需要在其业务基础上改造会比较顺手。关于微服务，尤其是电商中的一些处理手段，很多思路都是学习于码吗在线中分布式电商项目。再加上慕课网廖师兄的spring cloud微服务实践。 前台项目 只要阅读readme文档即可。代码仓库为：https://github.com/sunweiguo/snailmall-front 学习不仅要有输入，更要有自己的输出，实践是提升的捷径！]]></content>
      <tags>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试-进程与线程]]></title>
    <url>%2F2019%2F01%2F19%2F%E9%9D%A2%E8%AF%95-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[都是操作系统管理的对象，比较容易混淆，但是又是两样完全不同的东西，因此区别很多。从他们区别也可以发散出很多关于操作系统比较重要的知识。所以面试比较常问。 1. 进程到底是什么？ 翻了一下书：《操作系统概念》第三章中提及了进程的概念。他是这样说的： 进程是执行中的程序，这是一种非正式说法。进程不只是程序代码（代码块），进程还包含了当前活动，通过程序计数器的值和处理器寄存器的内容来表示。此外，进程还包含进程堆栈段（包括临时数据，如函数参数、返回地址和局部变量）和数据段（包括全局变量）。进程还可能包含堆，是在进程运行期间动态分配的内存。 程序本身不是进程。程序只是被动实体，如存储在磁盘上包含一系列指令的文件内容。而进程是活动实体，它有一个程序计数器来表示下一个要执行的命令和相关资源集合。当一个可执行文件被装入内存时，一个程序才能成为进程。 总结一下进程是什么，是我个人理解：它是一个活动实体，运行在内存上。然后它占用很多独立的资源，比如：内存资源、程序运行肯定涉及CPU计算、占用的端口资源（公共的）、文件资源（公共的）、网络资源（公共的）等等等。要想执行这个进程，首先要有一个可执行文件，有了这个可执行文件，还要有相应的执行需要的资源。所以将可执行文件、当前进程的上下文、内存等资源结合起来，才是一个真正的进程。 那么，我们就可以理解一句话：进程是资源分配的基本单位。 进程中的内存空间（虽然空间大小都一样，下文会说明）是独立的，否则就会出现一种情况：修改自己程序中的某个指针就可以指向其他程序中的地址，然后拿到里面的数据，岂不是很恐怖的场景？ 如上图，进程中包含了线程。操作系统可能会运行几百个进程，进程中也可能有几个到几百个线程在运行。 文件和网络句柄是所有进程共享的，多个进程可以去打开同一个文件，去抢占同一个网络端口。 图中还有个内存。这个内存不是我们经常说的内存条，即物理内存，而是虚拟内存，是进程独立的，大小与实际物理内存无关。 2. 寻址空间 比如8086只有20根地址线，那么它的寻址空间就是1MB，我们就说8086能支持1MB的物理内存，及时我们安装了128M的内存条在板子上，我们也只能说8086拥有1MB的物理内存空间。 以前叫卖的32位的机子，32位是指寻址空间为2的32次方。32位的386以上CPU就可以支持最大4GB的物理内存空间了。 3. 为什么会有虚拟内存和物理内存的区别 正在运行的一个进程，他所需的内存是有可能大于内存条容量之和的，比如你的内存条是256M，你的程序却要创建一个2G的数据区，那么不是所有数据都能一起加载到内存（物理内存）中，势必有一部分数据要放到其他介质中（比如硬盘），待进程需要访问那部分数据时，在通过调度进入物理内存。 所以，虚拟内存是进程运行时所有内存空间的总和，并且可能有一部分不在物理内存中，而物理内存就是我们平时所了解的内存条。 关键的是不要把虚拟内存跟真实的插在主板上的内存条相挂钩，虚拟内存它是“虚拟的”不存在，假的啦，它只是内存管理的一种抽象！ 4. 虚拟内存地址和物理内存地址是如何映射呢 假设你的计算机是32位，那么它的地址总线是32位的，也就是它可以寻址0 ~ 0xFFFFFFFF（4G）的地址空间，但如果你的计算机只有256M的物理内存0x~0x0FFFFFFF（256M），同时你的进程产生了一个不在这256M地址空间中的地址，那么计算机该如何处理呢？回答这个问题前，先说明计算机的内存分页机制。 计算机会对虚拟内存地址空间（32位为4G）分页产生页（page），对物理内存地址空间（假设256M）分页产生页帧（page frame），这个页和页帧的大小是一样大的，所以呢，在这里，虚拟内存页的个数势必要大于物理内存页帧的个数。 在计算机上有一个页表（page table），就是映射虚拟内存页到物理内存页的，更确切的说是页号到页帧号的映射，而且是一对一的映射。但是问题来了，虚拟内存页的个数 &gt; 物理内存页帧的个数，岂不是有些虚拟内存页的地址永远没有对应的物理内存地址空间？ 不是的，操作系统是这样处理的。操作系统有个页面失效（page fault）功能。操作系统找到一个最少使用的页帧，让他失效，并把它写入磁盘，随后从磁盘中把把需要访问的数据所在的页放到最少使用的页帧中，并修改页表中的映射（即修改页号指向当前页帧），这样就保证所有的页都有被调度的可能了。这就是处理虚拟内存地址到物理内存的步骤。 至于里面如何实现的细节，我没有过多去探究。 5. 什么是虚拟内存地址和物理内存地址 虚拟内存地址由页号和偏移量组成。页号就是上面所说的。偏移量就是我上面说的页（或者页帧）的大小，即这个页（或者页帧）到底能存多少数据。 举个例子，有一个虚拟地址它的页号是4，偏移量是20，那么他的寻址过程是这样的：首先到页表中找到页号4对应的页帧号（比如为8），如果找不到对应的页桢，则用失效机制调入页。如果存在，把页帧号和偏移量传给MMU（CPU的内存管理单元）组成一个物理上真正存在的地址，接着就是访问物理内存中的数据了。 6. 线程里面有什么 写到这里，好像还与本标题无关，即进程和线程到底是什么关系和区别等。但是我们要知道，面试或者学习一个知识点，不是为了学习这个区别而学习， 我们应该学习为什么有进程和线程，有了进程还需要线程吗？有了线程还要进程吗？你说进程是资源分配的单位，分配的是什么资源呢？进程中的内存是咋管理的呢？虚拟内存和物理内存是什么？什么是虚拟内存地址和物理内存地址？等等等，所以面试是千变万化的，重要的是我们尽可能地多问自己几个为什么，然后从为什么开始去逐个击破，形成一个体系。 说说这个栈，我们知道，执行程序从主程序入口进入开始，可能会调用很多的函数，那么这些函数的参数和返回地址都会被压入栈中，包括这些函数中定义的临时局部变量都会压入栈中，随着函数的执行完毕，再逐层地弹出栈，回到主函数运行的地方，再继续执行。 PC(program counter)，就是程序计数器，指向的下一条指令执行的地址。 由此可见，操作系统运行的其实是一个一个的线程，而进程只是一个隔离资源的容器。 上面说到，PC是指向下一条指令执行的地址。而这些指令是放在内存中的。 我们的计算机大多数是存储程序型的。就是说数据和程序是同时存储在同一片内存里的。 所以我们经常会听到一个漏洞叫做“缓冲区溢出”：比如有一个地方让用户输入用户名，但是黑客输入很长很长的字符串进去，那么很有可能就会超出存放这个用户名的一片缓冲区，而直接侵入到存放程序的地方，那么黑客就可以植入程序去执行。解决方案就是限制输入的用户名长度，不要超过缓冲区大小。 还有一块是TLS(thread local storage)，我们知道进程有自己独立的内存，那么我们的线程能不能也有一小块属于自己的内存区域呢？ 这个东西，其实很简单，就是说，比如new一个对象，往往是在堆中开辟空间的，但是现在的情况是：在一个函数内，new出来一个对象，这个对象不引用外部对象，也不会被外部引用，是纯粹属于这个函数段，可以理解为这个对象是属于这个函数的局部临时变量。 此时，new这个对象就不需要再去堆中开辟空间了，因为一方面不需要共享，另一方面是在堆中开辟是比较慢的，并且可能有很多函数，这种局部对象零零总总加起来还是很多的，在堆中开辟会浪费空间。 所以，能不能在栈中就可以new出这个对象，反正用完就扔。TLS可以是现在这个。栈中直接new多方便多快，因为不需要走垃圾回收机制，还避免了线程安全问题。可以去搜索：栈上分配和逃逸分析 7. 线程VS进程 到这里，就清晰了很多。我们也可以多多少少理解他们的区别。 可以做个简单的比喻，便于记忆：进程=火车，线程=车厢 线程在进程下行进（单纯的车厢无法运行） 一个进程可以包含多个线程（一辆火车可以有多个车厢） 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘） 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易） 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源） 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢） 进程可以拓展到多机，进程最多适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上） 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－“互斥锁” 进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量” 再补充几句。 线程是调度的基本单位，进程是资源分配的基本单位 进程间没有共享内存，所以交互要通过TCP/IP端口的等方式来实现。线程间由于有共享内存，所以交互比较方便。 线程占用很多资源，而线程只需要分配栈和PC即可。 8. 针对虚拟内存和物理内存的总结 每个进程都有自己独立的4G(32位系统下)内存空间，各个进程的内存空间具有类似的结构 一个新进程建立的时候，将会建立起自己的内存空间，此进程的数据，代码等从磁盘拷贝到自己的进程空间（建立一个进程，就要把磁盘上的程序文件拷贝到进程对应的内存中去，对于一个程序对应的多个进程这种情况，浪费内存！），哪些数据在哪里，都由进程控制表中的task_struct记录 每个进程的4G内存空间只是虚拟内存空间，每次访问内存空间的某个地址，都需要把地址翻译为实际物理内存地址 所有进程共享同一物理内存，每个进程只把自己目前需要的虚拟内存空间映射并存储到物理内存上。 进程要知道哪些内存地址上的数据在物理内存上，哪些不在，还有在物理内存上的哪里，需要用页表来记录 页表的每一个表项分两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址（如果在的话） 当进程访问某个虚拟地址，去看页表，如果发现对应的数据不在物理内存中，则缺页异常 缺页异常的处理过程，就是把进程需要的数据从磁盘上拷贝到物理内存中，如果内存已经满了，没有空地方了，那就找一个页覆盖，当然如果被覆盖的页曾经被修改过，需要将此页写回磁盘 9. 关于进程和线程更深的认识 关于为什么要分进程和线程，先抛出结论： 进程process：进程就是时间总和=执行环境切换时间+程序执行时间------&gt;CPU加载执行环境-&gt;CPU执行程序-&gt;CPU保存执行环境 线程thread：线程也是时间总和=执行环境切换时间（共享进程的）+程序模块执行时间------&gt;CPU加载执行环境（共享进程的）-&gt;CPU执行程序摸块-&gt;CPU保存执行环境（共享进程的） 进程和线程都是描述CPU工作的时间段，线程是更细小的时间段。 那么，如果CPU时间片临幸本进程，那么这个进程在恢复执行环境之后，执行里面的若干线程就不需要再不停地切换执行环境了，所以说，线程相比于进程是比较轻量的。 在CPU看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。。 进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文 线程是什么呢？进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成：程序A得到CPU =》CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。 这里a，b，c的执行是共享了A的上下文，CPU在执行的时候没有进行上下文切换的。这里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境、更为细小的CPU时间段。 进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同。 整理自： https://www.zhihu.com/question/25532384 https://blog.csdn.net/moshenglv/article/details/52242153 https://blog.csdn.net/u012861978/article/details/53048077]]></content>
      <tags>
        <tag>操作系统相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步一步理解HTTPS]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F7.%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3HTTPS%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第七篇文章。HTTPS（SSL/TLS）的加密机制是前端后端ios安卓等都应了解的基本问题。也是面试经常问的点。 一、为什么需要加密？ 小时候看谍战片，情报发过来了之后，用一个小本本进行翻译，然后解密出情报。加密就是防止明文被别人看到甚至篡改嘛！ 回到互联网，因为http的内容是明文传输的，明文数据会经过中间代理服务器、路由器、wifi热点、通信服务运营商等多个物理节点，如果信息在传输过程中被劫持，传输的内容就完全暴露了，他还可以篡改传输的信息且不被双方察觉，这就是中间人攻击。所以我们才需要对信息进行加密。最简单容易理解的就是对称加密 。 二、什么是对称加密？ 小明写个求爱信给小红，小明担心小红的妈妈看到这封信的内容，他灵机一动，对信加个密，并确定好我用这个密钥加密的，小红收到之后也用这个密钥解密才行。 但是呢，这里有个麻烦的地方就是，小明和小红不在一个学校，这个钥匙呢，不方便直接送到手里。所以呢，小明得想办法把这个钥匙寄一个送给小红，好吧，就用最贵的顺丰吧！ 就是有一个密钥，它可以对一段内容加密，加密后只能用它才能解密看到原本的内容，和我们日常生活中用的钥匙作用差不多。 三、用对称加密可行吗？ 顺丰快递到了，结果小红不在家，小红的妈妈收到了，一看是个男同学寄的，怎么能忍住，赶紧打开，以看是一把钥匙，作为程序猿，妈妈得意一笑：哼哼，还能逃过我的眼睛？我赶紧复制一把藏着，我倒要看看他后面要寄啥来，还要加密？！ 果然小红的妈妈等到了来自小明寄过来的情书，解密一看，实锤早恋。 同样地，小明这边也非常危险，快递员刚出发，就被小明的妈妈拦截了，拿到了这个钥匙，那小明还没寄出的信已经被妈妈看光了。 所以问题的根本就是，这把钥匙要传输，传输就可能被截取。 回到互联网，如果通信双方都各自持有同一个密钥，且没有别人知道，这两方的通信安全当然是可以被保证的（除非密钥被破解）。 然而最大的问题就是这个密钥怎么让传输的双方知晓，同时不被别人知道。 如果由服务器生成一个密钥并传输给浏览器，那这个传输过程中密钥被别人劫持弄到手了怎么办？ 换种思路？试想一下，如果浏览器内部就预存了网站A的密钥，且可以确保除了浏览器和网站A，不会有任何外人知道该密钥，那理论上用对称加密是可以的，这样浏览器只要预存好世界上所有HTTPS网站的密钥就行啦！这么做显然不现实。 怎么办？所以我们就需要神奇的非对称加密。 四、什么是非对称加密？ 有两把密钥，通常一把叫做公钥、一把叫做私钥。 用公钥加密的内容必须用私钥才能解开，同样，私钥加密的内容只有公钥能解开。 五、用非对称加密可行吗？ 公钥呢，还是要通过快递员送给小红的。OK，假设小红要回信，写好了用公钥加密，小红的妈妈因为拿不到私钥，看不到信的内容。 OK，但是反过来呢？小明用私钥加密传给小红，那么小红的妈妈可就能解密了（因为公钥可能会被小红的妈妈拿到）。 回到互联网，服务器先把公钥直接明文传输给浏览器，之后浏览器向服务器传数据前都先用这个公钥加密好再传，这条数据的安全似乎可以保障了！因为只有服务器有相应的私钥能解开这条数据。 然而由服务器到浏览器的这条路怎么保障安全？ 如果服务器用它的的私钥加密数据传给浏览器，那么浏览器用公钥可以解密它，而这个公钥是一开始通过明文传输给浏览器的，这个公钥被谁劫持到的话，他也能用该公钥解密服务器传来的信息了。 所以目前似乎只能保证由浏览器向服务器传输数据时的安全性（其实仍有漏洞，下文会说）。 六、改良的非对称加密方案，似乎可以？ 小明和小红年纪不大，但是很聪明，针对这个情况，还是迅速升级加密方法。他们想到既然一组公钥私钥不够，那两组呢？ OK，小明和小红各造了一对。下面就是互相交换公钥。那么就变成： 下面就好办啦，小明写信用公钥B加密，那么信的内容只有小红能破解，因为小红是随身携带私钥B。相反，小红用公钥A对信加密，这样只有小明能破解，因为小明也是随身携带私钥A。好像很安全啦！除了下面提到的漏洞，唯一的缺点可能是：小红得花半天时间才能解密完这封信，有点受不了。 回到互联网。请看下面的过程： 某网站拥有用于非对称加密的公钥A、私钥A；浏览器拥有用于非对称加密的公钥B、私钥B。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器把公钥B明文传输给服务器。 之后浏览器向服务器传输的所有东西都用公钥A加密，服务器收到后用私钥A解密。由于只有服务器拥有这个私钥A可以解密，所以能保证这条数据的安全。 服务器向浏览器传输的所有东西都用公钥B加密，浏览器收到后用私钥B解密。同上也可以保证这条数据的安全。 的确可以！抛开这里面仍有的漏洞不谈（下文会讲），HTTPS的加密却没使用这种方案，为什么？最主要的原因是非对称加密算法非常耗时，特别是加密解密一些较大数据的时候有些力不从心。 七、非对称加密+对称加密？ 小明也知道，这个信很长，用非对称加密，太慢！办法也有，没有必要对那么长的信加密，我只要保证这个真正解密的钥匙不被别人拿到就行，那么他灵机一动想到这个方法： 小明和小红利用非对称加密对钥匙加密，姑且认为是这个钥匙被放在了一个盒子里，这个盒子也被锁起来了，只有小红或者小明才能打开盒子，再用钥匙去解密。 这个真正用于对称加密解密的钥匙别人就拿不到啦！ 自从用了这个方案，感觉又安全，解密又快，感情又深温了呢！ 回到互联网，步骤如下： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。 浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都用密钥X加密解密。 HTTPS的基本思想就是基于这个。但是这个方案也存在上面一直在说的漏洞。 八、中间人攻击 像妈妈这样级别的程序猿可能是那他们两没办法啦，但是呢，校区有个看门的大爷，以前是个黑客，也不知道咋回事，明明才50岁，但是看起来像80岁，头上光溜溜的，冬天冷呢。整天在那胡言乱语：docker牛逼啊，spring cloud牛逼啊，这个开源软件XXX写的真好，跟周围的老大爷老大妈根本谈不到一起去。 他也是闲的蛋疼，非要掺和，因为据说他以前单身30年，苦逼敲代码，不知道谈恋爱是啥滋味，姑且认为他好奇心重吧。 在小明第一次寄公钥A的时候，大爷出手了，截取下来。换成自己做的公钥B。然后送给小红。 小红哪里会知道这公钥被掉包了呢，所以直接就用了，按照正常步骤，小红想了一个随机字符串，这次就叫xiaomingwoxuanni吧，OK，用这个公钥B对这个字符串加个密，这个字符串就被锁进了用大爷公钥B锁的盒子里。 老大爷在门口守着呢，一看到小红寄东西了，又偷偷地截取下来，用自己的私钥B来解密这个盒子。轻易地拿到了里面的字符串，OK，怕小明察觉，再用小明寄来的公钥A加密传给小明，这样双方都不知道他们的钥匙已经被大爷给获取了。 小明和小红之间的信就用xiaomingwoxuanni这个钥匙进行对称加密和对称解密，完全不知道有个大爷就天天拿着这个字符串去解密信件，看的不亦乐乎，甚至还偷偷改几个字呢。 回到互联网。中间人的确无法得到浏览器生成的密钥B，这个密钥本身被公钥A加密了，只有服务器才有私钥A解开拿到它呀！然而中间人却完全不需要拿到密钥A就能干坏事了。请看： 某网站拥有用于非对称加密的公钥A、私钥A。 浏览器向网站服务器请求，服务器把公钥A明文给传输浏览器。 中间人劫持到公钥A，保存下来，把数据包中的公钥A替换成自己伪造的公钥B（它当然也拥有公钥B对应的私钥B）。 浏览器随机生成一个用于对称加密的密钥X，用公钥B（浏览器不知道公钥被替换了）加密后传给服务器。 中间人劫持后用私钥B解密得到密钥X，再用公钥A加密后传给服务器。 服务器拿到后用私钥A解密得到密钥X。 这样在双方都不会发现异常的情况下，中间人得到了密钥B。根本原因是浏览器无法确认自己收到的公钥是不是网站自己的。只要解决了这个公钥一定是这个网站发来的，那么基本就OK了 九、如何证明浏览器收到的公钥一定是该网站的公钥？ 现实生活中，如果想证明某身份证号一定是小明的，怎么办？看身份证。这里政府机构起到了“公信”的作用，身份证是由它颁发的，它本身的权威可以对一个人的身份信息作出证明。互联网中能不能搞这么个公信机构呢？给网站颁发一个“身份证”？ 十、数字证书 网站在使用HTTPS前，需要向“CA机构”申请颁发一份数字证书，即SSL证书，数字证书里有证书持有者、证书持有者的公钥等信息，服务器把证书传输给浏览器，浏览器从证书里取公钥就行了，证书就如身份证一样，可以证明“该公钥对应该网站”。然而这里又有一个显而易见的问题了，证书本身的传输过程中，如何防止被篡改？即如何证明证书本身的真实性？身份证有一些防伪技术，数字证书怎么防伪呢？解决这个问题我们就基本接近胜利了！ SSL证书内容： 证书的发布机构CA 证书的有效期 公钥 证书所有者 签名 十一、如何放防止数字证书被篡改？ 我们把证书内容生成一份“签名”，比对证书内容和签名是否一致就能察觉是否被篡改。这种技术就叫数字签名。 提到数字签名，其实原理很简单啦，就是比如我要传输一句话叫：“你给我转100块钱，我的账号是123456，转完了告诉我一声。”，如果不做任何处理，被刚才的老大爷截取了，他偷偷地改一下内容“你给我转200块钱，我的账号是654321，不要告诉任何人，尤其是你嫂子。” 是不是太坏了，弄不好被抓，大爷可不敢做大的，只敢骗个喝酒钱。 那么怎么防止大爷这种猥琐技术又高的人篡改呢？数字签名排上用场啦！ 以后再传消息就是“你给我转100块钱，我的账号是123456，转完了告诉我一声。”+“！……@&amp;@%#……！￥@￥！@%……#￥！%……”,后面那一串东西就是数字签名，简单来说，就是想办法对前面的内容进行非对称加密（这样别人根本不知道你加密的私钥是什么，也就伪装不了签名了）。传过去之后，我要对其进行解密，与传过来的明文一一对比参数，看有没有被改动过。一旦发现哪里不对应，说明已经被篡改了。 “CA机构”制作签名的过程： CA拥有非对称加密的私钥和公钥。 CA对证书明文信息进行hash。 对hash后的值用私钥加密，得到数字签名。 明文和数字签名共同组成了数字证书，这样一份数字证书就可以颁发给网站了。网站把这个数字证书传给浏览器。 那浏览器拿到服务器传来的数字证书后，如何验证它是不是真的？（有没有被篡改、掉包） 浏览器验证过程： 拿到证书，得到明文T，数字签名S。 用CA机构的公钥对S解密（由于是浏览器信任的机构，所以浏览器保有它的公钥。详情见下文），得到S’。 用证书里说明的hash算法对明文T进行hash得到T’。 比较S’是否等于T’，等于则表明证书可信。 为什么这样可以证明证书可信呢？我们来仔细想一下。 十二、中间人有可能篡改该证书吗？ 老大爷就算有天大的能耐，也拿不到加密的私钥，那么只是单纯地篡改明文，只会造成校验不通过。 回到互联网，假设中间人篡改了证书的原文，由于他没有CA机构的私钥，所以无法得到此时加密后签名，无法相应地篡改签名。 浏览器收到该证书后会发现原文和签名解密后的值不一致，则说明证书已被篡改，证书不可信，从而终止向服务器传输信息，防止信息泄露给中间人。 十三、中间人有可能把证书掉包吗？ 假设有另一个网站B也拿到了CA机构认证的证书，它想搞垮网站A，想劫持网站A的信息。于是它成为中间人拦截到了A传给浏览器的证书，然后替换成自己的证书，传给浏览器，之后浏览器就会错误地拿到B的证书里的公钥了，会导致上文提到的漏洞。 其实这并不会发生，因为证书里包含了网站A的信息，包括域名，浏览器把证书里的域名与自己请求的域名比对一下就知道有没有被掉包了。 总结：因为一个网站域名对应一个证书，你的证书根其他人的证书肯定是不一样的，那么你就算拿到了其他人的证书再掉包成自己的，也没用，毕竟浏览器那边只要看一下是不是我要查看的域名。 十四、为什么制作数字签名时需要hash一次？ 最显然的是性能问题，前面我们已经说了非对称加密效率较差，证书信息一般较长，比较耗时。而hash后得到的是固定长度的信息（比如用md5算法hash后可以得到固定的128位的值），这样加密解密就会快很多。 十五、怎么证明CA机构的公钥是可信的？ 让我们回想一下数字证书到底是干啥的？没错，为了证明某公钥是可信的，即“该公钥是否对应该网站/机构等”，那这个CA机构的公钥是不是也可以用数字证书来证明？没错，操作系统、浏览器本身会预装一些它们信任的根证书，如果其中有该CA机构的根证书，那就可以拿到它对应的可信公钥了。 实际上证书之间的认证也可以不止一层，可以A信任B，B信任C，以此类推，我们把它叫做信任链或数字证书链，也就是一连串的数字证书，由根证书为起点，透过层层信任，使终端实体证书的持有者可以获得转授的信任，以证明身份。 另外，不知你们是否遇到过网站访问不了、提示要安装证书的情况？这里安装的就是根证书。说明浏览器不认给这个网站颁发证书的机构，那么没有该机构的根证书，你就得手动下载安装（风险自己承担XD）。安装该机构的根证书后，你就有了它的公钥，就可以用它验证服务器发来的证书是否可信了。 也就是说，公钥是从证书中获取的。证书是网站从机构那边申请来的，证书+签名传给浏览器。只要校验通过，那么公钥必然没有被篡改过，并且一定是这个网站传来的，那么解决了我们最核心的问题：确定公钥是我们指定的网站传来的。 既然公钥是正确的，那么小红就会用正确的公钥对随机字符串加密，中间不会出现篡改。 十六、HTTPS必须在每次请求中都要先在SSL/TLS层进行握手传输密钥吗？ 这也是我当时的困惑之一，显然每次请求都经历一次密钥传输过程非常耗时，那怎么达到只传输一次呢？用session就行。 服务器会为每个浏览器（或客户端软件）维护一个session ID，在TSL握手阶段传给浏览器，浏览器生成好密钥传给服务器后，服务器会把该密钥存到相应的session ID下，之后浏览器每次请求都会携带session ID，服务器会根据session ID找到相应的密钥并进行解密加密操作，这样就不必要每次重新制作、传输密钥了！ 十七、HTTPS原理 下面再来看看HTTPS原理就特别简单啦！ HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：可以理解为HTTP+SSL/TLS， 即 HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL，用于安全的 HTTP 数据传输。 1234HTTPSSL/TLSTCPIP 我们只要知道，在SSL层里面可以完成校验和密钥的传输。 理解了上面，这个图也就没啥好解释的了。 整理自：https://zhuanlan.zhihu.com/p/43789231]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP基础知识提炼]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F6.HTTP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%8F%90%E7%82%BC%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第六篇文章。这里简单记录一些关于HTTP的基本概念，比较基础。下面的内容是对《图解HTTP》提炼的再提炼，主要原因是很多重要的东西前面已经详细说过了，还有一些东西知道即可，用到再去查，作为一个后端攻城狮，也没有必要了解那么琐碎。 HTTP是不保存状态的协议 HTTP是无状态协议。自身不对请求和响应之间通信状态进行保存（即不做持久化处理）。 HTTP之所以设计得如此简单，是为了更快地处理大量事物，确保协议的可伸缩性。 HTTP/1.1 随时无状态协议，但可通过 Cookie 技术保存状态。 告知服务器意图的HTTP方法 GET：获取资源 POST：传输实体主体 PUT：传输文件 HEAD：获得报文首部，与GET方法一样，只是不返回报文主体内容。用于确认URI的有效性及资源更新的日期时间等。 DELETE：删除文件，与PUT相反（响应返回204 No Content）。 OPTIONS：询问支持的方法，查询针对请求URI指定的资源支持的方法（Allow:GET、POST、HEAD、OPTIONS）。 TRACE：追踪路径 CONNECT：要求用隧道协议连接代理（主要使用SSL（Secure Sockets Layer，安全套接层）和TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输）。 URI、URL 官方解释都是什么乱起八糟的东西。各种博客也是跟着抄，这两者到底是什么关系和意义？ 统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来。 对应于实际例子就是：每个人都有身份证，这个身份证号码就对应这个人。比如张三的身份证号码为123456，那么我只要知道123456就可以找到这个人。 那什么是URL呢？从名字看是：统一资源定位器。 如果做类比，URL就是：动物住址协议://地球/中国/浙江省/杭州市/西湖区/某大学/14号宿舍楼/525号寝/张三.人 我们通过这个详细的地址也可以找到张三这个人。 那么他们俩到底是什么关系呢？ URI是以某种规则唯一地标识资源，手段不限，比如身份证号。当然了，地址可以唯一标识，那么也属于URI的一种手段。所以说URL是URI的子集。 回到Web上，假设所有的Html文档都有唯一的编号，记作html:xxxxx，xxxxx是一串数字，即Html文档的身份证号码，这个能唯一标识一个Html文档，那么这个号码就是一个URI。 而URL则通过描述是哪个主机上哪个路径上的文件来唯一确定一个资源，也就是定位的方式来实现的URI。 对于现在网址我更倾向于叫它URL，毕竟它提供了资源的位置信息，如果有一天网址通过号码来标识变成了http://741236985.html，那感觉叫成URI更为合适。 HTTP请求报文 返回结果的HTTP状态码 状态码的职责是当客户端向服务器端发送请求时，描述返回的请求结果。 状态码如200 OK，以3为数字和原因短语组成。 数字中的第一位定义了响应类别，后两位无分类。响应类别有以下五种： 类别 原因短语 1XX Informational(信息性状态码) 2XX Success（成功状态码） 3XX Redirection（重定向状态码） 4XX Client Error（客户端错误状态码） 5XX Server Error（服务器错误状态码） ⭐2XX 成功 200 OK：请求被正常处理 204 No Content：一般在只需从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。 206 Partial Content：客户端进行范围请求 ⭐3XX 重定向 301 Moved Permanently：永久重定向。表示请求的资源已被分配了新的URI，以后应使用资源现在所指的URI。 也就是说，如果已经把资源对应的URI保存为书签了，这时应该按Location首部字段提示的URI重新保存。 302 Found：临时性重定向。表示请求的资源已被分配了新的URI，希望用户（本次）能使用新的URI访问。 和301 Moved Permanently状态码相似，但302状态码代表的资源不是被永久移动，只是临时性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。比如，用户把URI保存成书签，但不会像301状态码出现时那样去更新书签，而是仍旧保留返回302状态码的页面对应的URI（在Chrome中，还是会保存为重定向后的URI，不解）。 303 See Other：表示由于请求对应的资源存在着另一个URI，应使用GET方法定向获取请求的资源。这与302类似，但303明确表示客户端应当采用GET方法获取资源。 304 Not Modified：该状态码表示客户端发送附带条件的请求（指采用GET方法的请求报文中包含If-Match,If-Modified-Since，If-None-March，If-Range，If-Unmodified-Since中任一首部。）时，服务器端允许请求访问资源，但因发生请求为满足条件的情况后，直接返回304（服务器端资源未改变，可直接使用客户端未过期的缓存）。304状态码返回时，不包含任何响应的主体部分。 304虽被划分在3XX类别，但是和重定向没有关系。 307 Temporary Redirect：临时重定向。与302有相同含义。307遵守浏览器标准，不会从POST变成GET。 ⭐4XX 客户端错误 4XX的响应结果表明客户端是发生错误的原因所在。 400 Bad Request：表示请求报文中存在语法错误。 401 Unauthorized：表示发送的请求需要有通过HTTP认证（BASIC认证、DIGEST认证）的认证信息。 403 Forbidden：表明对请求资源的访问被服务器拒绝了。服务器端可在实体的主体部分对原因进行描述（可选） 404 Not Found：表明服务器上无法找到请求的资源。除此之外，也可以在服务器端拒绝请求且不想说明理由时时用。 ⭐5XX 服务器错误 5XX的响应结果表明服务器本身发生错误。 500 Interval Server Error：表明服务器端在执行请求时发生了错误。也有可能是Web应用存在的bug或某些临时的故障。 503 Service Unavailable：表明服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。如果事先得知解除以上状况需要的时间，最好写入Retry-After首部字段再返回给客户端。 HTTP的瓶颈 一条连接上只可发送一个请求（前面讲到，持久化可保持TCP连接状态，但仍完成一次请求/响应后才能进行下一次请求/响应，而管线化方式可让一个TCP连接并行发送多个请求。） 请求只能从客户端开始。客户端不可以接收除响应以外的指令 请求/响应首部未经压缩就发送。首部信息越多延迟越大 发送冗长(重复)的首部。每次互相发送相同的首部造成的浪费较多 SPDY以会话层的形式加入，控制对数据的流动，但还是采用HTTP建立通信连接。因此，可照常使用HTTP的GET和POST等方法、Cookie以及HTTP报文等。 使用 SPDY后，HTTP协议额外获得以下功能。 多路复用流：通过单一的TCP连接，可以无限制处理多个HTTP请求。所有请求的处理都在一条TCP连接上完成，因此TCP的处理效率得到提高。 赋予请求优先级：SPDY不仅可以无限制地并发处理请求，还可以给请求逐个分配优先级顺序。这样主要是为了在发送多个请求时，解决因带宽低而导致响应变慢的问题。 压缩HTTP首部：压缩HTTP请求和响应的首部。 推送功能：支持服务器主动向客户端推送数据的功能。 服务器提示功能：服务器可以主动提示客户端请求所需的资源。由于在客户端发现资源之前就可以获知资源的存在，因此在资源已缓存等情况下，可以避免发送不必要的请求。 WebSocket 利用Ajax和Comet技术进行通信可以提升Web的浏览速度。但问题在于通信若使用HTTP协议，就无法彻底解决瓶颈问题。 WebSocket技术主要是为了解决Ajax和Comet里XMLHttpRequst附带的缺陷所引起的问题。 一旦Web服务器与客户端之间建立起WebSocket协议的通信连接，之后所有的通信都依靠这个专用协议进行。通信过程中可互相发送JSON、XML、HTML或图片等任意格式的数据。 WebSocket的主要特点： 推送功能：支持由服务器向客户端推送数据。 减少通信量：和HTTP相比，不但每次连接时的总开销减少，而且由于WebSocket的首部信息很小，通信量也相应较少。 为了实现WebSocket通信，在HTTP连接建立之后，需要完成一次“握手”的步骤。 握手·请求：为了实现WebSocket通信，需要用到HTTP的Upgrade首部字段，告知服务器通信协议发生改变，以达到握手的目的。 握手·响应：对于之前的请求，返回状态码101 Switching Protocols 的响应。 成功握手确立WebSocket连接后，通信时不再使用HTTP的数据帧，而采用WebSocket独立的数据帧。 由于是建立在HTTP基础上的协议，因此连接的发起方仍是客户端，而一旦确立WebSocket通信连接，不论服务器端还是客户端，任意一方都可直接向对方发送报文。 整理自：https://github.com/JChehe/blog/blob/master/posts/《图解HTTP》读书笔记.md]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手和四次挥手]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F5.TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第五篇文章。面试讲到TCP，那么基本都会问三次握手和四次挥手的过程，以及比如对于握手，为什么是三次，而不是两次或者四次，本章详细探讨其中的门道。 1.复习 首先针对http协议，我们有必要复习一下最重要的东西。 HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 在HTTP 1.0以0.9版本中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。 通常的做法是即使不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道 客户端“在线”。 若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 2.SOCKET原理 2.1 套接字（socket）概念 初次接触这个名词：“套接字”，说实话，心里是蒙蔽的，这是啥玩意，但是可以去搜索一下什么是套接管： 我们可以看出来，两个管子可能直接连的话连不起来，那么可以通过中间一个东西连接起来。 那么，现在就好理解了，两个程序要通信，需要知道对方的一些信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 它是什么呢？它是网络通信过程中端点的抽象表示，这个抽象里面就包含了网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。 那为什么一定要用它呢？ 在同一台计算机上，TCP协议与UDP协议可以同时使用相同的port而互不干扰。 操作系统根据套接字地址，可以决定应该将数据送达特定的进程或线程。这就像是电话系统中，以电话号码加上分机号码，来决定通话对象一般。 因为我们电脑上可能会跑很多的应用程序，TCP协议端口需要为这些同时运行的程序提供并发服务，或者说，传输层需要为应用层的多个进程提供通信服务，每个进程起一个TCP连接，那么这多个TCP连接可能是通过同一个 TCP协议端口传输数据。 如何区别哪个进程对应哪个TCP连接呢？ 许多计算机操作系统为应用程序与TCP／IP协议交互提供了套接字(Socket)接口。应 用层可以和传输层通过Socket接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。 2.2 建立socket连接 建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。 套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。 客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发 给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 2.3 SOCKET连接与TCP连接 创建Socket连接时，可以指定使用的传输层协议，Socket可以支持不同的传输层协议（TCP或UDP），当使用TCP协议进行连接时，该Socket连接就是一个TCP连接。 3.TCP基本字段 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 针对协议中的字段，我们只需要了解一下：ACK、SYN、序号这三个部分。 ACK : 确认 TCP协议规定，只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 SYN ： 在连接建立时用来同步序号。 当SYN=1而ACK=0时，表明这是一个连接请求报文。 对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此, SYN置1就表示这是一个连接请求或连接接受报文。 FIN 即终结的意思， 用来释放一个连接。 当 FIN = 1 时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接。 4.三次握手(重要，细读) 首先，TCP作为一种可靠传输控制协议，其核心思想：既要保证数据可靠传输，又要提高传输的效率，而用三次恰恰可以满足以上两方面的需求！ 然后，要明确TCP连接握手，握的是啥？ 答案：通信双方数据原点的序列号！ 我们在上面一篇文章知道，消息的完整是靠给每个消息包搞一个编号，依次地ACK确认。确认机制是累计的，意味着 X 序列号之前(不包括 X) 包都是被确认接收到的。 TCP可靠传输的精髓：TCP连接的一方A，由操作系统动态随机选取一个32位长的序列号（Initial Sequence Number）。 假设A的初始序列号为1000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，1001，1002，1003…，并把自己的初始序列号ISN告诉B。 让B有一个思想准备，什么样编号的数据是合法的，什么编号是非法的，比如编号900就是非法的，同时B还可以对A每一个编号的字节数据进行确认。 如果A收到B确认编号为2001，则意味着字节编号为1001-2000，共1000个字节已经安全到达。 同理B也是类似的操作，假设B的初始序列号ISN为2000，以该序列号为原点，对自己将要发送的每个字节的数据进行编号，2001，2002，2003…，并把自己的初始序列号ISN告诉A，以便A可以确认B发送的每一个字节。如果B收到A确认编号为4001，则意味着字节编号为2001-4000，共2000个字节已经安全到达。 好了，在理解了握手的本质之后，下面就可以总结上面图的握手过程了。 对于A与B的握手过程，可以总结为： A 发送同步信号SYN + A's Initial sequence number（丢失会A会重传） B 确认收到A的同步信号，并记录 A's ISN 到本地，命名 B's ACK sequence number B发送同步信号SYN + B's Initial sequence number （丢失B会周期性超时重传，直到收到A的确认） A确认收到B的同步信号，并记录 B's ISN 到本地，命名 A's ACK sequence number 很显然2和3 这两个步骤可以合并，只需要三次握手，可以提高连接的速度与效率。 这里就会引出一个问题，两次不行吗？ A 发送同步信号SYN + A’s Initial sequence number B发送同步信号SYN + B’s Initial sequence number + B’s ACK sequence number 这里有一个问题，A与B就A的初始序列号达成了一致，但是B无法知道A是否已经接收到自己的同步信号，如果这个同步信号丢失了，A和B就B的初始序列号将无法达成一致。 所以A必须再给B一个确认，以确认A已经接收到B的同步信号。 如果A发给B的确认丢了，该如何？ A会超时重传这个ACK吗？不会！TCP不会为没有数据的ACK超时重传。 那该如何是好？B如果没有收到A的ACK，会超时重传自己的SYN同步信号，一直到收到A的ACK为止。 写到这里，其实我们已经明白了，握手其实就是各自确认对方的序列号。因为后面的数据编号就会以此为基础，从而保证后续数据的可靠性。 谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 但是有的人指出，其实这只是表象，或者说并不是三次握手的设计初衷，我表示认同，这个防止已失效的连接请求报文段应该只是附加的一些好处，而不应该是解释为什么是三次握手的原因。 TCP初始阶段为什么是三次握手，原因总结如下： 为了实现可靠数据传输， TCP 协议的通信双方， 都必须维护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。 三次握手的过程即是通信双方相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤，如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认。 5.四次挥手 当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了； 但是，这个时候主机1还是可以接受来自主机2的数据； 当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的； 当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了； 主机1告诉主机2知道了，主机2收到这个确认之后就立马关闭自己。 主机1等待2MSL之后也关闭了自己。 针对最后一条消息，即主机1发送ack后，主机2接收到此消息，即认为双方达成了同步：双方都知道连接可以释放了，此时B可以安全地释放此TCP连接所占用的内存资源、端口号。 所以被动关闭的B无需任何wait time，直接释放资源。 但是主机1并不知道主机2是否接到自己的ACK，主机1是这么想的： 如果主机2没有收到自己的ACK，主机2会超时重传FiN，那么主机1再次接到重传的FIN，会再次发送ACK 如果主机2收到自己的ACK，也不会再发任何消息，包括ACK 无论是情况1还是2，A都需要等待，要取这两种情况等待时间的最大值，以应对最坏的情况发生，这个最坏情况是： 主机2没有收到主机1的ACK，那么超时之后主机2会重传FIN，也就是说，要浪费一个主机1发出ACK的最大存活时间(MSL)+FIN消息的最大存活时间(MSL) 不可能时间再多了，这个已经针对最糟糕的状况。 等待2MSL时间，A就可以放心地释放TCP占用的资源、端口号，此时可以使用该端口号连接任何服务器。 在等待的时间内，主机2可以重试多次，因为2MSL时间为240秒，超时重传只有0.5秒，1秒，2秒，，16秒。 当主机2重试次数达到上限，主机2会reset连接。 那么为什么是2MSL我们已经了解了，但是为什么要等这个时间呢？ 如果不等，释放的端口可能会重连刚断开的服务器端口，这样依然存活在网络里的老的TCP报文可能与新TCP连接报文冲突，造成数据冲突，为避免此种情况，需要耐心等待网络老的TCP连接的活跃报文全部死翘翘，2MSL时间可以满足这个需求。 整理自： https://www.zhihu.com/question/24853633 https://www.zhihu.com/question/67013338 https://github.com/jawil/blog/issues/14 https://www.jianshu.com/p/9968b16b607e]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP协议入门]]></title>
    <url>%2F2019%2F01%2F18%2Fnetwork%2F4.TCP%E5%8D%8F%E8%AE%AE%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第四篇文章。首先要明确：信道本身不可靠（丢包、重复包、出错、乱序），不安全。所以引出了七层或五层模型来保证。因此，任何一个东西的提出都是为了解决某个问题的。学习计算机，从他的历史出发，理解为什么会有不断低迭代，因为是为了解决某个痛点问题。比如HTTP的发展，为什么在HTTP1.0基础上还要提出HTTP1.1，为什么还要提出HTTP2.0，我们学习了他的发展历史之后就会明白了。同样，下面再说一说为什么要有TCP协议，TCP到底解决了什么问题。 一、回顾 首先简单回顾一下。 1.1 物理层 物理层是相当于物理连接，将0101以电信号的形式传输出去。 1.2 数据链路层 数据链路层，有一个叫做以太网协议，规定了电子信号是如何组成数据包的，这个协议的头里面，包含了自身的网卡信息，还有目的地的网卡信息（一般我们可以知道对方的IP，IP可以通过DNS解析到，然后根据ARP协议将IP转换为MAC地址）。那么，如果在同一个局域网内，我们就可以通过广播的方式找到对应MAC地址的主机。—即以太网协议解决了子网内部的点对点通信。 1.3 网络层 但是呢，以太网协议不能解决多个局域网通信，每个局域网之间不是互通的，那么以太网这种广播的方式不可用，就算可用，网络那么大，通过广播进行找，是一个可怕的场景。那么，IP协议可以连接多个局域网，简单来说，IP 协议定义了一套自己的地址规则，称为 IP 地址。物理器件，比如说路由器，就是基于IP协议，里面保存一套地址指路牌，想去哪个局域网，可以通过这个牌子来找，然后逐步路由到目标局域网，最后就可以找到那台主机了。IP层就是对应了网络层。 1.4 传输层 那么，此时解决了多个局域网路由问题，也解决了局域网内寻址问题，即我这台主机已经可以找到那台主机了，下面还有什么事情需要做呢？显然，找到主机还不行啊，比如我用微信发一条消息，我发到你主机了，但是你主机上的微信不知道这条消息发给他了，这里说的就是端口，信息要发到这个端口上，监听这个端口的程序才会收到消息。 1.5 应用层 OK，最上层的应用层，就是最贴近用户的，他的一系列协议只是为了让两台主机会互相都理解而已。 二、问题和解决 2.1 存在的问题 在明白了计算机网络为什么要这几层模型之后，我们再回到一开始，如何保证安全、可靠、完整地传输信息呢？ 很显然，上面提到的，只是保证信息能找到对方主机和端口，但是这个信息中途被拦截了、甚至被篡改了、信息延迟了（几分钟或者几个小时，或者几个世纪）、网络不通或者挂了，信息自己可不会告诉你他挂了或者要迟到一会，如果没有一个协议来保障可靠性，那么我这条消息发出去，能不能到、能不能及时到、能不能完整到、能不能不被篡改到等这些问题将会造成灾难，网络传输也就没有了意义。 计算机的前辈们，为我们提供了一系列的措施来尽可能保证信息能正确送达。 2.2 数据校验 首先在数据链路层，可以通过各种校验，比如奇偶校验等手段来判断数据包传的是否正确。 2.3 数据可靠性 好了，解决了数据是否正确之后，但是还不能保证线路是可靠的，加入某个包没发出去或者发错了，应该有一个出错重传机制，保证信息传输的可靠性。这就引出了TCP协议。 TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 下面来看看TCP是如何解决丢包、重复包、出错、乱序等不可靠的一些问题的。 三、滑动窗口协议的提出 这就引出了TCP中最终的一个东西：滑动窗口协议。 3.1 朴素的方法来确保可靠性 先从简单的角度出发，自己想一下，如何保证不丢包、不乱序。 按照顺序，发送一个确认一个，显然，吞吐量非常低。那么，一次性发几个包，然后一起确认呢？ 3.2 改进方案 那么就引出第二个问题，我一次性发几个包合适呢？就这引出了滑动窗口。 四、数据包编号和重传机制 4.1 数据包编号 在说明滑动窗口原理之前，必须要说一下TCP数据包的编号SEQ。 我们知道，由于以太网数据包大小限制，所以每个包承载的数据是有限的，如果发一个很大的包，必然是要拆分的。 发送的时候，TCP 协议为每个包编号（sequence number，简称 SEQ），以便接收的一方按照顺序还原。万一发生丢包，也可以知道丢失的是哪一个包。 第一个包的编号是一个随机数。为了便于理解，这里就把它称为1号包。假定这个包的负载长度是100字节，那么可以推算出下一个包的编号应该是101。这就是说，每个数据包都可以得到两个编号：自身的编号，以及下一个包的编号。接收方由此知道，应该按照什么顺序将它们还原成原始文件。 4.2 数据重传机制 TCP协议就是根据这些编号来重新还原文件的。并且接收端保证顺序性返回ACK确认，比如有两个包发过去，为1号和2号，2号接收成功，但是发现1号包还没接收到，所以2号的ACK是不会发回去的，这个时候，如果在重传时间内收到1号了，那么就把这两个包的ACK都返回回去，如果超时了，就重传1号包。知道1号包接收成功，后续的才会返回ACK。 具体是如何做到的呢？ 前面说过，每一个数据包都带有下一个数据包的编号。如果下一个数据包没有收到，那么 ACK 的编号就不会发生变化。 举例来说，现在收到了4号包，但是没有收到5号包。ACK 就会记录，期待收到5号包。过了一段时间，5号包收到了，那么下一轮 ACK 会更新编号。如果5号包还是没收到，但是收到了6号包或7号包，那么 ACK 里面的编号不会变化，总是显示5号包。这会导致大量重复内容的 ACK。 如果发送方发现收到三个连续的重复 ACK，或者超时了还没有收到任何 ACK，就会确认丢包，即5号包遗失了，从而再次发送这个包。通过这种机制，TCP 保证了不会有数据包丢失。 （图片说明：Host B 没有收到100号数据包，会连续发出相同的 ACK，触发 Host A 重发100号数据包。） 五、慢启动 下面再来说说慢启动。 服务器发送数据包，当然越快越好，最好一次性全发出去。但是，发得太快，就有可能丢包。带宽小、路由器过热、缓存溢出等许多因素都会导致丢包。线路不好的话，发得越快，丢得越多。 最理想的状态是，在线路允许的情况下，达到最高速率。但是我们怎么知道，对方线路的理想速率是多少呢？答案就是慢慢试。 TCP 协议为了做到效率与可靠性的统一，设计了一个慢启动（slow start）机制。开始的时候，发送得较慢，然后根据丢包的情况，调整速率：如果不丢包，就加快发送速度；如果丢包，就降低发送速度。 Linux 内核里面设定了（常量TCP_INIT_CWND），刚开始通信的时候，发送方一次性发送10个数据包，即&quot;发送窗口&quot;的大小为10。然后停下来，等待接收方的确认，再继续发送。 默认情况下，接收方每收到两个 TCP 数据包，就要发送一个确认消息。&quot;确认&quot;的英语是 acknowledgement，所以这个确认消息就简称 ACK。 ACK 携带两个信息： 期待要收到下一个数据包的编号 接收方的接收窗口的剩余容量 发送方有了这两个信息，再加上自己已经发出的数据包的最新编号，就会推测出接收方大概的接收速度，从而降低或增加发送速率。这被称为&quot;发送窗口&quot;，这个窗口的大小是可变的。 我们可以知道，发送发和接收方都维护了一个缓冲区，可以理解为窗口。根据接收速度可以调整发送速度，逐渐达到这条线路最高的传输速率。 ok，下面就可以研究一下滑动窗口了。 六、滑动窗口原理 正常情况下： （如图，123表示已经正常发送并且收到了ACK确认。4567属于已发送但是还没有收到ACK。8910表示待发送。这个窗口当前长度为7.正常情况下，4号包收到ACK，那么窗口就会右移一格。） 但是往往会出现一些问题，比如5号包迟迟收不到ACK，在接收端可能是没有收到5号包，但是可能会收到6号包甚至是7、8号包，那么此时只能等待5号包，如果5号包顺利到达了，那么就把5678号包的ACK都发给发送端，那么发送端滑动窗口向右右移四格。如果迟迟收不到5号包，只能重传。 以上就是关于TCP中比较重要的出错重传、编号、慢启动以及滑动窗口。这些保证了数据传输的可靠性。 整理自：http://www.ruanyifeng.com/blog/2017/06/tcp-protocol.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http的前世今生]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F3.http%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第三篇文章。作为一个后端攻城狮，每天打交道最多的就是HTTP协议，在如今火热的微服务实现方案中，除了阿里的dubbo，就是spring cloud，而spring cloud目前适用的服务间通信方式也就是基于HTTP 的 restful接口来实现。并且作为浏览器上用的最多的协议，无论是前端、后端还是测试都应该去熟悉它，软件的发展是循序渐进的，每次的迭代升级都是为了解决上一版本的痛点，HTTP协议的发展也是如此，本章着重讲解HTTP的前世今生，让我们更加了解HTTP。 HTTP 是基于 TCP/IP 协议的应用层协议。它不涉及数据包（packet）传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。 一、HTTP/0.9 1.1 简介 这是第一个定稿的HTTP协议。 内容非常简单，只有一个命令GET 没有HEADER等描述数据的信息 服务器发送完毕，就关闭TCP连接（一个HTTP请求在一个TCP连接中完成） 1.2 请求格式 比如发起一个GET请求： GET /index.html 上面命令表示，TCP 连接（connection）建立后，客户端向服务器请求（request）网页index.html。 1.3 响应格式 协议规定，服务器只能回应HTML格式的字符串，不能回应别的格式。 123&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 服务器发送完毕，就关闭TCP连接。 二、HTTP/1.0 2.1 简介 跟现在比较普遍适用的1.1版本已经相差不多。 增加很多命令，比如POST、HEAD等命令 增加status code 和 header 多字符集支持、多部分发送、权限、缓存等 首先，任何格式的内容都可以发送。这使得互联网不仅可以传输文字，还能传输图像、视频、二进制文件。这为互联网的大发展奠定了基础。 其次，除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。 再次，HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。 其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 2.2 请求格式 下面是一个1.0版的HTTP请求的例子。 123GET / HTTP/1.0User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: */* 可以看到，这个格式与0.9版有很大变化。 第一行是请求命令，必须在尾部添加协议版本（HTTP/1.0）。后面就是多行头信息，描述客户端的情况。 客户端请求的时候，可以使用Accept字段声明自己可以接受哪些数据格式。上面代码中，客户端声明自己可以接受任何格式的数据。 2.3 响应格式 服务器的回应如下： 12345678910HTTP/1.0 200 OK Content-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 回应的格式是&quot;头信息 + 一个空行（\r\n） + 数据&quot;。其中，第一行是&quot;协议版本 + 状态码（status code） + 状态描述&quot;。 2.4 Content-Type 字段 关于字符的编码，1.0版规定，头信息必须是 ASCII 码，后面的数据可以是任何格式。因此，服务器回应的时候，必须告诉客户端，数据是什么格式，这就是Content-Type字段的作用。 2.5 缺点 每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。 为了解决这个问题，有些浏览器在请求时，用了一个非标准的Connection字段。 Connection: keep-alive 这个字段要求服务器不要关闭TCP连接，以便其他请求复用。服务器同样回应这个字段。 Connection: keep-alive 一个可以复用的TCP连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段，不同实现的行为可能不一致，因此不是根本的解决办法。 三、HTTP/1.1 3.1 持久连接和管道机制 持久连接（以前的版本中，一个HTTP请求就创建一个TCP连接，请求返回之后就关闭TCP连接，然而建立一次TCP连接的过程是比较耗时的，效率会比较低，现在建立一个TCP连接后，后面的HTTP请求都可以复用这个TCP连接，即允许了在同一个连接里面发送多个请求，会提高效率） pipeline（解决了同一个TCP连接中客户端可以发送多个HTTP请求，但是对于服务端来说，对于进来的请求要按照顺序进行内容的返回，如果前一个请求处理时间长，而后一个请求处理时间端，即便后面一个请求已经处理完毕了，也要等待前一个请求处理完毕返回他才可以返回结果，这种串行的方式比较慢） 在1.1版本以前，每次HTTP请求，都会重新建立一次TCP连接，服务器响应后，就立刻关闭。众所周知，建立TCP连接的新建成本很高，因为需要三次握手，并且有着慢启动的特性导致发送速度较慢。而1.1版本添加的持久连接功能可以让一次TCP连接中发送多条HTTP请求，值得一提的是默认是，控制持久连接的Connection字段默认值是keep-alive，也就是说是默认打开持久连接，如果想要关闭，只需将该字段的值改为close。 Connection: close 而管道化则赋予了客户端在一个TCP连接中连续发送多个请求的能力，而不需要等到前一个请求响应，这大大提高了效率。值得一提的是，虽然客户端可以连续发送多个请求，但是服务器返回依然是按照发送的顺序返回。（强调的是request不需要等待上一个request的response，其实发送的request还是有顺序的，服务端按照这个顺序接收，依次返回响应） HTTP/1.1允许多个http请求通过一个套接字同时被输出 ，而不用等待相应的响应。然后请求者就会等待各自的响应，这些响应是按照之前请求的顺序依次到达。（me：所有请求保持一个FIFO的队列，一个请求发送完之后，不必等待这个请求的响应被接受到，下一个请求就可以被再次发出；同时，服务器端返回这些请求的响应时也是按照FIFO的顺序）。管道化的表现可以大大提高页面加载的速度，尤其是在高延迟连接中。 3.2 Content-Length 字段 一个TCP连接现在可以传送多个回应，势必就要有一种机制，区分数据包是属于哪一个回应的。这就是Content-length字段的作用，声明本次回应的数据长度。 Content-Length: 3495 上面代码告诉浏览器，本次回应的长度是3495个字节，后面的字节就属于下一个回应了。 在1.0版中，Content-Length字段不是必需的，因为浏览器发现服务器关闭了TCP连接，就表明收到的数据包已经全了。 3.3 分块传输编码 对于一些很耗时的动态操作来说，这意味着，服务器要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用&quot;流模式&quot;（stream）取代&quot;缓存模式&quot;（buffer）。 因此，1.1版规定可以不使用Content-Length字段，而使用&quot;分块传输编码&quot;（chunked transfer encoding）。只要请求或回应的头信息有Transfer-Encoding字段，就表明回应将由数量未定的数据块组成。 Transfer-Encoding: chunked 每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后是一个大小为0的块，就表示本次回应的数据发送完了。下面是一个例子。 1234567891011121314151617HTTP/1.1 200 OKContent-Type: text/plainTransfer-Encoding: chunked25This is the data in the first chunk1Cand this is the second one3con8sequence0 3.3 其他功能 1.1版还新增了许多动词方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。 另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名。 Host: www.example.com 有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础。 3.4 缺点 虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为&quot;队头堵塞&quot;（Head-of-line blocking）。 四、SPDY 协议 2009年，谷歌公开了自行研发的 SPDY 协议，主要解决 HTTP/1.1 效率不高的问题。 这个协议在Chrome浏览器上证明可行以后，就被当作 HTTP/2 的基础，主要特性都在 HTTP/2 之中得到继承。 五、HTTP/2 5.1 二进制协议 HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。 HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为&quot;帧&quot;（frame）：头信息帧和数据帧。 二进制协议的一个好处是，可以定义额外的帧。HTTP/2 定义了近十种帧，为将来的高级应用打好了基础。如果使用文本实现这种功能，解析数据将会变得非常麻烦，二进制解析则方便得多。 5.2 多工 HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了&quot;队头堵塞&quot;。 举例来说，在一个TCP连接里面，服务器同时收到了A请求和B请求，于是先回应A请求，结果发现处理过程非常耗时，于是就发送A请求已经处理好的部分， 接着回应B请求，完成后，再发送A请求剩下的部分。 这样双向的、实时的通信，就叫做多工（Multiplexing）。 5.3 数据流 因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。 5.4 头信息压缩 HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如Cookie和User Agent，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。 HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息使用gzip或compress压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。 5.5 服务器推送 HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。 常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析HTML源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。 整理自：http://www.ruanyifeng.com/blog/2016/08/http.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从上到下看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F2.%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第二篇文章。经过上一篇文章的详细介绍，我们了解了一个数据是如何从物理层一步一步到达应用层的，那么本章从上而下的角度来看看一条请求时如何从浏览器传递到服务器并且返回的。 这个过程看的就是用户从浏览器输入一条url之后，是如何发送过去的。 1.上一篇文章的小结 我们已经知道，网络通信就是交换数据包。电脑A向电脑B发送一个数据包，后者收到了，回复一个数据包，从而实现两台电脑之间的通信。数据包的结构，基本上是下面这样： 发送这个包，需要知道两个地址： 对方的MAC地址 对方的IP地址 有了这两个地址，数据包才能准确送到接收者手中。但是，前面说过，MAC地址有局限性，如果两台电脑不在同一个子网络，就无法知道对方的MAC地址，必须通过网关（gateway）转发。 上图中，1号电脑要向4号电脑发送一个数据包。它先判断4号电脑是否在同一个子网络，结果发现不是（后文介绍判断方法），于是就把这个数据包发到网关A。网关A通过路由协议，发现4号电脑位于子网络B，又把数据包发给网关B，网关B再转发到4号电脑。 1号电脑把数据包发到网关A，必须知道网关A的MAC地址。 发送数据包之前，电脑必须判断对方是否在同一个子网络，然后选择相应的MAC地址。 有了一台新电脑之后，要想上网，一种方式是自己配置静态IP： 很多人都没有进行过这个配置，因为一般情况下我们根本不需要这样。但是有的时候也会用到，比如我在电信实习的时候，他们每一个网口旁边都贴着这四个参数，你联网必须要适用他提供的一系列地址才行。其实经过上面的学习，我们已经知道，通信的时候，需要知道对方的IP（ARP知道对方的MAC地址）、子网掩码（确定所在的子网）、默认网关（不在一个子网，要通过网关取转发、路由）、DNS服务器（解析域名为IP地址）。 但是，这样的设置很专业，普通用户望而生畏，而且如果一台电脑的IP地址保持不变，其他电脑就不能使用这个地址，不够灵活。出于这两个原因，大多数用户使用&quot;动态IP地址上网&quot;。 2.DHCP协议 所谓&quot;动态IP地址&quot;，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。 这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做&quot;DHCP服务器&quot;。新的计算机加入网络，必须向&quot;DHCP服务器&quot;发送一个&quot;DHCP请求&quot;数据包，申请IP地址和相关的网络参数。 前面说过，如果两台计算机在同一个子网络，必须知道对方的MAC地址和IP地址，才能发送数据包。但是，新加入的计算机不知道DHCP服务器的两个地址，怎么发送数据包呢？ DHCP协议做了一些巧妙的规定。 首先，它是一种应用层协议，建立在UDP协议之上，所以整个数据包是这样的： （1）最前面的&quot;以太网标头&quot;，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 （2）后面的&quot;IP标头&quot;，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。 （3）最后的&quot;UDP标头&quot;，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。 因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。 当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道&quot;这个包是发给我的&quot;，而其他计算机就可以丢弃这个包。 接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个&quot;DHCP响应&quot;数据包。 这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 新加入的计算机收到这个响应包，于是就知道了自己的IP地址、子网掩码、网关地址、DNS服务器等等参数。 动态拿到最核心的四个参数：自己的IP地址、子网掩码、网关地址、DNS服务器，就可以联网了。 3.访问google的过程 我们假定，经过上一节的步骤，用户设置好了自己的网络参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后他打开浏览器，想要访问Google，在地址栏输入了网址：www.google.com。 这意味着，浏览器要向Google发送一个网页请求的数据包。 3.1 DNS协议 我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。 DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。 然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 3.2 子网掩码 接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。 已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。 3.3 应用层协议 浏览网页用的是HTTP协议，它的整个数据包构造是这样的： HTTP部分的内容，类似于下面这样： 123456789 GET / HTTP/1.1 Host: www.google.com Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1) ...... Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8 Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3 Cookie: ... ... 我们假定这个部分的长度为4960字节，它会被嵌在TCP数据包之中。 3.4 TCP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 3.5 IP协议 然后，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。 IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 3.6 以太网协议 最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 3.7 服务器响应 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。 根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的&quot;HTTP请求&quot;，接着做出&quot;HTTP响应&quot;，再用TCP协议发回来。 本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 整理自：http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从下到上看五层模型]]></title>
    <url>%2F2019%2F01%2F17%2Fnetwork%2F1.%E4%BB%8E%E4%B8%8B%E5%88%B0%E4%B8%8A%E7%9C%8B%E4%BA%94%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是计算机网络相关的第一篇文章。要想了解HTTP协议，必然要从最基本的计算机网络知识开始入手。本篇文章从下到上具体介绍五层经典模型，极速入门计算机网络。 经典五层模型 下面我们先来了解一下各层做的事情！ 1.物理层 电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波等方式。 这就叫做&quot;物理层&quot;，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 2.数据链路层 单纯的0和1没有任何意义，必须规定解读方式：多少个电信号算一组？每个信号位有何意义？ 这就是&quot;链接层&quot;的功能，它在&quot;实体层&quot;的上方，确定了0和1的分组方式。 2.1 以太网协议 早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做&quot;以太网&quot;（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做&quot;帧&quot;（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。 &quot;标头&quot;包含数据包的一些说明项，比如发送者、接受者、数据类型等等；&quot;数据&quot;则是数据包的具体内容。 &quot;标头&quot;的长度，固定为18字节。&quot;数据&quot;的长度，最短为46字节，最长为1500字节。因此，整个&quot;帧&quot;最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。 2.2 MAC地址 上面提到，以太网数据包的&quot;标头&quot;，包含了发送者和接受者的信息。那么，发送者和接受者是如何标识呢？ 以太网规定，连入网络的所有设备，都必须具有&quot;网卡&quot;接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。 每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。 前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 2.3 广播 定义地址只是第一步，后面还有更多的步骤。 首先，一块网卡怎么会知道另一块网卡的MAC地址？ 回答是有一种ARP协议，可以解决这个问题。下面介绍ARP。 其次，就算有了MAC地址，系统怎样才能把数据包准确送到接收方？ 回答是以太网采用了一种很&quot;原始&quot;的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 上图中，1号计算机向2号计算机发送一个数据包，同一个子网络的3号、4号、5号计算机都会收到这个包。它们读取这个包的&quot;标头&quot;，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做&quot;广播&quot;（broadcasting）。 有了数据包的定义、网卡的MAC地址、广播的发送方式，&quot;链接层&quot;就可以在多台计算机之间传送数据了。 3.网络层 以太网协议，依靠MAC地址发送数据。理论上，单单依靠MAC地址，上海的网卡就可以找到洛杉矶的网卡了，技术上是可以实现的。 但是，这样做有一个重大的缺点。以太网采用广播方式发送数据包，所有成员人手一&quot;包&quot;，不仅效率低，而且局限在发送者所在的子网络。也就是说，如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理的，否则互联网上每一台计算机都会收到所有包，那会引起灾难。 互联网是无数子网络共同组成的一个巨型网络，很像想象上海和洛杉矶的电脑会在同一个子网络，这几乎是不可能的。 因此，必须找到一种方法，能够区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用&quot;路由&quot;方式发送。（&quot;路由&quot;的意思，就是指如何向不同的子网络分发数据包，这是一个很大的主题，本文不涉及。）遗憾的是，MAC地址本身无法做到这一点。它只与厂商有关，与所处网络无关。 这就导致了&quot;网络层&quot;的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做&quot;网络地址&quot;，简称&quot;网址&quot;。 于是，&quot;网络层&quot;出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。 3.1 IP协议 规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。 目前，广泛采用的是IP协议第四版，简称IPv4。这个版本规定，网络地址由32个二进制位组成。 习惯上，我们用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。 互联网上的每一台计算机，都会分配到一个IP地址。这个地址分成两个部分，前一部分代表网络，后一部分代表主机。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。处于同一个子网络的电脑，它们IP地址的网络部分必定是相同的，也就是说172.16.254.2应该与172.16.254.1处在同一个子网络。 但是，问题在于单单从IP地址，我们无法判断网络部分。还是以172.16.254.1为例，它的网络部分，到底是前24位，还是前16位，甚至前28位，从IP地址上是看不出来的。 那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数&quot;子网掩码&quot;（subnet mask）。 所谓&quot;子网掩码&quot;，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.254.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。 知道&quot;子网掩码&quot;，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 比如，已知IP地址172.16.254.1和172.16.254.233的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，结果都是172.16.254.0，因此它们在同一个子网络。 总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。 3.2 IP数据包 根据IP协议发送的数据，就叫做IP数据包。不难想象，其中必定包括IP地址信息。 但是前面说过，以太网数据包只包含MAC地址，并没有IP地址的栏位。那么是否需要修改数据定义，再添加一个栏位呢？ 回答是不需要，我们可以把IP数据包直接放进以太网数据包的&quot;数据&quot;部分，因此完全不用修改以太网的规格。这就是互联网分层结构的好处：上层的变动完全不涉及下层的结构。 具体来说，IP数据包也分为&quot;标头&quot;和&quot;数据&quot;两个部分。&quot;标头&quot;部分主要包括版本、长度、IP地址等信息，&quot;数据&quot;部分则是IP数据包的具体内容。它放进以太网数据包后，以太网数据包就变成了下面这样。 IP数据包的&quot;标头&quot;部分的长度为20个字节，整个数据包的总长度最大为65,535字节。因此，理论上，一个IP数据包的&quot;数据&quot;部分，最长为65,515字节。前面说过，以太网数据包的&quot;数据&quot;部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。 3.3 ARP协议 关于&quot;网络层&quot;，还有最后一点需要说明。 因为IP数据包是放在以太网数据包里发送的，所以我们必须同时知道两个地址，一个是对方的MAC地址，另一个是对方的IP地址。通常情况下，对方的IP地址是已知的，但是我们不知道它的MAC地址。 所以，我们需要一种机制，能够从IP地址得到MAC地址。 这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的&quot;网关&quot;（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个&quot;广播&quot;地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 4. 传输层 有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。 接下来的问题是，同一台主机上有许多程序都需要用到网络，比如，你一边浏览网页，一边与朋友在线聊天。当一个数据包从互联网上发来的时候，你怎么知道，它是表示网页的内容，还是表示在线聊天的内容？ 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做&quot;端口&quot;（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 &quot;端口&quot;是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。不管是浏览网页还是在线聊天，应用程序会随机选用一个端口，然后与服务器的相应端口联系。 &quot;传输层&quot;的功能，就是建立&quot;端口到端口&quot;的通信。相比之下，“网络层&quot;的功能是建立&quot;主机到主机&quot;的通信。只要确定主机和端口，我们就能实现程序之间的交流。因此，Unix系统就把主机+端口，叫做&quot;套接字”（socket）。有了它，就可以进行网络应用程序开发了。 4.1 UDP协议 现在，我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。 UDP数据包，也是由&quot;标头&quot;和&quot;数据&quot;两部分组成。 &quot;标头&quot;部分主要定义了发出端口和接收端口，&quot;数据&quot;部分就是具体的内容。然后，把整个UDP数据包放入IP数据包的&quot;数据&quot;部分，而前面说过，IP数据包又是放在以太网数据包之中的，所以整个以太网数据包现在变成了下面这样： UDP数据包非常简单，&quot;标头&quot;部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 4.2 TCP协议 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 为了解决这个问题，提高网络可靠性，TCP协议就诞生了。这个协议非常复杂，但可以近似认为，它就是有确认机制的UDP协议，每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 因此，TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。 TCP数据包和UDP数据包一样，都是内嵌在IP数据包的&quot;数据&quot;部分。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 关于TCP细节以后再探讨。 5. 应用层 应用程序收到&quot;传输层&quot;的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。 &quot;应用层&quot;的作用，就是规定应用程序的数据格式。 举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了&quot;应用层&quot;。 这是最高的一层，直接面对用户。它的数据就放在TCP数据包的&quot;数据&quot;部分。因此，现在的以太网的数据包就变成下面这样。 *注：UDP头为8个字节，TCP头为20个字节 整理于：http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <tags>
        <tag>计算机网络相关</tag>
      </tags>
  </entry>
</search>
